{"/blog/":{"data":{"":" RSS Feed "},"title":"Blog"},"/database/":{"data":{"":" RSS Feed "},"title":"Database"},"/devops/":{"data":{"":" RSS Feed "},"title":"Devops"},"/devops/argo/":{"data":{"":" RSS Feed "},"title":"ArgoCI/CD"},"/devops/argo/argo00/":{"data":{"":"","1-argo-cd란#1️⃣ \u003cstrong\u003eArgo CD란?\u003c/strong\u003e":"","2-gitops-개념과-argo-cd의-역할#2️⃣ \u003cstrong\u003eGitOps 개념과 Argo CD의 역할\u003c/strong\u003e":"","3-argo-cd-vs-jenkins-vs-terraform-비교#3️⃣ \u003cstrong\u003eArgo CD vs Jenkins vs Terraform 비교\u003c/strong\u003e":"","4-argo-cd의-주요-기능-및-장점#4️⃣ \u003cstrong\u003eArgo CD의 주요 기능 및 장점\u003c/strong\u003e":"","5-argo-cd-아키텍처-개요#5️⃣ \u003cstrong\u003eArgo CD 아키텍처 개요\u003c/strong\u003e":"1️⃣ Argo CD란? Argo CD는 쿠버네티스 환경에서 GitOps 방식을 구현하는 지속적 배포(CD) 도구입니다. Git 저장소를 단일 소스로 삼아 애플리케이션 상태를 정의하고, 쿠버네티스 클러스터의 실제 상태를 지속적으로 동기화합니다.\n1. Argo CD의 특징 Git 기반 선언적 배포: Git 저장소를 통해 애플리케이션의 상태를 정의하고 관리 자동 동기화: 실제 클러스터 상태가 Git 저장소와 다를 경우 자동으로 동기화 수행 RBAC 지원: 역할 기반 접근 제어를 통해 보안 강화 GUI 및 CLI 지원: 웹 UI와 CLI를 통해 편리하게 배포 및 관리 가능 멀티 클러스터 지원: 여러 개의 쿠버네티스 클러스터를 한 곳에서 관리 2️⃣ GitOps 개념과 Argo CD의 역할 1. GitOps란? GitOps는 Git 저장소를 소스로 삼아 애플리케이션 및 인프라 상태를 관리하는 운영 방식입니다. 핵심 개념은 다음과 같습니다:\nGit을 단일 소스로 사용: 모든 애플리케이션 및 인프라 변경 사항이 Git에 저장됨 자동화된 동기화: 변경 사항을 감지하여 자동으로 쿠버네티스 클러스터에 적용 감사 가능성 보장: 모든 변경 이력이 Git에 기록되어 추적 가능 2. Argo CD의 GitOps 역할 Argo CD는 GitOps를 실현하는 주요 도구로서 다음과 같은 기능을 수행합니다:\nGit 저장소의 선언적 설정을 쿠버네티스 클러스터에 반영 실시간으로 클러스터 상태를 모니터링하고 자동 동기화 웹 UI, CLI, API를 통해 애플리케이션 배포 및 롤백 지원 🎯 GitOps 기반 Argo CD 배포 흐름 개발자 → Git 커밋 → Argo CD 감지 → 쿠버네티스에 배포 → 실시간 모니터링 3️⃣ Argo CD vs Jenkins vs Terraform 비교 1. 배포 도구 비교 도구 목적 사용 방식 GitOps 지원 Argo CD 지속적 배포 (CD) 선언적 (Git 기반) ✅ 네이티브 지원 Jenkins 지속적 통합 및 배포 (CI/CD) 스크립트 기반 (Imperative) ❌ 직접 지원하지 않음 Terraform 인프라 구축 및 관리 선언적 (HCL) ⚠️ 간접 지원 가능 2. 왜 Argo CD를 사용할까? GitOps 방식의 네이티브 지원 쿠버네티스 친화적인 배포 자동화된 동기화 및 롤백 기능 제공 UI를 통한 시각적인 상태 확인 가능 4️⃣ Argo CD의 주요 기능 및 장점 1. 주요 기능 ✅ 자동 동기화(Auto Sync): Git의 변경 사항을 감지하여 자동으로 배포 수행\n✅ 프로그레시브 딜리버리 지원: Blue/Green 및 Canary 배포 전략 적용 가능\n✅ Helm 및 Kustomize 지원: 다양한 배포 방식 지원\n✅ RBAC(Role-Based Access Control): 세밀한 권한 제어 가능\n✅ 멀티 클러스터 관리: 여러 쿠버네티스 클러스터를 한 번에 관리\n2. 장점 안정적인 배포: Git을 단일 소스로 사용하여 신뢰성 있는 배포 가능 빠른 롤백: 문제가 발생하면 이전 상태로 쉽게 복구 가능 감사 가능성: 모든 변경 사항이 Git에 기록되어 추적 가능 사용자 친화적 UI: 직관적인 대시보드를 제공하여 운영 편의성 증가 5️⃣ Argo CD 아키텍처 개요 1. Argo CD 구성 요소 📌 API Server: UI, CLI, API 요청을 처리하는 중앙 컨트롤러\n📌 Repository Server: Git 저장소에서 선언적 구성을 가져와 분석하는 서버\n📌 Application Controller: 쿠버네티스 상태를 감시하고 변경 사항을 반영하는 컨트롤러\n📌 Dex (Optional): 인증 및 SSO 지원을 위한 플러그인\n2. 아키텍처 다이어그램 +-------------------+ +----------------------+ | Git Repository |-----\u003e | Argo CD Repo Server | +-------------------+ +----------------------+ | v +-------------------+ +----------------------+ +--------------------+ | API Server |-----\u003e | Application Controller |---\u003e | Kubernetes Cluster | +-------------------+ +----------------------+ +--------------------+ 3. 동작 방식 사용자가 Git 저장소에 새로운 애플리케이션 설정을 Push Argo CD Repository Server가 변경 사항을 감지하고 동기화 요청 Application Controller가 쿠버네티스 클러스터 상태를 감시하고 적용 사용자는 UI 또는 CLI를 통해 배포 상태를 모니터링 "},"title":"Argo CD 개요"},"/devops/argo/argo01/":{"data":{"":"","1-argo-cd-설치-방법-helm-kustomize-yaml-배포#1️⃣ \u003cstrong\u003eArgo CD 설치 방법 (Helm, Kustomize, YAML 배포)\u003c/strong\u003e":"","2-argo-cd-cli-및-ui-사용법#2️⃣ \u003cstrong\u003eArgo CD CLI 및 UI 사용법\u003c/strong\u003e":"","3-argo-cd-rbac-role-based-access-control-설정#3️⃣ \u003cstrong\u003eArgo CD RBAC (Role-Based Access Control) 설정\u003c/strong\u003e":"","4-https-및-인증서-설정#4️⃣ \u003cstrong\u003eHTTPS 및 인증서 설정\u003c/strong\u003e":"","5-argo-cd-기본-설정값-argocd-cm-argocd-rbac-cm#5️⃣ \u003cstrong\u003eArgo CD 기본 설정값 (argocd-cm, argocd-rbac-cm)\u003c/strong\u003e":"1️⃣ Argo CD 설치 방법 (Helm, Kustomize, YAML 배포) Argo CD는 여러 가지 방법으로 설치할 수 있습니다. 대표적으로 YAML 배포, Helm Chart, Kustomize 방식이 있으며, 각 방식별 설치 방법을 살펴보겠습니다.\n1. YAML을 이용한 설치 Argo CD 공식 배포 YAML을 사용하여 설치할 수 있습니다.\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml ✅ 설치 확인\nkubectl get pods -n argocd 2. Helm을 이용한 설치 helm repo add argo https://argoproj.github.io/argo-helm helm repo update helm install argocd argo/argo-cd -n argocd --create-namespace ✅ 설치 확인\nkubectl get svc -n argocd 3. Kustomize를 이용한 설치 git clone https://github.com/argoproj/argo-cd.git cd argo-cd/manifests kubectl apply -k overlays/standalone ✅ 설치 확인\nkubectl get pods -n argocd 2️⃣ Argo CD CLI 및 UI 사용법 Argo CD는 CLI(Command Line Interface) 및 웹 UI를 제공합니다.\n1. CLI 설치 및 로그인 curl -sSL -o argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64 chmod +x argocd sudo mv argocd /usr/local/bin/ 로그인:\nargocd login \u003cARGOCD_SERVER\u003e --username admin --password \u003cPASSWORD\u003e 2. 웹 UI 접속 Argo CD 웹 UI에 접근하려면 다음 명령을 실행하세요:\nkubectl port-forward svc/argocd-server -n argocd 8080:443 ➡ 브라우저에서 https://localhost:8080 접속 후 로그인합니다.\n3️⃣ Argo CD RBAC (Role-Based Access Control) 설정 RBAC을 사용하면 특정 사용자 및 그룹에게 권한을 부여할 수 있습니다.\n1. RBAC 설정 파일 (argocd-rbac-cm.yaml) 다음은 예제 설정입니다.\napiVersion: v1 kind: ConfigMap metadata: name: argocd-rbac-cm namespace: argocd data: policy.csv: | p, admin, *, *, allow p, dev, applications, get, allow policy.default: 'role:readonly' 적용:\nkubectl apply -f argocd-rbac-cm.yaml 4️⃣ HTTPS 및 인증서 설정 Argo CD는 기본적으로 HTTP를 사용하지만, 보안을 위해 HTTPS 설정을 권장합니다.\n1. Let’s Encrypt 인증서 적용 (예제) kubectl create secret tls argocd-tls --cert=cert.pem --key=key.pem -n argocd 2. Ingress 설정을 통한 HTTPS 적용 (예제) apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd annotations: kubernetes.io/ingress.class: nginx spec: tls: - hosts: - argocd.example.com secretName: argocd-tls rules: - host: argocd.example.com http: paths: - path: / pathType: Prefix backend: service: name: argocd-server port: number: 443 적용:\nkubectl apply -f ingress.yaml 5️⃣ Argo CD 기본 설정값 (argocd-cm, argocd-rbac-cm) Argo CD의 주요 설정은 ConfigMap을 통해 관리됩니다.\n1. argocd-cm.yaml (설정 예제) apiVersion: v1 kind: ConfigMap metadata: name: argocd-cm namespace: argocd data: application.instanceLabelKey: argocd.argoproj.io/instance server.rbac.log.enforce.enable: \"true\" dex.config: | connectors: - type: github id: github name: GitHub 2. 설정 적용 방법 kubectl apply -f argocd-cm.yaml kubectl rollout restart deployment argocd-server -n argocd "},"title":"Argo CD 설치 및 기본 설정"},"/devops/argo/argo02/":{"data":{"":"","1-application-argo-cd의-애플리케이션-개념#1️⃣ \u003cstrong\u003eApplication: Argo CD의 애플리케이션 개념\u003c/strong\u003e":"","2-project-여러-애플리케이션을-관리하는-단위#2️⃣ \u003cstrong\u003eProject: 여러 애플리케이션을 관리하는 단위\u003c/strong\u003e":"","3-sync-쿠버네티스-클러스터와-git-상태-동기화#3️⃣ \u003cstrong\u003eSync: 쿠버네티스 클러스터와 Git 상태 동기화\u003c/strong\u003e":"","4-sync-policy-자동-vs-수동-동기화-모드#4️⃣ \u003cstrong\u003eSync Policy: 자동 vs 수동 동기화 모드\u003c/strong\u003e":"","5-health-status-애플리케이션-상태-모니터링#5️⃣ \u003cstrong\u003eHealth Status: 애플리케이션 상태 모니터링\u003c/strong\u003e":"","6-sync-hooks-배포-전후-작업-실행#6️⃣ \u003cstrong\u003eSync Hooks: 배포 전후 작업 실행\u003c/strong\u003e":"1️⃣ Application: Argo CD의 애플리케이션 개념 Argo CD에서 애플리케이션(Application)은 Git 저장소에 정의된 애플리케이션 구성을 쿠버네티스 클러스터에 배포하는 단위입니다. 애플리케이션은 Git 저장소와 Kubernetes 클러스터를 연결하는 역할을 합니다.\n1. 애플리케이션 구성 요소 애플리케이션은 다음과 같은 주요 요소들로 구성됩니다:\nGit 저장소 URL: 애플리케이션의 소스 코드 및 배포 설정이 저장된 Git 저장소 주소 클러스터: 애플리케이션이 배포될 Kubernetes 클러스터 네임스페이스: 애플리케이션이 배포될 Kubernetes 네임스페이스 리소스: 애플리케이션의 리소스들 (Deployment, Service, ConfigMap 등) 2. 애플리케이션 생성 예시 argocd app create my-app \\ --repo https://github.com/my-org/my-app.git \\ --path deploy/k8s \\ --dest-server https://kubernetes.default.svc \\ --dest-namespace default 2️⃣ Project: 여러 애플리케이션을 관리하는 단위 Project는 여러 애플리케이션을 관리하는 단위로, 애플리케이션 그룹을 정의하고, 리소스 접근 제어를 설정할 수 있는 기능을 제공합니다. 이를 통해 조직 내 여러 환경에 대해 더 효율적으로 관리할 수 있습니다.\n1. Project 구성 요소 Git 저장소: 각 애플리케이션의 Git 저장소를 정의 리소스 제한: 프로젝트 내 애플리케이션들이 사용할 수 있는 리소스의 범위를 제한 RBAC 설정: 각 애플리케이션에 대한 접근 권한을 제어 2. Project 생성 예시 argocd proj create my-project \\ --description 'My Project for production apps' \\ --src-repo https://github.com/my-org/ \\ --dest-namespace default \\ --dest-server https://kubernetes.default.svc 3️⃣ Sync: 쿠버네티스 클러스터와 Git 상태 동기화 Sync는 Argo CD의 핵심 기능으로, Git 저장소에 정의된 상태를 쿠버네티스 클러스터와 동기화합니다. Sync가 발생하면 Argo CD는 Git에서 지정된 리소스를 가져와 쿠버네티스 클러스터에 적용하고, 클러스터 상태를 업데이트합니다.\n1. Sync 동작 예시 사용자가 Git 저장소에서 애플리케이션의 설정을 변경 Argo CD가 Git 저장소를 모니터링하고 변경 사항을 감지 클러스터에 설정 변경 사항을 적용하여 동기화 완료 2. Sync 상태 확인 argocd app sync my-app 4️⃣ Sync Policy: 자동 vs 수동 동기화 모드 Sync Policy는 동기화 방식에 대한 설정으로, 자동 또는 수동 모드로 구분할 수 있습니다.\n1. 자동 동기화 (Auto Sync) 자동 동기화 모드에서는 Git 저장소와 클러스터의 상태가 다를 경우 자동으로 동기화가 이루어집니다.\n장점: 자동으로 배포되어 실시간으로 최신 상태를 유지할 수 있음 단점: 실수로 잘못된 설정이 배포될 위험 있음 2. 수동 동기화 (Manual Sync) 수동 동기화 모드에서는 Git 저장소와 클러스터의 상태가 다르더라도 수동으로 동기화를 실행해야 합니다.\n장점: 실수로 배포를 하지 않아 안전하게 관리 가능 단점: 수동으로 동기화 작업을 해야 하므로 관리가 번거로울 수 있음 3. Sync Policy 설정 예시 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app namespace: argocd spec: syncPolicy: automated: prune: true selfHeal: true retry: limit: 3 5️⃣ Health Status: 애플리케이션 상태 모니터링 Argo CD는 애플리케이션의 상태를 모니터링하고 이를 Health Status로 표시합니다. Health Status는 애플리케이션이 정상적으로 작동하고 있는지, 아니면 문제가 발생했는지를 나타냅니다.\n1. Health Status 유형 Healthy: 애플리케이션이 정상적으로 동작 중 Degraded: 애플리케이션이 예상보다 성능이 낮거나 일부 기능이 동작하지 않음 Progressing: 애플리케이션이 배포 중이거나 업데이트 중 Suspended: 애플리케이션이 중지된 상태 2. Health 상태 확인 argocd app get my-app 6️⃣ Sync Hooks: 배포 전후 작업 실행 Sync Hooks는 애플리케이션의 배포 전후에 특정 작업을 자동으로 실행할 수 있게 해주는 기능입니다. 이를 통해 배포 과정에서 필수적인 설정 작업을 자동화할 수 있습니다.\n1. Sync Hooks의 종류 PreSync: 애플리케이션 배포 전에 실행되는 작업 Sync: 애플리케이션 배포 중에 실행되는 작업 PostSync: 애플리케이션 배포 후에 실행되는 작업 Sync Failure: 배포 실패 시 실행되는 작업 2. Sync Hook 예시 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app spec: syncPolicy: syncOptions: - CreateNamespace=true hooks: - exec: command: [\"/bin/sh\", \"-c\", \"echo 'PreSync Hook'\"] - exec: command: [\"/bin/sh\", \"-c\", \"echo 'PostSync Hook'\"] "},"title":"Argo CD 핵심 개념"},"/devops/argo/argo03/":{"data":{"":"","1-gitops-개념-및-argo-cd의-역할#1️⃣ \u003cstrong\u003eGitOps 개념 및 Argo CD의 역할\u003c/strong\u003e":"","2-gitops-workflow-이해-pr-기반-배포#2️⃣ \u003cstrong\u003eGitOps Workflow 이해 (PR 기반 배포)\u003c/strong\u003e":"","3-git-브랜치-및-태그를-활용한-배포-전략#3️⃣ \u003cstrong\u003eGit 브랜치 및 태그를 활용한 배포 전략\u003c/strong\u003e":"","4-git-repository-구조-설계-mono-repo-vs-multi-repo#4️⃣ \u003cstrong\u003eGit Repository 구조 설계 (Mono Repo vs Multi Repo)\u003c/strong\u003e":"","5-gitops-기반-cicd-파이프라인-구성#5️⃣ \u003cstrong\u003eGitOps 기반 CI/CD 파이프라인 구성\u003c/strong\u003e":"1️⃣ GitOps 개념 및 Argo CD의 역할 GitOps는 Git을 단일 진리의 원천으로 사용하여 애플리케이션 및 인프라의 배포 및 관리를 자동화하는 방법론입니다. GitOps는 개발자가 코드를 Git에 푸시하고, 이 변경 사항을 통해 배포가 자동으로 이루어지도록 합니다. 이 과정에서 Argo CD는 GitOps의 핵심 툴로서, Git 저장소와 Kubernetes 클러스터 간의 동기화를 책임집니다.\n1. GitOps의 주요 개념 Git 저장소: 애플리케이션의 코드와 배포 설정이 저장된 Git 저장소. 자동화: Git에 대한 변경 사항이 자동으로 클러스터에 반영되며, Kubernetes 리소스가 자동으로 배포됨. 동기화: GitOps는 Git과 클러스터의 상태를 일치시키는 작업을 자동화합니다. 2. Argo CD의 역할 Argo CD는 GitOps 워크플로우에서 Git 저장소와 Kubernetes 클러스터 사이의 자동화된 동기화를 관리하는 툴입니다. 이를 통해 사용자는 코드 변경을 Git에서만 관리하고, 클러스터는 Git 상태를 반영하여 자동으로 배포가 이루어집니다.\n2️⃣ GitOps Workflow 이해 (PR 기반 배포) GitOps에서 가장 중요한 워크플로우는 Pull Request (PR) 기반 배포입니다. 개발자는 애플리케이션 변경 사항을 PR로 제출하고, 이를 승인한 후 자동으로 배포가 이루어집니다.\n1. PR 기반 배포 과정 PR 제출: 개발자가 기능을 추가하거나 버그를 수정한 후 GitHub, GitLab 등의 Git 저장소에 PR을 제출합니다. 자동화된 검토: PR을 제출하면, CI/CD 파이프라인이 자동으로 테스트를 실행하고, 검토가 완료되면 배포가 준비됩니다. 배포 승인: PR이 승인되면 GitOps 툴(Argo CD)이 자동으로 Kubernetes 클러스터에 배포를 시작합니다. 배포 후 검증: 배포가 완료되면 Argo CD는 상태를 Git과 비교하여 실제 클러스터와 동기화됩니다. 2. PR 기반 배포 예시 # PR 제출 후 CI/CD 파이프라인이 실행됩니다. # CI 파이프라인에서는 테스트와 빌드를 자동으로 실행합니다. # 이후 PR이 승인되면 Argo CD가 자동으로 배포를 진행합니다. 3️⃣ Git 브랜치 및 태그를 활용한 배포 전략 GitOps에서는 Git 브랜치와 태그를 활용하여 배포 전략을 세울 수 있습니다. 이를 통해 다양한 환경(개발, 테스트, 프로덕션)에서 배포를 제어할 수 있습니다.\n1. 브랜치 기반 배포 전략 dev 브랜치: 개발 중인 기능을 포함하는 브랜치, 개발 환경에 배포 staging 브랜치: 테스트 환경에 배포되는 브랜치 main/master 브랜치: 프로덕션 환경에 배포되는 브랜치 2. 태그 기반 배포 전략 태그를 사용하여 버전을 명확히 관리하고, 특정 버전을 프로덕션 환경에 배포할 수 있습니다.\n3. 브랜치 및 태그 예시 # 각 환경에 맞는 Git 브랜치를 사용하여 배포 환경을 구분합니다. argocd app set my-app --repo https://github.com/my-org/my-app.git --branch dev 4️⃣ Git Repository 구조 설계 (Mono Repo vs Multi Repo) GitOps에서는 Git 저장소 구조를 어떻게 설계하느냐가 중요합니다. 주로 Mono Repo와 Multi Repo 방식이 사용됩니다.\n1. Mono Repo 설명: 모든 애플리케이션과 인프라 코드를 하나의 Git 저장소에 관리하는 방식. 장점: 모든 리소스를 하나의 저장소에서 관리하므로 코드의 일관성을 유지하기 용이. 단점: 저장소가 커지면서 관리가 어려울 수 있음. 2. Multi Repo 설명: 각 애플리케이션과 인프라 코드를 별도의 Git 저장소에서 관리하는 방식. 장점: 각 애플리케이션의 리소스를 독립적으로 관리할 수 있어 유연함. 단점: 저장소 간 의존성 관리가 어려울 수 있음. 3. Git Repository 구조 예시 Mono Repo: 모든 서비스 및 인프라 코드를 하나의 저장소에 관리 my-app-repo/ ├── app1/ ├── app2/ └── infrastructure/ Multi Repo: 각 서비스마다 별도의 Git 저장소 관리 app1-repo/ app2-repo/ infrastructure-repo/ 5️⃣ GitOps 기반 CI/CD 파이프라인 구성 GitOps는 CI/CD 파이프라인과 결합하여 자동화된 배포 및 테스트 흐름을 만들어냅니다. GitOps 기반 CI/CD 파이프라인은 애플리케이션을 테스트하고 빌드한 후, Git에 반영된 변경 사항에 따라 자동으로 배포를 진행합니다.\n1. CI 파이프라인 테스트: 코드를 빌드하고, 유닛 테스트 및 통합 테스트를 실행 빌드: 도커 이미지를 빌드하고 레지스트리에 푸시 2. CD 파이프라인 배포: Git 저장소에서 자동으로 배포가 이루어집니다. Argo CD는 Git 저장소와 Kubernetes 클러스터 간의 상태를 동기화합니다. 모니터링: 배포 후 애플리케이션 상태를 모니터링하고, 문제가 있을 경우 롤백 3. CI/CD 파이프라인 예시 # CI 파이프라인 (GitHub Actions 예시) name: CI/CD Pipeline on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Build Docker image run: | docker build -t my-app . docker push my-app:latest deploy: needs: build runs-on: ubuntu-latest steps: - name: Deploy to Kubernetes using Argo CD run: | argocd app sync my-app "},"title":"Argo CD와 GitOps"},"/devops/argo/argo04/":{"data":{"":"","1-application-객체-생성-및-설정#1️⃣ \u003cstrong\u003eApplication 객체 생성 및 설정\u003c/strong\u003e":"","2-applicationset을-활용한-대규모-애플리케이션-배포#2️⃣ \u003cstrong\u003eApplicationSet을 활용한 대규모 애플리케이션 배포\u003c/strong\u003e":"","3-helm-chart-기반-애플리케이션-배포#3️⃣ \u003cstrong\u003eHelm Chart 기반 애플리케이션 배포\u003c/strong\u003e":"","4-kustomize를-이용한-배포-관리#4️⃣ \u003cstrong\u003eKustomize를 이용한 배포 관리\u003c/strong\u003e":"","5-manifest-파일yaml-직접-적용하기#5️⃣ \u003cstrong\u003eManifest 파일(YAML) 직접 적용하기\u003c/strong\u003e":"","6-argo-cd를-통한-마이크로서비스-배포-및-관리#6️⃣ \u003cstrong\u003eArgo CD를 통한 마이크로서비스 배포 및 관리\u003c/strong\u003e":"1️⃣ Application 객체 생성 및 설정 Argo CD에서 애플리케이션은 Kubernetes 클러스터에 배포할 애플리케이션을 나타내는 기본적인 단위입니다. Application 객체는 Git 저장소에서 Kubernetes 리소스를 가져와 자동으로 배포합니다.\n1. Application 객체란? Application 객체는 GitOps에서 애플리케이션을 정의하고, Git 리포지토리와 클러스터 간의 동기화를 관리하는 데 사용됩니다. 이를 통해 Argo CD는 애플리케이션의 상태를 Git과 동기화합니다.\n2. Application 객체 생성 예시 Application 객체를 생성하기 위해서는 먼저 Git 리포지토리와 Kubernetes 클러스터를 정의해야 합니다. 아래는 application.yaml을 사용한 예시입니다.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/my-app.git targetRevision: HEAD path: manifests project: default syncPolicy: automated: prune: true selfHeal: true 이 YAML 파일은 GitHub 리포지토리의 manifests 폴더에서 Kubernetes 리소스를 가져와 default 네임스페이스에 배포합니다. syncPolicy는 자동 동기화 및 자원 정리를 활성화합니다.\n2️⃣ ApplicationSet을 활용한 대규모 애플리케이션 배포 ApplicationSet은 여러 개의 애플리케이션을 관리할 수 있는 기능을 제공합니다. 이를 사용하면 대규모 애플리케이션 환경에서 여러 개의 Application 객체를 동적으로 생성하고 관리할 수 있습니다.\n1. ApplicationSet 개요 ApplicationSet은 여러 Git 리포지토리 또는 클러스터에 걸쳐 동일한 애플리케이션을 배포하는 데 유용합니다. 예를 들어, 여러 환경(개발, 스테이징, 프로덕션)에서 동일한 애플리케이션을 자동으로 배포할 수 있습니다.\n2. ApplicationSet 예시 아래는 ApplicationSet을 사용하여 여러 애플리케이션을 배포하는 예시입니다. git 소스를 기반으로 애플리케이션을 배포합니다.\napiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: my-app-set spec: generators: - git: repoURL: https://github.com/my-org/my-app.git revision: HEAD directories: - path: apps/* template: metadata: name: \"{{path.basename}}\" spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/my-app.git targetRevision: HEAD path: \"{{path}}\" project: default syncPolicy: automated: prune: true selfHeal: true 이 예시에서는 git 저장소의 여러 디렉토리 내 애플리케이션들을 동적으로 관리합니다.\n3️⃣ Helm Chart 기반 애플리케이션 배포 Helm Chart는 Kubernetes 애플리케이션의 패키징 방식으로, Argo CD는 Helm Chart를 사용하여 애플리케이션을 배포할 수 있습니다. Helm Chart는 애플리케이션의 리소스를 템플릿화하여 쉽게 배포하고 관리할 수 있게 도와줍니다.\n1. Helm Chart로 애플리케이션 배포 Helm Chart를 사용하여 Argo CD에서 애플리케이션을 배포하려면, 먼저 Helm Chart를 Git 리포지토리 또는 Helm 레지스트리에 저장해야 합니다.\n2. Helm Chart 배포 예시 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-helm-app namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/my-helm-chart.git targetRevision: HEAD chart: my-chart project: default syncPolicy: automated: prune: true selfHeal: true 이 예시는 Git 저장소에서 my-helm-chart Helm Chart를 가져와 default 네임스페이스에 배포합니다.\n4️⃣ Kustomize를 이용한 배포 관리 Kustomize는 Kubernetes 리소스의 오버레이 및 패치 관리 도구로, Argo CD는 Kustomize를 지원하여 다양한 환경에 맞는 배포를 관리할 수 있습니다.\n1. Kustomize란? Kustomize는 기본 리소스를 변형하여 다른 환경에 맞게 설정할 수 있는 도구입니다. 예를 들어, 개발, 테스트, 프로덕션 환경에서 동일한 애플리케이션을 배포할 수 있습니다.\n2. Kustomize 배포 예시 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-kustomize-app namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/my-kustomize-app.git targetRevision: HEAD path: overlays/prod project: default syncPolicy: automated: prune: true selfHeal: true 이 예시는 Git 저장소에서 prod 오버레이를 사용하여 프로덕션 환경에 맞게 배포합니다.\n5️⃣ Manifest 파일(YAML) 직접 적용하기 Argo CD는 Helm, Kustomize 외에도 Kubernetes 리소스를 직접 YAML 파일로 관리할 수 있습니다. 이 방법을 사용하면 애플리케이션을 더 세밀하게 제어할 수 있습니다.\n1. Manifest 파일 직접 적용 예시 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-manifest-app namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/my-manifest-app.git targetRevision: HEAD path: manifests project: default syncPolicy: automated: prune: true selfHeal: true 이 예시는 Git 저장소에서 Kubernetes 리소스가 정의된 manifests 폴더를 직접 가져와 배포합니다.\n6️⃣ Argo CD를 통한 마이크로서비스 배포 및 관리 Argo CD는 마이크로서비스 아키텍처를 관리하는 데 매우 유용한 도구입니다. 여러 개의 마이크로서비스를 각기 다른 Git 리포지토리 또는 Helm Chart로 관리하며, 각 서비스의 배포를 Argo CD로 자동화할 수 있습니다.\n1. 마이크로서비스 배포 예시 마이크로서비스 애플리케이션을 여러 개의 Application 객체로 관리하며, 각각 다른 Git 리포지토리에서 소스를 가져올 수 있습니다.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: service1 namespace: argocd spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/service1.git targetRevision: HEAD path: manifests project: default syncPolicy: automated: prune: true selfHeal: true 이와 같은 방식으로 각 마이크로서비스를 별도로 관리할 수 있습니다. Argo CD는 여러 서비스의 배포 및 상태를 한눈에 관리할 수 있는 기능을 제공합니다."},"title":"Argo CD 애플리케이션 관리"},"/devops/argo/argo05/":{"data":{"":"","1-자동-vs-수동-sync#1️⃣ \u003cstrong\u003e자동 vs 수동 Sync\u003c/strong\u003e":"","2-sync-옵션-및-동작-방식#2️⃣ \u003cstrong\u003eSync 옵션 및 동작 방식\u003c/strong\u003e":"","3-sync-waves--phases-배포-단계-제어#3️⃣ \u003cstrong\u003eSync Waves \u0026amp; Phases (배포 단계 제어)\u003c/strong\u003e":"","4-sync-hooks-presync-sync-postsync#4️⃣ \u003cstrong\u003eSync Hooks (PreSync, Sync, PostSync)\u003c/strong\u003e":"","5-blue-green-배포-및-canary-배포-적용#5️⃣ \u003cstrong\u003eBlue-Green 배포 및 Canary 배포 적용\u003c/strong\u003e":"","6-배포-롤백-rollback-및-히스토리-관리#6️⃣ \u003cstrong\u003e배포 롤백 (Rollback) 및 히스토리 관리\u003c/strong\u003e":"1️⃣ 자동 vs 수동 Sync Argo CD에서 애플리케이션의 상태를 Git 리포지토리와 동기화하는 방식에는 두 가지 주요 방식이 있습니다: 자동 Sync와 수동 Sync.\n1. 자동 Sync 자동 Sync는 Argo CD가 Git 리포지토리의 변경사항을 감지하고 이를 Kubernetes 클러스터에 자동으로 적용하는 방식입니다. 이 방식은 개발 및 운영 환경에서 애플리케이션 상태를 항상 최신 상태로 유지하는 데 유용합니다.\n자동 Sync는 Git 리포지토리의 변경사항을 자동으로 감지하여 즉시 클러스터에 반영합니다. 예를 들어, syncPolicy 설정에서 automated 옵션을 활성화하면 자동 Sync가 작동합니다. 예시: 자동 Sync 설정 syncPolicy: automated: prune: true # 사용되지 않는 리소스를 자동으로 삭제 selfHeal: true # 변경된 리소스를 자동으로 수정 2. 수동 Sync 수동 Sync는 사용자가 명시적으로 동기화 작업을 트리거할 때만 적용됩니다. 이 방식은 배포 전에 확인 및 검토를 원하는 경우 유용합니다.\n수동 Sync는 Argo CD UI나 CLI에서 명시적으로 동기화 작업을 트리거해야 합니다. 실수로 배포되는 것을 방지하려는 상황에서 사용됩니다. 예시: 수동 Sync Argo CD UI에서 “Sync” 버튼을 클릭하여 수동으로 동기화를 수행할 수 있습니다.\n2️⃣ Sync 옵션 및 동작 방식 Argo CD에서 Sync는 여러 가지 옵션을 제공하며, 이 옵션들은 애플리케이션의 배포 방식을 세밀하게 제어할 수 있도록 돕습니다.\n1. Sync 옵션 Prune: Git 리포지토리에서 삭제된 리소스를 Kubernetes 클러스터에서 자동으로 삭제합니다. SelfHeal: 리소스가 Git 리포지토리와 다를 경우, 자동으로 클러스터 상태를 복구합니다. 2. 동작 방식 Sync 동작은 Git 리포지토리와 Kubernetes 클러스터 간의 차이를 분석하고, 이를 기반으로 새로운 리소스를 생성하거나 기존 리소스를 수정/삭제합니다. 이 과정은 Argo CD의 Helm, Kustomize, 또는 Manifests를 통해 수행됩니다.\n3. 예시: Sync 옵션 설정 syncPolicy: automated: prune: true selfHeal: true 이 설정은 자동 Sync를 활성화하고, 불필요한 리소스를 정리하며, 변경 사항을 즉시 클러스터에 반영합니다.\n3️⃣ Sync Waves \u0026 Phases (배포 단계 제어) Argo CD는 애플리케이션 배포를 여러 단계로 나누어 배포할 수 있는 Sync Waves 기능을 제공합니다. 이를 통해 배포 작업의 순서를 제어하고, 복잡한 배포 작업을 효율적으로 관리할 수 있습니다.\n1. Sync Waves Sync Waves는 배포 작업을 여러 단계로 나누어 순차적으로 실행할 수 있게 합니다. 각 Wave는 동기화 단계의 그룹으로, 배포 순서를 설정할 수 있습니다.\n2. Phases Sync Phases는 배포 과정에서 각각의 리소스가 처리되는 순서를 정의합니다. 예를 들어, 데이터베이스 리소스를 먼저 배포하고, 그 다음에 애플리케이션을 배포하는 방식으로 설정할 수 있습니다.\n3. 예시: Sync Waves 설정 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app spec: syncPolicy: automated: prune: true selfHeal: true syncOptions: - SyncWave=1 # 첫 번째 배포 단계 - SyncWave=2 # 두 번째 배포 단계 4️⃣ Sync Hooks (PreSync, Sync, PostSync) Argo CD는 배포 과정 중에 특정 작업을 트리거할 수 있는 Sync Hooks를 제공합니다. Sync Hooks는 배포 작업의 각 단계(PreSync, Sync, PostSync)에 실행할 작업을 정의합니다.\n1. PreSync Hook PreSync는 배포가 시작되기 전에 실행되는 작업입니다. 예를 들어, 배포 전에 데이터베이스 마이그레이션을 실행하거나, 이전 배포 버전에서 데이터를 백업할 수 있습니다.\n2. Sync Hook Sync는 실제 애플리케이션 배포가 이루어지는 시점입니다. 이 단계에서 실제 Kubernetes 리소스를 배포합니다.\n3. PostSync Hook PostSync는 배포 후에 실행되는 작업입니다. 예를 들어, 배포 후 애플리케이션의 상태를 모니터링하거나, 추가적인 설정을 적용할 수 있습니다.\n4. 예시: Sync Hook 설정 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app spec: syncPolicy: automated: prune: true selfHeal: true syncHooks: - hook: PreSync command: [\"sh\", \"-c\", \"echo 'Running PreSync task'\"] - hook: PostSync command: [\"sh\", \"-c\", \"echo 'Running PostSync task'\"] 5️⃣ Blue-Green 배포 및 Canary 배포 적용 Argo CD는 Blue-Green 배포와 Canary 배포 전략을 지원하여, 애플리케이션 배포 시 더 안전하고 점진적인 배포를 할 수 있습니다.\n1. Blue-Green 배포 Blue-Green 배포는 새 버전의 애플리케이션을 배포할 때, 기존 버전과 새로운 버전을 동시에 유지하고, 트래픽을 새로운 버전으로 전환하는 전략입니다. 이 방식은 롤백이 쉬운 장점이 있습니다.\n2. Canary 배포 Canary 배포는 새 버전의 애플리케이션을 일부 사용자에게만 배포하고, 문제가 없을 경우 점진적으로 배포 대상을 확대하는 방식입니다. 이는 위험을 최소화하며 점진적으로 배포를 관리할 수 있습니다.\n3. 예시: Canary 배포 설정 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-canary-app spec: syncPolicy: automated: prune: true selfHeal: true destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/my-canary-app.git targetRevision: HEAD path: manifests project: default syncOptions: - CanaryStrategy=true 6️⃣ 배포 롤백 (Rollback) 및 히스토리 관리 배포 롤백은 새로운 버전의 배포가 문제가 발생했을 때 이전 안정적인 버전으로 되돌리는 작업입니다. Argo CD는 배포 히스토리를 관리하며, 손쉽게 롤백을 할 수 있습니다.\n1. 롤백 Argo CD는 배포 기록을 저장하여, 사용자가 이전 버전으로 쉽게 롤백할 수 있도록 지원합니다. 이 기능은 배포 중 문제가 발생했을 때 매우 유용합니다.\n2. 예시: 롤백 수행 Argo CD UI에서 “Rollback” 버튼을 클릭하거나 CLI에서 다음 명령어를 사용하여 롤백할 수 있습니다.\nargocd app rollback my-app --revision \u003crevision_number\u003e "},"title":"Argo CD Sync 및 배포 전략"},"/devops/argo/argo06/":{"data":{"":"","1-argo-cd-rbacrole-based-access-control-설정#1️⃣ \u003cstrong\u003eArgo CD RBAC(Role-Based Access Control) 설정\u003c/strong\u003e":"","2-ssosingle-sign-on-연동-oidc-ldap-github-oauth-등#2️⃣ \u003cstrong\u003eSSO(Single Sign-On) 연동 (OIDC, LDAP, GitHub OAuth 등)\u003c/strong\u003e":"","3-tls-및-인증서-관리#3️⃣ \u003cstrong\u003eTLS 및 인증서 관리\u003c/strong\u003e":"","4-private-git-repository-연동-github-gitlab-bitbucket#4️⃣ \u003cstrong\u003ePrivate Git Repository 연동 (GitHub, GitLab, Bitbucket)\u003c/strong\u003e":"","5-secret-management-sealed-secrets-hashicorp-vault-연동#5️⃣ \u003cstrong\u003eSecret Management (Sealed Secrets, HashiCorp Vault 연동)\u003c/strong\u003e":"","6-argo-cd에서-image-signing-및-sbom-적용#6️⃣ \u003cstrong\u003eArgo CD에서 Image Signing 및 SBOM 적용\u003c/strong\u003e":"1️⃣ Argo CD RBAC(Role-Based Access Control) 설정 RBAC는 사용자가 Argo CD에서 수행할 수 있는 작업을 제어하는 중요한 보안 기능입니다. 이를 통해 특정 사용자 또는 그룹에 대해 권한을 세밀하게 설정할 수 있습니다.\n1. RBAC의 개념 RBAC는 사용자나 그룹에게 역할(role)을 할당하고, 각 역할에 대해 허용된 작업을 정의합니다. 예를 들어, 관리자는 모든 작업을 할 수 있지만, 일반 사용자에게는 일부 작업만 허용할 수 있습니다.\n2. RBAC 설정 예시 Argo CD에서는 argocd-rbac-cm ConfigMap을 사용하여 RBAC 정책을 설정합니다. 이 설정은 사용자에게 특정 권한을 부여하는 데 사용됩니다.\n예시: 관리자와 일반 사용자 역할 설정 apiVersion: v1 kind: ConfigMap metadata: name: argocd-rbac-cm namespace: argocd data: policy.csv: | g, admin, role:admin g, user, role:readonly policy.default: \"role:readonly\" admin 그룹은 role:admin 역할을 가지며, 모든 작업을 수행할 수 있습니다. user 그룹은 role:readonly 역할을 가지며, 읽기 전용 액세스만 허용됩니다. 3. Role 설정 예시 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: argocd name: admin rules: - apiGroups: [\"\"] resources: [\"pods\", \"services\"] verbs: [\"get\", \"list\", \"create\", \"delete\"] 이 예시는 admin 역할에 Kubernetes 리소스를 관리할 수 있는 권한을 부여합니다.\n2️⃣ SSO(Single Sign-On) 연동 (OIDC, LDAP, GitHub OAuth 등) SSO는 여러 애플리케이션에 대해 하나의 인증 방법을 사용하는 방식으로, Argo CD는 OIDC(OpenID Connect), LDAP, GitHub OAuth 등 다양한 인증 방법을 지원합니다.\n1. OIDC 연동 OIDC는 JSON Web Token(JWT)을 사용한 인증 방식을 제공합니다. Argo CD는 OIDC를 사용하여 외부 인증 제공자(Google, Okta 등)와 통합할 수 있습니다.\n예시: OIDC 설정 apiVersion: argoproj.io/v1alpha1 kind: ConfigMap metadata: name: argocd-cm namespace: argocd data: oidc.config: | name: Google issuer: https://accounts.google.com clientID: \u003cyour-client-id\u003e clientSecret: \u003cyour-client-secret\u003e requestedScopes: openid, profile, email 2. LDAP 연동 LDAP을 사용하여 기존의 디렉터리 서비스와 연동할 수 있습니다. Argo CD는 LDAP를 통해 사용자 인증을 수행하고, LDAP 그룹을 이용하여 RBAC을 설정할 수 있습니다.\n예시: LDAP 설정 apiVersion: argoproj.io/v1alpha1 kind: ConfigMap metadata: name: argocd-cm namespace: argocd data: ldap.config: | url: ldap://ldap.example.com bindDN: \"cn=admin,dc=example,dc=com\" bindPassword: \"password\" userSearchBase: \"ou=users,dc=example,dc=com\" userSearchFilter: \"(uid={0})\" 3️⃣ TLS 및 인증서 관리 Argo CD는 애플리케이션과 Kubernetes 클러스터 간의 통신을 보호하기 위해 TLS(Transport Layer Security)를 사용합니다. 또한, 인증서를 관리하고 갱신하는 기능도 중요합니다.\n1. TLS 인증서 관리 Argo CD에서는 기본적으로 self-signed 인증서를 사용하지만, 이를 Let's Encrypt와 같은 외부 인증서로 교체할 수 있습니다.\n예시: 인증서 설치 apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: argocd-cert namespace: argocd spec: secretName: argocd-tls dnsNames: - argocd.example.com issuerRef: name: letsencrypt-prod kind: ClusterIssuer 2. 인증서 갱신 인증서를 자동으로 갱신하려면 cert-manager와 같은 도구를 사용하여 Argo CD의 인증서를 자동으로 갱신할 수 있습니다.\n4️⃣ Private Git Repository 연동 (GitHub, GitLab, Bitbucket) Argo CD는 비공개 Git 리포지토리와의 연동을 지원하여, 애플리케이션 소스를 안전하게 관리할 수 있습니다. 이를 통해 GitOps 방식을 구현할 수 있습니다.\n1. Git Credential 설정 비공개 리포지토리와 연동하기 위해서는 인증 정보를 설정해야 합니다.\n예시: Git 리포지토리 인증 설정 argocd repo add https://github.com/my-org/my-private-repo.git \\ --username \u003cyour-username\u003e --password \u003cyour-password\u003e 2. SSH 키를 사용한 인증 SSH 키를 사용하여 인증을 설정할 수도 있습니다.\n예시: SSH 키 설정 argocd repo add git@github.com:my-org/my-private-repo.git \\ --ssh-private-key-path ~/.ssh/id_rsa 5️⃣ Secret Management (Sealed Secrets, HashiCorp Vault 연동) Argo CD는 기밀 정보를 안전하게 관리하는 기능도 제공합니다. 이를 위해 Sealed Secrets 또는 HashiCorp Vault와의 연동을 지원합니다.\n1. Sealed Secrets 사용 Sealed Secrets는 암호화된 비밀을 Kubernetes 클러스터에 안전하게 저장하는 방법을 제공합니다.\n예시: Sealed Secrets 생성 kubectl create secret generic my-secret --from-literal=password=my-secret-password kubectl seal --cert my-cert.pem \u003c my-secret.yaml \u003e my-sealed-secret.yaml 2. HashiCorp Vault 연동 HashiCorp Vault는 기밀 정보를 안전하게 관리할 수 있는 도구로, Argo CD와 연동하여 Vault에서 비밀을 가져와 사용할 수 있습니다.\n예시: Vault 연동 설정 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app spec: source: repoURL: https://github.com/my-org/my-app.git targetRevision: HEAD destination: server: https://kubernetes.default.svc namespace: default secrets: - vault://path/to/secret 6️⃣ Argo CD에서 Image Signing 및 SBOM 적용 이미지 서명 및 SBOM(Software Bill of Materials)은 보안의 중요한 요소로, 애플리케이션 이미지를 서명하고, 소프트웨어 구성 요소를 추적하는 방법을 제공합니다.\n1. Image Signing Argo CD는 이미지 서명을 사용하여 신뢰할 수 있는 소스에서 이미지를 배포하는지 확인합니다. 이를 통해 악성 이미지 배포를 방지할 수 있습니다.\n예시: 이미지 서명 이미지 서명은 cosign 같은 도구를 사용하여 수행할 수 있습니다. 예시로, cosign을 사용하여 이미지를 서명하는 방법은 다음과 같습니다.\ncosign sign my-image:latest 2. SBOM 적용 SBOM을 사용하면 애플리케이션의 소프트웨어 구성 요소를 추적할 수 있습니다. Argo CD는 SBOM을 활용하여 애플리케이션의 종속성을 관리할 수 있습니다.\n예시: SBOM 생성 syft my-image:latest -o json \u003e sbom.json "},"title":"Argo CD 보안 및 접근 제어"},"/devops/argo/argo07/":{"data":{"":"","1-argo-cd와-jenkins-연동#1️⃣ \u003cstrong\u003eArgo CD와 Jenkins 연동\u003c/strong\u003e":"","2-argo-cd와-github-actions-연동#2️⃣ \u003cstrong\u003eArgo CD와 GitHub Actions 연동\u003c/strong\u003e":"","3-argo-cd와-gitlab-cicd-연동#3️⃣ \u003cstrong\u003eArgo CD와 GitLab CI/CD 연동\u003c/strong\u003e":"","4-argo-cd와-tekton을-활용한-cicd#4️⃣ \u003cstrong\u003eArgo CD와 Tekton을 활용한 CI/CD\u003c/strong\u003e":"","5-argo-cd를-활용한-kubernetes-operator-배포#5️⃣ \u003cstrong\u003eArgo CD를 활용한 Kubernetes Operator 배포\u003c/strong\u003e":"1️⃣ Argo CD와 Jenkins 연동 Jenkins는 매우 인기 있는 CI/CD 도구로, Argo CD와 연동하여 강력한 자동화된 배포 파이프라인을 구축할 수 있습니다. Argo CD는 GitOps 방식을 따르기 때문에 Jenkins와 연동하여 CI 파이프라인을 실행한 후, Argo CD는 Kubernetes 클러스터에 애플리케이션을 배포합니다.\n1. Jenkins와 Argo CD 연동 개요 Jenkins에서 애플리케이션을 빌드하고, Git 저장소에 배포 파일을 푸시하면 Argo CD가 이를 감지하고 배포합니다. Jenkins는 Argo CD API를 호출하여 Kubernetes에 애플리케이션을 배포할 수 있습니다.\n2. Jenkins에서 Argo CD API 호출 예시 pipeline { agent any stages { stage('Deploy to Kubernetes') { steps { script { sh 'argocd app sync my-app' sh 'argocd app wait my-app --sync' } } } } } 이 Jenkins 파이프라인은 argocd CLI 명령어를 통해 Argo CD에 애플리케이션을 동기화하고 기다립니다.\n3. Jenkins와 Argo CD 연동을 위한 설정 Jenkins에서 Argo CD에 접근하려면 argocd CLI를 설치하고, API 인증을 위한 토큰을 사용해야 합니다. 이 인증 정보를 Jenkins의 환경 변수로 설정해주면 됩니다.\n2️⃣ Argo CD와 GitHub Actions 연동 GitHub Actions는 GitHub에서 제공하는 CI/CD 도구로, GitHub 저장소에 통합된 자동화된 워크플로를 실행할 수 있습니다. Argo CD와 함께 사용하여, GitHub에서의 변경 사항을 자동으로 Kubernetes 클러스터에 배포할 수 있습니다.\n1. GitHub Actions 워크플로 예시 GitHub Actions에서 Argo CD를 호출하여 애플리케이션을 배포할 수 있습니다.\nname: Deploy to Kubernetes with Argo CD on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Set up kubectl uses: azure/setup-kubectl@v1 - name: Sync with Argo CD run: | argocd login argocd.example.com --username admin --password ${{ secrets.ARGOCD_PASSWORD }} argocd app sync my-app argocd app wait my-app --sync 2. GitHub Actions와 Argo CD 설정 GitHub Actions에서 Argo CD를 사용하려면 argocd CLI가 설치된 환경에서 로그인 및 애플리케이션 동기화 작업을 해야 합니다. 환경 변수로 Argo CD의 사용자 이름과 비밀번호를 설정합니다.\n3️⃣ Argo CD와 GitLab CI/CD 연동 GitLab CI/CD는 GitLab의 내장된 CI/CD 시스템으로, GitLab 저장소에서 자동화된 배포를 지원합니다. GitLab CI/CD와 Argo CD를 연동하여 애플리케이션을 자동으로 배포할 수 있습니다.\n1. GitLab CI/CD 파이프라인 예시 stages: - deploy deploy: stage: deploy script: - argocd login argocd.example.com --username admin --password $ARGOCD_PASSWORD - argocd app sync my-app - argocd app wait my-app --sync only: - master 2. GitLab과 Argo CD 설정 GitLab CI/CD에서 Argo CD를 사용하려면 GitLab Runner 환경에서 argocd CLI를 사용할 수 있도록 설정해야 하며, 인증 정보는 GitLab의 Secret Variables에 저장해야 합니다.\n4️⃣ Argo CD와 Tekton을 활용한 CI/CD Tekton은 Kubernetes 네이티브 CI/CD 파이프라인을 생성할 수 있는 도구입니다. Argo CD와 Tekton을 연동하여 Kubernetes 환경에서 CI/CD 파이프라인을 구축할 수 있습니다.\n1. Tekton과 Argo CD 연동 개요 Tekton은 파이프라인, 작업 및 리소스를 정의하여 CI/CD 파이프라인을 구축하고, Argo CD는 배포 후 애플리케이션의 상태를 관리합니다.\n2. Tekton 파이프라인 예시 apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: deploy-pipeline-run spec: pipelineRef: name: deploy-pipeline serviceAccountName: pipeline params: - name: APP_NAME value: my-app - name: REPO_URL value: https://github.com/my-org/my-app.git 3. Tekton과 Argo CD 설정 Tekton에서 작업을 실행하고, Argo CD는 이를 감지하여 Kubernetes 클러스터에 애플리케이션을 배포합니다. Tekton의 PipelineRun에서 Argo CD CLI를 호출하여 애플리케이션을 동기화하고 상태를 확인할 수 있습니다.\n5️⃣ Argo CD를 활용한 Kubernetes Operator 배포 Kubernetes Operator는 Kubernetes 클러스터 내에서 애플리케이션의 생애 주기를 관리하는 데 사용됩니다. Argo CD와 Kubernetes Operator를 연동하여 애플리케이션을 관리할 수 있습니다.\n1. Operator를 통한 애플리케이션 관리 Argo CD는 GitOps 방식으로 Kubernetes 리소스를 관리하는데, Kubernetes Operator는 특정 애플리케이션의 생애 주기 관리 및 상태 모니터링을 자동화합니다. 이를 통해 애플리케이션 배포 후 운영까지 Argo CD와 Kubernetes Operator가 협력하여 관리할 수 있습니다.\n2. Operator 배포 예시 Operator를 배포하는 예시를 보면, 특정 애플리케이션 리소스를 관리하는 CustomResource를 정의할 수 있습니다.\napiVersion: apps/v1 kind: Deployment metadata: name: my-operator spec: replicas: 1 template: spec: containers: - name: my-operator image: my-operator:latest 3. Argo CD와 Operator 배포 예시 Argo CD는 Git 리포지토리에서 변경 사항을 추적하여 Kubernetes 클러스터에 자동으로 배포할 수 있습니다. GitOps 방식으로 Kubernetes Operator의 배포를 관리할 수 있습니다.\nargocd app sync my-operator "},"title":"Argo CD와 CI/CD 통합"},"/devops/argo/argo08/":{"data":{"":"","1-argo-cd의-로그-및-이벤트-확인#1️⃣ \u003cstrong\u003eArgo CD의 로그 및 이벤트 확인\u003c/strong\u003e":"","2-application-상태-분석-healthy-progressing-degraded-suspended#2️⃣ \u003cstrong\u003eApplication 상태 분석 (Healthy, Progressing, Degraded, Suspended)\u003c/strong\u003e":"","3-argo-cd-notifications-설정-slack-discord-ms-teams-webhook-연동#3️⃣ \u003cstrong\u003eArgo CD Notifications 설정 (Slack, Discord, MS Teams, Webhook 연동)\u003c/strong\u003e":"","4-prometheus-grafana를-활용한-argo-cd-모니터링#4️⃣ \u003cstrong\u003ePrometheus, Grafana를 활용한 Argo CD 모니터링\u003c/strong\u003e":"","5-argo-cd-장애-복구-및-백업-전략#5️⃣ \u003cstrong\u003eArgo CD 장애 복구 및 백업 전략\u003c/strong\u003e":"1️⃣ Argo CD의 로그 및 이벤트 확인 Argo CD는 애플리케이션의 배포 및 동기화 상태를 모니터링하기 위한 다양한 로그 및 이벤트를 제공합니다. 이를 통해 애플리케이션의 상태를 확인하고 문제가 발생한 원인을 추적할 수 있습니다.\n1. Argo CD 서버 로그 확인 Argo CD 서버의 로그를 확인하려면 kubectl 명령어를 사용하여 로그를 추출할 수 있습니다. 예를 들어, argocd-server의 로그를 확인하려면 아래 명령어를 사용합니다.\nkubectl logs -n argocd deployment/argocd-server 이 명령어는 argocd-server의 최근 로그를 출력합니다.\n2. Application 관련 이벤트 확인 Argo CD에서 애플리케이션 상태를 변경할 때 발생하는 이벤트를 확인하려면, 다음 명령어를 사용할 수 있습니다.\nkubectl describe application my-app -n argocd 이 명령어는 my-app 애플리케이션의 상태와 관련된 이벤트를 출력합니다. 이벤트를 통해 배포 실패나 리소스 변경 사항을 추적할 수 있습니다.\n2️⃣ Application 상태 분석 (Healthy, Progressing, Degraded, Suspended) Argo CD는 애플리케이션의 상태를 Healthy, Progressing, Degraded, Suspended와 같은 상태로 나타냅니다. 각 상태는 애플리케이션의 현재 상태를 나타내며, 이를 통해 문제를 분석할 수 있습니다.\n1. Healthy 상태 Healthy 상태는 애플리케이션이 정상적으로 배포되었고, 모든 리소스가 예상대로 작동하는 상태입니다.\n2. Progressing 상태 Progressing 상태는 애플리케이션이 변경 중에 있음을 나타냅니다. 예를 들어, 배포 중인 상태이거나 리소스가 업데이트 중일 때 이 상태로 표시됩니다.\n3. Degraded 상태 Degraded 상태는 애플리케이션에 문제가 발생했음을 나타냅니다. 배포 실패나 리소스 충돌 등으로 인해 애플리케이션이 정상적으로 작동하지 않는 경우 발생합니다.\n4. Suspended 상태 Suspended 상태는 애플리케이션의 동기화가 일시 중지된 상태를 나타냅니다. 이는 수동 동기화 모드로 설정된 경우 발생할 수 있습니다.\n3️⃣ Argo CD Notifications 설정 (Slack, Discord, MS Teams, Webhook 연동) Argo CD는 다양한 알림 시스템과 연동할 수 있습니다. 이를 통해 애플리케이션의 배포 상태나 에러 발생 시 알림을 받을 수 있습니다.\n1. Slack 연동 예시 Slack과 Argo CD를 연동하려면, Slack Webhook을 생성한 후 Argo CD 알림 설정에 이를 추가해야 합니다.\napiVersion: v1 kind: ConfigMap metadata: name: argocd-notifications-cm namespace: argocd data: service.slack: | api_url: \"https://hooks.slack.com/services/your/webhook/url\" channel: \"#your-channel\" username: \"Argo CD\" 이 설정을 통해 애플리케이션의 상태가 변경될 때 Slack 채널로 알림을 받을 수 있습니다.\n2. MS Teams 연동 예시 MS Teams의 Webhook을 생성한 후, Argo CD의 알림 설정에 추가하여 MS Teams 채널로 알림을 보낼 수 있습니다.\napiVersion: v1 kind: ConfigMap metadata: name: argocd-notifications-cm namespace: argocd data: service.teams: | api_url: \"https://outlook.office.com/webhook/your/webhook/url\" channel: \"your-channel\" username: \"Argo CD\" 4️⃣ Prometheus, Grafana를 활용한 Argo CD 모니터링 Argo CD는 Prometheus와 Grafana를 활용하여 애플리케이션의 상태를 실시간으로 모니터링할 수 있습니다. Prometheus는 메트릭을 수집하고, Grafana는 이를 시각화하여 대시보드로 제공합니다.\n1. Prometheus 설정 Argo CD에서 Prometheus 메트릭을 수집하려면, 먼저 Argo CD의 Prometheus 메트릭을 활성화해야 합니다. Argo CD의 argocd-server에 Prometheus 메트릭을 노출할 수 있는 설정을 추가합니다.\napiVersion: apps/v1 kind: Deployment metadata: name: argocd-server namespace: argocd spec: containers: - name: argocd-server ports: - containerPort: 8080 - containerPort: 8082 name: metrics 2. Grafana 대시보드 설정 Grafana를 사용하여 Prometheus에서 수집된 데이터를 시각화할 수 있습니다. 예를 들어, Argo CD의 애플리케이션 동기화 상태, 실패율, 배포 시간 등을 모니터링할 수 있습니다.\n3. Prometheus와 Grafana 대시보드 예시 apiVersion: 1 kind: ConfigMap metadata: name: argocd-metrics-dashboard namespace: argocd data: dashboard.json: | { \"title\": \"Argo CD Metrics\", \"panels\": [ { \"type\": \"graph\", \"title\": \"Sync Status\", \"targets\": [ { \"expr\": \"argocd_application_sync_status\", \"legendFormat\": \"{{app_name}}\" } ] } ] } 5️⃣ Argo CD 장애 복구 및 백업 전략 Argo CD의 장애 복구 및 백업 전략은 애플리케이션과 상태 데이터를 안전하게 보존하고, 문제가 발생했을 때 빠르게 복구할 수 있도록 도와줍니다.\n1. Argo CD 리소스 백업 Argo CD의 핵심 리소스인 애플리케이션 및 설정을 주기적으로 백업해야 합니다. 이를 위해 kubectl 명령어를 사용하여 리소스를 YAML 파일로 추출할 수 있습니다.\nkubectl get applications -n argocd -o yaml \u003e argocd-applications-backup.yaml kubectl get configmap -n argocd \u003e argocd-config-backup.yaml 2. Argo CD 장애 복구 Argo CD 클러스터가 장애를 겪은 경우, 위에서 백업한 YAML 파일을 사용하여 빠르게 복구할 수 있습니다. 백업 파일을 클러스터에 적용하면 됩니다.\nkubectl apply -f argocd-applications-backup.yaml kubectl apply -f argocd-config-backup.yaml "},"title":"Argo CD 모니터링 및 문제 해결"},"/devops/argo/argo09/":{"data":{"":"","1-multi-tenant-argo-cd-구성#1️⃣ \u003cstrong\u003eMulti-Tenant Argo CD 구성\u003c/strong\u003e":"","2-multi-cluster-배포-전략-argo-cd--cluster-api#2️⃣ \u003cstrong\u003eMulti-Cluster 배포 전략 (Argo CD + Cluster API)\u003c/strong\u003e":"","3-argo-rollouts와-함께-canary--blue-green-배포#3️⃣ \u003cstrong\u003eArgo Rollouts와 함께 Canary \u0026amp; Blue-Green 배포\u003c/strong\u003e":"","4-argo-cd--service-mesh-istio-linkerd#4️⃣ \u003cstrong\u003eArgo CD + Service Mesh (Istio, Linkerd)\u003c/strong\u003e":"","5-argo-cd의-api-활용-및-자동화-webhooks-gitops-자동화-스크립트#5️⃣ \u003cstrong\u003eArgo CD의 API 활용 및 자동화 (Webhooks, GitOps 자동화 스크립트)\u003c/strong\u003e":"1️⃣ Multi-Tenant Argo CD 구성 Multi-Tenant Argo CD 구성은 여러 팀이나 부서가 하나의 Argo CD 인스턴스를 공유하여, 각자의 애플리케이션을 독립적으로 관리하는 환경을 제공합니다. 이를 통해 Argo CD를 효율적으로 분리하고, 각 팀의 애플리케이션을 논리적으로 격리할 수 있습니다.\n1. Multi-Tenant 아키텍처의 필요성 Multi-Tenant 환경에서 Argo CD를 설정하면, 각 팀이나 프로젝트가 자신만의 리소스를 관리하면서도 중앙 집중식으로 상태를 추적할 수 있습니다. 이를 통해 리소스를 절약하고, 관리의 복잡도를 줄일 수 있습니다.\n2. Argo CD의 Namespace 분리 각 팀을 위해 다른 네임스페이스를 사용하여 Argo CD 애플리케이션을 배포합니다. 이를 통해 각 팀의 애플리케이션 상태를 격리하고 관리할 수 있습니다.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app namespace: team-a spec: destination: namespace: team-a-app server: https://kubernetes.default.svc source: repoURL: https://github.com/my-org/my-repo path: charts/my-app project: default 3. Argo CD RBAC 설정 Argo CD의 RBAC(Role-Based Access Control)를 활용하여 각 팀이 자신의 애플리케이션만 관리할 수 있도록 권한을 설정합니다. 예를 들어, 특정 팀에만 접근을 허용하는 규칙을 만들 수 있습니다.\n2️⃣ Multi-Cluster 배포 전략 (Argo CD + Cluster API) Argo CD는 여러 클러스터에 걸쳐 애플리케이션을 배포하고 관리할 수 있습니다. 이때 Cluster API를 사용하여 클러스터를 자동으로 생성하고 관리하는 방법을 적용할 수 있습니다.\n1. Cluster API 소개 Cluster API는 Kubernetes 클러스터를 선언적으로 관리할 수 있는 API입니다. 이를 사용하여 새로운 클러스터를 생성하고, 기존 클러스터를 관리할 수 있습니다.\n2. Argo CD와 Cluster API 통합 Argo CD를 사용하여 다수의 클러스터에 애플리케이션을 배포하려면, 먼저 Cluster API를 통해 관리되는 클러스터들을 Argo CD에 등록해야 합니다.\nargocd cluster add \u003ccluster-name\u003e 3. Multi-Cluster 배포 Argo CD에서 Multi-Cluster 배포를 설정하면, 다양한 클러스터에 애플리케이션을 자동으로 배포하고 동기화할 수 있습니다.\nspec: destination: server: https://cluster-api-cluster-1 namespace: default 3️⃣ Argo Rollouts와 함께 Canary \u0026 Blue-Green 배포 Argo Rollouts는 Canary 배포와 Blue-Green 배포와 같은 고급 배포 전략을 지원하는 Argo CD의 확장입니다. 이를 통해 롤링 업데이트를 좀 더 세밀하게 제어할 수 있습니다.\n1. Argo Rollouts 소개 Argo Rollouts는 Argo CD의 확장으로, Canary와 Blue-Green 배포 전략을 쉽게 설정할 수 있습니다. 이는 점진적인 배포를 지원하여, 새로운 버전의 애플리케이션을 안전하게 배포하고 롤백할 수 있게 합니다.\n2. Canary 배포 설정 Canary 배포는 새로운 버전을 소수의 사용자에게만 배포하여, 문제가 없는지 확인한 후 전체 시스템에 배포하는 방법입니다. 이를 위해 아래와 같은 Rollout 리소스를 정의합니다.\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: my-app-rollout spec: replicas: 5 strategy: canary: steps: - setWeight: 20 - pause: {} - setWeight: 100 3. Blue-Green 배포 설정 Blue-Green 배포는 새로운 버전이 준비되면 트래픽을 완전히 새로운 버전으로 전환하여 배포하는 방법입니다.\napiVersion: argoproj.io/v1alpha1 kind: Rollout metadata: name: my-app-rollout spec: replicas: 5 strategy: blueGreen: activeService: my-app-active previewService: my-app-preview autoPromotionSeconds: 60 4️⃣ Argo CD + Service Mesh (Istio, Linkerd) Argo CD와 Service Mesh(예: Istio, Linkerd)를 통합하면, 애플리케이션의 배포와 운영에 있어 보다 고급 기능을 사용할 수 있습니다. Service Mesh는 마이크로서비스 간의 통신을 관리하고, 보안을 강화하며, 트래픽을 제어하는 데 유용합니다.\n1. Service Mesh 소개 Service Mesh는 애플리케이션의 서비스 간 통신을 관리하는 시스템입니다. Istio와 Linkerd는 Service Mesh의 대표적인 예입니다. Argo CD와 함께 사용하면 애플리케이션 배포 시 트래픽 제어나 보안 설정 등을 쉽게 할 수 있습니다.\n2. Istio와 Argo CD 통합 Istio를 Argo CD와 통합하면, 배포 시 Istio의 트래픽 관리, 보안 정책 등을 함께 적용할 수 있습니다.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: https://github.com/my-org/my-repo path: charts/my-app project: default syncPolicy: automated: prune: true selfHeal: true 3. Linkerd와 Argo CD 통합 Linkerd와 Argo CD를 통합하면, 보다 경량화된 Service Mesh 환경에서 트래픽 관리, 보안, 모니터링을 설정할 수 있습니다.\n5️⃣ Argo CD의 API 활용 및 자동화 (Webhooks, GitOps 자동화 스크립트) Argo CD는 API를 통해 자동화된 작업을 수행하거나 외부 시스템과 연동할 수 있습니다. Webhooks를 활용하여 Git 이벤트나 외부 시스템의 변경을 트리거로 자동화 작업을 실행할 수 있습니다.\n1. Webhooks를 활용한 자동화 Webhooks는 GitHub, GitLab 등에서 변경이 발생했을 때 Argo CD에 알림을 보내는 기능입니다. 이를 활용하여 GitOps 작업을 자동으로 트리거할 수 있습니다.\nargocd app sync my-app 2. GitOps 자동화 스크립트 작성 Argo CD API를 사용하여 애플리케이션의 상태를 자동으로 동기화하거나 배포할 수 있는 스크립트를 작성할 수 있습니다.\n#!/bin/bash argocd app sync my-app argocd app wait my-app --sync 3. 자동화된 파이프라인 설정 CI/CD 파이프라인에서 Argo CD API를 호출하여 배포를 자동화할 수 있습니다. 이를 통해 애플리케이션이 GitHub에서 변경될 때마다 자동으로 배포가 이루어지도록 할 수 있습니다.\nname: Deploy to Kubernetes on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Deploy to Argo CD run: | argocd login my-argocd-server argocd app sync my-app "},"title":"Argo CD 고급 활용 사례"},"/devops/docker/docker00/":{"data":{"1-컨테이너란-#1️⃣ 컨테이너란? 🏗️":"","2-vm-vs-컨테이너-차이-#2️⃣ VM vs. 컨테이너 차이 ⚖️":"","3-컨테이너-기술이-필요한-이유-#3️⃣ 컨테이너 기술이 필요한 이유 🌍":"","4-주요-컨테이너-런타임-비교-#4️⃣ 주요 컨테이너 런타임 비교 🔍":"","5-docker-기본-개념-#5️⃣ Docker 기본 개념 🚢":"","6-docker란-#6️⃣ Docker란? 🐳":"","7-docker의-주요-개념-#7️⃣ Docker의 주요 개념 📌":"컨테이너 기술 개요 🐳 1️⃣ 컨테이너란? 🏗️ 컨테이너(Container)는 애플리케이션과 필요한 모든 종속성을 하나의 패키지로 묶어, 어디서든 일관되게 실행할 수 있도록 만든 가상화 기술입니다.\n📌 컨테이너는 운영체제(OS) 수준에서 격리되어 실행되며, 물리적 서버 또는 가상 머신 위에서 가볍고 빠르게 배포 및 실행할 수 있습니다.\n2️⃣ VM vs. 컨테이너 차이 ⚖️ 항목 가상 머신(VM) 🏢 컨테이너(Container) 📦 실행 방식 하이퍼바이저(VMware, KVM 등)에서 실행 호스트 OS에서 직접 실행 OS 격리 각 VM이 자체 OS 포함 커널을 공유하며 독립된 환경 제공 성능 OS 부팅이 필요하여 상대적으로 느림 가벼운 실행 환경으로 빠름 자원 사용 각각의 VM이 메모리, CPU를 많이 사용 가벼운 프로세스 단위로 실행 배포 속도 OS 부팅 시간이 필요 즉시 실행 가능 🔍 컨테이너는 가볍고 빠르며, 자원 효율성이 뛰어나므로, 클라우드 및 마이크로서비스 환경에서 널리 사용됩니다!\n📌 컨테이너 아키텍처 비교 그림\n출처: 위키피디아\n3️⃣ 컨테이너 기술이 필요한 이유 🌍 ✅ 개발 환경의 일관성 유지 – “내 로컬에서는 잘 되는데?” 문제 해결\n✅ 빠른 배포 및 확장 – 경량 컨테이너를 사용하여 즉시 실행 가능\n✅ 효율적인 리소스 활용 – VM보다 메모리, CPU 사용량이 적음\n✅ 마이크로서비스 아키텍처 지원 – 각 서비스별 독립적인 실행 가능\n📌 실제 활용 예시\nNetflix, Google, Facebook 등은 컨테이너 기술을 활용하여 빠른 서비스 배포와 자동 확장을 수행 금융권, 공공기관에서도 컨테이너를 활용하여 보안 및 효율성 강화 4️⃣ 주요 컨테이너 런타임 비교 🔍 컨테이너 런타임 특징 Docker 가장 널리 사용되는 컨테이너 런타임, 사용 편리 containerd CNCF(Cloud Native Computing Foundation)에서 관리, 경량화된 Docker 대안 Firecracker AWS에서 개발, 경량 VM 기반 컨테이너, 보안성 강화 📌 컨테이너 런타임 구조\n출처: Medium\n5️⃣ Docker 기본 개념 🚢 6️⃣ Docker란? 🐳 Docker는 컨테이너를 쉽게 만들고, 배포하고, 실행할 수 있도록 도와주는 플랫폼입니다.\n📌 Docker의 주요 특징\n✅ 컨테이너 이미지를 빌드, 실행, 관리할 수 있음\n✅ 운영체제에 독립적인 환경을 제공\n✅ 경량화된 실행 방식으로 빠른 배포 가능\n7️⃣ Docker의 주요 개념 📌 1 Docker 이미지 (Image) 🏗️ 컨테이너 실행을 위한 템플릿 역할 여러 개의 계층(layer)로 구성 Docker Hub 또는 프라이빗 레지스트리에서 다운로드 가능 예시: nginx, mysql, redis 이미지 📌 Docker 이미지 구조\nBase Image (Ubuntu) ├── Layer 1: System Dependencies ├── Layer 2: Application Binaries ├── Layer 3: Configuration Files └── Container 2 Docker 컨테이너 (Container) 📦 이미지를 실행한 인스턴스 독립된 환경에서 실행되며, 필요한 애플리케이션 및 라이브러리를 포함 컨테이너 간 격리된 환경 제공 📌 컨테이너 실행 예제\ndocker run -d --name my_nginx -p 8080:80 nginx 3 Docker 레지스트리 (Registry) 📤 Docker 이미지를 저장하는 공간 공개 저장소(Docker Hub) 또는 프라이빗 저장소 사용 가능 📌 레지스트리 사용 예제\ndocker pull ubuntu:latest # Docker Hub에서 이미지 다운로드 docker tag my_app myrepo/my_app:v1.0 # 이미지 태깅 docker push myrepo/my_app:v1.0 # 프라이빗 레지스트리에 업로드 4 Docker 볼륨 (Volume) 🗄️ 컨테이너 데이터 영속성을 유지하는 저장소 컨테이너가 종료되어도 데이터 유지 가능 📌 볼륨 사용 예제\ndocker volume create my_volume docker run -d -v my_volume:/data my_container 5 Docker 네트워크 (Network) 🌐 컨테이너 간 통신을 관리하는 기능 기본적으로 bridge, host, overlay 네트워크 제공 📌 네트워크 사용 예제\ndocker network create my_network docker run -d --network my_network my_container 📌 Docker 네트워크 구조","컨테이너-기술-개요-#컨테이너 기술 개요 🐳":""},"title":"Container\u0026 Docker"},"/devops/docker/docker01/":{"data":{"1-docker-설치-linux-windows-mac#1️⃣ Docker 설치 (Linux, Windows, Mac)":"","2-docker-기본-명령어-정리-#2️⃣ Docker 기본 명령어 정리 📜":"","4-docker-이미지--컨테이너-관리-#4. Docker 이미지 \u0026amp; 컨테이너 관리 🎯":"","4-dockerfile-작성-방법-#4️⃣ Dockerfile 작성 방법 📝":"","5-docker-이미지-빌드-docker-build-#5️⃣ Docker 이미지 빌드 (docker build) 🏗️":"","6-컨테이너-실행-및-관리-#6️⃣ 컨테이너 실행 및 관리 🛠️":"","7-컨테이너-로그-확인-및-디버깅-#7️⃣ 컨테이너 로그 확인 및 디버깅 🔎":"","8-컨테이너-내부-접근-docker-exec-#8️⃣ 컨테이너 내부 접근 (docker exec) 💻":"","docker-설치-및-환경-구성-#Docker 설치 및 환경 구성 🛠️":"","결론-#결론 🎯":"Docker 설치 및 환경 구성 🛠️ 1️⃣ Docker 설치 (Linux, Windows, Mac) 📌 Docker는 다양한 운영체제에서 설치할 수 있으며, 아래 방법을 따라 설치할 수 있습니다.\n🔹 1 Linux (Ubuntu 기준) # 기존 Docker 패키지 제거 sudo apt-get remove docker docker-engine docker.io containerd runc # 패키지 업데이트 및 필수 패키지 설치 sudo apt-get update sudo apt-get install -y ca-certificates curl gnupg # Docker 공식 GPG 키 추가 sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo tee /etc/apt/keyrings/docker.asc \u003e /dev/null sudo chmod a+r /etc/apt/keyrings/docker.asc # Docker 저장소 추가 echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release \u0026\u0026 echo \"$VERSION_CODENAME\") stable\" | \\ sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null # Docker 패키지 설치 sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Docker 서비스 활성화 및 실행 sudo systemctl enable --now docker 🛠 설치 확인\ndocker --version docker run hello-world 🔹 2 Windows (Docker Desktop) 1️⃣ Docker 공식 사이트에서 Docker Desktop을 다운로드\n2️⃣ .exe 파일을 실행하고 설치 진행\n3️⃣ 설치 후 Docker 실행 → Settings에서 WSL 2 기반 엔진 사용 체크\n🛠 설치 확인\ndocker --version docker run hello-world 🔹 3 Mac (Docker Desktop) 1️⃣ Docker 공식 사이트에서 Docker Desktop for Mac 다운로드\n2️⃣ .dmg 파일을 실행하여 설치 진행\n3️⃣ 설치 후 Docker 실행\n🛠 설치 확인\ndocker --version docker run hello-world 2️⃣ Docker 기본 명령어 정리 📜 명령어 설명 docker run 컨테이너 실행 docker ps 실행 중인 컨테이너 목록 조회 docker stop \u003c컨테이너 ID\u003e 컨테이너 중지 docker rm \u003c컨테이너 ID\u003e 컨테이너 삭제 docker logs \u003c컨테이너 ID\u003e 컨테이너 로그 확인 docker exec -it \u003c컨테이너 ID\u003e bash 실행 중인 컨테이너 내부 접근 docker network ls 네트워크 목록 조회 📌 예제\ndocker run -d --name my-nginx -p 8080:80 nginx docker ps docker stop my-nginx docker rm my-nginx 4. Docker 이미지 \u0026 컨테이너 관리 🎯 4️⃣ Dockerfile 작성 방법 📝 📌 Dockerfile 기본 구조\n# 1. 베이스 이미지 설정 FROM ubuntu:latest # 2. 작업 디렉토리 설정 WORKDIR /app # 3. 필요한 패키지 설치 RUN apt-get update \u0026\u0026 apt-get install -y python3 # 4. 애플리케이션 실행 명령어 CMD [\"python3\", \"--version\"] 📌 Dockerfile을 활용하면 동일한 환경을 손쉽게 재현할 수 있습니다!\n5️⃣ Docker 이미지 빌드 (docker build) 🏗️ 📌 Docker 이미지 빌드\ndocker build -t my-python-app . 📌 이미지 목록 확인\ndocker images 📌 이미지 삭제\ndocker rmi my-python-app 6️⃣ 컨테이너 실행 및 관리 🛠️ 📌 컨테이너 실행 (포트 매핑 포함)\ndocker run -d -p 8080:80 --name my-nginx nginx 📌 실행 중인 컨테이너 목록 확인\ndocker ps 📌 모든 컨테이너 목록 확인 (중지된 컨테이너 포함)\ndocker ps -a 📌 컨테이너 중지 및 삭제\ndocker stop my-nginx docker rm my-nginx 7️⃣ 컨테이너 로그 확인 및 디버깅 🔎 📌 컨테이너 로그 확인\ndocker logs my-nginx 📌 컨테이너 상세 정보 조회\ndocker inspect my-nginx 📌 컨테이너의 실시간 로그 보기\ndocker logs -f my-nginx 8️⃣ 컨테이너 내부 접근 (docker exec) 💻 📌 컨테이너 내부로 진입\ndocker exec -it my-nginx bash 📌 특정 명령어 실행\ndocker exec my-nginx ls /usr/share/nginx/html 📌 컨테이너 종료 후 자동 삭제 (--rm 옵션 사용)\ndocker run --rm -it ubuntu bash 결론 🎯✅ Docker를 설치하고 기본적인 명령어를 익히면, 컨테이너 기반 환경을 쉽게 구성할 수 있습니다.\n✅ Docker Compose를 사용하면 여러 개의 컨테이너를 더욱 간편하게 관리할 수 있습니다.\n✅ Docker 이미지를 빌드하고, 컨테이너를 실행하며, 로그 확인 및 디버깅 방법을 익히는 것이 중요합니다!\n📌 추가 학습 리소스\nDocker 공식 문서 Docker Hub "},"title":"Docker Install"},"/devops/docker/docker02/":{"data":{"1-docker-compose-설치#1️⃣ Docker Compose 설치":"","2-docker-compose-기본-개념-#2️⃣ Docker Compose 기본 개념 📜":"","3-docker-composeyml-작성법-#3️⃣ \u003ccode\u003edocker-compose.yml\u003c/code\u003e 작성법 📝":"","4-여러-개의-컨테이너-구성-api-서버--db--웹서버#4️⃣ 여러 개의 컨테이너 구성 (API 서버 + DB + 웹서버)":"","5-docker-compose로-서비스-관리-#5️⃣ Docker Compose로 서비스 관리 🎛️":"","docker-compose-활용-#Docker Compose 활용 🛠️":"","결론-#결론 🎯":"Docker Compose 활용 🛠️ 1️⃣ Docker Compose 설치 Docker Compose는 여러 개의 컨테이너를 한 번에 정의하고 실행할 수 있는 도구입니다.\n🔹 1 Linux에서 Docker Compose 설치 sudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 🛠 설치 확인\ndocker-compose --version 📌 Docker Compose를 사용하면 docker-compose.yml 파일을 통해 여러 개의 컨테이너를 쉽게 관리할 수 있습니다!\n2️⃣ Docker Compose 기본 개념 📜 📌 Docker Compose란?\n여러 개의 컨테이너를 정의하고 관리할 수 있는 도구 docker-compose.yml 파일을 사용하여 서비스를 코드로 정의 단일 명령어로 컨테이너를 한 번에 실행, 중지, 삭제 가능 🔹 Docker Compose를 사용하면 이런 문제를 해결할 수 있음!\n✔️ 여러 개의 컨테이너를 손쉽게 실행 가능\n✔️ 컨테이너 간 네트워크를 자동으로 설정\n✔️ 환경 변수와 볼륨 설정 등을 코드로 관리\n✅ 사용 예시\nAPI 서버 + DB + 웹서버 마이크로서비스 아키텍처 CI/CD 파이프라인 📌 Docker Compose 설치 확인\ndocker-compose --version 3️⃣ docker-compose.yml 작성법 📝 📌 기본적인 docker-compose.yml 구조\nversion: \"3.8\" # Docker Compose 버전 services: app: image: nginx ports: - \"8080:80\" database: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: testdb 📌 서비스 정의\n키워드 설명 version Docker Compose 파일의 버전 services 실행할 컨테이너 목록 image 사용할 Docker 이미지 build Dockerfile을 사용하여 직접 빌드 ports 호스트와 컨테이너 간 포트 매핑 volumes 데이터 저장을 위한 볼륨 설정 environment 환경 변수 설정 4️⃣ 여러 개의 컨테이너 구성 (API 서버 + DB + 웹서버) 📌 예제: API 서버 (Flask) + MySQL + Nginx 구성\n🔹 1 프로젝트 폴더 구조 my_project/ ├── docker-compose.yml ├── app/ │ ├── Dockerfile │ ├── app.py │ ├── requirements.txt └── nginx/ ├── default.conf 🔹 2 Flask API 서버 구성 📌 Flask API 서버 코드 (app/app.py)\nfrom flask import Flask app = Flask(__name__) @app.route(\"/\") def home(): return \"Hello, Docker Compose!\" if __name__ == \"__main__\": app.run(host=\"0.0.0.0\", port=5000) 📌 Flask 의존성 파일 (app/requirements.txt)\nflask 📌 Flask 컨테이너 Dockerfile (app/Dockerfile)\n# 베이스 이미지 설정 FROM python:3.9 # 작업 디렉토리 설정 WORKDIR /app # 필요한 패키지 설치 COPY requirements.txt . RUN pip install -r requirements.txt # 애플리케이션 복사 COPY . . # Flask 실행 CMD [\"python\", \"app.py\"] 🔹 3 Nginx 설정 📌 Nginx 설정 파일 (nginx/default.conf)\nserver { listen 80; location / { proxy_pass http://app:5000; } } 🔹 4 docker-compose.yml 작성 📌 모든 컨테이너를 포함한 docker-compose.yml\nversion: \"3.8\" services: app: build: ./app ports: - \"5000:5000\" depends_on: - database database: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: testdb volumes: - db_data:/var/lib/mysql nginx: image: nginx volumes: - ./nginx/default.conf:/etc/nginx/conf.d/default.conf ports: - \"8080:80\" depends_on: - app volumes: db_data: 5️⃣ Docker Compose로 서비스 관리 🎛️ 🔹 1 컨테이너 실행 docker-compose up -d -d 옵션: 백그라운드에서 실행\n📌 실행 확인\ndocker-compose ps 🔹 2 컨테이너 중지 docker-compose down docker-compose down을 실행하면 모든 컨테이너와 네트워크가 종료됨\n📌 데이터를 유지한 채로 중지하려면?\ndocker-compose stop 🔹 3 특정 컨테이너만 실행/중지 docker-compose up -d app # app 서비스만 실행 docker-compose stop app # app 서비스만 중지 🔹 4 로그 확인 docker-compose logs -f 🔹 5 컨테이너 내부 접속 docker exec -it my_project_app_1 bash 📌 컨테이너 이름 확인\ndocker-compose ps 결론 🎯 ✅ Docker Compose를 활용하면 여러 개의 컨테이너를 손쉽게 관리할 수 있음\n✅ docker-compose.yml 파일을 통해 서비스를 코드로 정의하고 실행 가능\n✅ depends_on, volumes, environment 등을 활용하여 컨테이너 간 연동이 가능\n✅ 실무에서는 마이크로서비스, CI/CD, 개발 환경 구성 등에 활용\n📌 추가 학습 리소스\nDocker Compose 공식 문서 Docker Hub "},"title":"Docker-compose"},"/devops/docker/docker03/":{"data":{"1-네임스페이스와-cgroups-개념-#1️⃣ 네임스페이스와 cgroups 개념 🏗️":"","2-docker-컨테이너-리소스-제한-#2️⃣ Docker 컨테이너 리소스 제한 ⚖️":"","3-네트워크-격리-#3️⃣ 네트워크 격리 🌐":"","4-docker-컨테이너-보안-#4️⃣ Docker 컨테이너 보안 🛡️":"","결론-#결론 🎯":"컨테이너 격리 및 보안 🛡️ 1️⃣ 네임스페이스와 cgroups 개념 🏗️ 1. 네임스페이스(Namespace)란? 📌 네임스페이스(Namespace) 는 컨테이너 간의 격리를 제공하는 Linux 커널 기능입니다.\n컨테이너가 서로 독립된 환경에서 실행될 수 있도록 지원합니다.\n🔹 주요 네임스페이스 종류\n네임스페이스 설명 mnt 파일 시스템을 독립적으로 사용 pid 프로세스 ID를 분리하여 컨테이너 간 격리 net 네트워크 인터페이스를 독립적으로 사용 ipc 프로세스 간 통신(IPC) 자원을 분리 uts 호스트명과 도메인명을 컨테이너별로 분리 user UID/GID를 독립적으로 관리 (rootless 컨테이너) 📌 네임스페이스 확인 방법\nlsns # 현재 시스템의 네임스페이스 확인 📌 컨테이너 내 네임스페이스 확인\ndocker run -it --rm ubuntu bash lsns 2. cgroups(Control Groups)란? 📌 cgroups(Control Groups) 는 컨테이너의 CPU, 메모리, I/O 사용량을 제한하는 Linux 커널 기능입니다.\n🔹 주요 cgroups 설정 항목\n설정 설명 cpu.shares CPU 사용 비율 설정 memory.limit_in_bytes 최대 메모리 사용량 제한 blkio.throttle.read_bps_device 블록 I/O 속도 제한 📌 현재 cgroups 사용량 확인\ncat /sys/fs/cgroup/memory/memory.usage_in_bytes 📌 특정 컨테이너의 cgroups 확인\ndocker inspect --format='{{.HostConfig.Memory}}' \u003ccontainer_id\u003e 2️⃣ Docker 컨테이너 리소스 제한 ⚖️ 1. 컨테이너의 CPU 및 메모리 제한 설정 📌 메모리 제한 (--memory)\ndocker run -it --memory=512m ubuntu 📌 CPU 사용 제한 (--cpus)\ndocker run -it --cpus=1.5 ubuntu 📌 CPU 및 메모리 동시 제한\ndocker run -it --memory=512m --cpus=1 ubuntu 📌 실행 중인 컨테이너의 리소스 사용량 확인\ndocker stats 3️⃣ 네트워크 격리 🌐 1. Docker 네트워크 드라이버 종류 🔹 Docker 네트워크 모드\n모드 설명 bridge 기본 네트워크 모드, 가상 브릿지를 사용하여 컨테이너 연결 host 호스트 네트워크를 공유하여 네트워크 격리 해제 none 네트워크 기능 없음 overlay 여러 호스트 간 네트워크 연결 지원 (Swarm 모드) 📌 기본 브릿지 네트워크 확인\ndocker network ls 📌 새로운 브릿지 네트워크 생성\ndocker network create my_bridge 📌 컨테이너에 특정 네트워크 연결\ndocker run -it --network=my_bridge ubuntu 4️⃣ Docker 컨테이너 보안 🛡️ 1. Rootless 컨테이너 📌 Rootless 컨테이너 실행\ndockerd-rootless-setuptool.sh install Rootless 모드에서는 컨테이너가 호스트의 root 권한 없이 실행됨\n📌 Rootless 컨테이너 확인\ndocker info | grep \"rootless\" 2. seccomp를 활용한 시스템 콜 제한 📌 기본 seccomp 프로파일 적용\ndocker run --security-opt seccomp=default.json ubuntu 📌 사용 가능한 시스템 콜 목록 확인\ngrep \"syscalls\" /etc/docker/seccomp.json 3. AppArmor를 활용한 보안 정책 적용 📌 AppArmor 프로필 목록 확인\nsudo aa-status 📌 AppArmor 적용하여 컨테이너 실행\ndocker run --security-opt apparmor=my_profile ubuntu 결론 🎯✅ 네임스페이스와 cgroups를 사용하면 컨테이너 간 리소스 격리를 효과적으로 수행할 수 있음\n✅ CPU 및 메모리 제한을 설정하여 과도한 리소스 사용을 방지 가능\n✅ Docker 네트워크 드라이버를 활용하여 컨테이너 간 네트워크 격리를 설정 가능\n✅ Rootless 컨테이너, seccomp, AppArmor 등을 활용하여 보안을 강화 가능\n📌 추가 학습 리소스\nDocker Security 공식 문서 Seccomp \u0026 AppArmor ","컨테이너-격리-및-보안-#컨테이너 격리 및 보안 🛡️":""},"title":"Container Security"},"/devops/docker/docker04/":{"data":{"1-firecracker란-#1️⃣ Firecracker란? 🔥":"","2-docker와-firecracker-비교-#2️⃣ Docker와 Firecracker 비교 🆚":"","3-firecracker-설치-및-사용법-#3️⃣ Firecracker 설치 및 사용법 💻":"","4-firecracker로-컨테이너-실행하기-#4️⃣ Firecracker로 컨테이너 실행하기 🚀":"","firecracker-기반-경량-가상화-#Firecracker 기반 경량 가상화 ⚡":"","결론-#결론 🎯":"Firecracker 기반 경량 가상화 ⚡ 1️⃣ Firecracker란? 🔥 📌 Firecracker는 AWS에서 개발한 경량 가상화(VMM, Virtual Machine Monitor) 솔루션으로,\n서버리스 및 컨테이너 워크로드를 위한 초경량 가상 머신(MicroVM) 실행을 지원합니다.\n🔹 Firecracker의 주요 특징\n특징 설명 경량화 일반적인 VM보다 가벼운 MicroVM을 실행 빠른 부팅 125ms 이하의 부팅 속도 보안 강화 KVM 기반으로 프로세스 격리 지원 리소스 효율성 최소한의 메모리와 CPU 리소스 사용 📌 Firecracker 개념 아키텍처\n(출처: AWS)\nFirecracker는 AWS Lambda와 AWS Fargate에서 실제 사용되는 기술입니다!\n2️⃣ Docker와 Firecracker 비교 🆚 🔹 Firecracker vs. Docker 차이점\n항목 Firecracker Docker (containerd 기반) 격리 수준 KVM 기반 경량 VM 네임스페이스 \u0026 cgroups 부팅 속도 125ms 이하 즉시 실행 가능 리소스 사용량 더 적음 (경량 VM) 더 많음 (컨테이너 런타임) 보안성 하이퍼바이저 격리로 강화됨 프로세스 기반 격리 유즈 케이스 서버리스, 멀티테넌트 환경 일반적인 컨테이너 운영 📌 Firecracker는 Docker와 유사하지만 더 높은 보안성을 제공하는 경량 VM 기술입니다.\n3️⃣ Firecracker 설치 및 사용법 💻 1. Firecracker 바이너리 다운로드 📌 Firecracker 공식 바이너리 다운로드\ncurl -LOJ https://github.com/firecracker-microvm/firecracker/releases/latest/download/firecracker-x86_64 chmod +x firecracker-x86_64 📌 firecracker 실행 확인\n./firecracker-x86_64 --version 2. Firecracker 네트워크 설정 📌 TAP 네트워크 인터페이스 생성\nip tuntap add tap0 mode tap ip link set tap0 up 📌 브리지 네트워크 설정\nbrctl addbr br0 brctl addif br0 tap0 ip link set br0 up 4️⃣ Firecracker로 컨테이너 실행하기 🚀 1. MicroVM 부팅을 위한 설정 파일 생성 📌 MicroVM 설정 파일(vm-config.json) 생성\n{ \"boot-source\": { \"kernel_image_path\": \"./vmlinux\", \"boot_args\": \"console=ttyS0 reboot=k panic=1 pci=off\" }, \"drives\": [ { \"drive_id\": \"rootfs\", \"path_on_host\": \"./rootfs.ext4\", \"is_root_device\": true, \"is_read_only\": false } ], \"network-interfaces\": [ { \"iface_id\": \"eth0\", \"guest_mac\": \"AA:FC:00:00:01:01\", \"host_dev_name\": \"tap0\" } ], \"machine-config\": { \"vcpu_count\": 2, \"mem_size_mib\": 512 } } 2. Firecracker MicroVM 실행 📌 Firecracker 실행 및 VM 시작\n./firecracker-x86_64 --api-sock /tmp/firecracker.socket 📌 MicroVM 생성 요청 보내기\ncurl --unix-socket /tmp/firecracker.socket -i \\ -X PUT \"http://localhost/boot-source\" \\ -H \"Accept: application/json\" \\ -H \"Content-Type: application/json\" \\ -d \"@vm-config.json\" 📌 MicroVM 상태 확인\ncurl --unix-socket /tmp/firecracker.socket -i \\ -X GET \"http://localhost/machine-config\" 결론 🎯✅ Firecracker는 Docker보다 더 가벼운 경량 VM을 제공\n✅ KVM 기반으로 높은 보안성을 유지하면서 빠른 부팅 속도를 가짐\n✅ AWS Lambda 및 Fargate에서 사용되는 기술로 서버리스 환경에 최적화\n✅ Docker와 비교하여 격리 수준이 더 높아 보안성이 강화됨\n📌 추가 학습 리소스\nFirecracker 공식 문서 AWS Firecracker 소개 블로그 "},"title":"Firecracker"},"/devops/helm/helm00/":{"data":{"":"","1-helm-개요#1️⃣ \u003cstrong\u003eHelm 개요\u003c/strong\u003e":"","2-helm-기본-개념#2️⃣ \u003cstrong\u003eHelm 기본 개념\u003c/strong\u003e":"","3-helm-설치-및-기본-사용법#3️⃣ \u003cstrong\u003eHelm 설치 및 기본 사용법\u003c/strong\u003e":"1️⃣ Helm 개요 1. Helm이란? Helm은 Kubernetes 애플리케이션을 관리하기 위한 패키지 매니저입니다. Helm을 사용하면 복잡한 Kubernetes 리소스를 쉽게 정의하고, 배포 및 관리할 수 있습니다.\n2. Kubernetes에서 Helm이 필요한 이유 YAML 매니페스트 관리의 복잡성 감소 애플리케이션 배포 및 업데이트 자동화 재사용 가능한 구성 제공 여러 환경(개발, 테스트, 프로덕션)에서 일관된 배포 보장 3. Helm과 YAML 기반의 쿠버네티스 매니페스트 비교 Helm 없이 직접 Kubernetes 매니페스트를 작성하면 반복적이고 긴 YAML 파일을 관리해야 합니다. Helm은 이러한 매니페스트를 템플릿화하여 유지보수와 재사용성을 높여 줍니다.\n# 기존 Kubernetes Deployment 매니페스트 예제 apiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 3 template: spec: containers: - name: my-app image: my-app:1.0.0 Helm을 사용하면 다음과 같이 변수화할 수 있습니다.\n# Helm 템플릿 예제 apiVersion: apps/v1 kind: Deployment metadata: name: {{ .Release.Name }} spec: replicas: {{ .Values.replicaCount }} template: spec: containers: - name: my-app image: {{ .Values.image.repository }}:{{ .Values.image.tag }} 4. Helm의 주요 기능 및 장점 패키징 및 배포 자동화: Helm Chart를 이용하여 손쉽게 애플리케이션을 배포할 수 있습니다. 버전 관리: 이전 버전으로 롤백이 가능하여 안정적인 운영을 지원합니다. 템플릿화된 구성 관리: YAML 템플릿을 활용하여 다양한 환경에 맞게 설정을 변경할 수 있습니다. 레포지토리 기반 배포: 중앙 레포지토리(예: ArtifactHub)를 통해 Helm Chart를 공유하고 재사용할 수 있습니다. 2️⃣ Helm 기본 개념 1. Chart(차트): Helm의 패키징 단위 Helm의 Chart는 Kubernetes 애플리케이션을 배포하는 패키지입니다. Chart는 다음과 같은 구조를 가집니다.\nmychart/ Chart.yaml # 차트 메타데이터 values.yaml # 기본 설정 값 templates/ # 쿠버네티스 리소스 템플릿 charts/ # 종속 차트 2. Release(릴리스): Helm을 통해 배포된 애플리케이션 Release는 Helm Chart를 이용해 Kubernetes 클러스터에 배포된 인스턴스를 의미합니다. 동일한 Chart를 여러 번 배포할 수 있으며, 각각의 배포는 별도의 Release로 관리됩니다.\nhelm install my-release mychart/ 3. Repository(레포지토리): Helm 차트를 저장하는 공간 Helm Chart는 로컬이나 원격 저장소(레포지토리)에 저장됩니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami helm repo update 4. Values(값): 사용자 정의 설정을 적용하는 방법 values.yaml을 사용하면 기본값을 설정하고 사용자 정의 값을 적용할 수 있습니다.\nreplicaCount: 2 image: repository: nginx tag: latest helm install my-release mychart/ -f custom-values.yaml 5. Templating(템플릿): Helm이 동적으로 쿠버네티스 매니페스트를 생성하는 방식 Helm의 템플릿 엔진을 사용하면 동적인 매니페스트 생성을 할 수 있습니다.\napiVersion: apps/v1 kind: Deployment metadata: name: {{ .Release.Name }} spec: replicas: {{ .Values.replicaCount }} 3️⃣ Helm 설치 및 기본 사용법 1. Helm 설치 (Linux, macOS, Windows) Helm을 설치하는 방법은 운영 체제에 따라 다릅니다.\nLinux/macOS curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash Windows (choco 사용) choco install kubernetes-helm 2. Helm 명령어 기본 사용법 Helm에서 자주 사용하는 명령어를 정리했습니다.\n3. helm repo add (레포지토리 추가) helm repo add bitnami https://charts.bitnami.com/bitnami 4. helm search repo (차트 검색) helm search repo nginx 5. helm install (차트 설치) helm install my-release bitnami/nginx 6. helm list (설치된 차트 목록 확인) helm list 7. helm upgrade (업데이트) helm upgrade my-release bitnami/nginx 8. helm rollback (롤백) helm rollback my-release 1 9. helm uninstall (삭제) helm uninstall my-release 10. helm show values (차트 기본값 조회) helm show values bitnami/nginx "},"title":"Helm 개요"},"/devops/helm/helm01/":{"data":{"":"","3-helm-설치-및-기본-사용법#3️⃣ \u003cstrong\u003eHelm 설치 및 기본 사용법\u003c/strong\u003e":"3️⃣ Helm 설치 및 기본 사용법 1 Helm 설치 (Linux, macOS, Windows) Helm을 설치하는 방법은 운영 체제에 따라 다릅니다.\n✅ Linux 및 macOS Helm은 Homebrew 또는 스크립트로 설치할 수 있습니다.\nHomebrew를 사용하는 방법\nbrew install helm 스크립트를 사용하는 방법\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash ✅ Windows Windows에서는 choco 또는 scoop을 사용할 수 있습니다.\nChocolatey 사용\nchoco install kubernetes-helm Scoop 사용\nscoop install helm 2 Helm 명령어 기본 사용법 Helm을 설치한 후, 기본적인 명령어를 알아보겠습니다.\n3 Helm repo add (레포지토리 추가) Helm 차트 저장소(Repository)를 추가하려면 다음 명령어를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami 추가된 저장소 목록 확인:\nhelm repo list 4 Helm search repo (차트 검색) Helm 차트를 검색하려면 다음 명령어를 사용합니다.\nhelm search repo nginx 출력 예시:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 13.2.1 1.21.3 Chart for the NGINX web server 5 Helm install (차트 설치) Helm 차트를 설치하는 기본 명령어:\nhelm install my-nginx bitnami/nginx 설치된 Helm 리소스 확인:\nhelm list 6 Helm list (설치된 차트 목록 확인) 설치된 모든 차트를 확인하려면:\nhelm list 출력 예시:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION my-nginx default 1 2023-03-23 12:00:00.123456 +0000 UTC deployed nginx-13.2.1 1.21.3 7 Helm upgrade (업데이트) 설치된 Helm 차트를 업데이트하려면:\nhelm upgrade my-nginx bitnami/nginx --set service.type=LoadBalancer 8 Helm rollback (롤백) 이전 버전으로 되돌리려면:\nhelm rollback my-nginx 1 9 Helm uninstall (삭제) 설치된 차트를 삭제하려면:\nhelm uninstall my-nginx 🔟 Helm show values (차트 기본값 조회) Helm 차트의 기본 값을 확인하는 방법:\nhelm show values bitnami/nginx 출력 예시:\nreplicaCount: 1 image: repository: bitnami/nginx tag: 1.21.3 pullPolicy: IfNotPresent service: type: ClusterIP port: 80 "},"title":"Helm 설치 및 기본 사용법"},"/devops/helm/helm02/":{"data":{"":"","4-helm-chart-구조-상세-분석#4️⃣ \u003cstrong\u003eHelm Chart 구조 상세 분석\u003c/strong\u003e":"4️⃣ Helm Chart 구조 상세 분석 Helm Chart는 여러 파일과 디렉토리로 구성됩니다. 이 구조를 이해하면 Helm을 활용한 쿠버네티스 애플리케이션 배포를 효과적으로 할 수 있습니다.\n1 Chart.yaml (메타데이터) Chart.yaml 파일은 Helm Chart의 메타데이터를 포함합니다.\n📌 예제 (Chart.yaml) apiVersion: v2 name: my-chart description: A Helm chart for Kubernetes type: application version: 1.0.0 appVersion: 1.16.0 필드 설명 apiVersion Chart의 API 버전 (v2 권장) name 차트 이름 description 차트 설명 type application 또는 library version 차트의 버전 appVersion 배포되는 애플리케이션 버전 2 values.yaml (기본값 설정) values.yaml 파일은 기본 설정 값을 정의합니다. 사용자가 --set 옵션이나 values-override.yaml을 통해 값을 변경할 수 있습니다.\n📌 예제 (values.yaml) replicaCount: 2 image: repository: nginx tag: latest pullPolicy: IfNotPresent service: type: ClusterIP port: 80 이 값은 템플릿(templates/)에서 변수로 사용됩니다.\n3 templates/ 디렉토리 이 디렉토리에는 쿠버네티스 매니페스트 템플릿이 들어 있습니다. values.yaml에서 정의한 값과 결합하여 실제 배포 파일이 생성됩니다.\n4 _helpers.tpl (템플릿 함수) 템플릿 함수 파일로, 재사용 가능한 템플릿을 정의할 때 사용됩니다.\n📌 예제 (_helpers.tpl) {{- define \"my-chart.fullname\" -}} {{ .Release.Name }}-{{ .Chart.Name }} {{- end -}} 이제 deployment.yaml에서 호출할 수 있습니다.\nmetadata: name: {{ include \"my-chart.fullname\" . }} 5 deployment.yaml (배포 매니페스트) Deployment 리소스를 정의합니다.\n📌 예제 (deployment.yaml) apiVersion: apps/v1 kind: Deployment metadata: name: {{ include \"my-chart.fullname\" . }} spec: replicas: {{ .Values.replicaCount }} selector: matchLabels: app: {{ .Release.Name }} template: metadata: labels: app: {{ .Release.Name }} spec: containers: - name: nginx image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" ports: - containerPort: {{ .Values.service.port }} 6 service.yaml (서비스 매니페스트) 쿠버네티스 Service 리소스를 정의합니다.\n📌 예제 (service.yaml) apiVersion: v1 kind: Service metadata: name: {{ include \"my-chart.fullname\" . }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: {{ .Values.service.port }} selector: app: {{ .Release.Name }} 7 ingress.yaml (인그레스 매니페스트) Ingress를 사용하여 외부 트래픽을 관리할 수 있습니다.\n📌 예제 (ingress.yaml) apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: {{ include \"my-chart.fullname\" . }} spec: rules: - host: example.com http: paths: - path: / pathType: Prefix backend: service: name: {{ include \"my-chart.fullname\" . }} port: number: {{ .Values.service.port }} 8 configmap.yaml \u0026 secrets.yaml (설정 및 보안 정보) ✅ ConfigMap 예제 (configmap.yaml) apiVersion: v1 kind: ConfigMap metadata: name: {{ include \"my-chart.fullname\" . }}-config data: config.json: | { \"env\": \"production\" } ✅ Secret 예제 (secrets.yaml) apiVersion: v1 kind: Secret metadata: name: {{ include \"my-chart.fullname\" . }}-secret type: Opaque data: password: {{ \"my-password\" | b64enc }} 9 charts/ (서브 차트) charts/ 디렉토리는 서브 차트를 포함합니다. 예를 들어, PostgreSQL을 함께 설치하려면 여기에 PostgreSQL Chart를 추가할 수 있습니다.\ncharts/ ├── postgresql-10.3.11.tgz 🔟 .helmignore (제외 파일 목록) .helmignore 파일은 Chart 패키징 시 제외할 파일을 정의합니다.\n📌 예제 (.helmignore) *.md .git/ *.tgz tests/ "},"title":"Helm Chart 구조 상세 분석"},"/devops/helm/helm03/":{"data":{"":"","5-helm-templating-심화#5️⃣ \u003cstrong\u003eHelm Templating 심화\u003c/strong\u003e":"5️⃣ Helm Templating 심화 Helm은 Go 템플릿 엔진을 사용하여 Kubernetes 매니페스트를 동적으로 생성합니다. 이를 활용하면 환경별 설정 관리, 유연한 배포 구조, 중복 제거 등을 할 수 있습니다.\n1 템플릿 문법 ({{ }} 구문) Helm 템플릿에서 {{ }} 구문을 사용하여 변수, 조건문, 반복문 등을 적용할 수 있습니다.\n📌 예제 (템플릿 사용법) apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-config data: app_name: \"{{ .Chart.Name }}\" environment: \"{{ .Values.env }}\" 위 코드에서 {{ .Release.Name }}, {{ .Chart.Name }}, {{ .Values.env }} 등은 Helm에서 제공하는 내장 객체입니다.\n2 변수 사용 (.Values) values.yaml 파일에 정의된 값을 템플릿에서 불러올 수 있습니다.\n✅ values.yaml replicaCount: 3 image: repository: nginx tag: latest ✅ deployment.yaml spec: replicas: {{ .Values.replicaCount }} template: spec: containers: - name: nginx image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" 위처럼 values.yaml 값을 활용하면 환경별 설정 변경이 용이합니다.\n3 조건문 (if, else, with) Helm 템플릿에서 조건문을 사용하여 특정 설정을 활성화하거나 비활성화할 수 있습니다.\n📌 예제 (if, else) {{- if .Values.service.enabled }} apiVersion: v1 kind: Service metadata: name: {{ .Release.Name }}-service spec: type: {{ .Values.service.type }} ports: - port: 80 targetPort: 80 {{- else }} # 서비스가 비활성화되었습니다. {{- end }} .Values.service.enabled 값이 true이면 Service가 생성됩니다. false이면 해당 매니페스트가 생성되지 않습니다. 4 루프 (range) 배열 데이터가 있을 때 range 문법을 사용하면 반복문을 적용할 수 있습니다.\n✅ values.yaml volumes: - name: config mountPath: /etc/config - name: logs mountPath: /var/logs ✅ deployment.yaml (템플릿 적용) spec: template: spec: containers: - name: app image: my-app:latest volumeMounts: {{- range .Values.volumes }} - name: {{ .name }} mountPath: {{ .mountPath }} {{- end }} 위 코드를 적용하면, values.yaml의 volumes 리스트에 따라 자동으로 volumeMounts가 생성됩니다.\n5 함수 활용 (toYaml, include, tpl) Helm에는 다양한 내장 함수가 있으며, 특히 toYaml, include, tpl 함수는 템플릿을 다룰 때 유용합니다.\n📌 toYaml 함수 (YAML 포맷 유지) config: |- {{ .Values.config | toYaml | indent 2 }} toYaml은 YAML 데이터 구조를 그대로 유지하는 데 사용됩니다. 📌 include 함수 (템플릿 재사용) _helpers.tpl 파일에서 정의한 템플릿을 포함할 때 사용합니다.\n✅ _helpers.tpl\n{{- define \"app.fullname\" -}} {{ .Release.Name }}-{{ .Chart.Name }} {{- end -}} ✅ deployment.yaml\nmetadata: name: {{ include \"app.fullname\" . }} 📌 tpl 함수 (동적 값 렌더링) command: - \"/bin/sh\" - \"-c\" - \"{{ .Values.command | tpl . }}\" tpl 함수는 values.yaml 내부에서도 템플릿을 렌더링할 때 사용됩니다. 6 _helpers.tpl을 활용한 템플릿 최적화 Helm에서는 _helpers.tpl 파일을 사용하여 반복되는 템플릿을 모듈화할 수 있습니다.\n✅ _helpers.tpl {{- define \"app.labels\" -}} app: {{ .Release.Name }} chart: {{ .Chart.Name }}-{{ .Chart.Version }} {{- end -}} ✅ deployment.yaml metadata: labels: {{ include \"app.labels\" . | indent 4 }} 이렇게 하면 중복되는 레이블 정의를 하나의 템플릿으로 관리할 수 있습니다.\n7 기본값 (default) 및 사용자 입력값 적용 Helm에서는 default 함수를 사용하여 값이 없을 때 기본값을 설정할 수 있습니다.\n📌 기본값 적용 예제 replicaCount: {{ .Values.replicaCount | default 2 }} 만약 values.yaml에 replicaCount가 없으면 2가 기본값으로 사용됩니다. 📌 특정 설정이 없을 경우 기본값 적용 service: type: {{ .Values.service.type | default \"ClusterIP\" }} service.type 값이 설정되지 않았다면 \"ClusterIP\"을 기본값으로 사용합니다. "},"title":"Helm Templating 심화"},"/devops/helm/helm04/":{"data":{"":"","6-helm-values와-오버라이딩-전략#6️⃣ \u003cstrong\u003eHelm Values와 오버라이딩 전략\u003c/strong\u003e":"6️⃣ Helm Values와 오버라이딩 전략 Helm의 Values 파일은 배포 환경에서 유연한 설정을 가능하게 해줍니다. 이 섹션에서는 Helm에서 Values 파일을 어떻게 활용하고 오버라이딩 할 수 있는지에 대해 알아보겠습니다.\n1 기본 values.yaml 활용 Helm에서 values.yaml 파일은 기본 설정 파일로, 차트를 배포할 때 기본적으로 사용되는 값들을 정의합니다.\n📌 기본 values.yaml 예시 replicaCount: 3 image: repository: nginx tag: latest service: type: ClusterIP 이 파일에서 replicaCount, image.repository, service.type 등의 값을 정의하고, 이를 템플릿에서 사용할 수 있습니다.\n✅ 템플릿에서 values.yaml 활용 spec: replicas: {{ .Values.replicaCount }} containers: - name: nginx image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\" ports: - containerPort: 80 위와 같이 values.yaml에 정의된 값을 Helm 템플릿에서 사용할 수 있습니다.\n2 --set을 이용한 명령줄 값 오버라이딩 Helm 차트를 설치하거나 업그레이드할 때 --set 플래그를 사용하면 명령줄에서 직접 값을 오버라이딩할 수 있습니다.\n📌 명령줄에서 --set 사용 예시 helm install my-app my-chart --set replicaCount=5 --set service.type=LoadBalancer 이 명령어는 replicaCount를 5로, service.type을 LoadBalancer로 오버라이드합니다.\n✅ 템플릿에서 반영된 값 spec: replicas: {{ .Values.replicaCount }} service: type: {{ .Values.service.type }} 위 템플릿에서 --set으로 전달한 값들이 실제 배포 시 반영됩니다.\n3 -f custom-values.yaml로 커스텀 설정 적용 Helm은 -f 플래그를 사용하여 다른 values.yaml 파일을 지정할 수 있습니다. 이를 통해 환경별 설정이나 커스텀 설정을 적용할 수 있습니다.\n📌 custom-values.yaml 예시 replicaCount: 5 image: repository: custom-nginx tag: stable 📌 Helm 명령어로 적용 helm install my-app my-chart -f custom-values.yaml 이 명령어는 custom-values.yaml 파일에 정의된 값을 기본 values.yaml을 덮어쓰며 적용합니다.\n4 환경별 Values 파일 사용 전략 (dev, staging, prod) Helm을 사용하면 다양한 환경에 따라 다른 설정을 적용할 수 있습니다. 일반적으로 dev, staging, prod 환경을 위한 별도의 values 파일을 준비하여 사용하는 전략이 유용합니다.\n📌 values-dev.yaml (개발 환경) replicaCount: 1 image: repository: nginx tag: dev 📌 values-prod.yaml (프로덕션 환경) replicaCount: 3 image: repository: nginx tag: stable 📌 Helm 명령어로 환경별 값 적용 helm install my-app my-chart -f values-dev.yaml helm install my-app my-chart -f values-prod.yaml 이 방식은 환경별로 다르게 설정된 값을 적용할 수 있어 매우 유용합니다.\n5 Secret 관리 및 보안 강화 (helm-secrets 활용) helm-secrets는 민감한 데이터를 안전하게 관리하기 위한 Helm 플러그인입니다. 예를 들어, values.yaml에 민감한 정보가 포함될 수 있는데, 이를 암호화하고 관리할 수 있습니다.\n📌 helm-secrets 설치 helm plugin install https://github.com/jkroepke/helm-secrets 📌 secrets.yaml 파일 예시 apiVersion: v1 kind: Secret metadata: name: db-credentials data: username: {{ .Values.db.username | b64enc | quote }} password: {{ .Values.db.password | b64enc | quote }} 📌 values-secrets.yaml 파일 예시 db: username: myuser password: mysecretpassword 📌 암호화된 values-secrets.yaml 파일 사용 helm secrets install my-app my-chart -f values-secrets.yaml helm-secrets를 사용하면 values-secrets.yaml을 암호화하여 배포할 수 있어 보안을 강화할 수 있습니다."},"title":"Helm Values와 오버라이딩 전략"},"/devops/helm/helm05/":{"data":{"":"","7-helm-repository-관리#7️⃣ \u003cstrong\u003eHelm Repository 관리\u003c/strong\u003e":"7️⃣ Helm Repository 관리 Helm에서는 차트를 관리하고 배포하기 위해 레포지토리를 사용합니다. 이 섹션에서는 Helm 레포지토리를 관리하는 방법과 다양한 레포지토리 구축 방법을 설명합니다.\n1 Helm 공식 레포지토리 (artifactHub) Helm은 기본적으로 여러 공식 차트 레포지토리를 지원합니다. 그 중 하나가 Artifact Hub입니다. Artifact Hub는 Helm 차트와 같은 오픈소스 소프트웨어를 공유하는 플랫폼입니다.\n📌 Artifact Hub 예시 URL: https://artifacthub.io/ 여기서 사용자는 다양한 Helm 차트를 검색하고 설치할 수 있습니다. 예를 들어, MySQL 차트를 검색하여 설치할 수 있습니다. 📌 Helm 명령어로 공식 레포지토리 추가 helm repo add stable https://charts.helm.sh/stable helm repo update 이 명령어는 stable이라는 이름의 공식 Helm 레포지토리를 추가하고 최신 차트 목록을 업데이트합니다.\n2 Private Helm 레포지토리 구축 조직 내에서 자체 Helm 차트를 관리하고 배포할 필요가 있을 때는 Private Helm 레포지토리를 구축할 수 있습니다. 이를 통해 보안이 중요한 차트를 안전하게 배포할 수 있습니다.\n📌 기본적인 Private 레포지토리 구축 GitHub, GitLab 등의 자체 Git 저장소를 사용하거나, 직접 서버를 구축하여 Helm 차트를 저장할 수 있습니다. 📌 Private Helm 레포지토리 추가 helm repo add my-private-repo https://my-repo-url.com helm repo update 이 명령어는 지정된 URL에 있는 Private Helm 레포지토리를 추가하고 업데이트합니다.\n3 helm repo index로 로컬 차트 저장소 생성 helm repo index 명령어는 로컬 디렉토리에 저장된 차트를 Helm 차트 레포지토리로 변환할 때 사용됩니다. 로컬 서버에서 Helm 차트를 쉽게 관리할 수 있습니다.\n📌 로컬 차트 저장소 생성 차트가 포함된 디렉토리에서 helm repo index 명령어를 실행합니다. helm repo index ./charts --url http://myserver.com/charts 이 명령어는 ./charts 디렉토리의 차트를 인덱싱하여 index.yaml 파일을 생성합니다. 이 파일은 차트의 메타데이터와 함께 레포지토리 URL을 제공합니다.\n📌 인덱싱된 차트 파일 예시 apiVersion: v1 entries: my-app: - apiVersion: v2 appVersion: 1.0.0 name: my-app version: 1.0.0 description: My custom app chart urls: - http://myserver.com/charts/my-app-1.0.0.tgz 4 S3, GCS, Harbor, Nexus 등을 활용한 Helm 레포지토리 구축 S3, GCS, Harbor, Nexus와 같은 저장소 서비스를 사용하여 Helm 차트 레포지토리를 구축할 수 있습니다. 이를 통해 클라우드 환경에서도 헬름 차트의 배포 및 관리를 쉽게 할 수 있습니다.\n📌 S3를 활용한 Helm 레포지토리 구축 AWS S3 버킷을 만들고, 차트 파일을 업로드합니다.\nS3 버킷을 Helm 레포지토리로 사용하기 위해 helm repo index 명령어를 사용합니다.\nhelm repo index ./charts --url https://mybucket.s3.amazonaws.com/charts S3 URL을 통해 차트에 접근할 수 있습니다. helm repo add s3-repo https://mybucket.s3.amazonaws.com/charts 📌 Nexus 활용 예시 Nexus는 차트 관리뿐만 아니라 보안과 버전 관리가 중요한 경우에 유용합니다.\nNexus에서 Helm 레포지토리를 생성하고 차트를 업로드합니다. Nexus UI에서 Helm 리포지토리를 관리할 수 있습니다. 5 차트 버전 관리 (helm package) helm package 명령어는 Helm 차트를 **압축 파일(.tgz)**로 패키징하여 배포하거나 레포지토리에 업로드할 수 있게 합니다.\n📌 Helm 차트 패키징 helm package ./my-chart 이 명령어는 my-chart/ 디렉토리의 차트를 .tgz 파일로 패키징합니다. 이 패키지 파일은 레포지토리나 다른 서버에 업로드할 수 있습니다.\n📌 패키지된 차트 예시 my-chart-0.1.0.tgz 패키지된 차트는 버전 관리가 가능하며, 각 버전은 chart-name-version.tgz 형식으로 저장됩니다."},"title":"Helm Repository 관리"},"/devops/helm/helm06/":{"data":{"":"","8-helm-chart-배포-전략#8️⃣ \u003cstrong\u003eHelm Chart 배포 전략\u003c/strong\u003e":"8️⃣ Helm Chart 배포 전략 Helm은 Kubernetes 애플리케이션 배포의 자동화와 관리의 편리함을 제공합니다. 이번 섹션에서는 Helm을 활용한 다양한 배포 전략을 소개합니다.\n1 Helm과 GitOps (ArgoCD, FluxCD 연동) GitOps는 Git을 단일 소스로 사용하여 애플리케이션을 관리하는 방법론입니다. Helm은 GitOps 툴인 ArgoCD나 FluxCD와 함께 사용되어 자동화된 배포와 버전 관리를 가능하게 합니다.\n📌 ArgoCD와 Helm 연동 예시 ArgoCD에 Helm 차트를 연결하여 자동화된 배포를 설정합니다.\nArgoCD 애플리케이션 설정에서 Helm 차트 URL을 지정하고, 원하는 값을 Git 저장소에 저장합니다.\nGit 저장소에 변경 사항을 푸시하면 ArgoCD가 이를 감지하고 자동으로 배포를 실행합니다.\n# ArgoCD Application 예시 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: my-app spec: destination: server: https://kubernetes.default.svc namespace: default source: repoURL: 'https://github.com/my-org/helm-charts.git' chart: my-app-chart targetRevision: HEAD 2 Helm Rollback (버전 롤백) Helm은 배포된 차트를 쉽게 롤백할 수 있는 기능을 제공합니다. Helm Rollback을 사용하면 실수로 잘못된 버전을 배포했을 때 빠르게 이전 버전으로 되돌릴 수 있습니다.\n📌 Helm Rollback 예시 설치된 차트 목록을 확인하여 Release 이름과 버전 번호를 찾습니다. helm list 롤백할 차트의 버전으로 돌아갑니다. helm rollback \u003crelease-name\u003e \u003crevision-number\u003e 예를 들어, my-release라는 차트의 버전 1로 롤백하려면 아래와 같이 입력합니다.\nhelm rollback my-release 1 이 명령어는 배포된 차트를 버전 1로 롤백합니다.\n3 Helm Hooks 활용 (배포 전후 스크립트 실행) Helm Hooks는 배포 과정에서 특정 시점에 스크립트를 실행할 수 있도록 해줍니다. 예를 들어, 차트 배포 전후에 데이터베이스 마이그레이션 또는 환경 설정을 자동으로 수행할 수 있습니다.\n📌 Helm Hook 예시 Helm에서 Hooks를 사용하려면, templates/ 디렉토리 내에서 hook을 설정할 수 있습니다.\n# hooks/deployment-hook.yaml apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment annotations: \"helm.sh/hook\": pre-install # 설치 전 후크 spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app image: my-image 위 예시에서는 pre-install 훅을 사용하여 Helm 차트를 설치하기 전에 특정 작업을 실행합니다.\n📌 자주 사용되는 Helm Hooks 유형 pre-install: 차트 설치 전 post-install: 차트 설치 후 pre-upgrade: 차트 업그레이드 전 post-upgrade: 차트 업그레이드 후 pre-delete: 차트 삭제 전 post-delete: 차트 삭제 후 4 Helm과 CI/CD 파이프라인 통합 (GitHub Actions, Jenkins, GitLab CI) Helm은 CI/CD 파이프라인에서 매우 유용하게 활용됩니다. GitHub Actions, Jenkins, GitLab CI와 같은 툴과 통합하여 자동화된 배포와 버전 관리를 구현할 수 있습니다.\n📌 GitHub Actions와 Helm 연동 예시 .github/workflows/helm-deploy.yml 파일을 생성합니다. name: Helm Deploy on: push: branches: - main jobs: deploy: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Set up Helm uses: azure/setup-helm@v1 - name: Helm install run: | helm repo add my-repo https://my-repo-url helm repo update helm upgrade --install my-release my-repo/my-chart 이 예시는 GitHub Actions에서 main 브랜치에 푸시될 때마다 Helm 차트를 자동으로 배포하는 CI/CD 파이프라인을 설정한 것입니다.\n📌 Jenkins와 Helm 연동 예시 Jenkins의 파이프라인 설정에서 Helm 명령어를 사용하여 배포합니다. pipeline { agent any stages { stage('Checkout') { steps { git 'https://github.com/my-org/helm-charts.git' } } stage('Helm Install') { steps { sh 'helm repo add my-repo https://my-repo-url' sh 'helm repo update' sh 'helm upgrade --install my-release my-repo/my-chart' } } } } 이 예시는 Jenkins에서 Helm 차트를 자동으로 설치하고 업그레이드하는 파이프라인을 설정한 것입니다."},"title":"Helm Chart 배포 전략"},"/devops/helm/helm07/":{"data":{"":"","9-helm과-kubernetes-연동-심화#9️⃣ \u003cstrong\u003eHelm과 Kubernetes 연동 심화\u003c/strong\u003e":"9️⃣ Helm과 Kubernetes 연동 심화 Helm은 Kubernetes 애플리케이션을 관리하는 강력한 도구로, Kubernetes의 다양한 리소스를 효율적으로 설정하고 배포할 수 있게 도와줍니다. 이번 섹션에서는 Helm과 Kubernetes 리소스를 심화적으로 연동하는 방법을 다룹니다.\n1 Helm과 ConfigMap, Secret 연동 Helm은 ConfigMap과 Secret 리소스를 통해 애플리케이션의 환경 설정 및 비밀 정보를 관리할 수 있습니다. Helm 차트를 작성할 때 이러한 리소스를 쉽게 정의하고, 필요에 따라 값을 설정하여 Kubernetes에서 활용할 수 있습니다.\n📌 Helm에서 ConfigMap 사용 예시 values.yaml 파일에서 값을 정의하고, ConfigMap 리소스를 Helm 템플릿에서 참조합니다.\nvalues.yaml\nconfig: app_mode: production log_level: debug configmap.yaml (Helm 템플릿)\napiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-config data: APP_MODE: {{ .Values.config.app_mode }} LOG_LEVEL: {{ .Values.config.log_level }} 위 예시에서는 ConfigMap 리소스를 정의하고 values.yaml에서 제공하는 값을 참조하여 환경 변수를 설정합니다.\n📌 Helm에서 Secret 사용 예시 values.yaml\nsecrets: db_password: supersecretpassword secret.yaml (Helm 템플릿)\napiVersion: v1 kind: Secret metadata: name: {{ .Release.Name }}-secret type: Opaque data: DB_PASSWORD: {{ .Values.secrets.db_password | b64enc }} b64enc 함수는 값을 Base64 인코딩하여 Secret으로 안전하게 저장할 수 있도록 합니다.\n2 Helm을 활용한 Ingress Controller 설정 Helm을 사용하여 Ingress Controller를 설치하고 구성하는 방법을 배워봅니다. Ingress는 Kubernetes 클러스터 내에서 외부 요청을 적절한 서비스로 라우팅하는 역할을 합니다.\n📌 Ingress Controller 설치 예시 Helm을 사용하여 Ingress Controller를 설치합니다. 예를 들어, NGINX Ingress Controller를 설치합니다. helm install nginx-ingress ingress-nginx/ingress-nginx --namespace kube-system Ingress 리소스를 사용하여 트래픽을 라우팅합니다. ingress.yaml (Helm 템플릿)\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: {{ .Release.Name }}-ingress namespace: {{ .Release.Namespace }} spec: rules: - host: {{ .Values.ingress.host }} http: paths: - path: / pathType: Prefix backend: service: name: {{ .Release.Name }}-service port: number: 80 values.yaml\ningress: host: my-app.example.com 위 예시에서는 Ingress Controller가 my-app.example.com 도메인의 트래픽을 해당 서비스로 라우팅하도록 설정하고 있습니다.\n3 Helm으로 StatefulSet 배포 (데이터베이스, Kafka 등) StatefulSet은 상태를 가진 애플리케이션을 Kubernetes에서 관리하는 리소스입니다. 데이터베이스나 Kafka와 같은 상태ful 애플리케이션을 배포할 때 주로 사용됩니다.\n📌 StatefulSet 배포 예시 statefulset.yaml (Helm 템플릿)\napiVersion: apps/v1 kind: StatefulSet metadata: name: {{ .Release.Name }}-statefulset spec: serviceName: {{ .Release.Name }}-headless replicas: 3 selector: matchLabels: app: {{ .Release.Name }} template: metadata: labels: app: {{ .Release.Name }} spec: containers: - name: {{ .Release.Name }}-container image: {{ .Values.image.repository }}:{{ .Values.image.tag }} ports: - containerPort: 8080 volumeMounts: - name: data mountPath: /data volumeClaimTemplates: - metadata: name: data spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 1Gi values.yaml\nimage: repository: my-db-image tag: latest 위 예시는 StatefulSet을 사용하여 데이터베이스와 같은 상태ful 애플리케이션을 배포하는 방법을 보여줍니다. volumeClaimTemplates를 통해 Persistent Volume을 사용할 수 있습니다.\n4 Helm과 Persistent Volume (PVC) 사용법 **Persistent Volume (PVC)**은 Kubernetes에서 데이터 저장소를 관리하는 리소스로, 데이터를 지속적으로 저장하고 관리할 수 있습니다. Helm을 통해 PVC를 관리하여 애플리케이션에 필요한 저장소를 제공합니다.\n📌 Persistent Volume Claim (PVC) 사용 예시 pvc.yaml (Helm 템플릿)\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: {{ .Release.Name }}-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi 위 예시는 **Persistent Volume Claim (PVC)**을 정의하여 애플리케이션에 5Gi의 저장소를 요청하는 방법을 보여줍니다.\n📌 PVC와 StatefulSet 연동 예시 statefulset.yaml (Helm 템플릿)\nvolumeClaimTemplates: - metadata: name: data spec: accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 5Gi 이 예시에서는 StatefulSet 내에서 PVC를 사용하여 애플리케이션의 데이터 저장소를 관리합니다."},"title":"Helm과 Kubernetes 연동 심화"},"/devops/helm/helm08/":{"data":{"":"","-helm-고급-기능-및-베스트-프랙티스#🔟 \u003cstrong\u003eHelm 고급 기능 및 베스트 프랙티스\u003c/strong\u003e":"🔟 Helm 고급 기능 및 베스트 프랙티스 Helm은 Kubernetes에서 애플리케이션을 관리하는 중요한 도구로, 고급 기능과 베스트 프랙티스를 활용하면 더 효율적이고 안전한 배포가 가능합니다. 이 섹션에서는 Helm Chart 버전 관리, Service Mesh 연동, 보안 모범 사례 및 Operator 패턴과 같은 고급 기능을 다룹니다.\n1 Helm Chart 버전 관리 (appVersion, version 개념) Helm Chart의 버전 관리는 애플리케이션 버전과 차트 버전을 관리하는 중요한 개념입니다. version은 Helm 차트 자체의 버전을 의미하고, appVersion은 실제 애플리케이션의 버전을 나타냅니다. 이를 통해 Helm 차트를 업데이트하거나 롤백할 때 중요한 기준을 마련할 수 있습니다.\n📌 version vs appVersion version: Helm 차트의 버전 (Helm Chart 자체의 버전) appVersion: 배포할 애플리케이션의 버전 Chart.yaml 예시\napiVersion: v2 name: myapp description: A Helm chart for Kubernetes version: 1.0.0 # Helm Chart 버전 appVersion: 2.3.0 # 애플리케이션 버전 이 예시에서 version은 Helm 차트의 버전이고, appVersion은 실제 애플리케이션의 버전입니다. appVersion은 차트가 어떤 애플리케이션을 배포하는지 나타냅니다.\n2 Helm과 Service Mesh (Istio, Linkerd) 연동 Service Mesh는 마이크로서비스 간의 통신을 관리하는 데 사용됩니다. Helm을 활용하여 Kubernetes 클러스터에 Istio나 Linkerd와 같은 Service Mesh를 배포할 수 있습니다. Helm 차트는 이러한 복잡한 네트워크 구성을 쉽게 배포하고 관리할 수 있게 해줍니다.\n📌 Istio 설치 예시 Istio를 Helm을 사용하여 설치하는 방법을 알아봅니다.\nhelm repo add istio https://istio-release.storage.googleapis.com/charts helm repo update helm install istio-base istio/base --namespace istio-system helm install istiod istio/istiod --namespace istio-system 위 명령은 Istio의 기본 차트를 설치하고, Istio가 서비스 간 트래픽을 관리하도록 설정합니다.\n📌 Istio와 연동된 애플리케이션 배포 values.yaml\nglobal: istio: enabled: true Helm 차트를 사용하여 Istio와 통합된 애플리케이션을 배포하려면, 위와 같은 값을 values.yaml에 설정하여 Istio와 연동할 수 있습니다.\n3 Helm Chart 보안 모범 사례 Helm을 사용할 때 보안은 매우 중요한 요소입니다. 차트 작성 시 보안 모범 사례를 따르는 것이 중요합니다.\n📌 1. 민감한 데이터 관리 (Secrets) 애플리케이션 설정 중 비밀번호, API 키 등 민감한 정보를 Helm 차트에서 처리할 때는 Secret 리소스를 활용하여 보안적으로 안전하게 처리합니다.\napiVersion: v1 kind: Secret metadata: name: myapp-secret type: Opaque data: DB_PASSWORD: {{ .Values.secrets.dbPassword | b64enc }} 📌 2. 최소 권한 원칙 Kubernetes 리소스를 정의할 때는 최소 권한 원칙을 적용하여 필요한 최소한의 권한만을 할당합니다. 예를 들어, Role 및 RoleBinding을 사용하여 각 리소스에 대해 필요한 권한만 부여합니다.\napiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: read-only rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"list\"] 📌 3. 차트 저장소 보안 Helm 차트를 외부 저장소에서 다운로드할 때는 HTTPS를 사용하여 통신을 암호화하고, 신뢰할 수 있는 차트 저장소만 사용합니다.\n4 Helm과 Operator 패턴 연계 활용 Operator 패턴은 Kubernetes에서 애플리케이션의 라이프사이클을 관리하는 방법입니다. Helm과 Operator를 결합하여 애플리케이션 배포 및 운영을 자동화할 수 있습니다.\n📌 Operator 패턴 개요 Operator는 Kubernetes API와 통합되어 리소스를 자동으로 관리하는 컨트롤러입니다. 예를 들어, MySQL Operator는 MySQL 인스턴스를 자동으로 설치하고 관리합니다.\n📌 Helm과 Operator 연동 예시 Operator 패턴을 사용하여 MySQL을 자동으로 배포하고 관리하는 Helm 차트를 작성합니다.\nhelm install mysql-operator mysql-operator/mysql-operator --namespace operator 이 명령어는 MySQL Operator를 설치하고, Kubernetes에서 MySQL 데이터베이스의 배포, 업그레이드, 백업 등을 자동으로 관리하도록 설정합니다.\nvalues.yaml\noperator: enabled: true mysql: replicas: 3 storage: 10Gi 위 예시는 MySQL Operator를 사용하여 3개의 MySQL 인스턴스를 관리하는 Helm 차트를 설정하는 방법을 보여줍니다."},"title":"Helm 고급 기능 및 베스트 프랙티스"},"/devops/jenkins/":{"data":{"":"1️⃣ Jenkins 개요 및 기본 개념 2️⃣ Jenkins 설치 및 환경 구축 3️⃣ Jenkins 아키텍처 및 핵심 개념 4️⃣ Jenkins 기본 사용법 5️⃣ Jenkins Pipeline (선언형 vs 스크립트형) 7️⃣ Jenkins와 Docker/Kubernetes 연동 8️⃣ Jenkins Job 및 Build 단계 상세 설정 9️⃣ Jenkins와 CI/CD 구축 실습 🔟 Jenkins 플러그인 활용 및 고급 설정 1️⃣1️⃣ Jenkins 모니터링 및 성능 최적화 1️⃣2️⃣ Jenkins 보안 및 권한 관리 1️⃣3️⃣ Jenkins 장애 대응 및 트러블슈팅 1️⃣4️⃣ Jenkins 최신 트렌드 및 Best Practices\nRSS Feed "},"title":"Jenkins"},"/devops/jenkins/jenkins01/":{"data":{"":"","1-jenkins-개요-및-기본-개념#1️⃣ \u003cstrong\u003eJenkins 개요 및 기본 개념\u003c/strong\u003e":"1️⃣ Jenkins 개요 및 기본 개념 1. Jenkins란 무엇인가? (역사, 배경, 발전 과정) Jenkins는 2004년에 처음 출시된 오픈소스 자동화 서버로, 소프트웨어 개발의 CI/CD(지속적 통합/지속적 배포) 과정을 자동화하는 데 사용됩니다. Jenkins는 원래 “Hudson\"이라는 이름으로 시작되었으며, 오라클과의 갈등으로 인해 2011년에 “Jenkins\"로 이름이 변경되었습니다. 현재는 수많은 플러그인과 함께 다양한 개발 언어 및 플랫폼에서 사용할 수 있는 강력한 툴로 발전하였습니다.\n그림: Jenkins 로고 및 Hudson과의 역사적인 변천 2. CI/CD 개념 및 Jenkins의 역할 CI(지속적 통합): 개발자가 작성한 코드를 주기적으로 공유된 리포지토리에 통합하여, 새로운 코드 변경 사항을 빨리 테스트하고 배포할 수 있도록 합니다. CD(지속적 배포): 자동화된 빌드, 테스트, 배포 과정을 통해 소프트웨어를 빠르고 안전하게 배포합니다. Jenkins는 이러한 CI/CD 과정을 자동화하여 개발 및 배포 속도를 향상시키는 데 중요한 역할을 합니다.\n그림: CI/CD 파이프라인 예시 3. Jenkins의 주요 기능 및 특징 자동화: 반복적인 작업(빌드, 테스트, 배포 등)을 자동화하여 개발자의 시간을 절약합니다. 플러그인 시스템: Jenkins는 수천 개의 플러그인을 제공하여 다양한 툴과 통합할 수 있습니다. 다양한 빌드 도구 지원: Maven, Gradle, Ant 등 다양한 빌드 도구를 지원합니다. 분산 빌드: 여러 서버에서 빌드를 분산시켜 병렬 처리할 수 있습니다. 4. Jenkins의 주요 사용 사례 (CI/CD, 인프라 자동화, 빌드 및 테스트) CI/CD: Jenkins는 빌드, 테스트, 배포를 자동화하여 지속적인 통합 및 배포를 실현합니다. 인프라 자동화: Jenkins를 사용하여 인프라를 코드로 관리하고, 서버 배포 및 구성 자동화를 지원합니다. 빌드 및 테스트: Jenkins는 코드를 빌드하고, 테스트하여 품질을 유지하며, 배포 준비가 완료되었는지 확인합니다. 예시: Jenkins 파이프라인을 사용한 간단한 빌드 및 테스트 프로세스 5. 기존 CI/CD 툴 (GitHub Actions, GitLab CI, ArgoCD 등)과 비교 툴 특징 장점 단점 Jenkins 오픈소스 CI/CD 도구로, 다양한 플러그인과 커스터마이징 가능 유연성, 확장성, 커스터마이징 가능 복잡한 설정, 초기 설정에 시간 소모 GitHub Actions GitHub 리포지토리와 통합된 CI/CD 툴 간편한 설정, GitHub과 통합 기능이 제한적일 수 있음 GitLab CI GitLab에서 제공하는 CI/CD 툴, GitLab 리포지토리와 통합 GitLab과의 밀접한 통합 GitLab만 지원 ArgoCD Kubernetes 환경을 위한 GitOps 방식의 배포 자동화 도구 Kubernetes 환경에 최적화 CI 기능 부재 6. Jenkins의 전체적인 아키텍처 및 구성 요소 Jenkins는 Master와 Agent로 구성된 아키텍처를 가집니다.\nJenkins Master: 빌드 파이프라인을 관리하고, 빌드 작업을 분배하는 역할을 합니다. Jenkins Agent: 빌드를 실제로 수행하는 서버로, 여러 개의 에이전트를 추가하여 분산 빌드를 할 수 있습니다. 그림: Jenkins Master-Agent 아키텍처 이렇게 Jenkins는 유연하고 강력한 CI/CD 자동화 툴로, 다양한 환경에서 사용될 수 있습니다."},"title":"Jenkins 개요 및 기본 개념"},"/devops/jenkins/jenkins02/":{"data":{"":"","2-jenkins-설치-및-환경-구축#2️⃣ \u003cstrong\u003eJenkins 설치 및 환경 구축\u003c/strong\u003e":"2️⃣ Jenkins 설치 및 환경 구축 1. Jenkins 설치 방법 비교 (Docker, Bare Metal, Kubernetes) Jenkins는 다양한 환경에서 설치할 수 있습니다. 각 방법에 따라 설치 절차와 장단점이 다릅니다.\nDocker: Docker를 사용하면 Jenkins 설치가 간편하고, 컨테이너화된 환경에서 실행됩니다. 이 방법은 빠르게 Jenkins를 실행하고, 환경을 격리할 수 있는 장점이 있습니다.\n예시: Docker를 이용한 Jenkins 설치 명령 docker pull jenkins/jenkins:lts docker run -d -p 8080:8080 -p 50000:50000 jenkins/jenkins:lts Bare Metal: 물리적 서버에 Jenkins를 직접 설치하는 방법입니다. 이 방법은 안정적이고 성능이 중요할 때 적합하지만, 설정과 유지 관리가 상대적으로 복잡합니다.\n예시: Ubuntu에서 Jenkins 설치 sudo apt update sudo apt install openjdk-11-jdk wget -q -O - https://pkg.jenkins.io/keys/jenkins.io.key | sudo apt-key add - sudo sh -c 'echo deb http://pkg.jenkins.io/debian/ stable main \u003e /etc/apt/sources.list.d/jenkins.list' sudo apt update sudo apt install jenkins Kubernetes: Jenkins를 Kubernetes 클러스터에 배포하여 확장성과 가용성을 높일 수 있습니다. 여러 파드에서 Jenkins를 실행할 수 있으며, Jenkins를 Kubernetes 클러스터에 자동으로 배포할 수 있는 Helm 차트를 제공하는 경우도 많습니다.\n예시: Kubernetes에서 Jenkins 설치 helm install jenkins stable/jenkins 2. Jenkins 설치 전 사전 요구사항 (Java 버전, 권장 하드웨어 사양) Jenkins는 Java 8 이상 버전에서 실행됩니다. 따라서 Java가 설치되어 있어야 하며, Jenkins를 실행하기 전에 Java를 설치해야 합니다.\nJava 설치 예시 (Ubuntu) sudo apt update sudo apt install openjdk-11-jdk Jenkins 권장 하드웨어 사양: CPU: 2개 이상의 코어 메모리: 최소 4GB RAM 디스크 공간: 최소 10GB 이상 3. Jenkins LTS vs Weekly Release 차이 LTS (Long Term Support): LTS 버전은 안정성에 중점을 두고, 중요한 보안 업데이트와 버그 수정만 제공됩니다. 장기적으로 안정적인 환경을 원하는 경우 사용합니다.\nWeekly Release: 매주 새로운 기능과 업데이트가 포함된 버전으로, 최신 기능을 빨리 사용하고 싶은 개발자에게 적합합니다. 하지만 안정성에 있어 LTS 버전보다 낮을 수 있습니다.\n차이점 요약: 버전 특징 추천 대상 LTS 안정성, 보안 패치 중심 장기적으로 안정적인 운영 필요 Weekly 최신 기능, 빠른 업데이트, 새로운 기능 제공 최신 기능을 자주 사용해야 할 경우 4. Jenkins를 Linux/Mac/Windows에서 설치하기 Linux: 대부분의 Linux 배포판에서 패키지 관리자를 통해 설치할 수 있습니다.\n예시: CentOS에서 Jenkins 설치 sudo yum install java-11-openjdk sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat/jenkins.repo sudo rpm --import https://pkg.jenkins.io/redhat/jenkins.io.key sudo yum install jenkins sudo systemctl start jenkins Mac: Homebrew를 사용하여 간편하게 설치할 수 있습니다.\n예시: Mac에서 Jenkins 설치 brew install jenkins-lts Windows: Windows용 MSI 설치 파일을 다운로드하여 GUI 방식으로 설치할 수 있습니다.\n예시: Windows에서 Jenkins 설치 Jenkins 다운로드: Jenkins 공식 사이트 MSI 파일을 실행하고 설치 진행 5. Jenkins를 Docker 및 Kubernetes에서 실행하기 Docker: 위에서 설명한 것처럼 Docker를 사용하여 Jenkins를 컨테이너화하여 실행할 수 있습니다.\n예시: Docker Compose를 사용한 Jenkins 실행 version: '3' services: jenkins: image: jenkins/jenkins:lts ports: - \"8080:8080\" - \"50000:50000\" volumes: - jenkins_home:/var/jenkins_home volumes: jenkins_home: Kubernetes: Helm 차트를 사용하여 Kubernetes 클러스터에서 Jenkins를 설치할 수 있습니다.\n예시: Helm을 사용한 Jenkins 설치 helm install jenkins stable/jenkins 6. Jenkins의 기본 설정 및 관리자 계정 생성 Jenkins를 처음 실행하면 초기 설정 화면이 표시됩니다. 이때 관리자 계정을 생성하고 기본 설정을 진행할 수 있습니다.\n초기 설정 예시: Jenkins 웹 UI에 접속 (기본 포트: 8080) Unlock Jenkins 화면에서 초기 비밀번호를 입력 (파일 경로: /var/lib/jenkins/secrets/initialAdminPassword) 관리자 계정을 설정하고, 필요한 플러그인 설치 7. Jenkins UI 개요 및 주요 설정 옵션 Jenkins의 UI는 여러 섹션으로 나누어져 있으며, 주요 기능을 빠르게 접근할 수 있습니다.\nDashboard: Jenkins의 주요 작업을 관리하는 홈 화면 New Item: 새로운 작업(Job)을 생성하는 옵션 Manage Jenkins: Jenkins의 전반적인 설정을 관리하는 메뉴 Manage Plugins: 플러그인을 설치 및 관리하는 메뉴 그림: Jenkins UI 화면 Jenkins UI는 사용자 친화적이며, 기본적인 빌드, 배포, 테스트 작업을 쉽게 설정하고 관리할 수 있습니다."},"title":"Jenkins 설치 및 환경 구축"},"/devops/jenkins/jenkins03/":{"data":{"":"","3-jenkins-아키텍처-및-핵심-개념#3️⃣ \u003cstrong\u003eJenkins 아키텍처 및 핵심 개념\u003c/strong\u003e":"3️⃣ Jenkins 아키텍처 및 핵심 개념 1. Jenkins Master-Slave 아키텍처 Jenkins는 기본적으로 Master-Slave 구조로 설계되어 있으며, 이를 통해 분산 빌드를 지원합니다.\nMaster: Jenkins 서버의 중앙 관리자로, 파이프라인을 관리하고, 사용자 인터페이스(UI)를 제공합니다. Master는 주로 빌드 실행을 제어하고 관리하지만 실제 빌드를 수행하지 않습니다. Slave (Agent): 실제 빌드와 실행을 수행하는 서버로, Master에서 분배된 작업을 처리합니다. 여러 개의 Slave를 연결하여 빌드를 분산할 수 있습니다. 그림: Jenkins Master-Slave 아키텍처 2. Jenkins의 주요 컴포넌트 Jenkins는 여러 중요한 컴포넌트로 구성되어 있으며, 각 컴포넌트는 다양한 기능을 담당합니다.\nController (Master): Jenkins의 핵심 서버로, 사용자가 인터페이스를 통해 작업을 관리할 수 있도록 합니다. Controller는 빌드 작업을 할당하고, 플러그인 및 설정을 관리합니다.\nAgent (Worker/Node): 빌드 작업을 실제로 실행하는 서버입니다. 여러 개의 Agent를 설정하여 분산 빌드를 수행할 수 있습니다.\nJob: Jenkins에서 실행할 작업의 단위로, 빌드, 테스트, 배포 등의 작업을 포함할 수 있습니다.\nPipeline: 여러 Job을 정의하여 하나의 흐름으로 연결된 작업을 수행할 수 있는 스크립트입니다.\nBuild: Job이 실행될 때마다 생성되는 실행 단위입니다.\n3. Controller (Master): Job 관리 및 UI 제공 Jenkins의 Controller는 빌드 작업의 관리 및 분배를 담당하는 중심 서버입니다. 사용자는 이곳에서 빌드를 설정하고 관리하며, Jenkins의 UI를 통해 다양한 작업을 수행할 수 있습니다.\nJob 관리: Controller에서 새로운 Job을 생성하고, 기존 Job을 수정하거나 삭제할 수 있습니다. UI 제공: 사용자에게 직관적인 UI를 제공하여, Job 실행 상태나 빌드 결과를 확인할 수 있습니다. 4. Agent (Worker/Node): 실제 빌드 및 실행 수행 Jenkins의 Agent는 Master로부터 할당된 작업을 실제로 수행하는 서버입니다. 여러 개의 Agent를 설정하여 작업을 분산시킬 수 있으며, 다양한 환경에서 빌드를 병렬로 실행할 수 있습니다.\n에이전트 설정: Jenkins의 Master는 각 Agent에 작업을 분배하며, 각 Agent는 필요한 환경에서 빌드를 수행합니다. 빌드 분산: 여러 Agent를 통해 빌드를 분산시켜 빠르고 효율적으로 실행할 수 있습니다. 5. Pipeline, Job, Build의 개념 Pipeline: Jenkins의 Pipeline은 복잡한 빌드 및 배포 작업을 자동화하는 스크립트입니다. 여러 Job을 하나의 흐름으로 연결하여 실행할 수 있습니다.\n예시: 간단한 Declarative Pipeline pipeline { agent any stages { stage('Build') { steps { echo 'Building...' } } stage('Test') { steps { echo 'Testing...' } } } } Job: Jenkins에서 수행할 작업의 단위입니다. Job은 하나의 빌드 작업으로, 특정 작업을 수행하기 위한 설정을 포함합니다.\nBuild: Job이 실행될 때마다 생성되는 실행 단위로, 각 Build는 고유한 빌드 번호를 가집니다.\n6. Jenkins의 주요 디렉토리 구조 (/var/lib/jenkins, JENKINS_HOME) /var/lib/jenkins: Jenkins의 기본 설치 경로로, Jenkins의 설정 파일 및 데이터가 저장됩니다.\nJENKINS_HOME: Jenkins의 홈 디렉토리로, 모든 Jenkins 설정과 플러그인, 빌드 기록 등이 이 경로에 저장됩니다. 이 디렉토리는 Jenkins 서버의 상태를 복원하거나 마이그레이션하는 데 사용됩니다.\n그림: Jenkins 디렉토리 구조 7. Jenkins 환경 변수 (BUILD_ID, WORKSPACE, JENKINS_URL) Jenkins는 빌드와 실행 환경에 대한 다양한 환경 변수를 제공합니다. 이들 환경 변수는 Jenkins 내에서 자동으로 설정되며, 스크립트에서 참조하여 사용할 수 있습니다.\nBUILD_ID: 현재 빌드의 고유 ID로, Jenkins의 빌드 번호를 나타냅니다. WORKSPACE: 현재 실행 중인 Job의 작업 디렉토리 경로입니다. JENKINS_URL: Jenkins 서버의 URL입니다. 예를 들어, http://localhost:8080. 예시: 환경 변수 사용 pipeline { agent any stages { stage('Info') { steps { echo \"Build ID: ${env.BUILD_ID}\" echo \"Workspace: ${env.WORKSPACE}\" echo \"Jenkins URL: ${env.JENKINS_URL}\" } } } } Jenkins의 아키텍처와 핵심 개념을 이해하면, Jenkins의 효율적인 사용과 구성이 가능해집니다."},"title":"Jenkins 아키텍처 및 핵심 개념"},"/devops/jenkins/jenkins04/":{"data":{"":"","4-jenkins-기본-사용법#4️⃣ \u003cstrong\u003eJenkins 기본 사용법\u003c/strong\u003e":"4️⃣ Jenkins 기본 사용법 1. Jenkins 첫 실행 및 초기 설정 마법사 Jenkins를 처음 실행하면, 초기 설정 마법사가 나타나게 됩니다. 이 과정에서는 기본적인 설정을 완료하고, Jenkins가 정상적으로 실행되도록 준비할 수 있습니다.\nJenkins를 처음 실행하면, 브라우저에서 http://localhost:8080을 열면 Jenkins의 첫 페이지에 접근할 수 있습니다. 첫 페이지에서 제공된 unlock 키를 입력합니다. 이 키는 Jenkins가 설치된 서버의 /var/lib/jenkins/secrets/initialAdminPassword 파일에서 찾을 수 있습니다. 관리자의 초기 계정을 설정하고, Jenkins의 초기 설정을 완료합니다. 그림: Jenkins 첫 실행 화면 2. 플러그인 설치 및 관리 (Manage Plugins) Jenkins는 다양한 플러그인을 통해 기능을 확장할 수 있습니다. 플러그인을 설치하는 방법은 매우 간단합니다.\nJenkins 대시보드에서 Manage Jenkins \u003e Manage Plugins를 클릭합니다. 플러그인 관리 페이지에서 Available 탭을 통해 필요한 플러그인을 검색하고 설치할 수 있습니다. 설치가 완료되면 Jenkins를 재시작하여 플러그인이 활성화됩니다. 그림: 플러그인 설치 화면 3. Jenkins Job 유형 및 차이점 Jenkins는 다양한 종류의 Job을 지원합니다. 각 Job 유형은 특정한 목적에 맞게 설계되어 있으며, 사용자의 요구에 맞춰 선택할 수 있습니다.\n1. Freestyle Project Freestyle Project는 가장 기본적인 Jenkins Job입니다. 기본적인 빌드, 테스트, 배포 등을 설정할 수 있습니다. 사용자가 UI에서 쉽게 설정할 수 있으며, 간단한 프로젝트에 적합합니다.\n설정 방법: Jenkins 대시보드에서 새로운 Item을 클릭하고, Freestyle Project를 선택하여 Job을 생성합니다. 2. Pipeline Pipeline은 여러 작업을 하나의 스크립트로 정의하고, 코드로 관리할 수 있는 고급 Job 유형입니다. 복잡한 빌드 및 배포 흐름을 자동화하는 데 유용합니다.\n설정 방법: 새로운 Item에서 Pipeline을 선택하고, 필요한 Jenkinsfile을 작성하여 설정합니다. 3. Multi-branch Pipeline Multi-branch Pipeline은 Git 저장소 내의 여러 브랜치를 자동으로 인식하고, 각 브랜치마다 별도의 파이프라인을 실행합니다. 여러 브랜치에서 동시에 작업을 관리할 수 있습니다.\n설정 방법: Multi-branch Pipeline을 선택하고, Git 저장소를 연결한 후 Jenkins가 각 브랜치에서 파이프라인을 자동으로 생성하도록 합니다. 4. Multijob Project Multijob Project는 여러 개의 Job을 묶어서 실행하는 프로젝트 유형입니다. 각 Job은 독립적으로 실행되며, 서로 의존할 수 있습니다.\n4. 첫 번째 Jenkins Job 생성 및 실행 Jenkins 대시보드에서 새로운 Item을 클릭합니다. Freestyle Project를 선택하고, 프로젝트 이름을 입력합니다. 빌드 설정을 입력합니다. 예를 들어, Shell 또는 Batch 명령어를 사용하여 빌드를 실행할 수 있습니다. 설정이 완료되면 Save를 클릭하고, Build Now를 클릭하여 빌드를 실행합니다. 예시: Freestyle Project에서 빌드 명령어 설정 echo \"Hello, Jenkins!\" 그림: Jenkins Job 생성 및 실행 5. Build Trigger 설정 (수동, Poll SCM, Webhook, Timer) Jenkins에서는 다양한 방법으로 빌드가 트리거될 수 있습니다.\n수동 트리거: 사용자가 직접 Build Now 버튼을 클릭하여 빌드를 실행합니다. Poll SCM: Git과 같은 소스 코드 관리 시스템을 주기적으로 폴링하여 변경 사항이 있으면 자동으로 빌드를 실행합니다. Webhook: GitHub과 같은 외부 시스템에서 발생한 이벤트(예: Push 이벤트)에 의해 빌드를 자동으로 트리거할 수 있습니다. Timer: 일정한 시간 간격으로 자동으로 빌드를 실행할 수 있습니다. 예시: GitHub Webhook 설정 Jenkins에서 Webhook을 설정하려면 GitHub 저장소에서 Jenkins URL로 POST 요청을 보내도록 설정합니다. Jenkins에서 Build Triggers 항목에서 GitHub hook trigger for GITScm polling을 활성화합니다. 6. Build History 및 Console Output 분석 Jenkins는 각 빌드에 대한 기록을 저장합니다. 이를 통해 이전 빌드의 상태와 로그를 쉽게 확인할 수 있습니다.\nBuild History: Jenkins 대시보드에서 각 Job을 선택하면, 오른쪽에 빌드 기록이 표시됩니다. 각 빌드는 고유한 번호와 함께 상태를 표시합니다. Console Output: 빌드가 완료된 후, Build Now에서 실행된 Job을 클릭하여 Console Output을 확인할 수 있습니다. 이곳에서는 빌드 과정에서 발생한 오류나 로그를 확인할 수 있습니다. 그림: Build History 및 Console Output Jenkins에서 기본적인 Job을 설정하고 빌드를 트리거하는 방법을 익히는 것은 CI/CD 자동화의 첫걸음입니다. 다양한 트리거와 설정을 활용하여 효율적인 빌드를 자동화할 수 있습니다."},"title":"Jenkins 기본 사용법"},"/devops/jenkins/jenkins05/":{"data":{"":"","5-jenkins-pipeline-선언형-vs-스크립트형#5️⃣ \u003cstrong\u003eJenkins Pipeline (선언형 vs 스크립트형)\u003c/strong\u003e":"5️⃣ Jenkins Pipeline (선언형 vs 스크립트형) 1. Jenkins Pipeline 개념 및 필요성 Jenkins Pipeline은 Jenkins에서 CI/CD 프로세스를 자동화하기 위한 강력한 방법입니다. 파이프라인은 소스 코드의 빌드, 테스트, 배포 과정 등 여러 단계를 정의하고 자동으로 실행할 수 있게 해줍니다.\nCI/CD 자동화: 소프트웨어 개발 라이프사이클을 자동화하여 개발자가 수동으로 작업할 필요를 줄여줍니다. 일관된 빌드 프로세스: 파이프라인을 코드로 관리하기 때문에 빌드 과정이 일관되고 반복 가능합니다. 버전 관리: Jenkinsfile을 코드로 저장하고 버전 관리 시스템에 포함시킬 수 있어 파이프라인의 변경 이력을 관리할 수 있습니다. 그림: Jenkins Pipeline 흐름 2. Declarative Pipeline vs Scripted Pipeline Jenkins에서는 두 가지 종류의 파이프라인 구문을 제공합니다: 선언형(Declarative) 파이프라인과 스크립트형(Scripted) 파이프라인입니다.\n1. Declarative Pipeline 선언형 파이프라인은 더 간결하고 읽기 쉬운 형식으로, 구조화된 구문을 사용하여 파이프라인을 정의합니다. Jenkinsfile을 사용하여 파이프라인을 선언할 때 주로 사용됩니다. 선언형 파이프라인은 전체 파이프라인을 특정 블록 안에 작성하여 가독성을 높이고, 잘못된 구문을 방지합니다.\n주요 특징: 명확한 구조, 간결한 문법, 기본값 제공 구조: pipeline { ... }로 감싸서 정의 stages, steps, agent 등을 포함한 명확한 블록 구조 2. Scripted Pipeline 스크립트형 파이프라인은 더 유연하고 복잡한 동작을 정의할 수 있지만, 선언형 파이프라인보다는 상대적으로 복잡하고 가독성이 떨어질 수 있습니다. Groovy 스크립트를 기반으로 하며, 프로그래밍적인 자유도가 높습니다.\n주요 특징: 더 많은 유연성, 고급 스크립트 기능 구조: node { ... } 블록 안에 파이프라인 명령어를 작성 3. Jenkinsfile 기본 구조 Jenkinsfile은 Jenkins 파이프라인을 정의하는 파일로, 파이프라인의 모든 설정과 단계를 이 파일 안에서 정의합니다. Jenkinsfile은 보통 프로젝트 루트에 위치하며, Git과 같은 버전 관리 시스템에 저장됩니다.\n기본 구조 (선언형 파이프라인) pipeline { agent any stages { stage('Build') { steps { echo 'Building...' } } stage('Test') { steps { echo 'Testing...' } } stage('Deploy') { steps { echo 'Deploying...' } } } } 기본 구조 (스크립트형 파이프라인) node { stage('Build') { echo 'Building...' } stage('Test') { echo 'Testing...' } stage('Deploy') { echo 'Deploying...' } } 4. Declarative Pipeline 예제 및 실습 예제: 기본적인 Declarative Pipeline pipeline { agent any stages { stage('Checkout') { steps { checkout scm } } stage('Build') { steps { sh './build.sh' } } stage('Test') { steps { sh './test.sh' } } stage('Deploy') { steps { sh './deploy.sh' } } } } 이 예제에서는 Checkout, Build, Test, Deploy라는 4단계를 포함하는 간단한 파이프라인을 정의하고 있습니다. 각 단계에서는 셸 명령어를 실행합니다. 5. Scripted Pipeline 예제 및 실습 예제: Scripted Pipeline 사용 node { stage('Checkout') { checkout scm } stage('Build') { sh './build.sh' } stage('Test') { sh './test.sh' } stage('Deploy') { sh './deploy.sh' } } 이 예제는 스크립트형 파이프라인을 사용하여 동일한 빌드, 테스트, 배포 과정을 정의한 것입니다. 각 단계는 node 블록 내에서 stage로 구분되어 있습니다. 6. Pipeline 단계 (stages, steps, agent, post, environment) 1. stages stages는 파이프라인에서 실행될 여러 단계를 정의합니다. 각 단계는 stage로 구분되며, 순차적으로 실행됩니다.\n2. steps steps는 각 stage에서 수행할 실제 작업을 정의합니다. 예를 들어, 빌드, 테스트, 배포 명령어 등을 여기서 지정합니다.\n3. agent agent는 파이프라인 또는 특정 stage가 실행될 에이전트를 정의합니다. 예를 들어, any는 모든 에이전트에서 실행을 의미합니다.\n4. post post 블록은 파이프라인 실행 후에 수행할 작업을 정의합니다. 예를 들어, 빌드가 성공적일 때나 실패할 때의 후속 작업을 설정할 수 있습니다.\n5. environment environment는 파이프라인 전체에서 사용될 환경 변수를 설정하는 데 사용됩니다.\n7. Parallel Pipeline 및 Multistage Pipeline 1. Parallel Pipeline 병렬 파이프라인은 여러 작업을 동시에 실행할 수 있도록 정의합니다. 이를 통해 빌드 시간을 단축할 수 있습니다.\npipeline { agent any stages { stage('Build') { parallel { stage('Frontend') { steps { sh './build_frontend.sh' } } stage('Backend') { steps { sh './build_backend.sh' } } } } } } 2. Multistage Pipeline 멀티스테이지 파이프라인은 여러 단계를 정의하여 복잡한 빌드 프로세스를 여러 단계로 나누어 관리할 수 있도록 합니다.\npipeline { agent any stages { stage('Build') { steps { sh './build.sh' } } stage('Test') { steps { sh './test.sh' } } stage('Deploy') { steps { sh './deploy.sh' } } } } 8. Jenkins Shared Library 활용 Jenkins Shared Library는 여러 파이프라인에서 공통으로 사용할 수 있는 스크립트 및 함수들을 관리하는 방법입니다. 이를 통해 코드 중복을 줄이고 파이프라인의 재사용성을 높일 수 있습니다.\n예시: vars 디렉터리 내에 공통적으로 사용할 함수를 정의하고, Jenkinsfile에서 @Library 어노테이션을 사용하여 호출합니다. 9. Pipeline 성능 최적화 (parallel, timeout, retry) 1. parallel 병렬 작업을 사용하여 여러 단계를 동시에 실행하여 빌드 시간을 단축할 수 있습니다.\n2. timeout 작업이 특정 시간 내에 완료되지 않으면 자동으로 실패하도록 설정할 수 있습니다.\npipeline { agent any stages { stage('Build') { steps { timeout(time: 30, unit: 'MINUTES') { sh './build.sh' } } } } } 3. retry 작업이 실패할 경우, 재시도할 수 있는 기능을 제공합니다.\npipeline { agent any stages { stage('Build') { steps { retry(3) { sh './build.sh' } } } } } Jenkins 파이프라인은 CI/CD 프로세스를 자동화하고, 빌드, 테스트, 배포 과정의 효율성을 높이는 중요한 도구입니다. 선언형과 스크립트형 파이프라인을 이해하고, 다양한 기능을 활용해보세요."},"title":"Jenkins Pipeline (선언형 vs 스크립트형)"},"/devops/jenkins/jenkins06/":{"data":{"":"","6-jenkins와-git-github-gitlab-연동#6️⃣ \u003cstrong\u003eJenkins와 Git, GitHub, GitLab 연동\u003c/strong\u003e":"6️⃣ Jenkins와 Git, GitHub, GitLab 연동 1. Jenkins와 Git 저장소 연동 Jenkins는 Git을 지원하여, Git 저장소의 소스 코드를 자동으로 가져와 빌드 및 배포를 실행할 수 있습니다. Jenkins와 Git 연동을 통해 코드 변경 사항을 추적하고, 실시간으로 빌드와 테스트가 가능합니다.\n1.1. Git 연동 설정 방법 Jenkins 설치 후 Git 플러그인 설치: 먼저 Jenkins에 Git 플러그인을 설치해야 합니다. Git 저장소 URL 설정: Jenkins에서 Job을 생성한 후, Git 저장소의 URL을 입력하여 소스 코드를 가져옵니다. 인증 관리: SSH 키나 사용자 인증 정보를 설정하여, Git 서버와 연결할 수 있도록 합니다. git 'https://github.com/yourusername/yourrepository.git' 그림: Jenkins에서 Git 연동 설정 화면 2. GitHub Webhook을 이용한 자동 빌드 GitHub Webhook은 GitHub 저장소에서 코드가 푸시될 때 자동으로 Jenkins에서 빌드를 트리거할 수 있게 해주는 기능입니다. 이를 통해 개발자는 GitHub에 커밋을 푸시할 때마다 자동으로 Jenkins 빌드를 실행할 수 있습니다.\n2.1. GitHub Webhook 설정 방법 Jenkins에 GitHub Plugin 설치: 먼저 GitHub 플러그인을 설치합니다. Jenkins Job 설정: GitHub Webhook을 사용하려면 GitHub 프로젝트와 연결된 Jenkins Job을 만들어야 합니다. 빌드 트리거를 GitHub hook trigger for GITScm polling으로 설정합니다. GitHub Webhook 설정: GitHub에서 리포지토리의 Settings -\u003e Webhooks로 이동 후, Jenkins URL을 입력하고 Webhook을 추가합니다. 예시: GitHub Webhook 설정 triggers { githubPush() } 그림: GitHub Webhook 설정 화면 3. GitLab CI/CD와 비교 GitLab CI/CD와 Jenkins는 모두 CI/CD 도구이지만, 그 사용 방식에는 차이가 있습니다. GitLab CI/CD는 GitLab 자체 내장 기능으로 제공되며, Jenkins는 외부 도구로 설치해야 합니다. 두 도구 모두 유사한 기능을 제공하지만, Jenkins는 플러그인 확장성을 통해 다양한 시스템과 연동할 수 있다는 장점이 있습니다.\n3.1. 차이점 설치 및 설정: Jenkins는 독립적인 서버에서 운영되며, GitLab CI/CD는 GitLab 내부에 포함되어 있어 설정이 간편합니다. 플러그인: Jenkins는 수많은 플러그인으로 기능을 확장할 수 있습니다. UI/UX: GitLab CI/CD는 GitLab UI와 통합되어 있어 일관된 경험을 제공합니다. Jenkins는 별도의 UI를 제공하여 다양한 설정을 할 수 있습니다. 예시: GitLab CI/CD 설정 stages: - build - test - deploy build: script: - echo \"Building...\" test: script: - echo \"Testing...\" deploy: script: - echo \"Deploying...\" 4. Branch 기반 자동화 (Multi-branch Pipeline) Jenkins는 Multi-branch Pipeline을 사용하여 여러 Git 브랜치에 대해 자동으로 파이프라인을 실행할 수 있습니다. 이는 브랜치마다 다른 파이프라인을 설정할 수 있게 해주며, 특히 GitHub나 GitLab에서 여러 브랜치를 다루는 프로젝트에 유용합니다.\n4.1. Multi-branch Pipeline 설정 Multi-branch Pipeline Job 생성: Jenkins에서 새로운 Multi-branch Pipeline Job을 생성합니다. Git 저장소 연결: GitHub나 GitLab 리포지토리와 연결합니다. Branch Discovery 설정: 각 브랜치에 대해 자동으로 파이프라인을 생성하고 실행합니다. multibranchPipelineJob('My-Repo') { branchSources { git { id('my-repo-id') remote('https://github.com/yourusername/yourrepository.git') credentialsId('your-credentials-id') } } } 그림: Multi-branch Pipeline 설정 화면 5. Git Tag, Pull Request 트리거 설정 Git에서는 Git Tag와 **Pull Request(PR)**를 트리거로 사용하여 빌드를 실행할 수 있습니다. Jenkins는 특정 태그가 푸시되거나 PR이 생성될 때 빌드를 실행하도록 설정할 수 있습니다.\n5.1. Git Tag 트리거 triggers { gitTagPush('v*') } 5.2. PR 트리거 GitHub와 GitLab에서 PR이 열리거나 수정될 때 Jenkins에서 자동으로 빌드를 트리거할 수 있습니다. 이를 위해 GitHub Pull Request Builder Plugin이나 GitLab CI Plugin을 사용합니다.\n6. GitHub Actions vs Jenkins 비교 GitHub Actions와 Jenkins는 모두 CI/CD 도구이지만, 사용 방식과 장점이 다릅니다.\n6.1. 차이점 설정 용이성: GitHub Actions는 GitHub 내에서 바로 설정할 수 있으며, 별도의 서버가 필요 없습니다. Jenkins는 별도로 설치하고 구성해야 합니다. 플러그인: Jenkins는 다양한 플러그인을 통해 기능을 확장할 수 있지만, GitHub Actions는 GitHub 환경 내에서만 동작합니다. UI/UX: GitHub Actions는 GitHub UI와 통합되어 있어 GitHub 사용자에게 더 직관적인 경험을 제공합니다. 예시: GitHub Actions Workflow name: CI/CD Pipeline on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Run build run: ./build.sh 7. Jenkins Credentials를 이용한 Git 인증 관리 Jenkins에서는 Git 인증을 위해 Jenkins Credentials를 사용하여 인증 정보를 안전하게 관리할 수 있습니다. 이를 통해 SSH 키나 사용자 이름과 비밀번호를 안전하게 저장하고, Jenkins Job에서 사용할 수 있습니다.\n7.1. Jenkins Credentials 설정 방법 Jenkins 대시보드에서 Manage Jenkins -\u003e Manage Credentials로 이동합니다. Git 저장소 인증 정보 추가: SSH 키나 사용자 이름/비밀번호 등을 입력하여 Git 인증 정보를 저장합니다. Job에서 인증 정보 사용: Jenkinsfile이나 Job 설정에서 이 인증 정보를 참조하여 Git과 연결합니다. git credentialsId: 'my-credentials-id', url: 'https://github.com/yourusername/yourrepository.git' 그림: Jenkins Credentials 설정 화면 Jenkins와 Git, GitHub, GitLab을 연동하면 CI/CD 파이프라인의 자동화가 훨씬 쉬워집니다. Git 저장소와 Jenkins의 통합을 통해 효율적인 소프트웨어 개발과 배포가 가능해집니다."},"title":"Jenkins와 Git, GitHub, GitLab 연동"},"/devops/jenkins/jenkins07/":{"data":{"":"","7-jenkins와-dockerkubernetes-연동#7️⃣ \u003cstrong\u003eJenkins와 Docker/Kubernetes 연동\u003c/strong\u003e":"7️⃣ Jenkins와 Docker/Kubernetes 연동 1. Docker 기반 Jenkins 실행 및 관리 Jenkins는 Docker와 쉽게 통합되어 Docker 컨테이너 내에서 Jenkins를 실행할 수 있습니다. 이를 통해 Jenkins 인프라를 간편하게 관리하고 확장할 수 있습니다. Docker를 활용하면 Jenkins 서버와 에이전트를 컨테이너로 배포하여 빠르게 환경을 구축할 수 있습니다.\n1.1. Docker로 Jenkins 실행 Docker를 사용하여 Jenkins를 실행하려면 Docker 이미지를 다운로드하고 실행할 수 있습니다. 아래는 Jenkins의 공식 Docker 이미지를 사용하여 Jenkins를 실행하는 방법입니다.\ndocker pull jenkins/jenkins:lts docker run -d -p 8080:8080 -p 50000:50000 --name jenkins jenkins/jenkins:lts 이 명령어는 Jenkins를 Docker 컨테이너로 실행하며, 8080 포트와 50000 포트를 개방하여 Jenkins UI와 에이전트 연결을 가능하게 합니다.\n1.2. Jenkins와 Docker 연동 설정 Jenkins 내부에서 Docker를 사용할 수 있도록 Docker Plugin을 설치해야 합니다. Docker Plugin을 통해 Jenkins는 Docker 컨테이너 내에서 빌드를 실행하거나 Docker 이미지를 빌드할 수 있습니다.\ndocker.build(\"my-image\") 그림: Docker로 실행된 Jenkins UI 2. Jenkins에서 Docker 컨테이너를 사용한 빌드 Jenkins에서 Docker를 사용하여 빌드를 수행할 수 있습니다. Docker 컨테이너를 빌드, 테스트, 배포하는 과정은 Jenkins 파이프라인에서 Docker를 활용하여 자동화할 수 있습니다.\n2.1. Docker를 이용한 빌드 예시 pipeline { agent any stages { stage('Build') { steps { script { docker.build('my-docker-image') } } } } } 위의 예시는 Jenkins에서 Docker를 사용하여 이미지를 빌드하는 방법을 보여줍니다. docker.build() 명령어를 통해 Docker 이미지를 생성하고 Jenkins가 이를 실행합니다.\n2.2. Docker 에이전트 사용 Jenkins에서 Docker 컨테이너를 에이전트로 사용할 수 있습니다. 이를 통해 Jenkins의 빌드 및 테스트가 Docker 환경에서 실행되도록 할 수 있습니다. Docker 에이전트를 사용하는 방법은 다음과 같습니다.\npipeline { agent { docker { image 'maven:3-alpine' label 'docker-agent' } } stages { stage('Build') { steps { sh 'mvn clean install' } } } } 그림: Docker 컨테이너에서 빌드 실행 3. Jenkins에서 Kubernetes Pod를 Agent로 사용하는 방법 Jenkins는 Kubernetes와 연동하여 Kubernetes Pod를 동적 에이전트로 사용할 수 있습니다. 이를 통해 Jenkins는 Kubernetes 클러스터 내에서 필요할 때마다 Pod를 생성하여 빌드 작업을 분산 처리할 수 있습니다.\n3.1. Kubernetes Plugin 설치 Jenkins에서 Kubernetes를 사용하려면 Kubernetes Plugin을 설치해야 합니다. 설치 후 Kubernetes 클러스터를 Jenkins에 연결하여 Jenkins가 Kubernetes 환경에서 Pod를 동적으로 생성할 수 있도록 설정합니다.\n3.2. Kubernetes에서 에이전트 자동 생성 Jenkins는 Kubernetes 클러스터에서 에이전트를 동적으로 생성하여 작업을 실행할 수 있습니다. 아래는 Kubernetes에서 Jenkins 에이전트를 자동으로 생성하는 예시입니다.\npipeline { agent { kubernetes { label 'jenkins-agent' defaultContainer 'jnlp' } } stages { stage('Build') { steps { container('maven') { sh 'mvn clean install' } } } } } 위의 예시는 Kubernetes 클러스터에서 jnlp라는 기본 컨테이너를 사용하여 Jenkins 빌드를 실행하는 방법을 보여줍니다.\n그림: Kubernetes에서 Jenkins 에이전트 생성 4. Kubernetes Plugin을 활용한 동적 Jenkins Agent 생성 Jenkins는 Kubernetes Plugin을 활용하여 클러스터 내에서 동적으로 Jenkins 에이전트를 생성할 수 있습니다. Kubernetes 클러스터의 리소스를 효율적으로 사용하고, 필요할 때마다 에이전트를 자동으로 생성하여 빌드를 실행할 수 있습니다.\n4.1. Kubernetes Plugin 설정 Kubernetes 클러스터 연결: Jenkins 대시보드에서 Manage Jenkins -\u003e Configure System으로 이동하여 Kubernetes 클러스터를 설정합니다. Pod Template 설정: Jenkins 에이전트를 실행할 Kubernetes Pod 템플릿을 설정하여, 빌드에 필요한 컨테이너들을 정의합니다. 예시: Pod Template 설정 kubernetes { label 'jenkins-agent' defaultContainer 'jnlp' containers { containerTemplate(name: 'maven', image: 'maven:3-alpine', ttyEnabled: true, command: 'cat') } } 그림: Kubernetes 에이전트 설정 화면 5. Jenkins와 Helm을 이용한 Kubernetes 배포 자동화 Jenkins는 Helm을 사용하여 Kubernetes 클러스터에 배포를 자동화할 수 있습니다. Helm 차트를 사용하여 애플리케이션 배포를 정의하고 Jenkins 파이프라인에서 이를 자동으로 실행할 수 있습니다.\n5.1. Helm을 이용한 Kubernetes 배포 예시 pipeline { agent any stages { stage('Deploy') { steps { script { sh 'helm upgrade --install my-app ./charts/my-app' } } } } } 위 예시에서는 Jenkins 파이프라인을 사용하여 Helm 차트를 통해 Kubernetes에 애플리케이션을 배포하는 방법을 보여줍니다.\n그림: Helm을 통한 Kubernetes 배포 6. Jenkins + ArgoCD를 활용한 GitOps 구축 GitOps는 Git을 단일 진리의 원천으로 사용하여 배포 및 관리를 자동화하는 방법론입니다. Jenkins와 ArgoCD를 결합하면 GitOps 흐름을 구축하여, 코드 변경 시 자동으로 Kubernetes에 배포를 실행할 수 있습니다.\n6.1. Jenkins와 ArgoCD 연동 Jenkins에서 빌드된 Docker 이미지를 Git 리포지토리에 푸시한 후, ArgoCD는 해당 Git 리포지토리의 변화를 감지하고 Kubernetes 클러스터에 자동으로 배포를 수행합니다.\n예시: Jenkins와 ArgoCD 연동 pipeline { agent any stages { stage('Push to Git') { steps { script { // Docker 이미지 푸시 sh 'docker push my-docker-image' } } } stage('Notify ArgoCD') { steps { script { // ArgoCD API를 호출하여 배포를 트리거 sh 'curl -X POST http://argocd-server/api/v1/applications/my-app/sync' } } } } } 그림: Jenkins와 ArgoCD 연동 흐름 Jenkins와 Docker, Kubernetes를 연동하면 CI/CD 파이프라인을 더욱 효율적이고 확장 가능한 형태로 구축할 수 있습니다. Jenkins는 Docker와 Kubernetes의 유연성을 이용하여 다양한 배포 환경을 지원하고, Helm과 ArgoCD를 통해 배포 자동화 및 GitOps 방식을 쉽게 구현할 수 있습니다."},"title":"Jenkins와 Docker/Kubernetes 연동"},"/devops/jenkins/jenkins08/":{"data":{"":"","8-jenkins-job-및-build-단계-상세-설정#8️⃣ \u003cstrong\u003eJenkins Job 및 Build 단계 상세 설정\u003c/strong\u003e":"8️⃣ Jenkins Job 및 Build 단계 상세 설정 1. Build Step (Shell Script, Gradle, Maven, Makefile 등) Jenkins에서 Build Step은 빌드 프로세스 중에 실행될 구체적인 명령을 정의하는 단계입니다. Jenkins는 다양한 빌드 툴 및 방법을 지원하여, 다양한 환경에서 빌드를 자동화할 수 있습니다.\n1.1. Shell Script Jenkins의 가장 기본적인 빌드 단계는 Shell Script를 실행하는 것입니다. 이 방법은 유닉스 계열 시스템에서 자주 사용됩니다. 예를 들어, 간단한 쉘 명령을 실행하여 빌드를 처리할 수 있습니다.\n#!/bin/bash echo \"Hello, Jenkins!\" Jenkins에서 Execute shell을 선택하고 위의 쉘 스크립트를 입력하면 빌드가 실행됩니다.\n1.2. Gradle 빌드 Gradle은 Java 프로젝트를 위한 빌드 툴입니다. Jenkins에서 Gradle을 사용하려면 Gradle Plugin을 설치하고, 아래와 같이 빌드를 설정할 수 있습니다.\ngradle build 1.3. Maven 빌드 Maven은 또 다른 Java 기반 빌드 툴로, Maven을 사용하는 프로젝트는 Jenkins에서 다음과 같이 설정할 수 있습니다.\nmvn clean install 1.4. Makefile 빌드 Makefile을 사용하여 C/C++ 프로젝트나 다른 언어의 빌드를 관리하는 방법입니다. make 명령어를 사용하여 빌드를 실행할 수 있습니다.\nmake 그림: Jenkins에서의 다양한 Build Step 2. Build Trigger (SCM Polling, Webhook, Timer, Upstream) Build Trigger는 빌드를 자동으로 트리거하는 조건을 설정하는 기능입니다. 여러 가지 트리거 옵션을 통해 다양한 방식으로 빌드를 자동화할 수 있습니다.\n2.1. SCM Polling SCM Polling은 Git, SVN 등 소스 코드 관리 시스템의 변경 사항을 주기적으로 확인하여 빌드를 트리거하는 방법입니다. 이를 통해 소스 코드가 변경되었을 때 자동으로 빌드를 실행할 수 있습니다.\npollSCM('* * * * *') 위 예시는 매 분마다 소스 코드 변경 여부를 확인하여 빌드를 트리거합니다.\n2.2. Webhook Webhook은 GitHub, GitLab, Bitbucket 등 외부 Git 서비스에서 코드 변경이 발생하면 Jenkins에게 알리는 방식입니다. Webhook을 설정하면 소스 코드가 푸시될 때마다 Jenkins가 자동으로 빌드를 실행합니다.\n2.3. Timer Timer는 일정 시간 간격으로 빌드를 트리거하는 방법입니다. 예를 들어, 매일 정해진 시간에 빌드를 실행할 수 있습니다.\nH 12 * * 1-5 위의 예시는 매주 월요일부터 금요일까지, 매일 오후 12시에 빌드를 실행하는 스케줄입니다.\n2.4. Upstream Upstream 트리거는 다른 Job이 성공적으로 완료되었을 때 해당 Job을 실행하는 방식입니다. 예를 들어, 빌드가 완료된 후 배포 Job을 트리거할 수 있습니다.\n그림: Jenkins Build Trigger 설정 화면 3. Post-build Actions (Artifact 저장, Email Notification, Slack 연동) Post-build Actions는 빌드가 완료된 후 실행할 작업을 설정하는 기능입니다. 빌드가 완료되면 다양한 후속 작업을 정의할 수 있습니다.\n3.1. Artifact 저장 빌드 후 생성된 파일들을 Artifact로 저장할 수 있습니다. 이를 통해 빌드된 결과물을 저장하고 나중에 참조할 수 있습니다. 예를 들어, Maven으로 빌드한 .jar 파일을 저장할 수 있습니다.\narchiveArtifacts '**/target/*.jar' 3.2. Email Notification 빌드 상태에 따라 이메일을 발송할 수 있습니다. 빌드가 성공하거나 실패할 때, 팀에게 이메일 알림을 보낼 수 있습니다.\nemailext subject: 'Build Status: ${BUILD_STATUS}', body: 'The build has finished: ${BUILD_URL}' 3.3. Slack 연동 Jenkins와 Slack을 연동하여 빌드 상태를 Slack 채널에 전송할 수 있습니다. 이를 통해 팀원들이 실시간으로 빌드 상태를 확인할 수 있습니다.\nslackSend channel: '#build-notifications', color: 'good', message: \"Build #${BUILD_NUMBER} finished successfully!\" 그림: Post-build Actions 설정 화면 4. Build Parameter 활용 (choice, string, password, boolean) Build Parameters는 사용자가 빌드를 실행할 때 입력할 수 있는 매개변수입니다. Jenkins에서는 다양한 유형의 파라미터를 지원하여 빌드를 유연하게 제어할 수 있습니다.\n4.1. Choice Choice 파라미터를 사용하면 사용자가 선택할 수 있는 옵션을 제공할 수 있습니다. 예를 들어, 배포 환경을 선택하도록 할 수 있습니다.\nchoice(name: 'DEPLOY_ENV', choices: ['dev', 'staging', 'production'], description: 'Select the deployment environment') 4.2. String String 파라미터를 사용하면 사용자가 문자열을 입력할 수 있습니다. 예를 들어, 버전 번호를 입력받는 경우입니다.\nstring(name: 'VERSION', defaultValue: '1.0.0', description: 'Enter the version to deploy') 4.3. Password Password 파라미터는 민감한 정보를 입력할 때 사용됩니다. 예를 들어, 비밀번호를 입력받는 경우입니다.\npassword(name: 'PASSWORD', description: 'Enter the password') 4.4. Boolean Boolean 파라미터는 참(True) 또는 거짓(False) 값을 선택하는 옵션입니다.\nbooleanParam(name: 'IS_PRODUCTION', defaultValue: false, description: 'Is this a production environment?') 그림: Jenkins Build Parameter 설정 화면 5. Build 상태 분석 (SUCCESS, FAILURE, UNSTABLE) Jenkins에서는 빌드의 상태를 확인하여 각 상태에 따른 후속 작업을 정의할 수 있습니다. 주요 빌드 상태는 다음과 같습니다.\nSUCCESS: 빌드가 성공적으로 완료됨 FAILURE: 빌드가 실패함 UNSTABLE: 빌드가 경고 상태로 종료됨 (예: 테스트 실패 등) 5.1. 빌드 상태에 따른 처리 Jenkins에서 빌드 상태에 따라 다른 작업을 수행할 수 있습니다. 예를 들어, 빌드가 실패한 경우 이메일 알림을 보내고, 성공한 경우 다음 단계로 진행하도록 설정할 수 있습니다.\npost { success { echo 'Build succeeded!' } failure { echo 'Build failed!' } unstable { echo 'Build unstable!' } } 그림: 빌드 상태 분석 화면 6. Jenkins Artifacts 저장 및 활용 Artifacts는 빌드 후 생성된 결과물로, Jenkins에서는 이를 저장하고 나중에 활용할 수 있습니다. 예를 들어, 빌드 후 생성된 아티팩트를 저장하고, 다른 Job에서 이를 활용할 수 있습니다.\n6.1. Artifacts 저장 빌드 후 아티팩트를 저장하려면 archiveArtifacts 명령어를 사용합니다. 예를 들어, Maven 빌드를 사용한 후 .jar 파일을 저장할 수 있습니다.\narchiveArtifacts '**/target/*.jar' 6.2. Artifacts 활용 저장된 아티팩트를 다른 Job에서 사용할 수 있습니다. 예를 들어, 아티팩트를 다운로드하여 배포 작업을 수행할 수 있습니다.\ncopyArtifacts('Build-Job') { includePatterns('**/*.jar') target('builds/') } 그림: Artifacts 저장 및 활용 화면 Jenkins는 매우 강력한 빌드 및 배포 자동화 툴로, 다양한 빌드 옵션과 후속 작업을 설정할 수 있습니다. Jenkins Job과 Build 단계를 잘 활용하면, 효율적이고 자동화된 CI/CD 파이프라인을 구축할 수 있습니다."},"title":"Jenkins Job 및 Build 단계 상세 설정"},"/devops/jenkins/jenkins09/":{"data":{"":"","9-jenkins와-cicd-구축-실습#9️⃣ \u003cstrong\u003eJenkins와 CI/CD 구축 실습\u003c/strong\u003e":"9️⃣ Jenkins와 CI/CD 구축 실습 1️⃣ CI/CD 개념 복습 및 실무 적용 사례 CI/CD(Continuous Integration / Continuous Deployment)는 개발과 배포의 자동화를 통해 소프트웨어 개발 프로세스를 개선하는 방법론입니다. CI/CD는 개발자들이 자주 코드를 통합하고, 자동화된 테스트와 배포를 통해 더 빠르고 안정적인 소프트웨어 배포를 가능하게 합니다.\n1.1. Continuous Integration (CI) CI는 소프트웨어 개발에서 개발자들이 작성한 코드가 버전 관리 시스템에 자주 통합되는 프로세스를 의미합니다. 코드 변경 사항이 버전 관리 시스템에 푸시될 때마다 자동으로 빌드와 테스트가 수행되어 코드의 품질을 지속적으로 확인합니다.\n목표: 변경 사항을 작은 단위로 자주 통합하여 충돌을 최소화 주요 도구: Jenkins, Git, Maven, Gradle, TestNG, JUnit 등 1.2. Continuous Deployment (CD) CD는 코드가 성공적으로 빌드되고 테스트된 후, 자동으로 배포까지 이루어지는 프로세스입니다. 개발자가 코드를 푸시하면 CI가 자동으로 빌드하고, 테스트 후 CD 파이프라인이 배포까지 처리합니다.\n목표: 자동화된 배포를 통해 빠르고 안정적인 배포 환경을 유지 주요 도구: Jenkins, Docker, Kubernetes, Ansible, Helm 등 1.3. 실무 적용 사례 기업에서는 CI/CD를 통해 소프트웨어 품질을 높이고, 개발과 배포 속도를 향상시키는 방법을 사용합니다. 예를 들어, Amazon, Netflix, Facebook과 같은 대형 기업은 Jenkins와 같은 CI/CD 도구를 활용하여 수천 개의 코드 변경 사항을 실시간으로 배포하고 있습니다.\n2️⃣ Jenkins를 이용한 CI/CD 기본 Pipeline 구축 Jenkins에서 CI/CD 파이프라인을 구축하려면 여러 단계를 자동화하고, 이를 Pipeline으로 정의해야 합니다. Jenkins는 Declarative Pipeline과 Scripted Pipeline 두 가지 방식으로 파이프라인을 작성할 수 있습니다.\n2.1. Declarative Pipeline 예제 pipeline { agent any stages { stage('Build') { steps { script { echo 'Building the project...' // 빌드 작업 (예: Maven, Gradle) } } } stage('Test') { steps { script { echo 'Running tests...' // 테스트 작업 (예: JUnit) } } } stage('Deploy') { steps { script { echo 'Deploying the project...' // 배포 작업 (예: Docker, Kubernetes) } } } } } 2.2. Pipeline 단계 설명 Build: 코드를 빌드하는 단계입니다. Maven, Gradle, Makefile 등 다양한 빌드 도구를 사용할 수 있습니다. Test: 빌드 후 테스트를 실행하는 단계입니다. JUnit, TestNG 등을 이용해 단위 테스트를 수행합니다. Deploy: 테스트가 성공하면 자동으로 배포하는 단계입니다. Docker, Kubernetes를 이용하여 애플리케이션을 배포합니다. 3️⃣ Code → Build → Test → Deploy 자동화 CI/CD 파이프라인에서 Code → Build → Test → Deploy 단계는 코드 변경부터 최종 배포까지의 자동화 흐름을 의미합니다. Jenkins는 이러한 파이프라인을 설정하고, 코드를 푸시할 때마다 자동으로 실행되도록 할 수 있습니다.\n3.1. Code: Git 저장소에 코드 푸시 개발자는 GitHub, GitLab, Bitbucket 등의 Git 저장소에 코드를 푸시합니다. Jenkins는 이 저장소를 모니터링하고, 코드 푸시 시 자동으로 파이프라인을 실행합니다.\n3.2. Build: 코드 빌드 코드가 푸시되면 Jenkins는 빌드 단계에서 Maven, Gradle 등 빌드 도구를 사용하여 프로젝트를 컴파일하고 패키징합니다. 이때 Build라는 Jenkins Job이 실행됩니다.\n3.3. Test: 자동화된 테스트 빌드가 완료되면 Jenkins는 자동으로 유닛 테스트, 통합 테스트 등을 실행합니다. JUnit, TestNG 등의 프레임워크를 사용하여 테스트를 자동화할 수 있습니다.\n3.4. Deploy: 자동 배포 테스트가 성공하면 Jenkins는 자동으로 배포를 시작합니다. Docker, Kubernetes, Helm 등을 이용하여 자동으로 프로덕션 환경에 배포할 수 있습니다.\n4️⃣ Java(Spring Boot), Python, Node.js CI/CD 구축 예제 Jenkins를 사용하여 다양한 언어 및 프레임워크에 대한 CI/CD를 설정할 수 있습니다. 아래는 각각의 예시입니다.\n4.1. Java(Spring Boot) CI/CD 구축 pipeline { agent any stages { stage('Build') { steps { script { echo 'Building the Spring Boot project...' sh './mvnw clean install' } } } stage('Test') { steps { script { echo 'Running tests...' sh './mvnw test' } } } stage('Deploy') { steps { script { echo 'Deploying the Spring Boot application...' // Docker 이미지 생성 후 배포 } } } } } 4.2. Python CI/CD 구축 pipeline { agent any stages { stage('Build') { steps { script { echo 'Building the Python project...' sh 'pip install -r requirements.txt' } } } stage('Test') { steps { script { echo 'Running tests...' sh 'pytest' } } } stage('Deploy') { steps { script { echo 'Deploying the Python application...' // Docker, AWS Lambda 등으로 배포 } } } } } 4.3. Node.js CI/CD 구축 pipeline { agent any stages { stage('Build') { steps { script { echo 'Building the Node.js project...' sh 'npm install' } } } stage('Test') { steps { script { echo 'Running tests...' sh 'npm test' } } } stage('Deploy') { steps { script { echo 'Deploying the Node.js application...' // Docker, Kubernetes 등으로 배포 } } } } } 5️⃣ Jenkins와 Ansible을 이용한 인프라 자동화 Ansible은 IT 자동화 도구로, Jenkins와 연동하여 인프라의 자동화 배포를 수행할 수 있습니다. 예를 들어, Jenkins에서 빌드가 완료된 후 Ansible을 이용하여 서버에 소프트웨어를 자동으로 배포할 수 있습니다.\npipeline { agent any stages { stage('Build') { steps { script { echo 'Building the project...' // 빌드 작업 } } } stage('Deploy') { steps { script { echo 'Deploying with Ansible...' sh 'ansible-playbook deploy.yml' } } } } } 6️⃣ Blue-Green Deployment 및 Canary Deployment 적용 Jenkins를 사용하여 Blue-Green Deployment와 Canary Deployment를 설정할 수 있습니다. 이 두 가지 배포 방식은 애플리케이션의 다운타임을 최소화하고, 배포 리스크를 줄이는 데 사용됩니다.\n6.1. Blue-Green Deployment Blue-Green Deployment는 두 개의 독립된 환경(Blue와 Green)을 만들어서 하나는 현재 배포된 버전, 다른 하나는 새로운 버전으로 유지합니다. 새로운 버전을 배포할 때, 기존 버전을 차단하고 새로운 버전을 활성화합니다.\n6.2. Canary Deployment Canary Deployment는 일부 사용자에게만 새로운 버전을 배포하여, 점진적으로 새로운 버전을 배포하는 방법입니다. 이 방법은 배포 리스크를 최소화하는 데 유리합니다.\n그림: Blue-Green 및 Canary Deployment 예시 Jenkins를 이용한 CI/CD 파이프라인 구축은 소프트웨어 개발의 속도와 품질을 높이는 데 중요한 역할을 합니다. Jenkins의 자동화 기능을 잘 활용하면 효율적인 배포 및 테스트 환경을 구축할 수 있습니다."},"title":"Jenkins와 CI/CD 구축 실습"},"/devops/jenkins/jenkins10/":{"data":{"":"","-jenkins-플러그인-활용-및-고급-설정#🔟 \u003cstrong\u003eJenkins 플러그인 활용 및 고급 설정\u003c/strong\u003e":"🔟 Jenkins 플러그인 활용 및 고급 설정 1️⃣ Jenkins 주요 플러그인 소개 및 설치 Jenkins는 플러그인을 통해 기능을 확장할 수 있습니다. 플러그인은 Jenkins의 핵심 기능을 보강하고 다양한 외부 도구와 연동을 가능하게 합니다. 여기서는 Jenkins에서 자주 사용되는 플러그인들과 그 설치 방법을 다룹니다.\n1.1. Jenkins 플러그인 설치 방법 Jenkins 대시보드에서 Manage Jenkins를 클릭합니다. Manage Plugins 메뉴를 선택합니다. 플러그인 관리 화면에서 Available 탭에서 필요한 플러그인을 검색하여 설치합니다. 2️⃣ Git Plugin Git Plugin은 Jenkins에서 Git 리포지토리와 연동하여 빌드, 테스트, 배포 등 다양한 작업을 자동화하는 데 사용됩니다. 이를 통해 GitHub, GitLab, Bitbucket 등에서 코드 변경 사항을 자동으로 감지하고 빌드를 트리거할 수 있습니다.\n2.1. 설치 방법 Manage Jenkins → Manage Plugins → Available 탭에서 Git Plugin을 검색하여 설치합니다. 설치 후, Jenkins Job에서 Source Code Management 섹션에 Git을 선택하고, GitHub URL 및 인증 정보를 입력합니다. 2.2. 예제 pipeline { agent any stages { stage('Clone Git Repository') { steps { git url: 'https://github.com/your-repository.git', branch: 'main' } } // 빌드 및 테스트 단계 } } 3️⃣ Pipeline Plugin Pipeline Plugin은 Jenkins에서 파이프라인을 정의하고 실행할 수 있게 해주는 플러그인입니다. Jenkinsfile을 통해 여러 단계로 나누어 빌드, 테스트, 배포를 자동화할 수 있습니다.\n3.1. 설치 방법 Manage Jenkins → Manage Plugins → Available 탭에서 Pipeline Plugin을 검색하여 설치합니다. 설치 후, Jenkins의 New Item에서 Pipeline을 선택하고 Jenkinsfile을 설정합니다. 3.2. 예제 pipeline { agent any stages { stage('Build') { steps { echo 'Building the project' } } stage('Test') { steps { echo 'Running tests' } } stage('Deploy') { steps { echo 'Deploying the project' } } } } 4️⃣ Blue Ocean Blue Ocean은 Jenkins의 최신 UI로, 파이프라인을 시각적으로 표현하고, 사용자가 쉽게 작업할 수 있도록 돕습니다. 파이프라인의 각 단계를 드래그 앤 드롭 방식으로 관리할 수 있습니다.\n4.1. 설치 방법 Manage Jenkins → Manage Plugins → Available 탭에서 Blue Ocean을 검색하여 설치합니다. 설치 후, Jenkins 대시보드에서 Blue Ocean 메뉴를 선택하면 새로운 UI로 파이프라인을 시각화할 수 있습니다. 5️⃣ Slack Notification Slack Notification 플러그인은 Jenkins 빌드 상태를 Slack 채널로 알릴 수 있게 해줍니다. 이를 통해 빌드가 성공했는지 실패했는지를 실시간으로 팀원에게 전달할 수 있습니다.\n5.1. 설치 방법 Manage Jenkins → Manage Plugins → Available 탭에서 Slack Notification을 검색하여 설치합니다. 설치 후, Manage Jenkins → Configure System에서 Slack Webhook URL을 설정합니다. 5.2. 예제 pipeline { agent any stages { stage('Build') { steps { echo 'Building the project' } } } post { success { slackSend (channel: '#your-channel', message: 'Build Success!') } failure { slackSend (channel: '#your-channel', message: 'Build Failed!') } } } 6️⃣ SonarQube Scanner SonarQube Scanner는 코드 품질을 분석하고 결과를 SonarQube 대시보드에 보여주는 플러그인입니다. 이를 통해 코드 품질을 지속적으로 모니터링할 수 있습니다.\n6.1. 설치 방법 Manage Jenkins → Manage Plugins → Available 탭에서 SonarQube Scanner를 검색하여 설치합니다. 설치 후, Manage Jenkins → Configure System에서 SonarQube 서버를 설정합니다. 6.2. 예제 pipeline { agent any stages { stage('Build') { steps { echo 'Building the project' } } stage('Code Quality Analysis') { steps { script { // SonarQube 분석 명령 sh 'mvn sonar:sonar' } } } } } 7️⃣ Nexus Repository Manager Nexus Repository Manager 플러그인은 빌드 아티팩트를 관리하는 데 유용합니다. 빌드 후 생성된 아티팩트를 Nexus에 업로드하여 다른 프로젝트에서 재사용할 수 있습니다.\n7.1. 설치 방법 Manage Jenkins → Manage Plugins → Available 탭에서 Nexus Artifact Uploader를 검색하여 설치합니다. 설치 후, Jenkins의 Post-build Actions에서 Nexus 업로드 옵션을 설정합니다. 7.2. 예제 pipeline { agent any stages { stage('Build') { steps { echo 'Building the project' } } } post { success { nexusPublisher( nexusInstanceId: 'your-nexus-id', nexusRepository: 'your-repository', packages: [ [ $class: 'MavenArtifact', artifactId: 'your-artifact', version: '1.0.0', filePath: 'target/your-artifact.jar' ] ] ) } } } 8️⃣ Kubernetes Plugin Kubernetes Plugin은 Jenkins와 Kubernetes 클러스터를 연동하여 Jenkins Agent를 Kubernetes Pod로 자동화하여 실행하는 기능을 제공합니다.\n8.1. 설치 방법 Manage Jenkins → Manage Plugins → Available 탭에서 Kubernetes Plugin을 검색하여 설치합니다. 설치 후, Manage Jenkins → Configure System에서 Kubernetes 클러스터 정보를 설정합니다. 8.2. 예제 pipeline { agent { kubernetes { label 'my-pod' defaultContainer 'jnlp' } } stages { stage('Build') { steps { echo 'Building in Kubernetes pod' } } } } 9️⃣ Jenkins 글로벌 설정 (Manage Jenkins 설정 옵션) Jenkins 글로벌 설정은 Jenkins 환경을 설정하는 곳으로, 관리자가 Jenkins의 동작 방식 및 플러그인 동작 방식을 설정할 수 있습니다. 예를 들어, 이메일 서버 설정, JDK 및 Maven 경로 설정 등이 있습니다.\n9.1. 설정 방법 Manage Jenkins → Configure System에서 필요한 설정을 추가합니다. 예를 들어, Email Notification을 설정하려면, SMTP 서버 정보를 입력하고, 이메일 발송 기능을 활성화합니다. 🔟 Jenkins의 REST API 활용 (/api/json, curl 예제) Jenkins는 REST API를 통해 외부 시스템과 연동할 수 있습니다. curl을 이용하여 Jenkins 정보를 요청하거나 빌드를 트리거할 수 있습니다.\n10.1. 예제 curl -X POST http://your-jenkins-url/job/your-job-name/build 10.2. 빌드 상태 조회 curl http://your-jenkins-url/job/your-job-name/lastBuild/api/json 1️⃣1️⃣ Role-Based Access Control (RBAC) 설정 RBAC는 Jenkins에서 사용자별로 권한을 관리하는 기능입니다. 이를 통해 특정 사용자에게만 빌드 실행 권한, 관리 권한 등을 부여할 수 있습니다.\n11.1. 설정 방법 Manage Jenkins → Configure Global Security에서 Role-Based Authorization Strategy를 활성화합니다. Manage and Assign Roles에서 사용자의 역할을 설정합니다. 1️⃣2️⃣ Jenkins 사용자 및 권한 관리 Jenkins는 각 사용자에게 권한을 부여하여 작업을 제어할 수 있습니다. 사용자마다 접근 가능한 기능을 제한할 수 있습니다.\n12.1. 설정 방법 Manage Jenkins → Configure Global Security에서 Matrix-based security를 선택하고, 각 사용자에게 필요한 권한을 설정합니다. "},"title":"Jenkins 플러그인 활용 및 고급 설정"},"/devops/jenkins/jenkins11/":{"data":{"":"","11-jenkins-모니터링-및-성능-최적화#1️⃣1️⃣ \u003cstrong\u003eJenkins 모니터링 및 성능 최적화\u003c/strong\u003e":"1️⃣1️⃣ Jenkins 모니터링 및 성능 최적화 1️⃣ Jenkins System Monitoring (JVM Heap, Garbage Collection) Jenkins는 Java 기반으로 실행되기 때문에 JVM의 성능이 중요합니다. JVM Heap 크기, Garbage Collection 등을 모니터링하고 최적화함으로써 Jenkins의 성능을 향상시킬 수 있습니다.\n1.1. JVM Heap 모니터링 JVM의 힙 메모리 크기를 확인하려면 Jenkins의 시스템 관리 화면에서 Manage Jenkins → System Information 메뉴로 이동하여 메모리 사용량을 확인할 수 있습니다. JVM 힙 메모리가 부족할 경우, Jenkins 성능이 저하될 수 있습니다.\n1.2. Garbage Collection 모니터링 Garbage Collection 로그를 통해 메모리 관리 상태를 추적하고, GC 시간과 빈도를 모니터링합니다. GC 시간이 길거나 빈번할 경우, Jenkins의 응답 시간이 느려질 수 있습니다. -Xlog:gc* 옵션을 사용하여 GC 로그를 활성화하고 분석할 수 있습니다.\n-Xlog:gc*:file=/var/log/jenkins/gc.log 1.3. GC 로그 분석 Garbage Collection이 지나치게 자주 발생하는지 확인하고, 메모리 힙 크기를 조정하여 성능을 개선할 수 있습니다.\n2️⃣ Jenkins Worker Node 확장 및 로드 밸런싱 Jenkins의 빌드가 많아질수록 하나의 마스터 노드에서 처리하는 것은 성능에 부담을 줄 수 있습니다. 이를 해결하기 위해 Worker Node를 추가하고 로드 밸런싱을 적용할 수 있습니다.\n2.1. Worker Node 추가 Jenkins는 마스터-슬레이브 아키텍처를 지원하며, 여러 Worker Node를 추가하여 빌드를 분산할 수 있습니다. 이를 통해 빌드 성능을 향상시킬 수 있습니다.\nManage Jenkins → Manage Nodes and Clouds → New Node를 선택합니다. Worker Node의 유형을 선택하고, 해당 노드의 설정을 구성합니다. 2.2. 로드 밸런싱 Jenkins에는 여러 가지 로드 밸런싱 방식이 있으며, 클러스터 환경에서 여러 Worker Node가 동시에 작업을 처리할 수 있도록 설정할 수 있습니다. Jenkins Swarm Plugin을 사용하여 여러 Worker Node를 연결하고 부하 분산을 할 수 있습니다.\n3️⃣ Jenkins Job Queue 최적화 (throttle-concurrent-builds) Jenkins의 빌드가 많아지면 Job Queue가 지연될 수 있습니다. 이를 해결하기 위해 Throttle Concurrent Builds 플러그인을 사용하여 한 번에 실행되는 빌드 수를 제한하고, 과도한 대기 상태를 방지할 수 있습니다.\n3.1. Throttle Concurrent Builds 플러그인 설치 Manage Jenkins → Manage Plugins → Available 탭에서 Throttle Concurrent Builds를 검색하여 설치합니다. 설치 후, Jenkins Job의 설정에서 동시 실행되는 빌드 수를 제한할 수 있습니다. 3.2. 설정 방법 Job 설정에서 Build Triggers 아래 Throttle Concurrent Builds를 체크하고, 최대 동시 실행 빌드 수를 설정합니다.\nthrottleConcurrentBuilds(3) // 동시에 3개 이상의 빌드가 실행되지 않도록 제한 4️⃣ Jenkins 로그 분석 (jenkins.log, build.log) Jenkins의 로그를 분석하면 문제의 원인을 파악할 수 있습니다. jenkins.log는 Jenkins 시스템 전반의 로그를 담고 있으며, build.log는 각 빌드의 상세 로그를 담고 있습니다.\n4.1. jenkins.log 분석 jenkins.log 파일은 /var/log/jenkins/jenkins.log에 위치하고 있으며, Jenkins의 동작 중 발생한 오류나 경고를 기록합니다. 이 로그를 주기적으로 확인하여 시스템 상태를 점검합니다.\n4.2. build.log 분석 각각의 빌드에 대한 로그는 Jenkins 대시보드에서 빌드 번호를 클릭하여 확인할 수 있습니다. 빌드 로그에서 오류가 발생한 부분을 찾아 문제를 해결할 수 있습니다.\ntail -f /var/log/jenkins/jenkins.log tail -f /var/lib/jenkins/jobs/your-job-name/builds/latest/build.log 5️⃣ Jenkins 캐시 최적화 (Pipeline Caching, Docker Layer Caching) Jenkins의 빌드 시간 단축을 위해 캐시를 활용하는 방법입니다. Pipeline Caching과 Docker Layer Caching을 이용하여 빌드를 빠르게 만들 수 있습니다.\n5.1. Pipeline Caching Pipeline 캐시는 빌드가 반복될 때마다 캐시된 데이터를 활용하여 빌드를 가속화합니다. 예를 들어, Maven의 ~/.m2/repository 디렉토리를 캐시하여 빌드 속도를 향상시킬 수 있습니다.\npipeline { agent any stages { stage('Build') { steps { cache(path: '/root/.m2/repository', key: 'maven-cache') { sh 'mvn clean install' } } } } } 5.2. Docker Layer Caching Docker 이미지 빌드를 빠르게 할 수 있는 Docker Layer Caching을 설정하면 이미 빌드된 레이어를 재사용할 수 있습니다. 이를 통해 Docker 이미지 빌드를 최적화할 수 있습니다.\npipeline { agent { docker 'maven:3.6-jdk-8' } stages { stage('Build') { steps { script { docker.build(\"my-app\", \"--cache-from=my-app:latest .\") } } } } } 6️⃣ Jenkins Backup 및 Disaster Recovery 전략 Jenkins는 중요한 시스템이므로 정기적으로 백업을 수행하고, 장애 발생 시 복구할 수 있는 전략이 필요합니다.\n6.1. Jenkins Backup 전략 Jenkins의 백업은 주로 Jenkins Home 디렉토리를 백업하는 방식으로 이루어집니다. /var/lib/jenkins 디렉토리를 주기적으로 백업하여 복구 시점을 확보할 수 있습니다.\ntar -czf jenkins_backup_$(date +%F).tar.gz /var/lib/jenkins 6.2. Disaster Recovery 전략 Jenkins의 복구 전략으로는 **Jenkins Configuration as Code (JCasC)**를 활용하여 시스템 설정을 코드로 관리하고, 서버 장애 시 복구할 수 있도록 합니다.\nJenkins Configuration as Code 플러그인 설치 Jenkins 설정을 YAML 형식으로 저장하여 복구 시 빠르게 적용할 수 있습니다. jenkins: systemMessage: \"Jenkins configured by JCasC\" securityRealm: local: users: - id: admin password: 'admin' 이러한 Jenkins 모니터링 및 성능 최적화 방법들을 통해 Jenkins 환경을 효율적으로 관리하고, 안정적으로 운영할 수 있습니다."},"title":"Jenkins 모니터링 및 성능 최적화"},"/devops/jenkins/jenkins12/":{"data":{"":"","12-jenkins-보안-및-권한-관리#1️⃣2️⃣ \u003cstrong\u003eJenkins 보안 및 권한 관리\u003c/strong\u003e":"1️⃣2️⃣ Jenkins 보안 및 권한 관리 1️⃣ Jenkins의 보안 설정 (Manage Jenkins → Configure Global Security) Jenkins는 강력한 보안 설정을 제공하여 서버와 데이터의 안전을 보장합니다. 기본적으로 Configure Global Security 옵션을 통해 다양한 보안 기능을 설정할 수 있습니다.\n1.1. 보안 설정 메뉴 접근 Jenkins 대시보드에서 Manage Jenkins → Configure Global Security를 클릭합니다. 여기에서 Jenkins의 보안 관련 다양한 설정을 할 수 있습니다. 1.2. 설정할 수 있는 보안 항목들 Security Realm: 사용자 인증 방식 설정 (기본적으로 Jenkins 내장 사용자나 LDAP, Active Directory 등을 사용할 수 있습니다). Authorization: 사용자의 권한을 관리하는 방식 설정 (Role-Based Access Control(RBAC) 또는 Matrix-based security). Agent/Slave → Master Security: 마스터와 에이전트 간의 보안 연결 설정. CSRF Protection: Cross-Site Request Forgery 방지. 2️⃣ 사용자 인증 및 LDAP 연동 Jenkins는 LDAP을 통한 사용자 인증을 지원합니다. 이를 통해 조직의 중앙 인증 시스템을 사용하여 Jenkins에 로그인할 수 있습니다.\n2.1. LDAP 연동 설정 Manage Jenkins → Configure Global Security로 이동하여 Security Realm을 LDAP로 선택합니다. LDAP 서버의 주소와 필요한 인증 정보를 입력합니다. ldap: server: ldap://your-ldap-server:389 rootDN: dc=yourcompany,dc=com userSearchBase: ou=users,dc=yourcompany,dc=com groupSearchBase: ou=groups,dc=yourcompany,dc=com managerDN: cn=admin,dc=yourcompany,dc=com managerPassword: yourpassword 2.2. LDAP 그룹과 권한 연동 LDAP 그룹과 연동하여 각 사용자에 맞는 권한을 자동으로 부여할 수 있습니다. 예를 들어, Jenkins 관리자는 LDAP 그룹에서 admins로 설정된 사용자에게만 관리 권한을 부여할 수 있습니다.\n3️⃣ Role-Based Access Control (RBAC) 설정 RBAC(Role-Based Access Control)은 사용자가 역할에 따라 특정 작업을 수행할 수 있도록 제한하는 방식입니다. Jenkins에서는 이를 Role Strategy Plugin을 사용하여 설정할 수 있습니다.\n3.1. RBAC 플러그인 설치 Manage Jenkins → Manage Plugins → Available 탭에서 Role-based Authorization Strategy 플러그인을 설치합니다. 설치 후, Manage Jenkins → Configure Global Security에서 Authorization을 Role-Based Strategy로 설정합니다. 3.2. 역할 설정 Role Management 메뉴에서 다양한 역할을 설정하고, 각 역할에 대한 권한을 지정합니다. 예를 들어, admin 역할을 부여한 사용자는 모든 설정을 변경할 수 있으며, developer 역할을 가진 사용자는 빌드만 실행할 수 있습니다. role: admin: - Manage Jenkins - Configure System developer: - Job: Build - Job: View 4️⃣ Secrets 및 Credentials 관리 (withCredentials, Vault) Jenkins에서는 빌드 및 배포 과정에서 사용하는 비밀번호나 API 키와 같은 민감한 정보를 안전하게 관리할 수 있습니다. Credentials Plugin을 사용하여 이를 관리합니다.\n4.1. withCredentials 사용 withCredentials는 Jenkins Pipeline에서 비밀번호와 같은 비밀 데이터를 안전하게 사용할 수 있게 해줍니다.\npipeline { agent any environment { MY_SECRET = credentials('my-secret-id') // Jenkins Credentials에서 설정한 ID } stages { stage('Build') { steps { script { sh \"echo ${MY_SECRET}\" } } } } } 4.2. Vault 연동 Vault Plugin을 사용하여 HashiCorp Vault에서 비밀 데이터를 가져올 수 있습니다. 이를 통해 외부 시스템에서 안전하게 비밀 데이터를 관리할 수 있습니다.\nVault Plugin 설치 후, Vault 서버에 접근할 수 있는 설정을 진행합니다. Jenkins 파이프라인에서 Vault를 통해 비밀 정보를 사용할 수 있습니다. vaultSecret = vault( path: 'secret/myapp', secretValues: [[path: 'username', target: 'MY_USERNAME'], [path: 'password', target: 'MY_PASSWORD']] ) 5️⃣ Jenkins Agent 보안 (SSH, TLS) Jenkins에서는 마스터와 에이전트 간의 안전한 통신을 위해 SSH와 TLS를 사용할 수 있습니다.\n5.1. SSH를 통한 Agent 연결 Jenkins의 마스터와 에이전트는 SSH를 통해 연결할 수 있습니다. 이를 통해 에이전트 노드와 안전하게 통신할 수 있습니다.\nManage Jenkins → Manage Nodes and Clouds → New Node에서 에이전트 유형을 SSH로 선택합니다. SSH 키를 설정하여 인증을 진행합니다. 5.2. TLS 설정 Jenkins는 TLS를 사용하여 마스터와 에이전트 간의 안전한 연결을 할 수 있습니다. 이 설정은 Configure Global Security에서 TCP port for inbound agents 옵션을 SSL/TLS로 설정하여 활성화할 수 있습니다.\n6️⃣ Audit Log 및 보안 로그 분석 Jenkins에서 발생하는 모든 보안 관련 이벤트는 Audit Log로 기록됩니다. 이를 통해 보안 사고나 문제 발생 시 분석할 수 있습니다.\n6.1. Audit Plugin 설치 Audit Trail Plugin을 설치하면 Jenkins의 보안 로그를 외부 파일로 기록할 수 있습니다.\nManage Jenkins → Manage Plugins에서 Audit Trail Plugin을 설치합니다. 설치 후, Manage Jenkins → System Log에서 로그를 설정하고 기록할 경로를 지정할 수 있습니다. 6.2. 보안 로그 분석 로그에는 로그인 시도, 권한 변경, 작업 실행 등 중요한 정보가 기록됩니다. 이를 통해 누가 언제 어떤 작업을 했는지 추적할 수 있습니다.\ntail -f /var/log/jenkins/audit.log 이러한 Jenkins의 보안 및 권한 관리 기능을 잘 활용하면, 보다 안전하고 효율적인 Jenkins 환경을 구축할 수 있습니다."},"title":"Jenkins 보안 및 권한 관리"},"/devops/jenkins/jenkins13/":{"data":{"":"","13-jenkins-장애-대응-및-트러블슈팅#1️⃣3️⃣ \u003cstrong\u003eJenkins 장애 대응 및 트러블슈팅\u003c/strong\u003e":"1️⃣3️⃣ Jenkins 장애 대응 및 트러블슈팅 1️⃣ Jenkins Service 다운 문제 해결 Jenkins 서비스가 다운되면 먼저 시스템의 상태를 점검하고 원인을 찾아야 합니다. 서비스가 다운된 원인은 여러 가지가 있을 수 있으며, 가장 일반적인 원인으로는 메모리 부족, 디스크 공간 부족, 또는 잘못된 설정 등이 있습니다.\n1.1. 서비스 상태 점검 Jenkins 로그 확인:\n/var/log/jenkins/jenkins.log에 기록된 오류 메시지를 통해 문제를 파악할 수 있습니다. Jenkins가 시작되지 않으면, 로그 파일에서 특정 오류 메시지를 확인하고 이를 해결해야 합니다. tail -f /var/log/jenkins/jenkins.log 서비스 재시작:\nJenkins 서비스가 중단된 경우, 서비스 재시작을 시도해 볼 수 있습니다. sudo systemctl restart jenkins 디스크 사용량 확인:\n디스크가 가득 차면 Jenkins 서비스가 다운될 수 있습니다. 디스크 사용량을 확인하고 불필요한 파일을 삭제합니다. df -h 2️⃣ Pipeline 실행 오류 분석 (exit code, stderr) Pipeline 실행 중 발생하는 오류는 주로 exit code나 stderr를 통해 확인할 수 있습니다. 오류 코드와 오류 메시지를 분석하여 문제를 해결합니다.\n2.1. Exit Code 분석 Jenkins에서 빌드가 실패하면 exit code가 발생합니다. 일반적으로 exit code 0은 성공을 의미하고, 1 이상은 실패를 나타냅니다. 이때, stderr에서 오류 메시지를 확인할 수 있습니다.\n예시:\nscript { sh ''' echo \"Building project...\" exit 1 # Failure ''' } Exit Code 1은 일반적인 실패 코드이며, 이 경우 stderr에서 보다 구체적인 오류 메시지가 기록됩니다. 이를 통해 무엇이 잘못되었는지 파악할 수 있습니다.\n2.2. stderr 출력 확인 stderr는 실행 중 발생한 오류 메시지를 출력합니다. 빌드 실패 시, 해당 메시지를 분석하여 문제의 원인을 찾아 해결합니다.\necho \"Error: Missing dependency!\" \u003e\u00262 3️⃣ Git Checkout 오류 해결 (403 Forbidden, 403 Authentication Required) Git을 체크아웃할 때 발생하는 403 오류는 인증 관련 문제일 가능성이 큽니다. GitHub 또는 GitLab에서 인증이 제대로 이루어지지 않았을 때 발생할 수 있습니다.\n3.1. Git 인증 설정 확인 SSH 키를 사용하는 경우, Jenkins에서 사용 중인 SSH 키가 Git 서버에 등록되어 있는지 확인합니다.\nJenkins의 SSH 키를 Git 서버에 등록합니다. Git Plugin을 통해 Jenkins에서 Git 리포지토리에 접근할 수 있도록 설정합니다. 3.2. GitHub Personal Access Token 사용 **GitHub Personal Access Token (PAT)**을 생성하여 username:token 형식으로 인증을 진행할 수 있습니다. PAT을 Jenkins의 Credentials에 등록하여 사용할 수 있습니다. 3.3. Jenkins Credentials 확인 Manage Jenkins → Manage Credentials에서 Git credentials가 올바르게 설정되어 있는지 확인합니다. 4️⃣ Webhook 문제 해결 (403 No valid token, 404 Not Found) Webhook 설정 오류는 Jenkins와 외부 서비스(예: GitHub, GitLab) 간의 통신 문제로 발생할 수 있습니다. 주로 403 또는 404 오류가 발생하는데, 이는 토큰 문제나 잘못된 URL로 인해 발생합니다.\n4.1. Webhook URL 확인 Webhook URL이 정확한지 확인합니다. Webhook URL은 Jenkins 서버 주소와 **/github-webhook/**와 같은 추가 경로를 포함해야 합니다.\n예시:\nhttp://your-jenkins-server/github-webhook/ 4.2. 유효한 Token 확인 Secret Token을 설정하여 외부에서 발생하는 요청이 Jenkins에 유효한 요청인지 확인합니다. GitHub에서 Webhook을 설정할 때 Secret Token을 입력하고, Jenkins의 해당 Webhook 설정에서 이를 확인합니다. 5️⃣ OOM (Out of Memory) 문제 해결 (JVM Heap Dump, GC Logs) Out of Memory (OOM) 문제는 Jenkins가 메모리를 모두 소모하여 발생하는 문제입니다. 주로 JVM Heap 설정을 통해 메모리를 조정할 수 있습니다.\n5.1. JVM Heap Dump 설정 Jenkins의 JVM Heap 크기를 늘려야 할 수 있습니다. /etc/default/jenkins 파일을 수정하여 JVM 옵션을 설정합니다.\nJENKINS_JAVA_OPTIONS=\"-Xmx2g -Xms512m\" 5.2. GC Logs 확인 Garbage Collection (GC) 로그를 통해 메모리 누수나 불필요한 객체 생성 등을 확인할 수 있습니다.\nGC 로그를 활성화하려면 JVM 옵션에 -Xloggc:/var/log/jenkins/gc.log를 추가합니다. GC 로그를 분석하여 메모리 관리가 잘 되고 있는지 확인합니다. 예시:\n-Xloggc:/var/log/jenkins/gc.log 6️⃣ Disk Full 상태 해결 (Workspace Cleanup, Build Rotation) 디스크 공간 부족 문제는 Jenkins의 빌드가 쌓여서 발생할 수 있습니다. 이를 해결하려면 Workspace Cleanup 및 Build Rotation을 설정해야 합니다.\n6.1. Workspace Cleanup 빌드 후 생성된 불필요한 파일을 삭제하여 디스크 공간을 확보합니다. Workspace Cleanup Plugin을 사용하여 빌드가 끝난 후 자동으로 작업 공간을 청소할 수 있습니다. 6.2. Build Rotation 설정 Build Discarder 기능을 활용하여 오래된 빌드를 자동으로 삭제할 수 있습니다.\n각 Job의 Build Discarder를 설정하여 특정 조건에서 빌드를 삭제하도록 합니다. 예를 들어, 마지막 10번의 빌드만 유지하고 나머지는 삭제하는 방법을 설정할 수 있습니다. Max # of builds to keep: 10 위와 같은 방식으로 Jenkins에서 발생할 수 있는 다양한 장애 문제를 효과적으로 해결할 수 있습니다. 각 문제에 대해 로그 분석과 설정을 점검하여 빠르게 원인을 파악하고 해결하는 것이 중요합니다."},"title":"Jenkins 장애 대응 및 트러블슈팅"},"/devops/jenkins/jenkins14/":{"data":{"":"","14-jenkins-최신-트렌드-및-best-practices#1️⃣4️⃣ \u003cstrong\u003eJenkins 최신 트렌드 및 Best Practices\u003c/strong\u003e":"1️⃣4️⃣ Jenkins 최신 트렌드 및 Best Practices 1️⃣ GitOps와 Jenkins의 역할 GitOps는 Git을 소스 코드와 인프라의 단일 진실의 원천으로 사용하는 DevOps 관행입니다. Jenkins는 GitOps 파이프라인을 구현하는 데 중요한 역할을 하며, Jenkins를 사용하여 GitHub와 GitLab의 레포지토리에서 CI/CD 작업을 자동화할 수 있습니다.\n1.1. GitOps의 개념 GitOps의 핵심은 모든 인프라 변경을 Git에서 관리하고, 자동화된 파이프라인을 통해 이를 배포하는 것입니다. Jenkins는 GitOps 파이프라인의 핵심 도구로 사용될 수 있으며, GitHub Actions와 같은 다른 도구와 연동하여 효율적인 CI/CD 흐름을 구축할 수 있습니다.\n1.2. Jenkins와 GitOps의 연동 Jenkins는 GitOps에서 정의된 파이프라인을 트리거하는 역할을 합니다. Git 레포지토리에서 변경이 감지되면 Jenkins가 이를 받아들이고 자동으로 빌드 및 배포를 시작합니다. 예를 들어, Kubernetes 클러스터에 배포할 때 Jenkins는 Helm 차트와 같은 배포 도구를 사용하여 Git에서 정의된 설정을 적용합니다.\n2️⃣ Jenkins vs GitHub Actions vs GitLab CI/CD Jenkins는 오랫동안 CI/CD의 표준 도구로 자리잡았지만, GitHub Actions와 GitLab CI/CD 같은 도구들도 인기를 얻고 있습니다. 각 도구의 장단점을 이해하는 것이 중요합니다.\n2.1. Jenkins 유연성: Jenkins는 다양한 플러그인과 커스터마이징을 통해 복잡한 CI/CD 파이프라인을 구축할 수 있습니다. 확장성: 여러 플러그인과 다양한 노드를 이용한 분산 빌드를 지원합니다. 커뮤니티: Jenkins는 큰 커뮤니티와 다양한 지원을 받습니다. 2.2. GitHub Actions 통합성: GitHub Actions는 GitHub과 완벽하게 통합되며, GitHub 레포지토리와의 연동이 매우 편리합니다. 단순성: YAML 파일로 구성된 간단한 CI/CD 파이프라인을 설정할 수 있습니다. 2.3. GitLab CI/CD 기본 제공: GitLab은 CI/CD가 기본적으로 제공되며, GitLab 레포지토리와의 통합이 강력합니다. 통합 파이프라인: GitLab 자체에서 코드, 테스트, 빌드, 배포가 모두 처리되는 구조로, 엔터프라이즈 환경에 유리합니다. 3️⃣ Jenkins를 AWS/GCP/Azure에서 활용하는 방법 Jenkins는 AWS, GCP, Azure와 같은 클라우드 환경에서 매우 유용하게 활용될 수 있습니다. 각 클라우드 플랫폼에서 Jenkins를 설정하고 활용하는 방법을 살펴보겠습니다.\n3.1. AWS에서 Jenkins 사용 EC2 인스턴스에서 Jenkins를 실행하고, EBS를 사용하여 빌드 아티팩트를 저장할 수 있습니다. ECS 또는 EKS를 이용하여 Jenkins를 컨테이너화하여 배포할 수 있습니다. 3.2. GCP에서 Jenkins 사용 **GKE (Google Kubernetes Engine)**에 Jenkins를 배포하여, Kubernetes 환경에서 효율적으로 파이프라인을 실행할 수 있습니다. Cloud Storage를 활용하여 빌드 아티팩트를 저장하고, Cloud Build와 연동하여 파이프라인을 관리할 수 있습니다. 3.3. Azure에서 Jenkins 사용 **Azure Kubernetes Service (AKS)**에 Jenkins를 배포하여 클라우드 환경에서 Jenkins를 관리할 수 있습니다. Azure Blob Storage를 사용하여 빌드 아티팩트를 안전하게 저장할 수 있습니다. 4️⃣ Jenkins as Code (Jenkins Configuration as Code, JCasC) Jenkins Configuration as Code (JCasC)는 Jenkins의 모든 설정을 YAML 파일로 관리할 수 있게 해주는 플러그인입니다. 이를 통해 Jenkins 서버를 코드로 관리하고, 환경을 손쉽게 재구성할 수 있습니다.\n4.1. JCasC의 장점 자동화된 환경 설정: Jenkins 서버의 모든 설정을 코드로 관리하여 자동화된 방식으로 환경을 구축할 수 있습니다. 버전 관리: Jenkins 설정을 Git과 같은 버전 관리 시스템에 저장할 수 있어, 설정 변경을 추적하고 롤백할 수 있습니다. 4.2. JCasC 예제 JCasC를 사용하여 Jenkins 설정을 YAML로 관리하는 예제는 아래와 같습니다.\njenkins: systemMessage: \"Welcome to Jenkins\" numExecutors: 2 scmCheckoutRetryCount: 3 securityRealm: local: users: - id: \"admin\" password: \"admin123\" 5️⃣ Jenkins와 Terraform을 이용한 IaC 자동화 Jenkins와 Terraform을 함께 사용하면 인프라를 코드로 관리하고 자동으로 배포할 수 있습니다. Jenkins를 통해 Terraform을 실행하여 클라우드 환경을 자동으로 구축하고 관리할 수 있습니다.\n5.1. Terraform과 Jenkins의 통합 Terraform 플러그인을 사용하여 Jenkins에서 Terraform을 실행하고 인프라를 자동화할 수 있습니다. Jenkins 파이프라인에서 Terraform을 호출하여 환경을 구축하거나 변경 사항을 배포할 수 있습니다. 5.2. Terraform 실행 예제 pipeline { agent any stages { stage('Terraform Apply') { steps { script { sh 'terraform init' sh 'terraform apply -auto-approve' } } } } } 6️⃣ Serverless Jenkins (Jenkins X) Jenkins X는 서버리스 환경에서 Jenkins를 실행할 수 있는 도구입니다. Jenkins X는 Kubernetes 환경에 최적화되어 있으며, 자동화된 파이프라인을 GitOps 방식으로 관리할 수 있습니다.\n6.1. Jenkins X의 특징 Kubernetes와 통합: Jenkins X는 Kubernetes 클러스터에서 자동으로 빌드를 실행하고 배포할 수 있습니다. 자동화된 파이프라인: GitOps 방식으로 파이프라인을 관리하고, 코드 변경 시 자동으로 빌드, 테스트, 배포가 이루어집니다. 6.2. Jenkins X 설치 및 사용 Jenkins X는 jx CLI를 사용하여 설치하고 관리할 수 있습니다. 설치 후 Kubernetes 클러스터에 Jenkins X를 배포하여 CI/CD 파이프라인을 관리할 수 있습니다.\njx install Jenkins는 다양한 최신 트렌드와 Best Practices를 지원하며, 클라우드 환경에서의 사용, GitOps, Jenkins as Code 등의 최신 기술을 활용하여 효율적인 CI/CD 파이프라인을 구축할 수 있습니다. 이들 도구를 잘 활용하면 더욱 빠르고 안정적인 배포 환경을 만들 수 있습니다."},"title":"Jenkins 최신 트렌드 및 Best Practices"},"/devops/kubernetes/kubernetes/k8s00/":{"data":{"":"","#":"","1-쿠버네티스란#1️⃣ \u003cstrong\u003e쿠버네티스란?\u003c/strong\u003e":"📌 쿠버네티스(Kubernetes) 개요 1️⃣ 쿠버네티스란? 쿠버네티스(Kubernetes, K8s)는 컨테이너 오케스트레이션(Container Orchestration) 플랫폼으로, 다수의 컨테이너를 자동으로 배포하고 관리하는 시스템이다.\nGoogle이 내부에서 사용하던 Borg 시스템을 기반으로 개발한 오픈소스 프로젝트로, 현재는 CNCF(Cloud Native Computing Foundation)에서 관리하고 있다.\n🔹 컨테이너 오케스트레이션 개념 컨테이너 오케스트레이션은 다수의 컨테이너 애플리케이션을 자동으로 배포, 확장 및 운영하는 프로세스를 의미한다.\n컨테이너 단독 실행 환경에서는 다음과 같은 문제점이 발생할 수 있다:\n수동 관리 부담: 여러 개의 컨테이너를 배포하고 유지보수하는 작업이 복잡하다. 서비스 확장 어려움: 트래픽 증가 시 컨테이너를 동적으로 확장(scale-out)하기 어렵다. 장애 대응 부족: 컨테이너가 갑자기 종료되면, 이를 자동으로 복구하는 메커니즘이 필요하다. Kubernetes는 이러한 문제를 해결하기 위해 자동화된 컨테이너 관리 시스템을 제공한다.\n🔹 왜 쿠버네티스가 필요한가? (Docker Swarm, Mesos 비교) 컨테이너 오케스트레이션을 위한 다양한 솔루션이 존재하지만, Kubernetes가 사실상 업계 표준으로 자리 잡았다.\n특징 Kubernetes Docker Swarm Apache Mesos 확장성 대규모 클러스터 지원, 복잡한 서비스에도 적합 소규모 서비스에 적합, 대규모 확장 어려움 다양한 워크로드 지원, 빅데이터 및 VM도 관리 가능 자동화 기능 자동 복구, 오토스케일링, 헬스 체크 지원 기본적인 자동화 제공, 기능이 제한적 워크로드 자동 분배 가능 네트워크 모델 강력한 네트워크 정책 및 서비스 디스커버리 지원 간단한 네트워크 구조 네트워크 구성 자유롭지만 복잡함 커뮤니티 \u0026 지원 가장 활발한 오픈소스 프로젝트, 광범위한 지원 Docker 생태계와 통합이 쉬움 특정 기업(예: Twitter, Airbnb 등) 중심의 사용 사례 👉 Kubernetes가 더 적합한 경우\n복잡한 마이크로서비스 아키텍처 운영 대규모 트래픽을 처리해야 하는 서비스 다양한 환경(온프레미스, 클라우드 등)에서 동일한 배포 방식 유지 👉 Docker Swarm이 적합한 경우\n빠르고 간단한 컨테이너 오케스트레이션 필요 Kubernetes의 복잡성을 피하고 싶은 경우 👉 Apache Mesos가 적합한 경우\n컨테이너뿐만 아니라 VM, 빅데이터 처리까지 포함하는 복합적인 환경 운영 🔹 주요 기능 및 특징 Kubernetes는 컨테이너를 효과적으로 관리하기 위해 다양한 기능을 제공한다.\n✅ 자동 배포 및 스케일링\n원하는 개수의 컨테이너를 배포하고, 트래픽에 따라 자동으로 확장/축소 가능 HPA(Horizontal Pod Autoscaler)를 사용하여 CPU, 메모리 사용량에 따라 자동 조정 ✅ 서비스 디스커버리 \u0026 로드 밸런싱\nPod의 IP 변경 없이 서비스 이름으로 접근 가능 외부 트래픽을 관리하는 Ingress 기능 제공 ✅ 자동 복구(Self-healing)\n장애 발생 시 컨테이너를 자동으로 재시작 헬스 체크(Liveness, Readiness Probe)를 통해 상태 확인 ✅ 스토리지 오케스트레이션\n로컬 볼륨, 클라우드 스토리지(GCP, AWS, Azure) 지원 PVC(Persistent Volume Claim)으로 동적 스토리지 관리 가능 ✅ 보안 및 접근 제어\nRBAC(Role-Based Access Control)로 사용자 권한 관리 네트워크 정책(Network Policy)으로 Pod 간 통신 제어 ✅ 멀티 클러스터 및 하이브리드 클라우드 지원\n온프레미스와 클라우드를 혼합한 하이브리드 환경에서도 운영 가능 "},"title":"Kubernetes"},"/devops/kubernetes/kubernetes/k8s01/":{"data":{"-결론#\u003cstrong\u003e🔹 결론\u003c/strong\u003e":"쿠버네티스 아키텍처 (Kubernetes Architecture) 쿠버네티스(Kubernetes)는 컨테이너화된 애플리케이션을 자동으로 배포, 확장 및 운영할 수 있도록 설계된 오픈소스 컨테이너 오케스트레이션 플랫폼이다.\n이를 가능하게 하는 구조는 크게 컨트롤 플레인(Control Plane)과 워커 노드(Worker Node) 로 나뉜다.\n출처: Kubernetes 공식 문서\n1️⃣ 컨트롤 플레인 (Control Plane) 컨트롤 플레인은 쿠버네티스 클러스터의 두뇌 역할을 하며, 클러스터의 상태를 유지하고 변경 사항을 관리한다.\n여러 개의 노드를 제어하는 역할을 하며, 주요 구성 요소는 다음과 같다.\n🔹 1.1 API Server (kube-apiserver) 역할:\n모든 쿠버네티스 리소스를 관리하는 중심적인 컴포넌트 클라이언트(kubectl, Dashboard, 외부 API 요청)와 쿠버네티스 내부 컴포넌트 간의 통신을 처리 etcd와 직접 통신하여 클러스터 상태를 저장하고 조회 예제:\nkubectl을 사용하여 새로운 Pod를 생성할 때 API 서버가 이를 어떻게 처리하는지 보자.\nkubectl run nginx --image=nginx ✅ 위 명령어 실행 시 API 서버는 다음 과정을 수행한다:\n사용자의 요청을 JSON 형태로 변환 etcd에 저장하여 클러스터의 상태를 업데이트 Scheduler가 해당 Pod를 적절한 노드에 배치하도록 요청 Kubelet이 해당 노드에서 컨테이너를 실행 🔹 1.2 etcd 역할:\n쿠버네티스 클러스터의 상태 정보를 저장하는 분산 Key-Value 저장소 고가용성을 위해 다중 노드로 구성 가능 모든 클러스터 리소스(Pod, Service, ConfigMap 등)의 선언적 상태를 저장 예제:\netcd에 저장된 데이터를 확인하는 명령어:\nETCDCTL_API=3 etcdctl get /registry/pods --prefix --keys-only ✅ 위 명령어를 실행하면 현재 클러스터에 존재하는 모든 Pod 목록을 가져올 수 있다.\n🔹 1.3 Scheduler (kube-scheduler) 역할:\n새로운 Pod가 생성될 때 적절한 워커 노드를 선택 CPU, 메모리, 스토리지, 노드 라벨, affinity 등의 리소스 조건을 기반으로 스케줄링 결정 예제:\nPod가 특정 노드에서 실행되도록 강제하는 nodeSelector 설정:\napiVersion: v1 kind: Pod metadata: name: my-nginx spec: nodeSelector: disktype: ssd # SSD를 사용하는 노드에 배치 containers: - name: nginx image: nginx ✅ 위 설정을 적용하면 SSD 스토리지가 있는 노드에서만 Pod가 실행된다.\n🔹 1.4 Controller Manager (kube-controller-manager) 역할:\n쿠버네티스의 다양한 컨트롤러(Controller)를 실행하는 역할 각 리소스의 현재 상태(Current State)와 원하는 상태(Desired State) 를 비교하고 조정 주요 컨트롤러: ReplicaSet Controller: 원하는 개수만큼 Pod 유지 Node Controller: 노드 상태 감지 및 장애 감지 Service Account Controller: 서비스 계정과 API 토큰 관리 예제:\nPod 개수를 자동으로 유지하는 ReplicaSet 예제:\napiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx-replicaset spec: replicas: 3 # 항상 3개의 Pod 유지 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ✅ 만약 하나의 Pod가 삭제되면 Controller Manager가 자동으로 새로운 Pod를 생성하여 3개를 유지한다.\n2️⃣ 워커 노드 (Worker Node) 컨트롤 플레인에서 지시한 작업을 실제 수행하는 노드이다.\n각 워커 노드는 다음과 같은 핵심 컴포넌트로 구성된다.\n🔹 2.1 Kubelet 역할:\n노드에서 실행되는 핵심 에이전트 API 서버로부터 Pod 배치 명령을 받아 실행 컨테이너 상태를 모니터링하고, 장애 발생 시 재시작 예제:\n특정 노드의 Kubelet 상태 확인:\nsystemctl status kubelet ✅ Kubelet이 실행 중이어야 노드가 정상적으로 작동한다.\n🔹 2.2 Kube Proxy 역할:\nKubernetes 네트워크를 관리하는 서비스 Service를 통해 Pod 간 트래픽을 라우팅 iptables 또는 IPVS 기반으로 트래픽을 처리 예제:\n쿠버네티스 서비스 목록 확인:\nkubectl get svc ✅ Service가 생성되면 Kube Proxy가 이를 인식하고 네트워크 경로를 자동으로 설정한다.\n🔹 2.3 컨테이너 런타임 (Container Runtime) 역할:\n컨테이너 실행을 담당하는 소프트웨어 대표적인 런타임: Docker containerd CRI-O Firecracker 예제:\n현재 노드에서 사용 중인 컨테이너 런타임 확인:\nkubectl get nodes -o wide ✅ CONTAINER-RUNTIME 필드에서 어떤 런타임이 사용되는지 확인할 수 있다.\n3️⃣ 서비스 디스커버리 및 네트워킹 개념 쿠버네티스에서 Pod는 동적으로 생성/삭제되므로, IP 주소가 변경될 가능성이 높다.\n이를 해결하기 위해 Service와 Ingress를 활용하여 트래픽을 안정적으로 관리한다.\n🔹 3.1 Cluster Networking 쿠버네티스 네트워킹의 주요 특징:\n모든 Pod는 고유한 IP를 가지며 동일한 네트워크 내에서 서로 통신 가능 CNI (Container Network Interface)를 사용하여 네트워크 정책을 관리\n(예: Flannel, Calico, Cilium) 🔹 3.2 Service를 통한 트래픽 관리 Pod가 삭제되더라도 고정된 엔드포인트를 제공하는 Service 예제:\napiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 ✅ 위 설정을 적용하면 my-service라는 고정된 DNS 이름을 사용하여 Pod와 통신할 수 있다.\n🔹 결론 쿠버네티스의 아키텍처는 컨트롤 플레인과 워커 노드로 구성되며,\n각 컴포넌트가 유기적으로 작동하여 컨테이너를 안정적으로 관리한다.\n쿠버네티스를 운영할 때는 API Server, Scheduler, Controller Manager, Kubelet, Kube Proxy의 역할을 명확히 이해하는 것이 중요하다.","1-컨트롤-플레인-control-plane#\u003cstrong\u003e1️⃣ 컨트롤 플레인 (Control Plane)\u003c/strong\u003e":"","2-워커-노드-worker-node#\u003cstrong\u003e2️⃣ 워커 노드 (Worker Node)\u003c/strong\u003e":"","3-서비스-디스커버리-및-네트워킹-개념#\u003cstrong\u003e3️⃣ 서비스 디스커버리 및 네트워킹 개념\u003c/strong\u003e":"","쿠버네티스-아키텍처-kubernetes-architecture#\u003cstrong\u003e쿠버네티스 아키텍처 (Kubernetes Architecture)\u003c/strong\u003e":""},"title":"Architecture"},"/devops/kubernetes/kubernetes/k8s02/":{"data":{"-결론#\u003cstrong\u003e🔹 결론\u003c/strong\u003e":"쿠버네티스 설치 및 환경 구성 (Kubernetes Installation) 쿠버네티스를 구축하는 방법은 여러 가지가 있다.\nMinikube: 로컬 개발 환경에서 쿠버네티스를 실행하는 가장 쉬운 방법 kubeadm: 온프레미스 또는 가상화 환경에서 쿠버네티스 클러스터를 직접 구축 클라우드 서비스 (EKS, GKE, AKS): AWS, GCP, Azure에서 제공하는 매니지드 쿠버네티스 서비스 각 방법을 자세히 살펴보자.\n1️⃣ Minikube를 활용한 로컬 환경 구축 🔹 Minikube란? 개발 및 테스트 용도로 로컬에서 쉽게 쿠버네티스를 실행할 수 있도록 도와주는 도구 하나의 노드로 구성된 싱글 노드 클러스터를 생성 Docker, VirtualBox, Hyper-V 등의 가상 머신에서 실행 가능 🔹 Minikube 설치 (Linux, macOS, Windows) 📌 Linux/macOS 설치 # Minikube 다운로드 및 실행 권한 부여 curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube 📌 Windows 설치 (PowerShell 사용) choco install minikube ✅ Windows 사용자는 Hyper-V 또는 VirtualBox를 미리 설치해야 한다.\n🔹 Minikube 클러스터 시작 minikube start --driver=docker ✅ Minikube는 기본적으로 Docker 컨테이너를 VM처럼 사용하여 쿠버네티스를 실행한다.\n🔹 Minikube 상태 확인 minikube status 🔹 Minikube에서 테스트용 Nginx 실행 kubectl create deployment nginx --image=nginx kubectl expose deployment nginx --type=NodePort --port=80 minikube service nginx --url ✅ Minikube에서 실행된 서비스의 URL을 반환한다.\n2️⃣ kubeadm을 이용한 클러스터 구축 🔹 kubeadm이란? 실제 운영 환경에서 쿠버네티스를 구축할 때 사용하는 CLI 도구 빠르고 간단하게 멀티 노드 쿠버네티스 클러스터를 생성할 수 있다. 온프레미스 환경(VMware, Bare Metal) 또는 클라우드 VM에서 사용 가능 🔹 kubeadm 설치 📌 패키지 업데이트 및 의존성 설치 (Ubuntu 기준) sudo apt-get update \u0026\u0026 sudo apt-get install -y apt-transport-https ca-certificates curl 📌 쿠버네티스 패키지 저장소 추가 curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list sudo apt-get update 📌 kubeadm, kubelet, kubectl 설치 sudo apt-get install -y kubelet kubeadm kubectl 🔹 쿠버네티스 클러스터 초기화 sudo kubeadm init ✅ 실행 후, kubeadm이 Master 노드를 설정하고 kubeadm join 명령어를 출력한다.\n🔹 워커 노드 클러스터에 추가 각 워커 노드에서 다음 명령어 실행 (마스터 노드에서 제공한 kubeadm join 명령어 사용):\nsudo kubeadm join \u003cmaster-ip\u003e:6443 --token \u003ctoken\u003e --discovery-token-ca-cert-hash sha256:\u003chash\u003e 🔹 kubectl을 사용하여 클러스터 확인 kubectl get nodes ✅ 정상적으로 모든 노드가 Ready 상태로 나오면 성공적으로 클러스터가 구성된 것이다.\n3️⃣ 클라우드 기반 쿠버네티스 서비스 (EKS, GKE, AKS) 🔹 Amazon Elastic Kubernetes Service (EKS) 📌 EKS CLI 설치\ncurl -O https://s3.us-west-2.amazonaws.com/amazon-eks/eksctl/latest/linux/eksctl chmod +x eksctl sudo mv eksctl /usr/local/bin ✅ EKS는 AWS에서 관리하는 쿠버네티스 서비스로, 클러스터 생성이 자동화되어 있어 편리하다.\n📌 EKS 클러스터 생성\neksctl create cluster --name my-cluster --region ap-northeast-2 ✅ EKS에서 쿠버네티스 클러스터가 자동으로 배포된다.\n🔹 Google Kubernetes Engine (GKE) 📌 GKE 클러스터 생성\ngcloud container clusters create my-cluster --zone=us-central1-a ✅ GKE는 Google Cloud에서 제공하는 쿠버네티스 매니지드 서비스이다.\n🔹 Azure Kubernetes Service (AKS) 📌 AKS 클러스터 생성\naz aks create --resource-group myResourceGroup --name myAKSCluster --node-count 3 --generate-ssh-keys ✅ AKS는 Azure에서 제공하는 쿠버네티스 서비스로, Azure CLI를 사용하여 배포할 수 있다.\n4️⃣ 쿠버네티스 설치 방식 비교 설치 방식 장점 단점 Minikube 간단한 설치, 로컬 환경 테스트에 적합 프로덕션 환경에는 부적합 kubeadm 직접 클러스터 설정 가능, 운영 환경 구축 가능 네트워크, 보안 설정 추가 필요 EKS, GKE, AKS 클라우드 기반 자동 관리, 확장성 우수 비용 발생, 클라우드 벤더 종속성 🔹 결론 쿠버네티스를 운영하는 방법은 로컬 개발 환경(Minikube), 직접 구축(kubeadm), 클라우드 기반(EKS, GKE, AKS) 등 다양하다.\n개발/테스트 환경에서는 Minikube가 유용하며, 운영 환경에서는 kubeadm 또는 클라우드 서비스 사용을 추천한다.","1-minikube를-활용한-로컬-환경-구축#\u003cstrong\u003e1️⃣ Minikube를 활용한 로컬 환경 구축\u003c/strong\u003e":"","2-kubeadm을-이용한-클러스터-구축#\u003cstrong\u003e2️⃣ kubeadm을 이용한 클러스터 구축\u003c/strong\u003e":"","3-클라우드-기반-쿠버네티스-서비스-eks-gke-aks#\u003cstrong\u003e3️⃣ 클라우드 기반 쿠버네티스 서비스 (EKS, GKE, AKS)\u003c/strong\u003e":"","4-쿠버네티스-설치-방식-비교#\u003cstrong\u003e4️⃣ 쿠버네티스 설치 방식 비교\u003c/strong\u003e":"","쿠버네티스-설치-및-환경-구성-kubernetes-installation#\u003cstrong\u003e쿠버네티스 설치 및 환경 구성 (Kubernetes Installation)\u003c/strong\u003e":""},"title":"Installation"},"/devops/kubernetes/kubernetes/k8s03/":{"data":{"-결론#\u003cstrong\u003e✅ 결론\u003c/strong\u003e":"쿠버네티스 오브젝트 (Kubernetes Core Objects) 쿠버네티스에서 오브젝트(Object)란 쿠버네티스 클러스터에서 관리하는 리소스를 의미한다.\n모든 오브젝트는 YAML 또는 JSON 형식으로 정의되며, API를 통해 생성 및 관리할 수 있다.\n아래 주요 오브젝트들을 각각 상세하게 설명할게.\n1️⃣ Pod (파드) 🔹 Pod란? 쿠버네티스에서 배포할 수 있는 가장 작은 단위 하나 이상의 컨테이너를 포함하며, 같은 네트워크 및 스토리지 환경을 공유 같은 Pod 내 컨테이너는 localhost로 통신 가능 🔹 Pod의 구조 🔽 Pod 내부 구성 요소 (컨테이너, 네트워크, 볼륨)\nPod ├── 컨테이너(Container) │ ├── 애플리케이션 실행 (예: Nginx, Python) │ ├── 환경 변수 설정 │ ├── 파일 시스템 공유 │ ├── 같은 네트워크 환경 사용 ├── 네트워크 (IP 주소) │ ├── Pod 내부에서는 localhost로 통신 가능 │ ├── 다른 Pod과는 Service를 통해 통신 ├── 볼륨(Volume) ├── 같은 Pod 내 컨테이너 간 데이터 공유 가능 🔽 Pod 데이터 흐름 예시 (단일 컨테이너)\n사용자 요청 -\u003e Service -\u003e Pod -\u003e 컨테이너 실행 🔹 Pod 예제 (YAML) apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 ✅ kubectl apply -f pod.yaml 명령어로 실행 가능\n2️⃣ Deployment (디플로이먼트) 🔹 Deployment란? Pod을 자동으로 관리하는 고급 컨트롤러 Pod을 배포하고, 롤링 업데이트 및 롤백 가능 특정 개수의 Pod을 유지하여 무중단 배포 지원 🔹 Deployment 구조 🔽 Deployment → ReplicaSet → Pod\n사용자 요청 ↓ Deployment (배포 관리) ↓ ReplicaSet (Pod 개수 관리) ↓ Pod (애플리케이션 실행) 🔹 Deployment 예제 apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: nginx image: nginx ports: - containerPort: 80 ✅ replicas: 3 → 3개의 Pod을 항상 유지\n3️⃣ Service (서비스) 🔹 Service란? Pod은 동적으로 IP가 변하기 때문에, 이를 해결하기 위한 고정된 네트워크 엔드포인트 Pod의 IP를 자동으로 탐색하고, 로드 밸런싱 기능 제공 🔹 Service 종류 ClusterIP (기본값) → 내부에서만 접근 가능 NodePort → 클러스터 외부에서 접근 가능 LoadBalancer → 클라우드 환경에서 로드 밸런서 사용 🔽 Service 데이터 흐름\n사용자 요청 ↓ Service (고정 IP 제공, 로드 밸런싱) ↓ Pod (애플리케이션 실행) 🔹 Service 예제 apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 80 type: NodePort ✅ kubectl apply -f service.yaml 명령어로 실행 가능\n4️⃣ Ingress (인그레스) 🔹 Ingress란? 외부에서 클러스터 내부의 여러 서비스로 트래픽을 라우팅하는 역할 도메인 기반 라우팅, HTTPS 적용, 인증 기능 제공 🔽 Ingress 데이터 흐름\n사용자 요청 (도메인 예: mysite.com) ↓ Ingress (도메인 기반 트래픽 라우팅) ↓ Service (고정 IP 제공) ↓ Pod (애플리케이션 실행) 🔹 Ingress 예제 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: - host: mysite.com http: paths: - path: / pathType: Prefix backend: service: name: my-service port: number: 80 ✅ kubectl apply -f ingress.yaml 명령어로 실행 가능\n5️⃣ ConfigMap \u0026 Secret 🔹 ConfigMap (설정 값 저장) 환경 변수, 설정 파일 등을 저장하는 오브젝트 🔽 ConfigMap 예제\napiVersion: v1 kind: ConfigMap metadata: name: my-config data: database_url: \"mysql://db.example.com\" 🔹 Secret (민감한 정보 저장) 비밀번호, API 키, 인증서 등 보안이 필요한 데이터를 저장 🔽 Secret 예제\napiVersion: v1 kind: Secret metadata: name: my-secret type: Opaque data: password: dGVzdHBhc3N3b3Jk # base64 인코딩된 값 ✅ echo -n \"testpassword\" | base64 명령어로 암호화 가능\n6️⃣ PersistentVolume (PV) \u0026 PersistentVolumeClaim (PVC) 🔹 PV \u0026 PVC란? Pod이 종료되어도 데이터가 유지되도록 하는 스토리지 PV: 실제 스토리지 리소스를 정의 PVC: Pod에서 사용하는 요청 단위 🔽 스토리지 데이터 흐름\nPod 요청 ↓ PersistentVolumeClaim (PVC) ↓ PersistentVolume (PV) ↓ 스토리지 (NFS, AWS EBS, Ceph 등) 🔹 PV \u0026 PVC 예제 apiVersion: v1 kind: PersistentVolume metadata: name: my-pv spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: manual hostPath: path: \"/mnt/data\" apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi ✅ Pod에서 volumeMounts 설정하여 사용 가능\n✅ 결론 쿠버네티스의 주요 오브젝트(Pod, Deployment, Service, Ingress 등)는 애플리케이션을 배포하고 운영하는 데 필수적인 요소이다.\n위 내용을 이해하면 쿠버네티스의 기본적인 구조를 확실히 알 수 있다! 🚀","1-pod-파드#\u003cstrong\u003e1️⃣ Pod (파드)\u003c/strong\u003e":"","2-deployment-디플로이먼트#\u003cstrong\u003e2️⃣ Deployment (디플로이먼트)\u003c/strong\u003e":"","3-service-서비스#\u003cstrong\u003e3️⃣ Service (서비스)\u003c/strong\u003e":"","4-ingress-인그레스#\u003cstrong\u003e4️⃣ Ingress (인그레스)\u003c/strong\u003e":"","5-configmap--secret#\u003cstrong\u003e5️⃣ ConfigMap \u0026amp; Secret\u003c/strong\u003e":"","6-persistentvolume-pv--persistentvolumeclaim-pvc#\u003cstrong\u003e6️⃣ PersistentVolume (PV) \u0026amp; PersistentVolumeClaim (PVC)\u003c/strong\u003e":"","쿠버네티스-오브젝트-kubernetes-core-objects#\u003cstrong\u003e쿠버네티스 오브젝트 (Kubernetes Core Objects)\u003c/strong\u003e":""},"title":"Core Objects"},"/devops/kubernetes/kubernetes/k8s04/":{"data":{"#":"","-결론#\u003cstrong\u003e✅ 결론\u003c/strong\u003e":"Workload \u0026 Deploy (워크로드 및 배포)쿠버네티스에서 **워크로드(Workload)**란 클러스터에서 실행되는 애플리케이션, 서비스, 작업(Job) 등을 의미한다.\n쿠버네티스는 다양한 배포(Deploy) 전략을 지원하며, 이를 통해 안정적인 서비스 운영이 가능하다.\n1️⃣ ReplicaSet (레플리카셋) 🔹 ReplicaSet이란? 특정 개수의 Pod을 항상 유지하도록 관리하는 컨트롤러 Pod이 죽으면 새로운 Pod을 생성하여 가용성을 보장 Deployment에서 내부적으로 사용됨 (직접 사용하는 경우는 드뭄) 🔽 ReplicaSet 데이터 흐름\n사용자 요청 ↓ ReplicaSet (Pod 개수 관리) ↓ Pod (애플리케이션 실행) 🔹 ReplicaSet 예제 apiVersion: apps/v1 kind: ReplicaSet metadata: name: my-replicaset spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: nginx image: nginx ports: - containerPort: 80 ✅ kubectl apply -f replicaset.yaml 명령어로 실행 가능\n✅ kubectl get rs 로 상태 확인\n2️⃣ Deployment (디플로이먼트) 🔹 Deployment란? ReplicaSet을 관리하며, 자동화된 롤링 업데이트 및 롤백 기능 제공 서비스 무중단 배포를 가능하게 함 🔽 Deployment 데이터 흐름\n사용자 요청 ↓ Deployment (배포 관리) ↓ ReplicaSet (Pod 개수 관리) ↓ Pod (애플리케이션 실행) 🔹 Deployment 예제 apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 3 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: nginx image: nginx ports: - containerPort: 80 ✅ kubectl apply -f deployment.yaml 명령어로 실행\n✅ kubectl rollout status deployment my-deployment 로 배포 상태 확인\n3️⃣ StatefulSet (스테이트풀셋) 🔹 StatefulSet이란? 상태를 유지해야 하는 애플리케이션(예: 데이터베이스, Kafka, Redis 등)에 사용 각 Pod에 고유한 네트워크 ID(hostname) 제공 PersistentVolume을 자동으로 연결하여 Pod이 재시작되더라도 데이터를 유지 🔽 StatefulSet 데이터 흐름\n사용자 요청 ↓ StatefulSet (상태 유지 관리) ↓ Pod (각각 고유한 ID와 저장소 유지) ↓ 스토리지 (PersistentVolume) 🔹 StatefulSet 예제 apiVersion: apps/v1 kind: StatefulSet metadata: name: my-db spec: serviceName: \"my-db\" replicas: 3 selector: matchLabels: app: database template: metadata: labels: app: database spec: containers: - name: postgres image: postgres ports: - containerPort: 5432 volumeMounts: - name: data mountPath: /var/lib/postgresql/data volumeClaimTemplates: - metadata: name: data spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi ✅ kubectl apply -f statefulset.yaml 명령어로 실행\n4️⃣ DaemonSet (데몬셋) 🔹 DaemonSet이란? 모든 노드에서 특정 Pod을 실행하도록 보장 로깅, 모니터링, 보안 에이전트 같은 시스템 레벨의 애플리케이션 실행에 사용됨 (예: Falco, Fluentd, Prometheus 노드 익스포터) 🔽 DaemonSet 데이터 흐름\n모든 노드에서 ↓ DaemonSet (각 노드에 Pod 배포) ↓ Pod (노드별 필수 애플리케이션 실행) 🔹 DaemonSet 예제 apiVersion: apps/v1 kind: DaemonSet metadata: name: my-daemonset spec: selector: matchLabels: app: monitoring-agent template: metadata: labels: app: monitoring-agent spec: containers: - name: node-exporter image: prom/node-exporter ✅ kubectl apply -f daemonset.yaml 명령어로 실행\n5️⃣ Job \u0026 CronJob 🔹 Job이란? 단발성 작업을 실행하는 워크로드 실행이 완료되면 자동 종료됨 🔽 Job 데이터 흐름\n사용자 요청 (일회성 작업) ↓ Job (실행 및 완료) ↓ Pod 종료 🔹 Job 예제 apiVersion: batch/v1 kind: Job metadata: name: my-job spec: template: spec: containers: - name: batch-job image: busybox command: [\"echo\", \"Hello, Kubernetes!\"] restartPolicy: Never ✅ kubectl apply -f job.yaml 명령어로 실행\n🔹 CronJob이란? 스케줄링된 주기적인 작업을 실행 crontab과 유사하게 작동 🔽 CronJob 데이터 흐름\n설정된 일정 (예: 매일 12시) ↓ CronJob 실행 ↓ Job 실행 후 완료 🔹 CronJob 예제 apiVersion: batch/v1 kind: CronJob metadata: name: my-cronjob spec: schedule: \"0 12 * * *\" # 매일 12시에 실행 jobTemplate: spec: template: spec: containers: - name: scheduled-task image: busybox command: [\"echo\", \"Scheduled Task Running!\"] restartPolicy: Never ✅ kubectl apply -f cronjob.yaml 명령어로 실행\n✅ 결론 쿠버네티스는 다양한 워크로드 배포 전략을 지원하며, 애플리케이션의 특성에 맞게 적절한 컨트롤러(Deployment, StatefulSet, DaemonSet 등)를 선택하는 것이 중요하다.","1-replicaset-레플리카셋#\u003cstrong\u003e1️⃣ ReplicaSet (레플리카셋)\u003c/strong\u003e":"","2-deployment-디플로이먼트#\u003cstrong\u003e2️⃣ Deployment (디플로이먼트)\u003c/strong\u003e":"","3-statefulset-스테이트풀셋#\u003cstrong\u003e3️⃣ StatefulSet (스테이트풀셋)\u003c/strong\u003e":"","4-daemonset-데몬셋#\u003cstrong\u003e4️⃣ DaemonSet (데몬셋)\u003c/strong\u003e":"","5-job--cronjob#\u003cstrong\u003e5️⃣ Job \u0026amp; CronJob\u003c/strong\u003e":"","workload--deploy-워크로드-및-배포#\u003cstrong\u003eWorkload \u0026amp; Deploy (워크로드 및 배포)\u003c/strong\u003e":""},"title":"Workload\u0026 Deploy"},"/devops/kubernetes/kubernetes/k8s05/":{"data":{"#":"","-결론#\u003cstrong\u003e✅ 결론\u003c/strong\u003e":"Kubernetes Networking (네트워킹) 1️⃣ 쿠버네티스 네트워킹 개요 🔹 쿠버네티스 네트워킹의 특징 쿠버네티스 네트워크 모델은 기존 VM 네트워크와 다르게 설계되었어.\n모든 Pod은 고유한 IP를 가진다. Pod 간 NAT 없이 직접 통신 가능하다. 각 노드는 클러스터 네트워크를 공유한다. 🔽 쿠버네티스 네트워크 개요\n(출처: Kubernetes 공식 문서)\n2️⃣ Pod 네트워킹 🔹 Pod 간 통신 쿠버네티스의 기본 네트워크 모델은 Pod 간 직접 통신을 허용 동일한 노드뿐만 아니라 다른 노드의 Pod과도 NAT 없이 통신 가능 🔽 Pod 네트워크 개요 (CNI 플러그인 활용)\n(출처: Kubernetes 공식 문서)\n🔹 네트워크 플러그인 (CNI: Container Network Interface) Pod 간 네트워킹을 구현하는 방법에는 다양한 CNI 플러그인이 있어.\nFlannel: 가장 간단한 네트워크 오버레이 Calico: 네트워크 정책 및 BGP 지원 Cilium: eBPF 기반 고성능 네트워크 Weave: 자동 멀티캐스트 지원 🔽 CNI 구성 예시\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: allow-app spec: podSelector: matchLabels: app: my-app policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: web ✅ 특정 Pod(app=my-app)으로 web Pod만 접근 가능하도록 제한\n3️⃣ Service (서비스) 🔹 Service란? 쿠버네티스에서 Pod은 동적으로 생성되고 사라지기 때문에, 항상 같은 IP를 가지지 않아.\n서비스(Service)는 Pod의 변경에도 불구하고 일정한 엔드포인트(IP)를 제공하여, 안정적인 네트워크 통신이 가능하도록 해줘.\n🔽 Service 데이터 흐름\n클라이언트 요청 ↓ Service (고정된 엔드포인트) ↓ Pod으로 트래픽 전달 🔹 Service 유형 ClusterIP (기본값) → 내부 통신용 가상 IP NodePort → 노드의 포트를 통해 외부에서 접근 가능 LoadBalancer → 클라우드 환경에서 외부 LB 사용 ExternalName → 외부 도메인으로 매핑 🔽 Service 아키텍처\n🔹 Service 예제 apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 ✅ kubectl apply -f service.yaml\n4️⃣ Ingress (인그레스) 🔹 Ingress란? 외부에서 쿠버네티스 내부로 HTTP(S) 요청을 라우팅 도메인 기반 트래픽 제어 SSL/TLS 종단 처리 가능 🔽 Ingress 아키텍처\n🔹 Ingress 예제 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: my-ingress spec: rules: - host: myapp.example.com http: paths: - path: / pathType: Prefix backend: service: name: my-service port: number: 80 ✅ kubectl apply -f ingress.yaml\n✅ myapp.example.com으로 접속 시 내부 my-service로 연결됨\n5️⃣ Network Policy (네트워크 정책) 🔹 Network Policy란? 기본적으로 쿠버네티스 네트워크는 모든 Pod 간 통신을 허용하지만,\n보안 강화를 위해 특정 Pod만 접근하도록 설정할 수도 있어.\n이를 Network Policy를 통해 관리할 수 있어.\n🔽 네트워크 정책 예제 (특정 Pod만 허용)\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} # 모든 Pod 대상 policyTypes: - Ingress ✅ 모든 인바운드 트래픽 차단\n✅ kubectl apply -f network-policy.yaml\n✅ 결론쿠버네티스 네트워킹은 Pod 간 통신, 서비스 디스커버리, 외부 트래픽 관리, 보안 정책 적용까지 매우 중요한 요소.","1-쿠버네티스-네트워킹-개요#\u003cstrong\u003e1️⃣ 쿠버네티스 네트워킹 개요\u003c/strong\u003e":"","2-pod-네트워킹#\u003cstrong\u003e2️⃣ Pod 네트워킹\u003c/strong\u003e":"","3-service-서비스#\u003cstrong\u003e3️⃣ Service (서비스)\u003c/strong\u003e":"","4-ingress-인그레스#\u003cstrong\u003e4️⃣ Ingress (인그레스)\u003c/strong\u003e":"","5-network-policy-네트워크-정책#\u003cstrong\u003e5️⃣ Network Policy (네트워크 정책)\u003c/strong\u003e":"","kubernetes-networking-네트워킹#\u003cstrong\u003eKubernetes Networking (네트워킹)\u003c/strong\u003e":""},"title":"Networking"},"/devops/kubernetes/kubernetes/k8s06/":{"data":{"-결론#✅ 결론":"쿠버네티스 보안을 강화하려면 네트워크 보안, 컨테이너 보안, RBAC, 이미지 보안을 종합적으로 적용필요.\nNetworkPolicy를 활용해 불필요한 통신 차단 Rootless 컨테이너, seccomp, AppArmor로 컨테이너 보안 강화 RBAC으로 사용자 권한 최소화 SBOM, 취약점 스캐닝으로 안전한 이미지를 사용 보안이 강한 쿠버네티스를 운영하면 공격을 예방하고, 안정적인 서비스 제공이 가능! 🚀🔥","1-쿠버네티스-보안-개요#1️⃣ 쿠버네티스 보안 개요":"","2-네트워크-보안#2️⃣ 네트워크 보안":"","3-컨테이너-보안#3️⃣ 컨테이너 보안":"","4-rbac-역할-기반-접근-제어#4️⃣ RBAC (역할 기반 접근 제어)":"","5-이미지-보안-sbom--취약점-스캔#5️⃣ 이미지 보안 (SBOM \u0026amp; 취약점 스캔)":"Kubernetes Security (쿠버네티스 보안) 1️⃣ 쿠버네티스 보안 개요 쿠버네티스 환경에서는 Pod 격리, 네트워크 보안, 접근 제어 등이 중요해.\n잘못된 설정이 보안 취약점을 만들 수 있으므로, 보안 모범 사례를 따라야 해.\n1. 쿠버네티스 보안 주요 영역 ✅ 네트워크 보안 → 네트워크 정책 적용 (NetworkPolicy)\n✅ 컨테이너 보안 → Rootless 컨테이너, seccomp, AppArmor\n✅ RBAC(Role-Based Access Control) → 사용자 권한 관리\n✅ 이미지 보안 → SBOM(소프트웨어 구성 분석), 취약점 스캔\n🔽 쿠버네티스 보안 개요 2️⃣ 네트워크 보안 기본적으로 Pod 간 모든 통신이 허용되지만, 보안 강화를 위해 네트워크 정책을 적용할 수 있어.\n1. Network Policy 개념 ✅ 특정 Pod 간 통신만 허용/차단 가능\n✅ Ingress (들어오는 트래픽) / Egress (나가는 트래픽) 제어 가능\n✅ CNI 플러그인이 지원해야 동작함\n🔽 NetworkPolicy 동작 흐름 2. Network Policy 예제 (웹서버만 DB 접속 허용) apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-web-to-db spec: podSelector: matchLabels: app: db policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: web ✅ kubectl apply -f network-policy.yaml 실행\n✅ web Pod만 db Pod에 접근 가능\n3️⃣ 컨테이너 보안 1. Rootless 컨테이너 ✅ 기본적으로 컨테이너는 root 권한을 가질 수 있어 보안 위험\n✅ Rootless 모드로 실행하면 비특권(non-root) 사용자가 컨테이너를 실행 가능\n🔽 Rootless 컨테이너 개념 ✅ Rootless 컨테이너 실행 예제\ndocker run -u 1000:1000 --rm -it ubuntu bash ✅ -u 1000:1000 옵션을 사용해 일반 사용자로 실행\n2. seccomp (시스템 호출 제한) ✅ 컨테이너가 **불필요한 시스템 호출(Syscall)**을 못하게 차단\n✅ 공격자가 악성 코드로 시스템을 탈취하는 것을 방지\n🔽 seccomp 예제 (불필요한 시스템 호출 차단)\napiVersion: v1 kind: Pod metadata: name: seccomp-demo annotations: seccomp.security.alpha.kubernetes.io/pod: runtime/default spec: containers: - name: app image: ubuntu command: [ \"sleep\", \"3600\" ] ✅ runtime/default 정책을 적용하여 보안 강화\n3. AppArmor (리눅스 보안 프로파일) ✅ 컨테이너가 특정 파일, 네트워크, 리소스에 접근하지 못하게 제한\n✅ 시스템 주요 파일에 대한 읽기/쓰기 제한 가능\n🔽 AppArmor 예제 (읽기 전용 파일 시스템)\napiVersion: v1 kind: Pod metadata: name: apparmor-demo annotations: container.apparmor.security.beta.kubernetes.io/app: runtime/default spec: containers: - name: app image: ubuntu securityContext: readOnlyRootFilesystem: true ✅ readOnlyRootFilesystem: true로 파일 시스템을 읽기 전용으로 설정\n4️⃣ RBAC (역할 기반 접근 제어) 쿠버네티스에서는 **RBAC(Role-Based Access Control)**을 통해 사용자 권한을 관리할 수 있어.\n1. RBAC 개념 ✅ 사용자, 그룹, 서비스 계정의 권한을 정의\n✅ Role, RoleBinding, ClusterRole, ClusterRoleBinding 사용\n🔽 RBAC 개요 2. RBAC 예제 (특정 네임스페이스에 대한 읽기 권한 부여) apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: dev name: dev-reader rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"list\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: dev-reader-binding namespace: dev subjects: - kind: User name: alice apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: dev-reader apiGroup: rbac.authorization.k8s.io ✅ alice 사용자는 dev 네임스페이스에서 Pod 읽기만 가능\n5️⃣ 이미지 보안 (SBOM \u0026 취약점 스캔) 쿠버네티스 환경에서는 컨테이너 이미지가 안전한지 확인하는 것이 중요해.\nSBOM(소프트웨어 구성 분석)과 취약점 스캔을 활용하면 보안성을 높일 수 있어.\n1. SBOM (Software Bill of Materials) ✅ 컨테이너 이미지가 어떤 라이브러리로 구성되었는지 분석\n✅ Snyk, Trivy, Grype 같은 도구로 SBOM 생성 가능\n🔽 Trivy를 사용한 SBOM 생성 예제\ntrivy sbom --format spdx-json -o sbom.json nginx:latest ✅ nginx:latest 이미지의 SBOM을 sbom.json 파일로 저장\n2. 취약점 스캔 ✅ 컨테이너 이미지의 보안 취약점(CVE) 자동 스캔\n✅ Trivy로 취약점 검사 가능\n🔽 Trivy를 사용한 취약점 검사\ntrivy image nginx:latest ✅ nginx:latest 이미지의 보안 취약점 확인","kubernetes-security-쿠버네티스-보안#Kubernetes Security (쿠버네티스 보안)":""},"title":"Security"},"/devops/kubernetes/kubernetes/k8s07/":{"data":{"":"","-결론#✅ 결론":"쿠버네티스에서는 로깅 \u0026 모니터링을 통해 운영 안정성을 보장필요.\nkubectl logs → 기본 Pod 로그 확인 Fluentd, Loki → 중앙 집중형 로그 관리 Prometheus, Grafana → 클러스터 리소스 모니터링 Alertmanager → 장애 발생 시 실시간 알람 ","1-로깅--모니터링-개요#1️⃣ 로깅 \u0026amp; 모니터링 개요":"","2-로깅-시스템#2️⃣ 로깅 시스템":"","3-모니터링-시스템#3️⃣ 모니터링 시스템":"","4-알람--장애-감지#4️⃣ 알람 \u0026amp; 장애 감지":"1️⃣ 로깅 \u0026 모니터링 개요 쿠버네티스에서는 클러스터 상태, 애플리케이션 로그, 메트릭 수집이 중요.\n문제 발생 시 원인 분석과 대응을 위해 적절한 로깅 \u0026 모니터링 시스템이 필요.\n1. 주요 구성 요소 ✅ 로깅 (Logging) → Pod \u0026 애플리케이션 로그 수집\n✅ 메트릭 (Metrics) → CPU, 메모리, 네트워크 사용량 분석\n✅ 모니터링 (Monitoring) → 클러스터 상태 감시 \u0026 알람 설정\n🔽 쿠버네티스 로깅 \u0026 모니터링 개요 2️⃣ 로깅 시스템 쿠버네티스에서는 로그 수집을 위해 3가지 방식을 사용할 수 있어.\n1. 기본 로깅 (kubectl logs) ✅ kubectl logs 명령어로 Pod의 stdout, stderr 로그 조회 가능\n✅ 기본적으로 컨테이너가 종료되면 로그도 사라짐 (휘발성)\n🔽 kubectl logs 예제\nkubectl logs my-pod kubectl logs -f my-pod # 실시간 로그 확인 2. 로그 수집 아키텍처 로그를 중앙에서 관리하려면 **로그 수집기(Fluentd, Loki, Elastic Stack 등)**를 활용해야 해.\n✅ Node Level Logging → 각 노드에서 로그 파일 수집\n✅ Sidecar Pattern → 로그 수집을 위한 별도 컨테이너 사용\n✅ Centralized Logging → Fluentd, Loki 등을 사용하여 로그 저장\n🔽 로그 수집 아키텍처 3. Fluentd를 활용한 로그 수집 ✅ Fluentd는 쿠버네티스 로그를 ElasticSearch, Loki, S3 등으로 전송하는 도구야.\n✅ DaemonSet을 사용해 클러스터 내 모든 노드에서 로그를 수집 가능\n🔽 Fluentd DaemonSet 배포 예제\napiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd spec: template: spec: containers: - name: fluentd image: fluent/fluentd volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog hostPath: path: /var/log ✅ kubectl apply -f fluentd.yaml 실행\n✅ 모든 노드에서 /var/log 경로의 로그를 수집\n3️⃣ 모니터링 시스템 쿠버네티스에서는 Prometheus, Grafana, Metrics Server를 활용해 모니터링을 수행해.\n1. Metrics Server (리소스 모니터링) ✅ kubectl top 명령어로 Pod, 노드의 CPU \u0026 메모리 사용량 조회 가능\n✅ HPA (Horizontal Pod Autoscaler)와 연동 가능\n🔽 kubectl top 명령어 예제\nkubectl top nodes kubectl top pods 2. Prometheus \u0026 Grafana 모니터링 아키텍처 ✅ Prometheus → 쿠버네티스 메트릭 수집\n✅ Grafana → Prometheus 데이터를 시각화\n🔽 Prometheus \u0026 Grafana 아키텍처 3. Prometheus \u0026 Grafana 배포 예제 ✅ Helm을 사용해 Prometheus \u0026 Grafana 설치\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm install prometheus prometheus-community/kube-prometheus-stack ✅ kubectl port-forward로 Grafana 접속\nkubectl port-forward svc/prometheus-grafana 3000:80 ✅ 웹 브라우저에서 http://localhost:3000 접속 후 Grafana 로그인 (admin / prom-operator)\n4️⃣ 알람 \u0026 장애 감지 1. Alertmanager를 활용한 알람 설정 ✅ Alertmanager는 Prometheus에서 임계값 초과 시 알림을 전송해.\n✅ Slack, Email, PagerDuty 등으로 알림 전송 가능\n🔽 Alertmanager 예제\ngroups: - name: node-alerts rules: - alert: HighCPUUsage expr: node_cpu_seconds_total \u003e 80 for: 5m labels: severity: critical annotations: description: \"노드 CPU 사용량이 80%를 초과함!\" ✅ CPU 사용량이 80%를 초과하면 알림 발송"},"title":"Loging\u0026 Monitoring"},"/devops/kubernetes/kubernetes/k8s08/":{"data":{"-결론#✅ 결론":"쿠버네티스 오토스케일링을 활용하면 부하에 맞춰 리소스를 자동으로 조절 가능.\n1️⃣ HPA → Pod 개수 자동 확장\n2️⃣ VPA → Pod 리소스 자동 조정\n3️⃣ Cluster Autoscaler → 노드 자동 확장","1-오토스케일링autoscaling-개요#1️⃣ 오토스케일링(AutoScaling) 개요":" 쿠버네티스에서는 부하에 따라 리소스를 자동으로 확장(AutoScaling) 가능.\n✅ 트래픽 증가 시 자동 확장 → 성능 유지\n✅ 트래픽 감소 시 자동 축소 → 비용 절감\n쿠버네티스에서 제공하는 오토스케일링 방식:\nHPA (Horizontal Pod Autoscaler) → Pod 개수 자동 확장 VPA (Vertical Pod Autoscaler) → Pod의 CPU \u0026 메모리 할당량 자동 조정 Cluster Autoscaler → 노드(Node) 수 자동 확장 🔽 쿠버네티스 오토스케일링 개요","2-hpa-horizontal-pod-autoscaler#2️⃣ HPA (Horizontal Pod Autoscaler)":"HPA는 CPU, 메모리 사용량을 기반으로 Pod 개수를 자동으로 조정.\n1. HPA 동작 방식 ✅ Metrics Server가 Pod 리소스 사용량 수집\n✅ HPA 컨트롤러가 리소스 사용량 확인\n✅ 트래픽 증가 시 Pod 추가, 감소 시 Pod 제거\n🔽 HPA 아키텍처 흐름도\n2. HPA 설정 예제 ✅ Deployment (my-app) 배포\napiVersion: apps/v1 kind: Deployment metadata: name: my-app spec: replicas: 1 selector: matchLabels: app: my-app template: metadata: labels: app: my-app spec: containers: - name: my-app image: nginx resources: requests: cpu: \"100m\" limits: cpu: \"200m\" ✅ HPA 생성 (CPU 50% 이상 사용 시 확장)\napiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: my-app-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: my-app minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 ✅ HPA 적용 후 확인\nkubectl apply -f hpa.yaml kubectl get hpa ","3-vpa-vertical-pod-autoscaler#3️⃣ VPA (Vertical Pod Autoscaler)":"VPA는 Pod 자체의 CPU \u0026 메모리 할당량을 자동으로 조정.\n1. VPA 동작 방식 ✅ VPA 컨트롤러가 Pod 리소스 사용량 분석\n✅ 적절한 CPU \u0026 메모리 할당량을 추천\n✅ Pod 재시작 후 새로운 리소스 설정 적용\n🔽 VPA 아키텍처 흐름도\n2. VPA 설정 예제 ✅ VPA 설치 (Metrics Server 필요)\nkubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler.yaml ✅ VPA 설정 (my-app에 적용)\napiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: my-app-vpa spec: targetRef: apiVersion: apps/v1 kind: Deployment name: my-app updatePolicy: updateMode: Auto ✅ VPA 적용 후 리소스 추천 확인\nkubectl describe vpa my-app-vpa ","4-cluster-autoscaler#4️⃣ Cluster Autoscaler":"Cluster Autoscaler는 노드 수를 자동으로 조정하는 기능.\n✅ HPA가 Pod 개수를 늘려도 노드가 부족하면? → Cluster Autoscaler가 새로운 노드 추가\n✅ Pod이 줄어들어 노드가 불필요하면? → 자동으로 노드 제거\n🔽 Cluster Autoscaler 아키텍처\n1. Cluster Autoscaler 설정 ✅ AWS EKS에서 Autoscaler 설치\nhelm repo add autoscaler https://kubernetes.github.io/autoscaler helm install cluster-autoscaler autoscaler/cluster-autoscaler ✅ Cluster Autoscaler 설정 파일\napiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler spec: template: spec: containers: - name: cluster-autoscaler image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0 command: - ./cluster-autoscaler - --cloud-provider=aws - --nodes=1:10:my-node-group ✅ 클러스터 노드 상태 확인\nkubectl get nodes kubectl describe nodes ","kubernetes-autoscaling#Kubernetes Autoscaling":"Kubernetes Autoscaling"},"title":"Autoscaling"},"/devops/kubernetes/kubernetes/k8s09/":{"data":{"-실습-결론#✅ 실습 결론":"이제 쿠버네티스의 핵심 개념을 직접 실습하면서 익힐 수 있어! 🚀🔥\n🔹 Deployment, Service, Ingress로 애플리케이션 배포\n🔹 ConfigMap, Secret을 활용한 환경 설정\n🔹 Persistent Volume \u0026 PVC를 활용한 데이터 저장\n🔹 오토스케일링, 로깅, 모니터링까지 확장 가능","-쿠버네티스-hands-on-실습-가이드#🔥 \u003cstrong\u003e쿠버네티스 Hands-On 실습 가이드\u003c/strong\u003e":"","1-실습-개요-hands-on-overview#1️⃣ 실습 개요 (Hands-On Overview)":"","2-애플리케이션-배포-deployment--service#2️⃣ 애플리케이션 배포 (Deployment + Service)":"","3-ingress로-외부-트래픽-라우팅#3️⃣ Ingress로 외부 트래픽 라우팅":"","4-configmap--secret-활용#4️⃣ ConfigMap \u0026amp; Secret 활용":"","5-persistent-volume--storage-class#5️⃣ Persistent Volume \u0026amp; Storage Class":"🔥 쿠버네티스 Hands-On 실습 가이드 ✅ Pod, Deployment, Service, Ingress, ConfigMap, Secret, Storage, Autoscaling, Security, Monitoring 등\n✅ 실제 운영 환경과 유사하게 쿠버네티스 리소스를 조합하여 배포. ✅ 각 단계별로 설명 + 예제 코드 + 명령어 + 실행 결과까지 포함.\n1️⃣ 실습 개요 (Hands-On Overview) 우리가 진행할 실습은 실제 운영 환경과 유사하게 설계되었어.\n이 실습을 통해 다음을 학습할 수 있어:\n✅ 쿠버네티스 오브젝트 (Pod, Deployment, Service, Ingress, ConfigMap, Secret, PVC, NetworkPolicy)\n✅ 네트워킹 (CNI, ClusterIP, NodePort, LoadBalancer, Ingress)\n✅ 스토리지 (Persistent Volume, StorageClass)\n✅ 오토스케일링 (HPA, VPA, Cluster Autoscaler)\n✅ 로깅 \u0026 모니터링 (Prometheus, Grafana, Fluentd)\n✅ 보안 (RBAC, Network Policy, Secret 관리)\n🔥 실습 환경 구성 📌 클러스터 구성:\nMinikube / Kind (로컬) EKS / GKE / AKS (클라우드 환경) 📌 기본 도구 설치:\n# Kubernetes CLI (kubectl) curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\" chmod +x kubectl mv kubectl /usr/local/bin/ # Minikube (로컬 쿠버네티스 클러스터) curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 chmod +x minikube-linux-amd64 sudo mv minikube-linux-amd64 /usr/local/bin/minikube minikube start 2️⃣ 애플리케이션 배포 (Deployment + Service) 1. Deployment \u0026 Service 생성 ✅ 예제 애플리케이션 (Nginx + Redis) 배포\napiVersion: apps/v1 kind: Deployment metadata: name: web-app spec: replicas: 3 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: nginx image: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: web-service spec: selector: app: web ports: - protocol: TCP port: 80 targetPort: 80 type: ClusterIP ✅ 배포 후 확인\nkubectl apply -f web-deployment.yaml kubectl get pods -o wide kubectl get svc 🔽 Deployment \u0026 Service 흐름도\n3️⃣ Ingress로 외부 트래픽 라우팅 ✅ Ingress Controller 설치\nminikube addons enable ingress kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml ✅ Ingress 설정 (도메인 기반 라우팅)\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: web-ingress spec: rules: - host: web.local http: paths: - path: / pathType: Prefix backend: service: name: web-service port: number: 80 ✅ Ingress 적용 후 확인\nkubectl apply -f ingress.yaml kubectl get ingress curl -H \"Host: web.local\" http://\u003cINGRESS-IP\u003e 🔽 Ingress 흐름도\n4️⃣ ConfigMap \u0026 Secret 활용 ✅ ConfigMap (환경 변수 설정)\napiVersion: v1 kind: ConfigMap metadata: name: app-config data: ENV: \"production\" LOG_LEVEL: \"debug\" ✅ Secret (민감 정보 관리)\napiVersion: v1 kind: Secret metadata: name: app-secret type: Opaque data: DB_PASSWORD: c2VjcmV0MTIz # base64 인코딩된 \"secret123\" ✅ Pod에서 ConfigMap \u0026 Secret 사용하기\napiVersion: apps/v1 kind: Deployment metadata: name: backend-app spec: replicas: 2 selector: matchLabels: app: backend template: metadata: labels: app: backend spec: containers: - name: app image: my-backend:latest envFrom: - configMapRef: name: app-config - secretRef: name: app-secret ✅ ConfigMap \u0026 Secret 확인\nkubectl get configmap app-config -o yaml kubectl get secret app-secret -o yaml 5️⃣ Persistent Volume \u0026 Storage Class ✅ 스토리지 설정\napiVersion: v1 kind: PersistentVolume metadata: name: my-pv spec: storageClassName: standard capacity: storage: 5Gi accessModes: - ReadWriteOnce hostPath: path: \"/data\" ✅ PVC와 연결\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: storageClassName: standard accessModes: - ReadWriteOnce resources: requests: storage: 1Gi ✅ Pod에서 사용\napiVersion: v1 kind: Pod metadata: name: storage-test spec: containers: - name: test-container image: busybox command: [ \"sleep\", \"3600\" ] volumeMounts: - mountPath: \"/data\" name: storage volumes: - name: storage persistentVolumeClaim: claimName: my-pvc ✅ 스토리지 적용 후 확인\nkubectl get pv kubectl get pvc "},"title":"hands-on"},"/devops/kubernetes/kubernetestraining/":{"data":{"":"","kubernetes-standard-architecture#\u003cstrong\u003eKubernetes Standard Architecture\u003c/strong\u003e":"","설치순서--순서는-필자의-주관으로-강제가-아님-#\u003cstrong\u003e설치순서 ( 순서는 필자의 주관으로 강제가 아님. )\u003c/strong\u003e":"Kubernetes Standard Architecture 해당 섹션에는 2023 표준 아키텍처를 기준으로 필자의 주관을 참조여 구성됩니다. 설치순서 ( 순서는 필자의 주관으로 강제가 아님. ) 각 컴포넌트 중, 색상으로 표시한 컴포넌트를 기준으로 설치됩니다. 1. Container Runtime 툴: containerd, Docker 설명: 쿠버네티스가 컨테이너를 실행하는 런타임을 설치합니다. 가이드: 각 노드에 containerd 또는 Docker를 설치합니다. kubelet이 사용할 컨테이너 런타임을 설정합니다. 2. Cluster Provisioning 툴: kubeadm, KubeSpray, Terraform 설명: 쿠버네티스 클러스터를 생성하고, 노드들을 구성합니다. 이 단계에서는 쿠버네티스 클러스터의 기본 인프라를 배포합니다. 가이드: Kubeadm: kubeadm init 명령어를 사용하여 클러스터를 초기화합니다. 각 노드를 클러스터에 추가합니다. KubeSpray: Ansible을 사용하여 다중 노드 클러스터를 자동으로 배포합니다. Terraform: 인프라를 코드로 정의하여 클라우드 환경에서 쿠버네티스 클러스터를 프로비저닝합니다. 3. Container Network Interface 툴: Calico, MetalLB, NGINX, K8GB 설명: 클러스터 내의 네트워킹을 설정합니다. Calico를 통해 네트워크 정책과 CNI를 설정합니다. 가이드: Calico를 설치하여 네트워크 정책과 Pod 간의 통신을 설정합니다. MetalLB를 사용하여 로드밸런서를 구성합니다. NGINX 또는 K8GB로 인그레스 컨트롤러를 설정합니다. 4. Usability Tools 툴: Helm, OpenLens 설명: 클러스터에서 쉽게 애플리케이션을 배포하고 관리할 수 있도록 도와주는 도구들을 설정합니다. 가이드: Helm을 사용하여 패키지 매니저로서 애플리케이션을 설치합니다. OpenLens를 통해 클러스터를 관리하고 모니터링합니다. 5. Container Natvie Storage 툴: Rook Ceph, Ceph 설명: 클러스터 내에서 스토리지를 설정하여 애플리케이션이 사용할 수 있게 합니다. 가이드: Rook/Ceph를 사용하여 퍼시스턴트 볼륨을 제공합니다. Velero를 설치하여 클러스터 백업 및 복원을 설정합니다. 6. Backup and Restore Service 툴: Velero 설명: Kubernetes 클러스터의 백업 및 복원을 관리합니다. Velero를 사용하여 클러스터의 상태와 데이터를 안전하게 저장하고, 필요시 복구할 수 있습니다. 가이드: Velero를 설치하여 클러스터의 백업 및 복구 작업을 자동화합니다. 클러스터 내의 퍼시스턴트 볼륨 및 네임스페이스 데이터를 백업하고 복구할 수 있는 설정을 구성합니다 7. Mesh Service 툴: Istio 설명: 서비스 간의 통신을 관리하고, 서비스 메시 패턴을 설정합니다. 가이드: Istio를 설치하여 마이크로서비스 간의 트래픽 관리, 모니터링, 보안을 설정합니다. 8. Access Management Service 툴: Keycloak 설명: SSO(Single Sign-On) 및 ID 관리 시스템을 설정합니다. 가이드: Keycloak을 설치하고, 클러스터의 RBAC 및 OIDC 인증을 통합합니다. SSO를 통해 애플리케이션 액세스를 제어합니다. 9. Key Management Service 툴: HashiCorp Vault 설명: 클러스터에서 사용하는 키와 인증서 관리 서비스를 설정합니다. 가이드: Vault를 설정하여 비밀 정보 및 인증서를 관리합니다. Kubernetes와 통합하여 비밀 데이터를 관리합니다. 10. Container Images Managemet Service 툴: Harbor 설명: 쿠버네티스 클러스터에서 사용하는 이미지를 저장하고 관리할 컨테이너 이미지 레지스트리를 설정합니다. 가이드: Harbor를 설치하여 내부 레지스트리를 설정하고, 이미지를 푸시하고 풀할 수 있게 합니다. 11. Monitoring and Logging 툴: Prometheus, Grafana, Fluentd, Elasticsearch, Kibana 설명: 클러스터의 모니터링 및 로깅을 설정합니다. 가이드: Prometheus를 설치하여 메트릭을 수집하고 Grafana를 통해 시각화합니다. Fluentd, Elasticsearch, Kibana를 설정하여 로그를 수집하고 분석합니다. 12. CI/CD Pipeline 툴: Jenkins, GitLab, Argo 설명: 애플리케이션의 지속적 통합과 지속적 배포를 설정합니다. 가이드: Jenkins/GitLab을 설정하여 빌드 파이프라인을 설정합니다. Argo를 사용하여 지속적 배포(CD)를 자동화합니다. 13. Multi Cluster Management 툴: Karmada 설명: 여러 클러스터를 관리하고 페더레이션 설정을 합니다. 가이드: Karmada를 설정하여 멀티 클러스터 관리 및 페더레이션을 지원합니다. "},"title":"Kubernetes Training"},"/devops/kubernetes/kubernetestraining/0.-resource/":{"data":{"":"","requirements-source#\u003cstrong\u003eRequirements source\u003c/strong\u003e":"","기본설정#\u003cstrong\u003e기본설정\u003c/strong\u003e":"Requirements source 기초 환경구성 Role Hostname OS 커널 CPU Mem IP Manager ctrl Ubuntu 24.04.1 LTS Linux 6.8.0-45-generic 1 2048M nat 10.1.61.179 / Nat 7.7.7.254 ControlPlan cp0 Ubuntu 24.04.1 LTS Linux 6.8.0-45-generic 1 2048M Nat 7.7.7.100 DataPlan node0 Ubuntu 24.04.1 LTS Linux 6.8.0-45-generic 1 2048M Nat 7.7.7.110 매우 대충 그렸지만, 넓은 아량으로 이해 부탁드립니다. 😄 진도를 나아가며 점차 추가해나아갈 예정입니다. 기본설정 NTP 설정 # NTP를 설정합니다. # NTP 서버가 있으면, 해당 서버로 시간대를 잡아두어도 좋습니다. $ timedatectl set-timezone Asia/Seoul ","기초-환경구성#\u003cstrong\u003e기초 환경구성\u003c/strong\u003e":""},"title":"Requirements source"},"/devops/kubernetes/kubernetestraining/0.-resource/file0/":{"data":{"":"","kubernets-manager-node#\u003cstrong\u003eKubernets Manager Node\u003c/strong\u003e":"Kubernets Manager Node Kubernets Manager Node Manager Node는 노드는 클러스터에 대한 요청을 수신하고 분배하는 역할을 하는 로드 밸런서 역할을 하는 서버를 의미합니다.\n일반적으로 NGINX, HAProxy와 같은 프록시 서버를 사용하며, 여기서는 Nginx로 구성합니다.\n해당 노드의 주 역할은 아래와 같습니다.\n역할 설명 예시 요청 라우팅 클라이언트의 API 요청을 적절한 서비스나 백엔드 노드로 전달합니다. NGINX가 클라이언트 요청을 받아 특정 서비스로 라우팅합니다. 예를 들어, /api/users 요청은 사용자 서비스로 전달됩니다. 로드 밸런싱 요청을 여러 백엔드 노드에 균등하게 분배하여 성능을 최적화합니다. HAProxy가 두 개의 인스턴스(예: app-1, app-2)로 요청을 분배합니다. 각 인스턴스가 50%의 트래픽을 처리합니다. SSL 종료 HTTPS 요청을 처리하고 SSL 인증서를 관리하여 보안을 강화합니다. NGINX가 SSL 인증서를 사용하여 클라이언트와의 연결을 암호화합니다. 내부 통신은 HTTP로 유지합니다. 캐싱 정적 콘텐츠를 캐싱하여 반복적인 요청에 대한 응답 속도를 향상시킵니다. NGINX가 자주 요청되는 이미지 파일을 캐싱하여, 사용자가 이미지를 요청할 때마다 백엔드 서버에 요청하지 않습니다. 트래픽 관리 요청의 수를 제한하거나 특정 요청을 차단하여 트래픽을 관리합니다. HAProxy가 특정 IP에서의 요청 수를 제한하여 DDoS 공격을 방어합니다. 예를 들어, 100초당 10회 요청으로 제한할 수 있습니다. 상태 검사 백엔드 서비스의 상태를 주기적으로 확인하여, 실패한 서비스에 요청을 보내지 않도록 합니다. HAProxy가 app-1 인스턴스의 상태를 확인하고, 문제가 발생한 경우 해당 인스턴스에 요청을 보내지 않도록 합니다. 간단하게 기존에 사용하던 Docker engine을 사용하지 않고 ContainerD를 사용 하게된 이유를 정리하자면 쿠버네티스는 여러 컨테이너 런타임과 통신할 수 있도록 하는 CRI라는 표준 인터페이스를 설계함.\nDocker engine은 CRI 인터페이스가 생기기전 존재한 기술로 CRI 인터페이스와 맞지 않았음.\n이를 해결하기 위해 dockershim이라는 어댑터 컴포넌트를 개발하였음.\n이 쉼의 존재는 kubelet 자체에 많은 불필요한 복잡성을 도입했고, 일부 통합은 이 쉼 때문에 Docker에 대해 일관성 없게 구현되었으며, 이로 인해 유지 관리 부담이 증가하게 됨.\n이처럼 벤더 특정 코드(특정 제품인 Docker에 종속적인 코드)를 유지 관리하는 것은 쿠버네티스의 오픈 소스 철학에 부합하지 않았고, 2022년 4월 v1.24에서 완전한 제거를 발표하였음. Kubernets Manager Node Install # nginx를 설치합니다. $ apt -y install nginx libnginx-mod-stream # nginx에 프록시를 세팅합니다. $ vi /etc/nginx/nginx.conf stream { upstream k8s-api { server 7.7.7.100:6443; # contolplan IP } server { listen 6443; proxy_pass k8s-api; } } # Nginx의 default 웹을 비활성화합니다. $ unlink /etc/nginx/sites-enabled/default $ systemctl restart nginx # kubectl 레포지터리를 등록하고 설치합니다. $ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg $ echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\" | tee /etc/apt/sources.list.d/kubernetes.list $ apt update $ apt -y install kubectl # (option) kubectl alias k $ apt -y install bash-completion $ source /usr/share/bash-completion/bash_completion $ kubectl completion bash | tee /etc/bash_completion.d/kubectl \u003e /dev/null $ echo 'alias k=kubectl' \u003e\u003e~/.bashrc $ echo 'complete -o default -F __start_kubectl k' \u003e\u003e~/.bashrc $ exec bash "},"title":"Manger Node"},"/devops/kubernetes/kubernetestraining/1.-runtime/containerd/file0/":{"data":{"":"","containerd#\u003cstrong\u003eContainerD\u003c/strong\u003e":"ContainerD Containerd는 Docker 사에서 moby project 와 함께 발표하여 CNCF 에 기증한 오픈소스입니다.\nContainerD 2022년 4월 Docker engine이 쿠버네티스에서 사용중단을 발표한 후, 높은 점유율을 이루고 있습니다.\n간단하게 기존에 사용하던 Docker engine을 사용하지 않고 ContainerD를 사용 하게된 이유를 정리하자면 쿠버네티스는 여러 컨테이너 런타임과 통신할 수 있도록 하는 CRI라는 표준 인터페이스를 설계함.\nDocker engine은 CRI 인터페이스가 생기기전 존재한 기술로 CRI 인터페이스와 맞지 않았음.\n이를 해결하기 위해 dockershim이라는 어댑터 컴포넌트를 개발하였음.\n이 쉼의 존재는 kubelet 자체에 많은 불필요한 복잡성을 도입했고, 일부 통합은 이 쉼 때문에 Docker에 대해 일관성 없게 구현되었으며, 이로 인해 유지 관리 부담이 증가하게 됨.\n이처럼 벤더 특정 코드(특정 제품인 Docker에 종속적인 코드)를 유지 관리하는 것은 쿠버네티스의 오픈 소스 철학에 부합하지 않았고, 2022년 4월 v1.24에서 완전한 제거를 발표하였음. 그렇다고, ContainerD가 Docker와 완전히 다른 것이 아닌 게, containerD는 도커를 컨테이너 런타임으로 사용할 때 내부적으로 사용되는 컨테이너 런타임 기술로, 이젠 이러한 기술을 도커에서 빼서 containerd만 따로 사용하는 것입니다.\n즉, 현재는 Docker에서 컨테이너 런타임을 분리하여 ContainerD를 사용하고, 여전히 관련 이미지 개발 등은 Docker을 사용하는 것입니다.\nDocker Engine, ContainerD에 간단 비교는 아래를 참고해주세요.\nDocker Engine vs ContainerD 특성 Docker Engine ContainerD 설명 Docker의 전체 컨테이너 관리 플랫폼 경량화된 컨테이너 런타임 주요 기능 이미지 빌드, 배포, 컨테이너 실행 컨테이너 실행, 이미지 관리 API Docker API 제공 CRI (Container Runtime Interface) 지원 설치 및 설정 복잡한 설치 및 설정 비교적 간단한 설치 및 설정 부하 상대적으로 높은 리소스 소모 더 경량화되어 리소스 효율성 높음 의존성 여러 추가 도구 (예: Docker Compose) 필요 Kubernetes에서 직접 사용 가능 주요 사용 사례 개발 및 테스트 환경 프로덕션 환경, 특히 Kubernetes에서 사용 References [kubernetes] ☸️쿠버네티스의 도커 지원 중단에 대한 모든 것 "},"title":" Concept"},"/devops/kubernetes/kubernetestraining/1.-runtime/containerd/file1/":{"data":{"":"","containerd-install#\u003cstrong\u003eContainerD Install\u003c/strong\u003e":"ContainerD Install ContainerD 환경구성 # CRI를 사용하기 위한 네트워크 설정 # net.bridge.bridge-nf-call-iptables: 브리징된 네트워크 인터페이스에서 트래픽을 iptables를 통해 필터링 할 수 있도록 허용 # net.bridge.bridge-nf-call-ip6tables: IPv6 네트워크에서도 브리지된 트래픽을 ip6tables를 통해 필터링할 수 있도록 허용 # net.ipv4.ip_forward: IP 포워딩을 활성화 $ cat \u003e /etc/sysctl.d/99-k8s-cri.conf \u003c\u003cEOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 EOF # 변경사항 적용 $ sysctl --system # OverlayFS: 파일 시스템을 로드하여 컨테이너의 파일 시스템을 효율적으로 관리 # br_netfilter: 브리지 네트워크 트래픽을 iptables로 전달할 수 있도록 하는 모듈 $ modprobe overlay; modprobe br_netfilter $ echo -e overlay\\\\nbr_netfilter \u003e /etc/modules-load.d/k8s.conf # iptables 백엔드를 iptables-legacy로 전환 # 쿠버네티스는 일부 환경에서 iptables-legacy 백엔드와 더 잘 호환되기 때문에 이를 설정 # iptables-legacy: 전통적인 iptables # iptables-nft: nftables 기반의 iptables 구현 $ update-alternatives --config iptables There are 2 choices for the alternative iptables (providing /usr/sbin/iptables). Selection Path Priority Status ------------------------------------------------------------ * 0 /usr/sbin/iptables-nft 20 auto mode 1 /usr/sbin/iptables-legacy 10 manual mode 2 /usr/sbin/iptables-nft 20 manual mode Press \u003center\u003e to keep the current choice[*], or type selection number: 1 update-alternatives: using /usr/sbin/iptables-legacy to provide /usr/sbin/iptables (iptables) in manual mode # swap off $ swapoff -a $ sed -i '/\\/swap.img/s/^/#/' /etc/fstab # apparmor 프로파일 비활성화 $ apparmor_parser -R /etc/apparmor.d/runc $ apparmor_parser -R /etc/apparmor.d/crun $ ln -s /etc/apparmor.d/runc /etc/apparmor.d/disable/ $ ln -s /etc/apparmor.d/crun /etc/apparmor.d/disable/ ContainerD 설치 # ContainerD를 설치합니다. $ apt -y install containerd # ContainerD의 디렉터리를 생성합니다. $ mkdir /etc/containerd # ContinerD의 기본 설정파일을 생성합니다. $ containerd config default | tee /etc/containerd/config.toml # sandbox_image 버전을 설정합니다. sed -i \"s|^\\( *sandbox_image *= *\\).*|\\1\\\"registry.k8s.io/pause:3.9\\\"|\" /etc/containerd/config.toml # Cgroup의 사용여부를 수정합니다. sed -i \"s|^\\( *SystemdCgroup *= *\\).*|\\1true|\" /etc/containerd/config.toml # 설정적용 $ systemctl restart containerd # 설치확인 $ containerd -version containerd github.com/containerd/containerd 1.7.12 $ systemctl status containerd ● containerd.service - containerd container runtime Loaded: loaded (/usr/lib/systemd/system/containerd.service; enabled; preset:\u003e Active: active (running) since Mon 2024-09-23 09:31:59 UTC; 8min ago Docs: https://containerd.io Process: 1416 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCC\u003e Main PID: 1418 (containerd) Tasks: 7 Memory: 13.3M (peak: 14.9M) CPU: 1.004s CGroup: /system.slice/containerd.service container.d config 정리 섹션 설명 disabled_plugins 비활성화된 플러그인 목록을 정의합니다. imports 가져올 플러그인 목록을 정의합니다. oom_score OOM(Out-Of-Memory) 점수를 설정하여 우선 순위를 결정합니다. root Containerd의 루트 디렉토리를 설정합니다. state 상태 정보를 저장할 디렉토리 위치를 설정합니다. cgroup cgroup 설정 경로를 지정합니다. debug 디버깅 설정 (주소, 포맷, 레벨 등)과 관련된 옵션입니다. grpc GRPC 서버 설정 (소켓 주소, 메시지 크기 제한 등)을 지정합니다. metrics 메트릭 수집을 위한 설정입니다. plugins 다양한 Containerd 플러그인의 설정을 정의합니다. plugins.\"io.containerd.gc.v1.scheduler\" 가비지 컬렉션 스케줄러의 설정입니다. plugins.\"io.containerd.grpc.v1.cri\" CRI(Container Runtime Interface) 관련 설정입니다. plugins.\"io.containerd.grpc.v1.cri\".cni CNI(Container Network Interface) 설정입니다. plugins.\"io.containerd.grpc.v1.cri\".containerd Containerd 자체 설정을 정의합니다. plugins.\"io.containerd.grpc.v1.cri\".registry 이미지 레지스트리와 관련된 설정입니다. plugins.\"io.containerd.runtime.v1.linux\" Linux 런타임 설정입니다. plugins.\"io.containerd.snapshotter.v1.overlayfs\" OverlayFS 스냅샷터 설정입니다. stream_processors 스트림 프로세서 관련 설정을 정의합니다. timeouts Containerd 내부에서 다양한 작업의 타임아웃을 정의합니다. ttrpc TTRPC 관련 설정입니다. "},"title":" install"},"/devops/kubernetes/kubernetestraining/10.-cims/harbor/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/10.-cims/harbor/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/11.-ml/elasticsearch/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/11.-ml/elasticsearch/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/11.-ml/fluentd/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/11.-ml/fluentd/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/11.-ml/kibana/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/11.-ml/kibana/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/12.ci/cd/argocd/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/12.ci/cd/argocd/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/13.-mcm/karmada/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/13.-mcm/karmada/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/2.-provisioning/kubeadm/file0/":{"data":{"":"","kubeadm#\u003cstrong\u003eKubeadm\u003c/strong\u003e":"Kubeadm Kubeadm은 쿠버네티스 클러스터 생성을 위한 “빠른 경로\"의 모범 사례로 kubeadm init 및 kubeadm join 을 제공하도록 만들어진 도구\n쿠버네티스는 분산 시스템 관리 도구로, 컨테이너화된 애플리케이션을 효율적으로 배포, 관리, 확장할 수 있게 해주지만, 초기 설정 및 유지 관리 작업은 복잡할 수 있으며, 이를 간편하게 해주기 위한 프로비저닝 툴 중 하나가 kubeadm\nkubeadm은 실행 가능한 최소 클러스터를 시작하고 실행하는 데 필요한 작업을 수행\n설계 상, 시스템 프로비저닝이 아닌 부트스트랩(bootstrapping)만 다룬다.\n즉, 대시보드, 모니터링 솔루션 및 클라우드별 애드온과 같은 다양한 있으면 좋은(nice-to-have) 애드온을 설치하는 것은 범위에 포함되지 않는다.\nKubeadm 명령어 명령어 설명 예시 kubeadm init 쿠버네티스 컨트롤 플레인 노드를 부트스트랩합니다. kubeadm init --pod-network-cidr=10.244.0.0/16 kubeadm join 워커 노드를 부트스트랩하고 클러스터에 조인시킵니다. kubeadm join \u003cmaster-ip\u003e:\u003cport\u003e --token \u003ctoken\u003e --discovery-token-ca-cert-hash \u003chash\u003e kubeadm upgrade 현재 클러스터를 새로운 버전으로 업그레이드합니다. kubeadm upgrade apply v1.20.0 kubeadm config 클러스터 초기화 및 업그레이드 시 설정 구성을 관리합니다. kubeadm config print init-defaults kubeadm token kubeadm join 명령어에 필요한 토큰을 생성하거나 관리합니다. kubeadm token create --print-join-command kubeadm reset kubeadm init 또는 kubeadm join으로 인한 변경 사항을 모두 되돌립니다. kubeadm reset kubeadm certs 쿠버네티스 인증서를 관리합니다. kubeadm certs renew all kubeadm kubeconfig 쿠버네티스 클러스터의 kubeconfig 파일을 관리합니다. kubeadm kubeconfig user kubeadm version kubeadm의 현재 버전을 출력합니다. kubeadm version kubeadm alpha 미리 보기 기능으로 커뮤니티 피드백을 위해 제공되는 실험적 기능을 실행합니다. kubeadm alpha certs renew References Kubeadm "},"title":"Kubeadm Concept"},"/devops/kubernetes/kubernetestraining/2.-provisioning/kubeadm/file1/":{"data":{"":"","kubeadm-install#\u003cstrong\u003eKubeadm Install\u003c/strong\u003e":"Kubeadm Install kubeadm을 설치하기 위한 환경은 전 인덱스를 참고 부탁드립니다. # 쿠버네티스 패키지 저장소에서 GPG 키를 다운로드 $ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg # 쿠버네티스 저장소를 추가 $ echo \"deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /\" | tee /etc/apt/sources.list.d/kubernetes.list # 패키지 목록 업데이트 $ apt update # kubeadm kubelet kubectl Install $ apt -y install kubeadm kubelet kubectl 명령어 설명 주요 기능 및 역할 예시 kubeadm 쿠버네티스 클러스터를 초기화하고 관리하는 도구입니다. 클러스터 초기화, 클러스터 업그레이드, 인증서 관리, 토큰 관리 kubeadm initkubeadm join kubelet 각 노드에서 실행되는 주요 에이전트로, 컨테이너의 상태를 관리하는 역할을 수행합니다. 컨테이너 생성 및 관리, Pod 상태 확인 및 보고- 노드 상태 확인 /etc/systemd/system/kubelet.service로 실행 kubectl 쿠버네티스 클러스터를 관리하고 조작하기 위한 CLI(Command Line Interface) 도구입니다. 리소스 조회 및 관리, 애플리케이션 배포 및 업데이트, 로그 확인 kubectl get podskubectl apply -f deployment.yaml "},"title":"Kubeadm install"},"/devops/kubernetes/kubernetestraining/2.-provisioning/kubeadm/file2/":{"data":{"":"","kubeadm-update#\u003cstrong\u003eKubeadm update\u003c/strong\u003e":"Kubeadm update 차후 작성예정 "},"title":"Kubeadm update"},"/devops/kubernetes/kubernetestraining/3.-cni/calico/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/3.-cni/calico/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/3.-cni/metallb/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/3.-cni/metallb/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/3.-cni/nginx/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/3.-cni/nginx/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/4.-tools/helm/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/4.-tools/helm/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/4.-tools/openlens/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/4.-tools/openlens/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/5.-cns/rook/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/5.-cns/rook/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/6.-br/velero/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/6.-br/velero/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/7.-mesh/istio/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/7.-mesh/istio/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/8.-ams/keycloak/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/8.-ams/keycloak/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/9.-kms/vault/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/9.-kms/vault/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/kubernetes/kubernetestraining/99.-etc/0/file0/":{"data":{"":"aaa"},"title":" Concept"},"/devops/kubernetes/kubernetestraining/99.-etc/0/file1/":{"data":{"":"aaa"},"title":" install"},"/devops/terraform/":{"data":{"":" RSS Feed "},"title":"Terraform"},"/portfolio/":{"data":{"":" Open source projects powered by Hextra "},"title":"Portfolio"},"/programing/":{"data":{"":" RSS Feed "},"title":"Programing"},"/programing/fastapi/fastapi/fastapi00/":{"data":{"fast-api란#\u003cstrong\u003eFAST API란?\u003c/strong\u003e":"","fast-api의-특징#\u003cstrong\u003eFAST API의 특징\u003c/strong\u003e":"","fastapi#\u003cstrong\u003eFastAPI\u003c/strong\u003e":"","fastapi-활용-사례#\u003cstrong\u003eFastAPI 활용 사례\u003c/strong\u003e":"FastAPI\nFastAPI 공식문서 FAST API란? FastAPI는 Python으로 작성된 현대적인 웹 프레임워크로, asyncio를 기반으로 한 ASGI(Asynchronous Server Gateway Interface)를 사용하여 비동기 작업을 통해 고성능과 간단한 코드 작성 방식을 제공합니다. 비동기 작업은 요청이 들어올 때 블로킹되지 않고 여러 작업을 병렬로 처리하므로, I/O 바운드 작업(예: 데이터베이스 쿼리, 외부 API 호출)에 대해 매우 높은 성능을 발휘합니다. # 예시 from fastapi import FastAPI app = FastAPI() @app.get(\"/async-example\") async def async_example(): data = await fetch_data_from_api() db_result = await fetch_data_from_db() return {\"api_data\": data, \"db_result\": db_result} FastAPI는 비동기 처리와 함께 Pydantic의 데이터 검증을 활용하여 JSON 데이터의 직렬화 및 역직렬화를 최적화 Starlette와 Uvicorn 사용 FastAPI는 Starlette을 기반으로 하여 HTTP 및 WebSocket 처리 성능이 뛰어나며, Uvicorn이라는 ASGI 서버로 실행됩니다. Uvicorn은 단일 스레드로도 높은 요청 처리량을 제공하며, 멀티프로세스 옵션을 통해 대규모 서비스에서도 확장 가능합니다 Swagger UI 및 ReDoc 같은 UI로 API 문서를 확인하고 테스트가 가능합니다. FAST API의 역사 FastAPI는 2018년에 Sebastián Ramírez에 의해 처음 발표되었습니다. Flask와 Django와 같은 기존 프레임워크의 제한점을 보완하고, 비동기 프로그래밍의 필요성을 반영하여 설계되었습니다.\nFAST API의 특징 FAST API의 장점 빠른 개발 속도\n자동완성 및 OpenAPI 지원으로 빠르고 효율적인 API 개발이 가능합니다.\nfrom typing import Union from fastapi import FastAPI app = FastAPI() @app.get(\"/\") def read_root(): return {\"Hello\": \"World\"} @app.get(\"/items/{item_id}\") def read_item(item_id: int, q: Union[str, None] = None): return {\"item_id\": item_id, \"q\": q} 고성능\nFastAPI는 ASGI를 기반으로 하여 비동기 I/O 작업을 효율적으로 처리하며, Flask와 같은 기존 프레임워크보다 더 높은 성능을 자랑합니다.\n자동 문서 생성\nSwagger UI와 ReDoc을 통해 API 문서를 자동으로 생성합니다.\n데이터 검증 및 직렬화\nPydantic을 활용한 강력한 데이터 검증 기능을 제공합니다. 이를 통해 JSON 데이터를 Python 객체로 안전하게 변환하고, 잘못된 데이터 처리를 방지합니다.\nOAuth2 및 보안\nFastAPI는 OAuth2 및 다양한 인증 방식을 기본 지원하여 보안이 중요한 API 개발에 적합합니다.\nFAST API의 단점 학습 자료 부족\nFastAPI는 비교적 새로운 프레임워크로, Django와 Flask에 비해 학습 자료가 제한적입니다.\n코드 복잡성 증가\nFastAPI는 다양한 기능을 제공하지만, 프로젝트 규모가 커질수록 코드 구조가 복잡해질 수 있습니다. 이를 해결하려면 모듈화를 적절히 활용해야 합니다.\nPython Framework 비교 기능 FastAPI Flask Django 속도 매우 빠름 보통 보통 비동기 지원 지원 기본 미지원 Django 3.0+부터 제한적으로 지원 문서화 자동 생성 수동 작성 필요 수동 작성 필요 데이터 검증 기본 제공 (Pydantic) 수동 작성 필요 수동 작성 필요 커뮤니티 성장 중 매우 크고 활성화 매우 크고 활성화 출처 : Chat GPT\nFastAPI 활용 사례 머신러닝 모델 배포\nFastAPI는 비동기 기능 덕분에 고성능 머신러닝 API를 쉽게 구현할 수 있습니다. 모델 예측 결과를 빠르게 제공하거나 데이터를 실시간으로 처리하는 데 유리합니다.\nIoT 애플리케이션\n대규모 IoT 기기와의 데이터 통신 및 제어를 위한 API를 개발할 때 이상적입니다.","python-framework-비교#\u003cstrong\u003ePython Framework 비교\u003c/strong\u003e":""},"title":"0. FastAPI이란?"},"/programing/fastapi/fastapi/fastapi01/":{"data":{"":"","1-domain-driven-design-ddd-기반-구조#\u003cstrong\u003e1. Domain-Driven Design (DDD) 기반 구조\u003c/strong\u003e":"","2-service-oriented-architecture-soa#\u003cstrong\u003e2. Service-Oriented Architecture (SOA)\u003c/strong\u003e":"","3-monolithic-구조#\u003cstrong\u003e3. Monolithic 구조\u003c/strong\u003e":"","api-프로젝트-구조-비교#\u003cstrong\u003eAPI 프로젝트 구조 비교\u003c/strong\u003e":"","fastapi-기초다지기#\u003cstrong\u003eFastAPI 기초다지기\u003c/strong\u003e":"","구조-비교#\u003cstrong\u003e구조 비교\u003c/strong\u003e":"FastAPI 기초다지기 개발환경은 여기를 참조해주세요. FastAPI 프로젝트 구조 FastAPI 프로젝트를 만들고자 한다면 프로젝트 구조를 잘 만들어야 한다. 그런데 FastAPI에는 프로젝트의 구조를 정의하는 규칙이 없다. (?) 기본적으로는 아래의 구조를 가진다. ├── main.py ├── database.py ├── models.py ├── domain │ ├── answer │ ├── question │ └── user └── frontend 프로젝트를 설정하는 main main.py는 FastAPI 애플리케이션의 진입점 역할을 합니다. 여기서 라우터를 등록, 미들웨어 설정, 데이터베이스 연결 등을 수행합니다. from fastapi import FastAPI from domain.question import question_router from domain.answer import answer_router from domain.user import user_router app = FastAPI() # 라우터 등록 app.include_router(question_router.router, prefix=\"/questions\", tags=[\"questions\"]) app.include_router(answer_router.router, prefix=\"/answers\", tags=[\"answers\"]) app.include_router(user_router.router, prefix=\"/users\", tags=[\"users\"]) 주요 역할: FastAPI 객체 생성 라우터를 include_router로 등록 (선택) 데이터베이스 연결, 미들웨어 추가 데이터베이스를 설정하는 database database.py는 데이터베이스와의 연결 및 ORM 설정을 담당합니다. FastAPI에서는 일반적으로 SQLAlchemy 또는 Tortoise ORM을 사용합니다. from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker DATABASE_URL = \"sqlite:///./test.db\" engine = create_engine(DATABASE_URL, connect_args={\"check_same_thread\": False}) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) Base = declarative_base() 주요 역할: 데이터베이스 URL 정의 엔진과 세션 구성 Base 객체 생성 (모델 클래스의 기반) 모델을 관리하는 models models.py는 데이터베이스 테이블을 정의하는 파일입니다. SQLAlchemy를 사용하여 각 테이블의 스키마를 선언합니다. from sqlalchemy import Column, Integer, String, Text, ForeignKey from database import Base class Question(Base): __tablename__ = \"questions\" id = Column(Integer, primary_key=True, index=True) title = Column(String, index=True) content = Column(Text) user_id = Column(Integer, ForeignKey(\"users.id\")) API 프로젝트 구조 비교1. Domain-Driven Design (DDD) 기반 구조 구조 설명 도메인별로 비즈니스 로직, 데이터 처리, 라우팅, 스키마를 모듈화하여 관리합니다. 도메인이 복잡하거나 확장 가능한 시스템에서 적합합니다. 구조 예시 project/ ├── domain/ │ ├── question/ │ │ ├── question_router.py │ │ ├── question_crud.py │ │ └── question_schema.py │ ├── answer/ │ └── user/ ├── main.py ├── database.py └── models.py 2. Service-Oriented Architecture (SOA) 구조 설명 기능별로 서비스를 분리하여 관리합니다. 각 서비스는 독립적이며, 서로 최소한의 의존성을 가집니다. 대규모 서비스에서 적합하며, 마이크로서비스로 전환하기 쉽게 확장 가능합니다. 구조 예시 project/ ├── services/ │ ├── user_service/ │ │ ├── router.py │ │ ├── service.py │ │ └── models.py │ ├── product_service/ │ │ ├── router.py │ │ ├── service.py │ │ └── models.py │ └── order_service/ │ ├── router.py │ ├── service.py │ └── models.py ├── main.py └── shared/ ├── database.py └── utils 3. Monolithic 구조 구조 설명 단일 코드베이스에서 모든 기능을 관리합니다. 소규모 프로젝트 또는 빠른 개발 주기를 요구하는 프로젝트에 적합합니다. 구조 예시 project/ ├── app/ │ ├── routers/ │ │ ├── user_router.py │ │ ├── product_router.py │ │ └── order_router.py │ ├── models/ │ │ ├── user_model.py │ │ ├── product_model.py │ │ └── order_model.py │ └── services/ │ ├── user_service.py │ ├── product_service.py │ └── order_service.py ├── main.py └── database.py 구조 비교 구조 장점 단점 적합한 프로젝트 Domain-Driven Design (DDD) 모듈화로 유지보수 및 확장성 용이 초기 설계 및 도메인 정의에 시간 소요 복잡하고 도메인이 명확한 프로젝트 Service-Oriented Architecture (SOA) 독립성 높은 서비스로 마이크로서비스 전환 용이, 대규모 시스템 관리 적합 서비스 간 통신 설계 및 관리의 복잡성 대규모 및 확장 가능한 프로젝트 Monolithic 구조 간단한 구조로 빠른 개발 가능 규모가 커지면 유지보수 어려움 소규모 프로젝트, 프로토타이핑 등 "},"title":"1. FastAPI 기초 다지기"},"/programing/fastapi/fastapi/fastapi02/":{"data":{"":"","fastapi-model#\u003cstrong\u003eFastAPI Model\u003c/strong\u003e":"FastAPI Model "},"title":"FastAPI Model"},"/programing/git/git/git00/":{"data":{"":"GIT Git이란? 깃(Git)은 2005년에 리누스 토르발스에 의해 개발된 분산 버전관리 시스템(Distributed Version Control Systems - DVCS)\n컴퓨터 파일의 변경사항을 추적하고 여러명의 사용자들 간에 파일에 대한 작업을 조율하는데 사용하는 형상관리 도구 이다.\nDVCS에서의 클라이언트는 단순히 파일의 마지막 스냅샷을 Checkout 하지 않고, 저장소를 히스토리와 더불어 전부 복제하여, 서버에 문제가 생기더라도 바로 복구가 가능하다.\nGIT과 SVN의 차이 단, Git은 기존 SVN(Subversion SVN)와 기능면에서는 유사해 보일 수는 있으나, 아래와 같은 차이를 가지고 있다.\n기능 SVN GIT 파일관리 중앙서버 업로드 로컬저장소 저장 후, 중앙서버 업로드 형상관리 동시 업로드 시 충돌가능 Branch, Merge로 충돌가능성이 낮음 작업관리 모든 작업이 서버에서 진행 작업은 로컬에서 진행 후, 업로드만 서버에 진행 형상관리 히스토리 관리 기능이 부족 히스토리 관리가 용이하게 구현되어있음 즉, Git과 SVN의 가장 큰 차이는 SVN은 서버단에서 작업을 수행하지만, GIT 로컬에서 자기만의 레포지터리를 생성 및, 분기를 이용한 효율적인 형상관리가 가능하다.\n대부분의 버전관리는 시간순으로 진행되나, GIT은 Branch를 통해 관리한다.\nGit은 대부분의 명령을 local에서 진행하기 때문에, 네트워크 등의 영향을 받지 않고, 무결성을 유지함에 보다 용이하다.\nGIT Status Git의 상태에는 크게 3가지가 있다. Committed란 데이터가 로컬 데이터베이스에 안전하게 저장됐다는 것을 의미 Modified는 수정한 파일을 아직 로컬 데이터베이스에 커밋하지 않은 상태 Staged란 현재 수정한 파일을 곧 커밋할 것이라고 표시한 상태 위는 Git의 워킹 트리를 나타내며, Git 디렉토리는 Git이 프로젝트의 메타데이터와 객체 데이터베이스를 저장하는 곳을 의미한다.\n워킹트리는 프로젝트의 특정 버전을 Checkout한 것이며, Git 디렉토리는 현재 작업하는 디스크에 존재하고, 디렉토리 안에 압축된 데이터베이스에서 파일을 가져와 워킹 트리를 생성한다.\nGit에서 Staging Area는 Index라고도 하며, Staging Area는 단순한 파일로, commit할 파일들에 대한 정보를 저장한다.\n즉, Git의 기본적으로 Git의 구동동작은 아래와 같다.\n워킹트리에서 파일을 수정 Staging Area에 파일을 Stage 해서 commit할 스냅샷을 만든다. 여기서 추가, 수정, 삭제 등의 작업이 가능하다. Staging Area에 있는 파일들을 commit해서 Git 디렉토리에 영구적인 스냅샷을 저장한다. 결과적으로 Git 디렉토리에 있는 파일들은 Committed 된 상태이며, 파일을 수정하고 Staging Area에 추가했다면 Staged 된 상태라고 할 수 있다.\n여기서 Checkout 후, 수정했지만 Staging Area에 추가되지 않았다면 Modified된 상태이다.\nGit lifecycle 워킹 디렉토리의 파일은 크게 Tracked(관리대상)과 Untracked(비관리대상)으로 나뉘며, Tracked 파일은 이미 스냅샷에 포함돼 있던 파일(레포지터리에 있던)이다. Tracked 파일은 다시 Unmodified(비수정)와 Modified(수정된), 그리고 Staged(commit으로 저장소에 기록될) 상태로 나뉘어진다. 이 외의 나머지는 untracked파일이며, 이는 워킹 디렉토리에 있는 파일 중 StagingArea에 포함되지 않는 파일이다. 처음 clone을 진행하면, 모든 파일은 Tracked(스냅샷에 포함)파일이지만, Unmodified(수정되지 않았기에)상태이다. 만약 clone 진행 후, 어떤 파일을 수정하게 되면 unmodified -\u003e modified 상태로 상태가 변경되며 실제로 commit을 진행하기 위한 staged 상태를 만들고, staged 상태의 파일을 commit하게 되며, git은 이러한 lifecycle을 반복하게 된다. Git 명령어 정리 표 명령어 설명 주요 옵션 및 설명 git add 변경 사항을 스테이징 영역에 추가 .: 모든 변경 사항 추가 git commit 스테이징된 변경 사항을 커밋 -m \"message\": 커밋 메시지 추가 git status 현재 작업 상태 확인 - git log 커밋 로그 확인 --oneline: 한 줄 요약 git show 특정 커밋의 변경 사항 및 메타데이터 표시 - git diff 변경 사항 비교 --staged: 스테이징된 변경 사항 비교 git push 로컬 커밋을 원격 저장소에 푸시 origin branch: 특정 브랜치로 푸시 git pull 원격 저장소의 변경 사항을 가져와 병합 --rebase: rebase 방식으로 병합 git clone 원격 저장소를 복제 --depth 1: 얕은 복제 (최신 히스토리만 복제) git branch 브랜치를 관리 (생성, 삭제, 목록 등) -d: 브랜치 삭제, -m: 브랜치 이름 변경 git reflog 모든 참조 로그 확인 - git reset 커밋 취소 또는 되돌리기 --hard: 변경 사항 삭제, --soft: 스테이징 유지 git stash 작업 중인 변경 사항 임시 저장 pop: 저장된 변경 사항 적용, list: 스태시 목록 git checkout 다른 브랜치 또는 커밋으로 전환 -b: 새 브랜치 생성 및 전환 git rebase 브랜치의 기반을 다른 브랜치로 변경 --interactive: 대화형 rebase git cherry-pick 특정 커밋을 현재 브랜치에 적용 - git revert 이전 커밋을 되돌리는 새로운 커밋 생성 - git tag 특정 커밋에 태그 추가 -a: 주석이 있는 태그 생성, -d: 태그 삭제 git blame 각 라인의 마지막 변경 내용 표시 - git fetch 원격 저장소의 변경 사항을 가져오기 --all: 모든 브랜치의 변경 사항 가져오기 git merge 다른 브랜치를 현재 브랜치로 병합 --no-ff: fast-forward 병합 방지, --squash: squash 병합 Git 설정 Git 최초 설정 Git을 설치하고 나면 Git의 사용 환경을 적합하게 설정해주어야 한다.\n이는 git config 라는 도구와 설정으로 내용을 확인하고 변경할 수 있다.\n/etc/gitconfig : 시스템의 모든 사용자와 모든 저장소에 적용되는 설정 ( git config –system ) ~/.gitconfig, ~/.config/git/config : 특정 사용자에게만 적용되는 설정 ( git config –global ) .git/config : 현재 디렉토리에만 적용되어 있는 설정 ( git config –local ) 위 설정들은 역순으로 우선시 되어 1 \u003c 2 \u003c 3 과 같은 우선순위를 가지고 있다.\n$ git config --global user.name \"John Doe\" $ git config --global user.email johndoe@example.com –global 옵션으로 설정하는 것은 한번이며 (전역), 만약 프로젝트마다 다른 이름과 메일주소를 사용하고 싶다면 –global 옵션을 빼고 사용한다. $ git config --list user.name=John Doe user.email=johndoe@example.com color.status=auto color.branch=auto color.interactive=auto color.diff=auto $ git config user.name john Doe 설정했던 옵션들은 git config –list 명령어로 확인할 수 있다. Git의 기초 $ cd $ git init 위 명령어를 통해 .git이라는 하위 디렉터리를 만들며, 이 안에는 저장소에 필요한 뼈대 파일(Skeleton)이 들어 있다. $ git add *.c $ git add LICENSE $ git commit -m 'initial project version' 위 명령어를 통해 GIt 저장소가 생성되었고 파일 버전 관리가 시작되었다. (commit) $ git clone https://github.com/libgit2/libgit2 $ git clone https://github.com/libgit2/libgit2 \u003cother name\u003e 다른 프로젝트의 참여하고 싶거나, git 저장소를 복사하고 싶을 때, git clone 명령어를 사용한다. Subversion과 같은 VCS에 익숙한 사용자에게는 “Checkout” 이 아닌, “clone” 이라는 점이 다르며, git은 서버에 있는 거의 모든 데이터를 복사한다. 즉, git은 서버의 모든 데이터 및 프로젝트의 히스토리를 전부 받아와, 실제 서버의 디스크가 망가져도, 로컬에서 복구가 가능한 특징을 가진다. (단, 서버설정파일은 제외) $ git status On branch master Your branch is up-to-date with 'origin/master'. nothing to commit, working directory clean 위 명령어를 실행하면 현재 하나도 수정되지 않았음을 알려준다. $ echo 'My Project' \u003e README $ git status On branch master Your branch is up-to-date with 'origin/master'. Untracked files: (use \"git add \u003cfile\u003e...\" to include in what will be committed) README nothing added to commit but untracked files present (use \"git add\" to track) README 파일은 “Untracked”에 속해 있는데 이것은 README 파일이 Untracked 상태라는 것을 의미한다. Git은 Untracked 파일을 아직 스냅샷(커밋)에 넣어지지 않은 파일이라고 본다. git init 명령을 실행한 후, git add (files) 명령을 실행했던 걸 기억할 것이다. 이 명령을 통해 디렉토리에 있는 파일을 추적하고 관리하도록 한다. git add 명령은 파일 또는 디렉토리의 경로를 아규먼트로 받는다. $ git add README $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes to be committed: (use \"git reset HEAD \u003cfile\u003e...\" to unstage) new file: README Changes not staged for commit: (use \"git add \u003cfile\u003e...\" to update what will be committed) (use \"git checkout -- \u003cfile\u003e...\" to discard changes in workingdirectory) modified: CONTRIBUTING.md git status 명령을 실행하면 README 파일이 Tracked 상태이면서 commit에 추가될 Staged 상태라는 것이 확인이 가능하다. “Changes to be commiteed” 에 들어 있는 파일은 Staged 상태라는 것을 의미한다. 즉, commit을 실행하면 add를 실행한 시점의 파일이 commit되어 stage -\u003e git history에 남게된다. CONTRIBUTING.md은 “Changes not staged for commit\"에 있으며, 이는 수정한 파일이 Tracked 상태이지만 아직 Staged 상태는 아니라는 것이며 이는 add 명령어를 통해 staged에 올릴 수 있음을 의미한다. $ git add CONTRIBUTING.md $ git status On branch master Your branch is up-to-date with 'origin/master'. Changes to be committed: (use \"git reset HEAD \u003cfile\u003e...\" to unstage) new file: README modified: CONTRIBUTING.md add 후 다시 staus를 입력하면 staged 상태로 올라간 것을 확인할 수 있다. $ git status -s M README MM Rakefile A lib/git.rb M lib/simplegit.rb ?? LICENSE.txt 이는 위처럼 status -s 옵션을 통해 간단하게 확인할 수 있으며 앞에 문자는 아래와 같은 의미를 가진다. A : New file M : Modified file MM : 작업 디렉터리 및 스테이지 변경 ?? : Untracked Unmodified 파일을 출력되지 않음 파일무시하기 $ cat .gitignore *.[oa] *~ .gitignore 파일을 만들고 그 안에 무시할 패턴을 적으면, 해당 패턴과 일치하는 파일들은 commit 되지 않는다. .gitignore은 아래와 같은 특징을 가진다. 아무것도 없는 라인이나, #로 시작하는 라인은 무시한다. 표준 Glob 패턴을 사용한다. 이는 프로젝트 전체에 적용된다. 슬래시(/)로 시작하면 하위 디렉토리에 적용되지(Recursivity) 않는다. 디렉토리는 슬래시(/)를 끝에 사용하는 것으로 표현한다. 느낌표(!)로 시작하는 패턴의 파일은 무시하지 않는다 기타 예제는 (gitignore_repo)[https://github.com/github/gitignore]를 참조하자. 변경사항 확인하기 $ git diff $ git diff --staged git status 명령은 특정파일의 Staged 상태인지는 확인할 수 있으나, 변경사항은 확인할 수 없다. git diff 명령을 사용하는데 Patch처럼 어떤 라인을 추가했고 삭제했는지가 궁금할 때 사용한다. $ git log commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon \u003cschacon@gee-mail.com\u003e Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 Author: Scott Chacon \u003cschacon@gee-mail.com\u003e Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test commit a11bef06a3f659402fe7563abf99ad00de2209e6 Author: Scott Chacon \u003cschacon@gee-mail.com\u003e Date: Sat Mar 15 10:31:28 2008 -0700 first commit $ git log -p -2 commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon \u003cschacon@gee-mail.com\u003e Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number diff --git a/Rakefile b/Rakefile ... $ git log --stat commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon \u003cschacon@gee-mail.com\u003e Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number Rakefile | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 ... git log 명령을 실행하면 저장소의 커밋 히스토리를 시간순으로 보여준다. 즉, 가장 최근의 커밋이 가장 먼저 나온다. 그리고 이어서 각 커밋의 SHA-1 체크섬, 저자 이름, 저자 이메일, 커밋한 날짜, 커밋 메시지를 보여준다. -p, –patch 는 굉장히 유용한 옵션이다. -p 는 각 커밋의 diff 결과를 보여준다. –stat 옵션으로 각 커밋의 통계 정보를 출력할 수 있다. –pretty 옵션이다. 이 옵션을 통해 히스토리 내용을 보여줄 때 기본 형식 이외에 여러 가지 중에 하나를 선택할 수 있다. 회귀 $ git commit -m 'initial commit' $ git add forgotten_file $ git commit --amend 위는 실수로 파일을 Stage 하는 것을 깜빡하고 빠트린 파일이 있으면 위와 같이 고칠 수 있다. 하지만, 이는 두 번째 commit이 첫 번째 commit이 완전히 뒤집어 쓰는 것(첫 번째 commit은 히스토리가 없어진다)으로 주의가 필요하다. $ git reset HEAD CONTRIBUTING.md Unstaged changes after reset: M CONTRIBUTING.md $ git status On branch master Changes to be committed: (use \"git reset HEAD \u003cfile\u003e...\" to unstage) renamed: README.md -\u003e README Changes not staged for commit: (use \"git add \u003cfile\u003e...\" to update what will be committed) (use \"git checkout -- \u003cfile\u003e...\" to discard changes in working directory) modified: CONTRIBUTING.md $ git status Changes not staged for commit: (use \"git add \u003cfile\u003e...\" to update what will be committed) (use \"git checkout -- \u003cfile\u003e...\" to discard changes in working directory) modified: CONTRIBUTING.md $ git checkout -- CONTRIBUTING.md $ git status On branch master Changes to be committed: (use \"git reset HEAD \u003cfile\u003e...\" to unstage) renamed: README.md -\u003e README reset과 staged는 비슷하게 보일 수 있으나, 서로 사용되는 영역이 다른다. reset은 staged영역을 조절하는 데 사용되며, checkout은 주로 브런치 간 이동이나 특정 파일의 변경 내용을 취소하는 데 사용된다. 리모트 저장소 $ git remote -v origin https://github.com/schacon/ticgit (fetch) origin https://github.com/schacon/ticgit (push) remote는 연결되어 있는 저장소를 의미하며, 이는 단지 네트워크 뿐이 아닌 다른 저장소도 의미함을 인지한다. 위 명령어를 통해 현재 연결되어 있는 저장소를 출력할 수 있다. $ git remote origin $ git remote add pb https://github.com/paulboone/ticgit $ git remote -v origin https://github.com/schacon/ticgit (fetch) origin https://github.com/schacon/ticgit (push) pb https://github.com/paulboone/ticgit (fetch) pb https://github.com/paulboone/ticgit (push) git remote add \u003c단축이름\u003e 으로 원격 저장소를 연격할 수 있다. ","#":"","git#GIT":"","git-설정#Git 설정":"","git과-svn의-차이#\u003cstrong\u003eGIT과 SVN의 차이\u003c/strong\u003e":"","git의-기초#Git의 기초":"","git이란#Git이란?":""},"title":"0. Git이란?"},"/programing/git/git/git03/":{"data":{"git#GIT":"","git-branch-이란#\u003cstrong\u003eGit Branch 이란?\u003c/strong\u003e":"","git-merge-이란#\u003cstrong\u003eGit Merge 이란?\u003c/strong\u003e":"","학습을-위한-git을-통한-형상-관리-및-협업-시나리오#\u003cstrong\u003e학습을 위한 Git을 통한 형상 관리 및 협업 시나리오\u003c/strong\u003e":"GIT Git Branch 이란? Branch란 독립적인 이력 관리 영역을 의미\nBranch이란 특정지점의 커밋에서 분기 해서 커밋을 이어 나가는 모습이 마치 나무의 가지가 뻗어 나가는것과 비슷하다고하여 붙여진 이름 브랜치를 사용하면 저장소를 따로 만들 필요 없이 한 저장소 안에서 기능추가,디버그, 테스트등의 작업을 동시에 할 수 있으며, 저장소 를 안정적이면서도 유연하게 운영이 가능\nGit Merge 이란? 다른 브랜치의 작업내용을 현재 작업 중인브 랜치에 병합할때 git merge를 사용 단, git merge를 사용하면 자동으로 병합되지만, 충돌이 발생시에는 사용자가 직접 충돌을 해결해야 됨 Fast-forward Merge 특징\n두 개의 브랜치가 존재\n하나의 브랜치에서 새로운 커밋을 생성\n다른 브랜치에서는 아무런 변경 사항이 없음\n브랜치는 분리되어 있지 않고 직선으로 연결된 것처럼 보이는 특징을 가지며, 단순히 두 브랜치의 포인터를 이동시켜 새로운 커밋을 가리키게 만드는 것만으로 병합이 이루어지는 병합을 Fast-forward 병합이라고 함, 이 때, 기존 브랜치가 새로운 커밋으로 이동\n3-way Merge 특징\n두 개의 브랜치가 존재\n각 브랜치에서는 서로 다른 변경 사항 존재\n이 두 브랜치를 병합\n병합진행 과정\n병합할 브랜치의 마지막 커밋 (merge 대상 브랜치)\n현재 브랜치의 마지막 커밋 (merge를 실행하는 브랜치)\n두 브랜치의 공통 조상 (공통 부모 커밋)\n이런 방식으로 병합을 수행하여 각 브랜치에서의 변경 사항을 적절히 통합\nFast-forward 병합: 두 브랜치의 이력이 간단하고 직선적인 경우에 발생합니다. 이전 커밋이 공통 부모가 되지 않으며, 단순히 브랜치의 포인터를 이동시켜 새로운 커밋\n3-way 병합: 두 브랜치가 서로 다른 변경 사항을 가지고 있을 때 발생하며, 이전 커밋이 공통 조상이 됩니다. 이전 커밋을 기반으로 변경 사항을 적절히 통합하여 새로운 병합 커밋\n학습을 위한 Git을 통한 형상 관리 및 협업 시나리오 Git 초기 설정 및 레포지터리 생성 mkdir my-project cd my-project git init # 원격 레포지터리 추가 git remote add origin https://github.com/username/my-project.git 첫 파일 추가 및 커밋 # 파일 생성 echo \"# My Project\" \u003e README.md # 파일을 스테이징하고 커밋 git add README.md git commit -m \"Initial commit\" # 원격 저장소로 푸시 git push origin master 브런치 생성 및 전환 # 새로운 브랜치 생성 및 전환 git checkout -b feature-branch 코드 수정 및 푸쉬 echo \"New feature code\" \u003e feature.txt # 변경 사항 스테이징 및 커밋 git add feature.txt git commit -m \"Add new feature\" 브랜치 병합 수행 (Fast-forward) # master 브랜치로 전환 git checkout master # 병합 수행 (Fast-forward) git merge --ff feature-branch # 병합 결과 확인 git log 브랜치 병합 수행 (No-fast-forward) # 새로운 기능 브랜치 생성 및 전환 git checkout -b feature-branch-2 # 파일 수정 및 커밋 echo \"Another new feature\" \u003e feature2.txt git add feature2.txt git commit -m \"Add another new feature\" # master 브랜치로 전환 및 병합 (No-fast-forward) git checkout master git merge --no-ff feature-branch-2 Git 협업 # 원격 저장소에서 변경 사항 가져오기 git fetch origin git pull origin master git push origin feature-branch Git 협업 # feature-branch 브랜치를 master 브랜치로 리베이스 git checkout feature-branch git rebase master Git 복구하기 # 마지막 커밋 되돌리기 (작업 내용 유지) git reset --soft HEAD~1 마지막 수정 인물 추정 # 파일의 각 라인의 변경 내용 표시 git blame \u003cfile\u003e "},"title":"3. Git Branch"},"/programing/git/git/git99/":{"data":{"":"","git-명령어-요약정리#GIT 명령어 요약정리":" Git 명령어는 git +명령어 이름 형태로 구성하며, 필요에 따라 -키 또는 –옵션을 추가가 가능\nGit 작업영역: Working Directory, Staging Area, Repository\nGit 파일의상태: Modified,Staged,Commited\n브런치(branch): 독립적인 이력 관리 영역 / git init시 master 브런치가 생성됨\nmaster: 해당브랜치의 끝(최신커밋)을 참조하는 개체 (Refs)\nHEAD: 포인터 역할을 수행하며, 커밋을 직접 참조 할 수 있을 뿐만 아니라 Refs(다른참조 개체)도 참조가능\ngit init # 저장소(repository) 생성 git config user.name \"［작성자 이름］\" # 사용자 이름 설정 --global 추가시 전역설정 git config user.email \"［이메일 계정]\" # 사용자 이메일 설정 --global 추가시 전역설정 git config --list # 저장소 설정 전체 출력 git config ［설정 항목] # 해당 항목 설정 값 출력(예 : git config user.name) git help # 도움말 git status # 저장소의 상태 정보 출력 git add ［파일 이름 \u0026 디렉터리] # 해당 파일을 Staging Area에 update git add . # Working Directory 안에 추가，수정된 모든 파일을 Staging Area에 을리기 git commit # 이력 저장，커밋 git commit -m \"［메시지］\" # vim을 사용하지 않고 인라인으로 메시지를 추가하여 커밋 git commit -am \"［메시지］\" # git add와 git commit을 한꺼번에 명령 (Untracked 파일은 제외) git status # 저장소 파일의 상태 출력 git status -s # 저장소 파일 상태를 간략하게 표시 git log # 저장소의 커밋 히스토리（로그，이력）를 출력 git log --pretty=oneline # 로그 출력 시 커밋을 한 줄로 간략하게 표시 （--pretty 옵션 사용） git log --oneline # 로그 출력 시 커밋을 한 줄로 표시 （해시도 앞자리 7글자만 출력） git log --graph # 로그를 그래프 형태로 출력 git show # 가장 최근 커밋의 상세 정보 출력 git show ［커밋 해시］ # 해당 커밋의 상세 정보 출력 git show HEAD # HEAD가 참조하는 커밋의 상세 정보 출력 git show HEAD〜n # HEAD를 기준으로 n단계 이전의 커밋 정보 출력 - git diff는 각 파일의 변경사항을 비교하는 명령어 git diff # 최근 커밋과 변경 사항이 발생한(Unstaged) 파일들의 내용 비교 git diff --staged # 현재 스테이지된 변경사항을 출력 git diff [commit hash1] [commit hash2] # commit1과 commit2를 비교 git reset # Staging Area 의파일전체 를언스테이징 상태로 되돌리기 git reset [파일이름(또는경로)] # 해당파일(또는경로)을 언스테이징 상태로 되돌리기 git checkout은 HEAD의 참조를 변경하는 명령 최신 커밋을 참조할 때 HEAD는 master를 간, 직접적으로 참조하고 있으며, HEAD는 master에서 떨어진 Detacghed 상태 또는 DetachedHEAD가 됨 gitcheckout [커밋해시] # 해당커밋으로파일상태변경 git checkout-: # HEAD가이전에참조했던커밋으로상태변경 git checkout master # HEAD가master를참조 git checkout HEAD~n # HEAD를기준으로 n단계이전커밋으로상태변경 git reset은 커밋을 취소하는 명령어이지만, 해당 커밋이 삭제되는 것이 아닌 브런치의 포인터를 바꾸는 개념 Working Dirtory Stating Area Repository –hard 변경 변경 변경 –mixed (default) 유지 변경 변경 –soft 유지 유지 변경 git reset [커밋주소] git remote # 현재 브랜치에 추가 된 원격 저장소 리스트 출력 git remote -v (--verbose) # 현재 브랜치에 추가 된 원격 저장소 리스트 출력(주소포함) git remote add [원격저장소이름] [원격저장소주소] # 해당이름으로 원격저장소의 주소등록 git remote rm [원격저장소이름] # 해당 원격 저장소를 삭제 git push -u (--set-upstream-to) [원격 저장소 이름] [로컬 저장소의 브런치] # 로컬저장소의 브랜치가 원격저장소를 추적하도록 설정하고, 파일들을 원격저장소로 저장 git push [원격 저장소 이름] [로컬 저장소 브런치] # 로컬 브런치의 변경사항을 원격 저장소 (레포지터리)에 Push (업로드) # upstream (-u) 설정 후, 원격 저장소, 로컬 저장소 브런치 생략이 가능 git pull [원격 저장소 이름] [로컬 저장소 브런치] # 원격저장소의 정보를 현재 로컬브랜치에 병합 (fetch + merge) # upstream (-u) 설정 후, 원격 저장소, 로컬 저장소 브런치 생략이 가능 git tag # 로컬저장소의 모든 태그를 조회 git tag [태그이름] # 현재 커밋에 태그를 생성(Lightweight 태그) git tag [태그이름] [커밋해시] # 해당 커밋에 태그를 생성 (Lightweight 태그) git tag -a [태그이름] -m \"[메시지]\" [커밋해시] # 메시지를 추가하여 태그 생성 (Annotated tag) git tag -am [태그이름] \"[메시지]\" # 현재 커밋에 메시지를 추가하여 태그 생성 (Annotated tag) git show [태그이름] # 해당 태그가 부착된 커밋의 상세정보 확인 git push --tags # 생성된 전체태그를 원격 저장소에 푸시(=git push origin -tags) git push [태그이름] # 해당태그를 원격저장소에 푸시 (= git push origin \"[태그이름]\") git tag -d [태그이름] # 해당 태그 삭제 git push -d [태그이름] # 원격저장소에서 해당 태그 삭제 git reset은 master의 참조를 특정 커밋으로 되돌리는 명령 git revert는 특정 커밋을 취소하는 내용의 새로운 커밋을 만드는 명령 git revert [커밋 해시] # 해당 커밋으로 되돌리기 git revert -n (--no-edit) [커밋 해시] # 커밋 메시지 수정 없이 기본메시지로 되돌리기 git revert n [커밋 해시] # 커밋 하지 않고 스테이징 상태로 되돌리기 git revert [커밋 해시1]..[커밋 해시2] # 해당 구간 만큼 커밋 되돌리기, 커밋 해시1은 되돌려지지 않음 git branch # 현재 브랜치 표시 git branch --list # 브랜치 목록 표시 git branch [브랜치이름] # 해당 브랜치이름으로 브랜치 생성 git checkout [브랜치이름] git switch [브랜치이름] # 해당브랜치로전환 git checkout -b [브랜치이름] git switch -c [브랜치이름] # 브랜치 생성과 동시에 전환 git branch -m [브랜치이름] [새로운 브랜치이름] # 브랜치 이름변경 git branch -d [브랜치이름] # 해당 로컬 브랜치 삭제 git push -d [repo] [브랜치이름] # 레포지터리 브런치 삭제 git merge [브랜치이름] # 해당 이름의 브랜치를 병합 git merge --ff [브랜치이름] # fast-for ward 관계에 있으면 커밋을 생성하지않고 현재 브랜치의 참조만 변경(default) git merge --no-ff [브랜치이름] # fast-forward 관계에 있어도 머지 커밋 생성 git merge --squash [브랜치이름] # 병합할 브랜치의 내용을 하나의 커밋에 합침 병합할 브랜치 정보는 생략 git rebase [브랜치이름] # 현재 브랜치가 해당 브랜치(브랜치이름)부터 분기하도록 재배치 git rebase --continue # 충돌 수정 후 재배치 진행 git rebase --abort # rebase 취소 "},"title":"99. Git 명령어 요약정리"},"/programing/go/go/go00/":{"data":{"":" ","1-go-언어의-역사-및-발전-배경#1️⃣ Go 언어의 역사 및 발전 배경":"Go 언어는 2007년 Google에서 Rob Pike, Ken Thompson, and Robert Griesemer와 같은 유명한 엔지니어들에 의해 설계되었으며, 2009년에 오픈 소스로 공개되었습니다. 그 목적은 성능이 뛰어나고 효율적인 동시성(concurrency)을 지원하는 프로그래밍 언어를 만드는 것이었습니다. Go 언어는 C 언어의 장점을 계승하면서도, 더 간결하고 모던한 개발 환경을 제공합니다.\nGo 언어의 발전 배경 Go는 시스템 언어로서 빠르고, 메모리 안전성, 병렬 처리 및 웹 서비스 개발 등에 강점을 가지고 있습니다. Google Cloud, Docker, Kubernetes, 그리고 많은 웹 서비스와 클라우드 인프라에서 사용됩니다.","2-go-언어의-특징#2️⃣ Go 언어의 특징":"간결성 Go 언어는 코드 작성의 간결함과 가독성에 중점을 두고 설계되었습니다. 문법이 간단하여 배우기 쉽고, 작성한 코드가 명확하게 읽힙니다. 예를 들어, Go는 세미콜론을 자동으로 처리하고, 중괄호로 블록을 구분합니다.\n예제: 간단한 “Hello, World!” 프로그램 package main import \"fmt\" func main() { fmt.Println(\"Hello, World!\") } 성능 Go는 C 언어와 비슷한 수준의 성능을 제공하며, 실행 속도가 빠릅니다. 이는 Go가 컴파일된 언어이기 때문에 가능하며, 가비지 컬렉션을 갖추고 있어서 메모리 관리를 효율적으로 처리할 수 있습니다.\n동시성 지원 Go의 가장 큰 특징 중 하나는 “goroutine\"을 통한 동시성(concurrency) 지원입니다. Go는 경량 스레드인 goroutine을 사용하여 매우 효율적으로 동시 작업을 처리할 수 있습니다.\n예제: goroutine을 사용한 동시성 처리 package main import \"fmt\" func sayHello() { fmt.Println(\"Hello from goroutine!\") } func main() { go sayHello() // goroutine을 실행 fmt.Println(\"Hello from main!\") } ","3-go-언어와-다른-언어c-python-java-비교#3️⃣ Go 언어와 다른 언어(C, Python, Java) 비교":"C와의 비교 Go는 C 언어의 간결함을 계승하면서도 더 높은 수준의 동시성 지원과 가비지 컬렉션을 제공합니다. C는 포인터 연산과 메모리 관리에 주의를 기울여야 하지만, Go는 메모리 안전성에 초점을 맞추고 있습니다.\nPython과의 비교 Go는 Python보다 더 빠르게 실행됩니다. Python은 인터프리터 언어인 반면, Go는 컴파일 언어로서 더 높은 성능을 제공합니다. 또한 Go는 멀티스레딩을 쉽게 구현할 수 있지만, Python은 GIL(Global Interpreter Lock)로 인해 동시성 처리에서 제한적입니다.\nJava와의 비교 Go는 Java보다 가벼운 런타임을 가집니다. Go의 코드 실행은 빠르며, JVM을 필요로 하지 않기 때문에 메모리 사용이 적고 실행 속도가 빠릅니다. Java는 객체 지향 언어로 클래스와 상속을 기반으로 하지만, Go는 간단한 구조체와 인터페이스로 이루어져 있습니다.","4-go-언어의-사용-사례#4️⃣ Go 언어의 사용 사례":"Web Development Go는 웹 서버 및 웹 애플리케이션 개발에 널리 사용됩니다. Go의 표준 라이브러리에는 HTTP 서버와 클라이언트가 내장되어 있어, RESTful API 서버나 고성능 웹 애플리케이션을 손쉽게 구축할 수 있습니다.\n예제: 간단한 웹 서버 package main import ( \"fmt\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \"Hello, Web!\") } func main() { http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } Microservices Go는 마이크로서비스 아키텍처에 적합한 언어입니다. 경량화된 구조와 빠른 성능 덕분에 대규모 분산 시스템에서 효율적으로 동작합니다.\nCloud Infrastructure Go는 Docker와 Kubernetes와 같은 클라우드 인프라 기술의 핵심 언어입니다. Go의 성능과 동시성 처리 능력은 클라우드 기반 애플리케이션의 요구 사항을 잘 충족합니다.","5-go-언어의-에코시스템#5️⃣ Go 언어의 에코시스템":"GoDoc GoDoc은 Go 언어의 공식 문서화 도구입니다. Go 패키지와 함수에 대한 문서를 자동으로 생성하여 개발자들이 쉽게 사용할 수 있도록 합니다.\nGo Modules Go Modules는 Go 프로젝트의 의존성을 관리하는 도구입니다. Go Modules를 사용하면 외부 라이브러리와의 의존성을 쉽게 관리할 수 있습니다.\nGo Repositories Go에는 수많은 오픈소스 패키지가 있으며, 이를 GitHub와 같은 플랫폼에서 찾을 수 있습니다. 많은 개발자들이 Go 언어를 사용하여 다양한 라이브러리와 도구들을 만들고 공유하고 있습니다."},"title":"Go 언어 소개 및 기본 개념"},"/programing/go/go/go01/":{"data":{"":" ","1-go-언어-설치-windows-macos-linux#1️⃣ Go 언어 설치 (Windows, macOS, Linux)":"Go를 사용하려면 먼저 Go 공식 웹사이트에서 설치 파일을 다운로드해야 합니다.\n1. Windows에 Go 설치하기 Go 공식 웹사이트에서 최신 버전의 Go 설치 파일을 다운로드합니다. 다운로드한 .msi 파일을 실행하고 설치를 진행합니다. 설치가 완료되면, 터미널에서 아래 명령어를 실행하여 설치 확인합니다. go version 2. macOS에 Go 설치하기 macOS에서는 Homebrew를 사용하여 Go를 설치할 수 있습니다.\nbrew install go 설치 후, 아래 명령어로 Go가 정상적으로 설치되었는지 확인합니다.\ngo version 3. Linux에 Go 설치하기 Go 공식 웹사이트에서 최신 버전의 Linux용 압축 파일(.tar.gz)을 다운로드합니다. 아래 명령어를 사용하여 Go를 /usr/local 경로에 설치합니다. wget https://go.dev/dl/go1.XX.X.linux-amd64.tar.gz sudo tar -C /usr/local -xzf go1.XX.X.linux-amd64.tar.gz PATH 환경 변수를 설정합니다. echo 'export PATH=$PATH:/usr/local/go/bin' \u003e\u003e ~/.bashrc source ~/.bashrc 설치 확인: go version ","2-go-개발-환경-설정-vscode-goland-atom-등#2️⃣ Go 개발 환경 설정 (VSCode, GoLand, Atom 등)":"Go 개발을 위한 대표적인 IDE(통합 개발 환경) 및 에디터는 다음과 같습니다.\n1. VSCode 설정하기 VSCode 공식 웹사이트에서 VSCode를 다운로드하고 설치합니다. “Go” 확장 프로그램을 설치합니다. (Ctrl+Shift+X → Go 검색 → 설치) 터미널에서 Go 관련 환경 변수를 설정하고 적용합니다. export GOPATH=$HOME/go export PATH=$PATH:$GOPATH/bin 2. GoLand 설정하기 GoLand 공식 웹사이트에서 설치 파일을 다운로드합니다. 설치 후 Go SDK를 설정하고 프로젝트를 생성합니다. 3. Atom 설정하기 Atom을 설치한 후, “go-plus” 패키지를 설치합니다. 추가로 “gofmt”, “golint” 등의 플러그인을 설치하여 코드 품질을 유지할 수 있습니다. ","3-go-modules-설정-및-사용-go-mod-init-go-mod-tidy-go-get#3️⃣ Go Modules 설정 및 사용 (go mod init, go mod tidy, go get)":"Go Modules는 의존성 관리를 쉽게 하기 위해 도입된 기능입니다. 프로젝트에서 Go Modules을 설정하려면 아래 단계를 따르면 됩니다.\n1. Go Modules 초기화 새 프로젝트를 생성하고 Go Modules을 활성화합니다.\nmkdir myproject \u0026\u0026 cd myproject go mod init myproject go.mod 파일이 생성되며, 프로젝트의 의존성을 관리하는 역할을 합니다.\n2. 의존성 추가 및 정리 외부 패키지를 추가할 때 go get을 사용합니다.\ngo get github.com/gin-gonic/gin 사용하지 않는 패키지를 정리하려면 go mod tidy 명령어를 실행합니다.\ngo mod tidy ","4-go-workspace-및-gopath-이해#4️⃣ Go Workspace 및 GOPATH 이해":"Go 1.18부터 “Go Workspace” 기능이 추가되었으며, 기존 GOPATH와 차이가 있습니다.\n1. GOPATH란? GOPATH는 Go의 기본 작업 디렉토리를 지정하는 환경 변수입니다.\nexport GOPATH=$HOME/go export PATH=$PATH:$GOPATH/bin 기본적으로 $GOPATH/src 아래에서 코드를 작성해야 했지만, Go Modules을 사용하면 프로젝트 폴더 어디에서나 개발이 가능합니다.\n2. Go Workspace 설정하기 Go 1.18부터 여러 개의 모듈을 하나의 Workspace에서 관리할 수 있습니다.\ngo work init go work use ./module1 go work use ./module2 go.work 파일이 생성되며, 여러 모듈을 한 번에 관리할 수 있습니다.","5-go-실행-및-빌드-go-run-go-build-go-install#5️⃣ Go 실행 및 빌드 (go run, go build, go install)":"Go 프로그램을 실행하고 빌드하는 방법은 다음과 같습니다.\n1. Go 코드 실행 (go run) Go 파일을 직접 실행할 수 있습니다.\ngo run main.go 2. Go 코드 빌드 (go build) 프로그램을 빌드하여 실행 가능한 바이너리를 생성할 수 있습니다.\ngo build -o myapp main.go 3. Go 패키지 설치 (go install) Go 패키지를 설치하고 실행할 수 있습니다.\ngo install myproject 설치된 실행 파일은 $GOPATH/bin에 저장됩니다.","6-go-의존성-관리와-버전-관리-go-111-이상#6️⃣ Go 의존성 관리와 버전 관리 (Go 1.11 이상)":"Go 1.11부터는 go modules를 사용하여 패키지 의존성을 관리할 수 있습니다.\n1. 특정 버전의 패키지 설치하기 Go에서는 특정 버전의 패키지를 설치할 수 있습니다.\ngo get github.com/gin-gonic/gin@v1.7.7 2. 의존성 업데이트 패키지의 최신 버전으로 업데이트하려면 다음 명령어를 실행합니다.\ngo get -u all 3. go mod tidy 사용 사용하지 않는 패키지를 정리합니다.\ngo mod tidy "},"title":"Go 언어 설치 및 환경 설정"},"/programing/go/go/go02/":{"data":{"":" ","3-반복문-for-loop-break-continue#3️⃣ 반복문 (for loop, break, continue)":"Go는 for 문을 사용하여 반복문을 구현합니다.\n1. 기본 for 문 package main import \"fmt\" func main() { for i := 0; i \u003c 5; i++ { fmt.Println(\"반복:\", i) } } 2. break 문 package main import \"fmt\" func main() { for i := 0; i \u003c 10; i++ { if i == 5 { break } fmt.Println(i) } } 3. continue 문 package main import \"fmt\" func main() { for i := 0; i \u003c 5; i++ { if i == 2 { continue } fmt.Println(i) } } ","3-변수-선언-및-데이터-타입-int-string-float-bool-등#3️⃣ 변수 선언 및 데이터 타입 (int, string, float, bool 등)":"Go에서는 변수를 선언할 때 var 키워드 또는 := 단축 선언을 사용합니다.\n1. 변수 선언 방법 package main import \"fmt\" func main() { var a int = 10 var b float64 = 3.14 var c string = \"Hello, Go!\" var d bool = true fmt.Println(a, b, c, d) // 단축 선언 e := 42 // 타입을 자동으로 추론 fmt.Println(e) } 2. 데이터 타입 타입 설명 예제 int 정수형 var x int = 10 float 실수형 var y float64 = 3.14 string 문자열 var s string = \"Hello\" bool 논리형 (true 또는 false) var b bool = true ","3-상수와-iota#3️⃣ 상수와 iota":"Go에서는 const 키워드를 사용하여 상수를 선언합니다.\n1. 기본적인 상수 선언 package main import \"fmt\" func main() { const PI float64 = 3.14159 const Greeting = \"Hello, Go!\" fmt.Println(PI, Greeting) } 2. iota 활용하기 iota는 열거값을 자동으로 증가시키는 기능을 합니다.\npackage main import \"fmt\" func main() { const ( A = iota // 0 B // 1 C // 2 ) fmt.Println(A, B, C) } ","3-연산자-산술-비교-논리-연산자#3️⃣ 연산자 (산술, 비교, 논리 연산자)":"Go는 다양한 연산자를 제공합니다.\n1. 산술 연산자 package main import \"fmt\" func main() { a, b := 10, 3 fmt.Println(\"덧셈:\", a+b) fmt.Println(\"뺄셈:\", a-b) fmt.Println(\"곱셈:\", a*b) fmt.Println(\"나눗셈:\", a/b) fmt.Println(\"나머지:\", a%b) } 2. 비교 연산자 package main import \"fmt\" func main() { a, b := 5, 10 fmt.Println(a == b) // false fmt.Println(a != b) // true fmt.Println(a \u003e b) // false fmt.Println(a \u003c b) // true } 3. 논리 연산자 package main import \"fmt\" func main() { x, y := true, false fmt.Println(x \u0026\u0026 y) // false fmt.Println(x || y) // true fmt.Println(!x) // false } ","3-조건문-if-else-switch#3️⃣ 조건문 (if, else, switch)":"Go에서는 if, else 및 switch 문을 사용하여 조건문을 작성할 수 있습니다.\n1. if-else 문 package main import \"fmt\" func main() { num := 10 if num \u003e 0 { fmt.Println(\"양수입니다.\") } else if num \u003c 0 { fmt.Println(\"음수입니다.\") } else { fmt.Println(\"0입니다.\") } } 2. switch 문 package main import \"fmt\" func main() { day := \"월요일\" switch day { case \"월요일\": fmt.Println(\"한 주의 시작!\") case \"금요일\": fmt.Println(\"불금!\") default: fmt.Println(\"평범한 하루\") } } ","3-패키지와-임포트-import-main-패키지#3️⃣ 패키지와 임포트 (import, main 패키지)":"Go는 패키지를 기반으로 구성됩니다.\n1. 기본 패키지 구조 package main // 메인 패키지 import \"fmt\" // 표준 패키지 가져오기 func main() { fmt.Println(\"Hello, World!\") } 2. 사용자 정의 패키지 mathutil.go라는 파일을 생성하고 아래 내용을 추가합니다.\npackage mathutil func Add(a int, b int) int { return a + b } 이제 main.go에서 해당 패키지를 사용해보겠습니다.\npackage main import ( \"fmt\" \"./mathutil\" ) func main() { result := mathutil.Add(10, 5) fmt.Println(\"결과:\", result) } ","3-함수-선언-및-호출#3️⃣ 함수 선언 및 호출":"Go에서는 func 키워드를 사용하여 함수를 선언합니다.\n1. 기본 함수 선언 package main import \"fmt\" func sayHello() { fmt.Println(\"Hello, Go!\") } func main() { sayHello() } 2. 매개변수와 반환값이 있는 함수 package main import \"fmt\" func add(a int, b int) int { return a + b } func main() { result := add(3, 5) fmt.Println(\"결과:\", result) } 3. 다중 반환값 함수 package main import \"fmt\" func swap(a, b string) (string, string) { return b, a } func main() { x, y := swap(\"Hello\", \"World\") fmt.Println(x, y) } "},"title":"Go 언어 기본 문법"},"/programing/go/go/go03/":{"data":{"":"","4-구조체struct와-메서드#4️⃣ 구조체(Struct)와 메서드":"","4-맵map과-그-활용법#4️⃣ 맵(Map)과 그 활용법":"","4-배열과-슬라이스slice#4️⃣ 배열과 슬라이스(Slice)":"","4-배열과-슬라이스의-차이점-및-선택-기준#4️⃣ 배열과 슬라이스의 차이점 및 선택 기준":" 4️⃣ 배열과 슬라이스(Slice) Go에서 배열과 슬라이스는 데이터를 저장하는 중요한 구조입니다.\n1. 배열(Array) 배열은 고정된 크기를 가지며, 같은 타입의 데이터를 저장할 수 있습니다.\npackage main import \"fmt\" func main() { var arr [5]int = [5]int{1, 2, 3, 4, 5} fmt.Println(arr) // [1 2 3 4 5] arr[2] = 10 fmt.Println(arr) // [1 2 10 4 5] } 2. 슬라이스(Slice) 슬라이스는 배열과 달리 크기가 동적으로 변할 수 있습니다.\npackage main import \"fmt\" func main() { slice := []int{1, 2, 3, 4, 5} fmt.Println(slice) // [1 2 3 4 5] slice = append(slice, 6) // 요소 추가 fmt.Println(slice) // [1 2 3 4 5 6] } 4️⃣ 맵(Map)과 그 활용법 Go의 맵은 키-값(key-value) 구조를 가지며, 빠른 조회가 가능합니다.\n1. 맵 선언 및 초기화 package main import \"fmt\" func main() { user := map[string]int{ \"Alice\": 25, \"Bob\": 30, } fmt.Println(user[\"Alice\"]) // 25 } 2. 맵 요소 추가 및 삭제 package main import \"fmt\" func main() { user := make(map[string]int) user[\"Alice\"] = 25 user[\"Bob\"] = 30 delete(user, \"Bob\") // \"Bob\" 삭제 fmt.Println(user) // map[Alice:25] } 4️⃣ 구조체(Struct)와 메서드 구조체는 여러 개의 데이터를 묶어 하나의 객체처럼 다룰 수 있습니다.\n1. 구조체 선언 및 사용 package main import \"fmt\" type Person struct { Name string Age int } func main() { p := Person{\"Alice\", 25} fmt.Println(p.Name, p.Age) // Alice 25 } 2. 구조체 메서드 추가 package main import \"fmt\" type Person struct { Name string Age int } func (p Person) Greet() { fmt.Println(\"Hello, my name is\", p.Name) } func main() { p := Person{\"Alice\", 25} p.Greet() // Hello, my name is Alice } 4️⃣ 포인터와 값 전달 방식 Go에서 포인터는 값의 메모리 주소를 저장하는 변수입니다.\n1. 포인터 기본 개념 package main import \"fmt\" func main() { var a int = 10 var p *int = \u0026a // a의 메모리 주소 저장 fmt.Println(*p) // 10 (포인터 역참조) } 2. 포인터를 이용한 값 변경 package main import \"fmt\" func changeValue(num *int) { *num = 20 } func main() { a := 10 changeValue(\u0026a) fmt.Println(a) // 20 } 4️⃣ 인터페이스(Interface) 이해 및 활용 인터페이스는 메서드의 집합을 정의하며, 다형성을 제공합니다.\n1. 인터페이스 선언 package main import \"fmt\" type Speaker interface { Speak() } type Person struct { Name string } func (p Person) Speak() { fmt.Println(\"Hello, my name is\", p.Name) } func main() { var s Speaker = Person{\"Alice\"} s.Speak() // Hello, my name is Alice } 4️⃣ 타입 변환과 타입 assertion Go는 강타입 언어이므로 명시적 타입 변환이 필요합니다.\n1. 타입 변환 package main import \"fmt\" func main() { var i int = 42 var f float64 = float64(i) // int → float64 변환 fmt.Println(f) } 2. 타입 assertion 인터페이스에서 구체적인 타입을 추출할 때 사용합니다.\npackage main import \"fmt\" func main() { var i interface{} = \"Hello\" s, ok := i.(string) // 타입 변환 확인 if ok { fmt.Println(s) // Hello } else { fmt.Println(\"변환 실패\") } } 4️⃣ 배열과 슬라이스의 차이점 및 선택 기준 특징 배열(Array) 슬라이스(Slice) 크기 고정됨 동적으로 변경 가능 선언 방식 var arr [5]int slice := []int{} 메모리 정해진 크기만큼 할당 동적 메모리 관리 사용처 크기가 변하지 않는 경우 크기가 변하는 경우 배열은 고정 크기의 데이터를 다룰 때 사용하고,\n슬라이스는 유연한 크기의 데이터를 다룰 때 사용합니다.","4-인터페이스interface-이해-및-활용#4️⃣ 인터페이스(Interface) 이해 및 활용":"","4-타입-변환과-타입-assertion#4️⃣ 타입 변환과 타입 assertion":"","4-포인터와-값-전달-방식#4️⃣ 포인터와 값 전달 방식":""},"title":"고급 데이터 타입"},"/programing/go/go/go04/":{"data":{"":"","-정리#🎯 정리":" 5️⃣ 익명 함수 (Anonymous Functions) 익명 함수는 이름 없이 정의되는 함수로, 변수에 할당하거나 즉시 실행할 수 있습니다.\n1. 익명 함수 정의 및 실행 package main import \"fmt\" func main() { func() { fmt.Println(\"Hello, Anonymous Function!\") }() } 2. 익명 함수 변수 할당 package main import \"fmt\" func main() { add := func(a, b int) int { return a + b } fmt.Println(add(3, 4)) // 7 } 5️⃣ 클로저(Closure)와 함수 반환 값 클로저는 외부 변수에 접근할 수 있는 함수를 의미합니다.\n1. 클로저 예제 package main import \"fmt\" func counter() func() int { i := 0 return func() int { i++ return i } } func main() { count := counter() fmt.Println(count()) // 1 fmt.Println(count()) // 2 } count()를 호출할 때마다 i 값이 유지됩니다.\n5️⃣ 가변 인자 함수 (Variadic Functions) 가변 인자는 여러 개의 인자를 받을 수 있는 함수입니다.\npackage main import \"fmt\" func sum(numbers ...int) int { total := 0 for _, num := range numbers { total += num } return total } func main() { fmt.Println(sum(1, 2, 3, 4, 5)) // 15 } 5️⃣ 데퍼링(Defer)과 지연 실행 defer 키워드는 함수 실행을 지연시킵니다.\n1. defer 기본 예제 package main import \"fmt\" func main() { defer fmt.Println(\"끝!\") fmt.Println(\"시작!\") } 출력 결과:\n시작! 끝! defer는 함수가 끝날 때 실행됩니다.\n5️⃣ 고루틴과 채널 (Goroutines \u0026 Channels) Go에서는 고루틴(Goroutine) 을 사용해 병렬 프로그래밍이 가능합니다.\n1. 고루틴 사용 package main import ( \"fmt\" \"time\" ) func hello() { fmt.Println(\"Hello, Goroutine!\") } func main() { go hello() time.Sleep(time.Second) // 고루틴 실행을 기다림 } 2. 채널을 이용한 데이터 전송 package main import \"fmt\" func main() { ch := make(chan string) go func() { ch \u003c- \"Hello, Channel!\" }() msg := \u003c-ch fmt.Println(msg) } 채널을 사용하면 고루틴 간 데이터를 안전하게 주고받을 수 있습니다.\n5️⃣ 에러 처리 (error 타입, panic, recover) 1. 기본적인 에러 처리 package main import ( \"errors\" \"fmt\" ) func divide(a, b int) (int, error) { if b == 0 { return 0, errors.New(\"0으로 나눌 수 없습니다\") } return a / b, nil } func main() { result, err := divide(10, 0) if err != nil { fmt.Println(\"에러 발생:\", err) } else { fmt.Println(\"결과:\", result) } } 2. panic \u0026 recover package main import \"fmt\" func main() { defer func() { if r := recover(); r != nil { fmt.Println(\"복구:\", r) } }() panic(\"패닉 발생!\") // 강제 종료 } recover()를 사용하면 패닉을 복구할 수 있습니다.\n5️⃣ 명령줄 인수 처리 (os.Args, flag) 1. os.Args 사용 package main import ( \"fmt\" \"os\" ) func main() { fmt.Println(\"Arguments:\", os.Args) } $ go run main.go arg1 arg2 실행 시 인수 출력\n2. flag 패키지 사용 package main import ( \"flag\" \"fmt\" ) func main() { name := flag.String(\"name\", \"World\", \"이름을 입력하세요\") flag.Parse() fmt.Println(\"Hello,\", *name) } $ go run main.go -name=Alice 실행 시 Hello, Alice 출력\n5️⃣ 타이머, 간격 처리 (time.Timer, time.Ticker) 1. 타이머 (일정 시간 후 실행) package main import ( \"fmt\" \"time\" ) func main() { timer := time.NewTimer(2 * time.Second) \u003c-timer.C fmt.Println(\"2초 후 실행됨\") } 2. Ticker (주기적 실행) package main import ( \"fmt\" \"time\" ) func main() { ticker := time.NewTicker(1 * time.Second) defer ticker.Stop() for i := 0; i \u003c 3; i++ { \u003c-ticker.C fmt.Println(\"1초마다 실행\") } } 🎯 정리 개념 설명 익명 함수 이름 없이 즉시 실행하거나 변수에 할당 가능 클로저 외부 변수를 유지하는 함수 가변 인자 여러 개의 인자를 받을 수 있는 함수 defer 함수 종료 시 실행되는 지연 함수 고루틴 Go에서 제공하는 경량 스레드 채널 고루틴 간 데이터 전송 도구 에러 처리 error, panic, recover 활용 명령줄 인수 os.Args, flag 패키지 사용 타이머 특정 시간 후 코드 실행 Ticker 일정한 간격으로 코드 실행 ","5-가변-인자-함수-variadic-functions#5️⃣ 가변 인자 함수 (Variadic Functions)":"","5-고루틴과-채널-goroutines--channels#5️⃣ 고루틴과 채널 (Goroutines \u0026amp; Channels)":"","5-데퍼링defer과-지연-실행#5️⃣ 데퍼링(Defer)과 지연 실행":"","5-명령줄-인수-처리-osargs-flag#5️⃣ 명령줄 인수 처리 (os.Args, flag)":"","5-에러-처리-error-타입-panic-recover#5️⃣ 에러 처리 (error 타입, panic, recover)":"","5-익명-함수-anonymous-functions#5️⃣ 익명 함수 (Anonymous Functions)":"","5-클로저closure와-함수-반환-값#5️⃣ 클로저(Closure)와 함수 반환 값":"","5-타이머-간격-처리-timetimer-timeticker#5️⃣ 타이머, 간격 처리 (time.Timer, time.Ticker)":""},"title":"Go 언어의 함수 및 고급 기능"},"/programing/go/go/go05/":{"data":{"":"","-정리#🎯 정리":" 6️⃣ Go의 동시성 모델 이해 (Goroutines, Channels) Go는 CSP(Communicating Sequential Processes) 모델을 기반으로 동시성을 구현합니다.\n🖼 Go 동시성 모델 개념도\n┌─────────────────────────────────────────┐ │ Main Goroutine (메인 루틴) │ │ ┌───────────────────────────────────┐ │ │ │ Worker Goroutine 1 │ │ │ ├───────────────────────────────────┤ │ │ │ Worker Goroutine 2 │ │ │ ├───────────────────────────────────┤ │ │ │ Channel 통신 │ │ │ └───────────────────────────────────┘ │ └─────────────────────────────────────────┘ Go에서는 Goroutine(고루틴)과 Channel(채널) 을 활용하여 동시성을 효율적으로 처리할 수 있습니다.\n6️⃣ 고루틴(Goroutine)의 기본 사용법 고루틴은 Go에서 제공하는 경량 스레드로, go 키워드를 사용하여 실행됩니다.\n1. 고루틴 생성 예제 package main import ( \"fmt\" \"time\" ) func hello() { fmt.Println(\"Hello, Goroutine!\") } func main() { go hello() // 고루틴 실행 time.Sleep(time.Second) // 고루틴이 실행될 시간을 줌 fmt.Println(\"Main Function Done\") } 출력 예시:\nHello, Goroutine! Main Function Done 고루틴은 비동기 실행되므로 time.Sleep을 사용하여 실행 시간을 확보해야 합니다.\n6️⃣ 채널(Channel)을 이용한 데이터 통신 채널은 고루틴 간 데이터를 안전하게 주고받는 도구입니다.\n1. 기본적인 채널 예제 package main import \"fmt\" func main() { ch := make(chan string) // 문자열을 주고받는 채널 생성 go func() { ch \u003c- \"Hello, Channel!\" }() msg := \u003c-ch // 채널에서 데이터 수신 fmt.Println(msg) } 출력 결과:\nHello, Channel! 6️⃣ 동기화 및 병렬 처리 (sync, WaitGroup, Mutex 등) 1. sync.WaitGroup 사용 예제 WaitGroup을 사용하면 고루틴이 모두 실행될 때까지 대기할 수 있습니다.\npackage main import ( \"fmt\" \"sync\" \"time\" ) func worker(id int, wg *sync.WaitGroup) { defer wg.Done() // 완료 시 Done 호출 fmt.Printf(\"Worker %d 시작\\n\", id) time.Sleep(time.Second) fmt.Printf(\"Worker %d 완료\\n\", id) } func main() { var wg sync.WaitGroup for i := 1; i \u003c= 3; i++ { wg.Add(1) go worker(i, \u0026wg) } wg.Wait() // 모든 고루틴이 끝날 때까지 대기 fmt.Println(\"모든 작업 완료\") } 출력 결과:\nWorker 1 시작 Worker 2 시작 Worker 3 시작 Worker 2 완료 Worker 1 완료 Worker 3 완료 모든 작업 완료 6️⃣ 채널 버퍼링 및 비동기 처리 기본 채널은 동기적이지만, 버퍼를 지정하면 비동기적으로 작동할 수 있습니다.\n1. 버퍼링된 채널 예제 package main import \"fmt\" func main() { ch := make(chan int, 2) // 크기가 2인 버퍼링 채널 ch \u003c- 1 ch \u003c- 2 fmt.Println(\u003c-ch) // 1 fmt.Println(\u003c-ch) // 2 } 버퍼링을 사용하면 즉시 데이터를 보내고 받을 필요 없이 채널에 저장할 수 있습니다.\n6️⃣ Select문을 활용한 멀티 채널 처리 select 문을 사용하면 여러 채널을 동시에 처리할 수 있습니다.\n1. Select문 예제 package main import ( \"fmt\" \"time\" ) func main() { ch1 := make(chan string) ch2 := make(chan string) go func() { time.Sleep(1 * time.Second) ch1 \u003c- \"채널 1 데이터\" }() go func() { time.Sleep(2 * time.Second) ch2 \u003c- \"채널 2 데이터\" }() for i := 0; i \u003c 2; i++ { select { case msg1 := \u003c-ch1: fmt.Println(\"수신:\", msg1) case msg2 := \u003c-ch2: fmt.Println(\"수신:\", msg2) } } } 출력 예시:\n수신: 채널 1 데이터 수신: 채널 2 데이터 6️⃣ 동시성 패턴 (Producer-Consumer, Fan-out, Fan-in) 1. Producer-Consumer 패턴 생산자(Producer)가 데이터를 생성하고, 소비자(Consumer)가 처리하는 패턴입니다.\npackage main import ( \"fmt\" \"time\" ) func producer(ch chan\u003c- int) { for i := 1; i \u003c= 5; i++ { fmt.Println(\"생산:\", i) ch \u003c- i time.Sleep(time.Second) } close(ch) } func consumer(ch \u003c-chan int) { for num := range ch { fmt.Println(\"소비:\", num) } } func main() { ch := make(chan int, 3) go producer(ch) consumer(ch) } 출력 예시:\n생산: 1 소비: 1 생산: 2 소비: 2 생산: 3 소비: 3 ... 2. Fan-out 패턴 하나의 채널 데이터를 여러 고루틴에서 병렬 처리하는 패턴입니다.\npackage main import ( \"fmt\" \"sync\" ) func worker(id int, ch \u003c-chan int, wg *sync.WaitGroup) { defer wg.Done() for task := range ch { fmt.Printf(\"Worker %d: %d 처리 중\\n\", id, task) } } func main() { taskCh := make(chan int, 5) var wg sync.WaitGroup for i := 1; i \u003c= 3; i++ { wg.Add(1) go worker(i, taskCh, \u0026wg) } for i := 1; i \u003c= 5; i++ { taskCh \u003c- i } close(taskCh) wg.Wait() } 출력 예시:\nWorker 1: 1 처리 중 Worker 2: 2 처리 중 Worker 3: 3 처리 중 Worker 1: 4 처리 중 Worker 2: 5 처리 중 🎯 정리 개념 설명 Goroutine Go의 경량 스레드 Channel 고루틴 간 데이터 통신 WaitGroup 여러 고루틴 동기화 Mutex 공유 데이터 동기화 Select 멀티 채널 처리 Producer-Consumer 데이터 생산-소비 패턴 Fan-out 하나의 채널을 여러 고루틴이 병렬 처리 ","6-go의-동시성-모델-이해-goroutines-channels#6️⃣ Go의 동시성 모델 이해 (Goroutines, Channels)":"","6-select문을-활용한-멀티-채널-처리#6️⃣ Select문을 활용한 멀티 채널 처리":"","6-고루틴goroutine의-기본-사용법#6️⃣ 고루틴(Goroutine)의 기본 사용법":"","6-동기화-및-병렬-처리-sync-waitgroup-mutex-등#6️⃣ 동기화 및 병렬 처리 (sync, WaitGroup, Mutex 등)":"","6-동시성-패턴-producer-consumer-fan-out-fan-in#6️⃣ 동시성 패턴 (Producer-Consumer, Fan-out, Fan-in)":"","6-채널-버퍼링-및-비동기-처리#6️⃣ 채널 버퍼링 및 비동기 처리":"","6-채널channel을-이용한-데이터-통신#6️⃣ 채널(Channel)을 이용한 데이터 통신":""},"title":"Go의 동시성 모델 이해"},"/programing/go/go/go06/":{"data":{"":"","-정리#🎯 정리":" 7️⃣ 표준 라이브러리 개요 Go의 표준 라이브러리는 다음과 같은 주요 기능을 제공합니다.\n라이브러리 설명 os, io, bufio, fmt 파일 및 입력/출력(I/O) 처리 strings, strconv 문자열 및 형 변환 처리 encoding/json JSON 인코딩/디코딩 net/http HTTP 웹 서버 및 클라이언트 기능 log, errors 오류 처리 및 로깅 regexp 정규 표현식 활용 crypto 암호화 및 보안 기능 7️⃣ 파일 입출력 (I/O) (os, io, bufio, fmt 패키지) Go에서는 os, io, bufio 패키지를 활용하여 파일을 읽고 쓰는 작업을 수행할 수 있습니다.\n1. 파일 생성 및 쓰기 (os.Create) package main import ( \"fmt\" \"os\" ) func main() { file, err := os.Create(\"example.txt\") if err != nil { fmt.Println(\"파일 생성 실패:\", err) return } defer file.Close() file.WriteString(\"Hello, Go 파일 I/O!\") fmt.Println(\"파일이 성공적으로 생성되었습니다.\") } 📂 example.txt 내용\nHello, Go 파일 I/O! 7️⃣ 문자열 처리 (strings, strconv 패키지) strings 패키지는 문자열 조작, strconv 패키지는 문자열과 숫자 간 변환을 지원합니다.\n1. strings 활용 예제 package main import ( \"fmt\" \"strings\" ) func main() { str := \"Go is Awesome!\" fmt.Println(strings.ToUpper(str)) // 대문자 변환 fmt.Println(strings.ReplaceAll(str, \"Awesome\", \"Great\")) // 문자열 치환 fmt.Println(strings.Split(str, \" \")) // 문자열 분할 } 출력 결과:\nGO IS AWESOME! Go is Great! [Go is Awesome!] 2. strconv 활용 예제 package main import ( \"fmt\" \"strconv\" ) func main() { numStr := \"123\" num, _ := strconv.Atoi(numStr) // 문자열 → 숫자 변환 fmt.Println(num + 10) str := strconv.Itoa(456) // 숫자 → 문자열 변환 fmt.Println(str) } 출력 결과:\n133 456 7️⃣ JSON 처리 (encoding/json 패키지) Go의 encoding/json 패키지는 JSON 데이터를 다룰 때 사용됩니다.\n1. 구조체를 JSON으로 변환 package main import ( \"encoding/json\" \"fmt\" ) type User struct { Name string `json:\"name\"` Age int `json:\"age\"` Email string `json:\"email\"` } func main() { user := User{Name: \"Alice\", Age: 25, Email: \"alice@example.com\"} jsonData, _ := json.Marshal(user) // JSON 변환 fmt.Println(string(jsonData)) } 출력 결과:\n{\"name\":\"Alice\",\"age\":25,\"email\":\"alice@example.com\"} 7️⃣ HTTP 및 웹 서버 (net/http, http.Request, http.ResponseWriter) Go의 net/http 패키지를 사용하면 웹 서버 및 클라이언트를 쉽게 구현할 수 있습니다.\n1. 간단한 웹 서버 구현 package main import ( \"fmt\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \"Hello, Go Web Server!\") } func main() { http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } 🖥 웹 브라우저에서 http://localhost:8080 접속하면\nHello, Go Web Server! 이 출력됩니다.\n7️⃣ 오류 처리 및 로깅 (log, errors 패키지) Go에서는 errors 패키지를 활용하여 오류를 생성하고, log 패키지를 통해 로그를 기록할 수 있습니다.\n1. 오류 생성 및 처리 package main import ( \"errors\" \"fmt\" ) func divide(a, b int) (int, error) { if b == 0 { return 0, errors.New(\"0으로 나눌 수 없습니다\") } return a / b, nil } func main() { result, err := divide(10, 0) if err != nil { fmt.Println(\"오류 발생:\", err) } else { fmt.Println(\"결과:\", result) } } 출력 결과:\n오류 발생: 0으로 나눌 수 없습니다 7️⃣ 정규 표현식 (regexp 패키지) Go의 regexp 패키지는 문자열에서 특정 패턴을 찾을 때 사용됩니다.\n1. 이메일 검증 예제 package main import ( \"fmt\" \"regexp\" ) func main() { email := \"test@example.com\" regex := `^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$` match, _ := regexp.MatchString(regex, email) fmt.Println(\"이메일 유효성 검사:\", match) } 출력 결과:\n이메일 유효성 검사: true 7️⃣ 암호화 및 보안 (crypto 패키지) Go의 crypto 패키지는 해싱 및 암호화 기능을 제공합니다.\n1. SHA-256 해싱 예제 package main import ( \"crypto/sha256\" \"fmt\" ) func main() { data := \"password123\" hash := sha256.Sum256([]byte(data)) fmt.Printf(\"SHA-256 Hash: %x\\n\", hash) } 출력 결과:\nSHA-256 Hash: ef92b778bafee3.... 🎯 정리 패키지 설명 os, io, bufio 파일 입출력 (I/O) 처리 strings, strconv 문자열 및 형 변환 처리 encoding/json JSON 직렬화 및 역직렬화 net/http HTTP 서버 및 클라이언트 log, errors 오류 처리 및 로깅 regexp 정규 표현식 활용 crypto 암호화 및 보안 ","7-http-및-웹-서버-nethttp-httprequest-httpresponsewriter#7️⃣ HTTP 및 웹 서버 (\u003ccode\u003enet/http\u003c/code\u003e, \u003ccode\u003ehttp.Request\u003c/code\u003e, \u003ccode\u003ehttp.ResponseWriter\u003c/code\u003e)":"","7-json-처리-encodingjson-패키지#7️⃣ JSON 처리 (\u003ccode\u003eencoding/json\u003c/code\u003e 패키지)":"","7-문자열-처리-strings-strconv-패키지#7️⃣ 문자열 처리 (\u003ccode\u003estrings\u003c/code\u003e, \u003ccode\u003estrconv\u003c/code\u003e 패키지)":"","7-암호화-및-보안-crypto-패키지#7️⃣ 암호화 및 보안 (\u003ccode\u003ecrypto\u003c/code\u003e 패키지)":"","7-오류-처리-및-로깅-log-errors-패키지#7️⃣ 오류 처리 및 로깅 (\u003ccode\u003elog\u003c/code\u003e, \u003ccode\u003eerrors\u003c/code\u003e 패키지)":"","7-정규-표현식-regexp-패키지#7️⃣ 정규 표현식 (\u003ccode\u003eregexp\u003c/code\u003e 패키지)":"","7-파일-입출력-io-os-io-bufio-fmt-패키지#7️⃣ 파일 입출력 (I/O) (\u003ccode\u003eos\u003c/code\u003e, \u003ccode\u003eio\u003c/code\u003e, \u003ccode\u003ebufio\u003c/code\u003e, \u003ccode\u003efmt\u003c/code\u003e 패키지)":"","7-표준-라이브러리-개요#7️⃣ 표준 라이브러리 개요":""},"title":"Go 언어의 표준 라이브러리"},"/programing/go/go/go07/":{"data":{"":"","-정리#🎯 정리":" 8️⃣ Go의 테스트 프레임워크 (testing 패키지) Go에서는 testing 패키지를 활용하여 테스트 코드를 작성할 수 있습니다.\n테스트 파일은 *_test.go 형식으로 작성하며, go test 명령어로 실행할 수 있습니다.\n📌 기본 테스트 구조\npackage mypackage import \"testing\" func TestFunctionName(t *testing.T) { // 테스트 코드 작성 } 8️⃣ 단위 테스트 (Unit Testing) 작성 방법 단위 테스트는 함수 단위로 올바르게 동작하는지 검증하는 테스트입니다.\nGo에서는 t.Errorf 또는 t.Fatal을 사용하여 테스트 결과를 검증합니다.\n📌 예제: 두 수의 합을 검증하는 테스트\npackage mathutil import \"testing\" func Add(a, b int) int { return a + b } func TestAdd(t *testing.T) { result := Add(2, 3) expected := 5 if result != expected { t.Errorf(\"Add(2, 3) = %d; want %d\", result, expected) } } ✅ 테스트 실행\ngo test -v 8️⃣ 벤치마크(Benchmark) 작성 및 성능 측정 Go에서는 Benchmark 테스트를 작성하여 함수의 실행 속도를 측정할 수 있습니다.\n📌 예제: 문자열 결합 성능 비교\npackage main import ( \"strings\" \"testing\" ) func BenchmarkStringConcat(b *testing.B) { str := \"\" for i := 0; i \u003c b.N; i++ { str += \"x\" } } func BenchmarkStringBuilder(b *testing.B) { var sb strings.Builder for i := 0; i \u003c b.N; i++ { sb.WriteString(\"x\") } } ✅ 벤치마크 실행\ngo test -bench=. 8️⃣ 예제 기반 테스트 (Example Tests) Go에서는 문서화와 함께 동작하는 예제 테스트(Example Test) 를 작성할 수 있습니다.\n📌 예제: ExampleAdd 함수 사용법을 문서화\npackage mathutil import ( \"fmt\" ) func Add(a, b int) int { return a + b } func ExampleAdd() { fmt.Println(Add(2, 3)) // Output: 5 } ✅ 예제 테스트 실행\ngo test 8️⃣ 테스트 주도 개발(TDD) 적용 TDD는 테스트를 먼저 작성한 후 코드 구현을 진행하는 개발 방법입니다.\n📌 TDD 예제: 문자열을 뒤집는 함수\n1️⃣ 테스트 코드 작성 (실패 예상)\npackage main import \"testing\" func TestReverseString(t *testing.T) { result := ReverseString(\"hello\") expected := \"olleh\" if result != expected { t.Errorf(\"ReverseString(hello) = %s; want %s\", result, expected) } } 2️⃣ 코드 작성 (테스트 통과)\npackage main func ReverseString(s string) string { runes := []rune(s) for i, j := 0, len(runes)-1; i \u003c j; i, j = i+1, j-1 { runes[i], runes[j] = runes[j], runes[i] } return string(runes) } ✅ TDD 실행\ngo test -v 8️⃣ Go 모의 객체(Mock) 테스트 테스트에서 외부 의존성을 제거하고 독립적인 테스트를 수행하기 위해 Mock을 사용할 수 있습니다.\n📌 예제: 데이터베이스 모의(Mock) 테스트\npackage main import \"testing\" type DB interface { GetUser(id int) string } type MockDB struct{} func (m *MockDB) GetUser(id int) string { return \"Mock User\" } func TestGetUser(t *testing.T) { mockDB := \u0026MockDB{} user := mockDB.GetUser(1) if user != \"Mock User\" { t.Errorf(\"Expected Mock User, got %s\", user) } } ✅ Mock을 활용하면 DB 없이도 독립적인 테스트 가능!\n8️⃣ 커버리지 분석 및 코드 커버리지 도구 활용 Go에서는 go test -cover 명령어를 사용하여 테스트 커버리지(코드가 얼마나 테스트되었는지 측정) 를 확인할 수 있습니다.\n✅ 테스트 커버리지 확인\ngo test -cover ✅ HTML 보고서 생성\ngo test -coverprofile=coverage.out go tool cover -html=coverage.out -o coverage.html ✅ 예제 출력\ncoverage: 85.0% of statements 8️⃣ 테스트와 CI/CD 파이프라인 설정 CI/CD 환경에서는 테스트 자동 실행이 중요합니다.\nGitHub Actions를 사용하여 go test를 자동 실행하는 예제를 보겠습니다.\n📌 .github/workflows/go.yml 설정\nname: Go Test on: push: branches: - main jobs: test: runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v2 - name: Setup Go uses: actions/setup-go@v2 with: go-version: 1.20 - name: Run tests run: go test ./... -v ✅ GitHub Actions에서 자동으로 테스트 실행! 🚀\n🎯 정리 기능 설명 testing 패키지 단위 테스트 프레임워크 제공 Benchmark 성능 측정 가능 Example 문서 기반 테스트 Mock 의존성 제거 테스트 Coverage 코드 테스트 커버리지 측정 CI/CD 자동화된 테스트 환경 구축 ","8-go-모의-객체mock-테스트#8️⃣ Go 모의 객체(Mock) 테스트":"","8-go의-테스트-프레임워크-testing-패키지#8️⃣ Go의 테스트 프레임워크 (\u003ccode\u003etesting\u003c/code\u003e 패키지)":"","8-단위-테스트-unit-testing-작성-방법#8️⃣ 단위 테스트 (Unit Testing) 작성 방법":"","8-벤치마크benchmark-작성-및-성능-측정#8️⃣ 벤치마크(Benchmark) 작성 및 성능 측정":"","8-예제-기반-테스트-example-tests#8️⃣ 예제 기반 테스트 (Example Tests)":"","8-커버리지-분석-및-코드-커버리지-도구-활용#8️⃣ 커버리지 분석 및 코드 커버리지 도구 활용":"","8-테스트-주도-개발tdd-적용#8️⃣ 테스트 주도 개발(TDD) 적용":"","8-테스트와-cicd-파이프라인-설정#8️⃣ 테스트와 CI/CD 파이프라인 설정":""},"title":"Go 언어의 테스트와 벤치마크"},"/programing/go/go/go08/":{"data":{"":"","-정리#🎯 정리":" 9️⃣ 고급 구조체 사용법 (내장 타입, 중첩 구조체) Go의 구조체는 내장 타입(Embedding) 과 중첩 구조체(Nested Struct) 를 활용하여 코드의 재사용성을 높일 수 있습니다.\n📌 예제: 내장 타입(Embedding)\npackage main import \"fmt\" // 기본 구조체 정의 type Person struct { Name string Age int } // Employee가 Person을 포함 (내장 타입) type Employee struct { Person Position string } func main() { e := Employee{ Person: Person{Name: \"Alice\", Age: 30}, Position: \"Developer\", } // Employee에서도 Name, Age 사용 가능 fmt.Println(e.Name, e.Age, e.Position) // Alice 30 Developer } 📌 예제: 중첩 구조체(Nested Struct)\ntype Address struct { City string State string } type User struct { Name string Address Address } func main() { user := User{Name: \"Bob\", Address: Address{City: \"Seoul\", State: \"KR\"}} fmt.Println(user.Name, user.Address.City) // Bob Seoul } ✅ 내장 타입은 구조체를 확장하는 데 유용하며, 중첩 구조체는 데이터를 그룹화하는 데 적합합니다.\n9️⃣ 인터페이스(Interface) 활용법 (빈 인터페이스, 명시적 타입) Go의 인터페이스는 다형성(Polymorphism) 을 지원하며, 빈 인터페이스(interface{}) 를 활용하면 다양한 타입을 처리할 수 있습니다.\n📌 예제: 명시적 인터페이스 사용\npackage main import \"fmt\" type Speaker interface { Speak() string } type Dog struct{} func (d Dog) Speak() string { return \"Woof!\" } func PrintSpeaker(s Speaker) { fmt.Println(s.Speak()) } func main() { d := Dog{} PrintSpeaker(d) // Woof! } 📌 예제: 빈 인터페이스(interface{}) 활용\npackage main import \"fmt\" func PrintAny(value interface{}) { fmt.Println(value) } func main() { PrintAny(42) // 42 PrintAny(\"Hello\") // Hello PrintAny([]int{1, 2, 3}) // [1 2 3] } ✅ 빈 인터페이스는 유연하지만, 타입 단언(Type Assertion) 또는 타입 스위치(Type Switch)를 사용해야 합니다.\n9️⃣ 동시성 패턴 (Worker Pool, Concurrency-safe Data Structures) Go에서는 Worker Pool 을 활용하여 효율적으로 작업을 병렬 처리할 수 있습니다.\n📌 예제: Worker Pool 구현\npackage main import ( \"fmt\" \"sync\" \"time\" ) func worker(id int, jobs \u003c-chan int, results chan\u003c- int, wg *sync.WaitGroup) { defer wg.Done() for job := range jobs { fmt.Printf(\"Worker %d processing job %d\\n\", id, job) time.Sleep(time.Second) // 작업 시뮬레이션 results \u003c- job * 2 } } func main() { jobs := make(chan int, 5) results := make(chan int, 5) var wg sync.WaitGroup for i := 1; i \u003c= 3; i++ { // 3명의 Worker 실행 wg.Add(1) go worker(i, jobs, results, \u0026wg) } for j := 1; j \u003c= 5; j++ { jobs \u003c- j } close(jobs) wg.Wait() close(results) for res := range results { fmt.Println(\"Result:\", res) } } ✅ Worker Pool을 사용하면 효율적인 동시 처리가 가능합니다.\n9️⃣ Go 언어에서의 디자인 패턴 (싱글턴, 팩토리, 전략 등) Go에서도 디자인 패턴 을 적용할 수 있습니다.\n📌 예제: 싱글턴(Singleton) 패턴\npackage main import ( \"fmt\" \"sync\" ) var instance *Database var once sync.Once type Database struct{} func GetDatabaseInstance() *Database { once.Do(func() { instance = \u0026Database{} }) return instance } func main() { db1 := GetDatabaseInstance() db2 := GetDatabaseInstance() fmt.Println(db1 == db2) // true (같은 인스턴스) } ✅ 싱글턴 패턴은 DB 연결 등 전역적으로 단 하나의 인스턴스를 유지할 때 사용됩니다.\n9️⃣ 이벤트 기반 프로그래밍 및 Go 루틴 활용 Go에서는 Go 루틴과 채널을 이용하여 이벤트 기반 프로그래밍 을 구현할 수 있습니다.\n📌 예제: 이벤트 핸들러 구현\npackage main import \"fmt\" type Event struct { Data string } type EventHandler func(Event) func main() { eventChannel := make(chan Event) go func() { for event := range eventChannel { fmt.Println(\"Event received:\", event.Data) } }() eventChannel \u003c- Event{Data: \"Hello Event\"} eventChannel \u003c- Event{Data: \"Another Event\"} close(eventChannel) } ✅ 이벤트 기반으로 데이터 흐름을 제어할 수 있습니다.\n9️⃣ 고급 에러 처리 패턴 (에러 체이닝, 커스텀 에러) Go에서는 errors 패키지를 활용하여 고급 에러 처리 를 할 수 있습니다.\n📌 예제: 에러 체이닝(Wrapping Errors)\npackage main import ( \"errors\" \"fmt\" \"os\" ) func readFile(filename string) error { _, err := os.Open(filename) if err != nil { return fmt.Errorf(\"failed to open file: %w\", err) } return nil } func main() { err := readFile(\"nonexistent.txt\") if err != nil { fmt.Println(\"Error:\", err) if errors.Is(err, os.ErrNotExist) { fmt.Println(\"File does not exist!\") } } } ✅ %w 를 사용하면 에러 체이닝이 가능합니다.\n9️⃣ 코드 리팩토링 및 성능 최적화 Go 코드의 성능 최적화 및 리팩토링 을 위해 다음 기법들을 활용합니다.\n기법 설명 메모리 최적화 슬라이스 미리 할당 (make([]int, n)) 데이터 구조 최적화 맵 대신 배열 사용 가능성 검토 CPU 최적화 sync.Pool 활용하여 메모리 재사용 I/O 최적화 bufio.Reader 로 파일 읽기 속도 향상 📌 예제: sync.Pool을 활용한 메모리 재사용\npackage main import ( \"fmt\" \"sync\" ) var bufferPool = sync.Pool{ New: func() interface{} { return make([]byte, 1024) }, } func main() { buf := bufferPool.Get().([]byte) fmt.Println(\"Buffer length:\", len(buf)) bufferPool.Put(buf) // 재사용 가능 } ✅ 메모리 할당을 줄여 성능을 최적화할 수 있습니다.\n🎯 정리 이번 강의에서는 고급 Go 패턴 및 기법 을 학습했습니다.\nGo의 강력한 기능을 활용하여 더욱 효율적인 코드 를 작성할 수 있습니다. 🚀","9-go-언어에서의-디자인-패턴-싱글턴-팩토리-전략-등#9️⃣ Go 언어에서의 디자인 패턴 (싱글턴, 팩토리, 전략 등)":"","9-고급-구조체-사용법-내장-타입-중첩-구조체#9️⃣ 고급 구조체 사용법 (내장 타입, 중첩 구조체)":"","9-고급-에러-처리-패턴-에러-체이닝-커스텀-에러#9️⃣ 고급 에러 처리 패턴 (에러 체이닝, 커스텀 에러)":"","9-동시성-패턴-worker-pool-concurrency-safe-data-structures#9️⃣ 동시성 패턴 (Worker Pool, Concurrency-safe Data Structures)":"","9-이벤트-기반-프로그래밍-및-go-루틴-활용#9️⃣ 이벤트 기반 프로그래밍 및 Go 루틴 활용":"","9-인터페이스interface-활용법-빈-인터페이스-명시적-타입#9️⃣ 인터페이스(Interface) 활용법 (빈 인터페이스, 명시적 타입)":"","9-코드-리팩토링-및-성능-최적화#9️⃣ 코드 리팩토링 및 성능 최적화":""},"title":"Go 언어의 고급 개념과 패턴 "},"/programing/go/go/go09/":{"data":{"":"","-정리#🎯 정리":"1️⃣ 외부 라이브러리 활용 (Go Modules로 패키지 관리) Go에서는 Go Modules 를 활용하여 외부 라이브러리를 쉽게 설치하고 관리 할 수 있습니다.\n📌 Go Modules 설정 방법\n# 프로젝트 폴더에서 모듈 초기화 go mod init github.com/username/myproject 📌 패키지 설치 및 관리\n# 외부 패키지 설치 go get github.com/gin-gonic/gin # 종속성 업데이트 go mod tidy 📌 예제: 외부 패키지 활용\npackage main import ( \"fmt\" \"github.com/google/uuid\" ) func main() { id := uuid.New() fmt.Println(\"Generated UUID:\", id) } ✅ Go Modules를 활용하면 패키지 관리를 쉽게 할 수 있습니다.\n2️⃣ 웹 프레임워크 (Gin, Echo, Fiber) 사용법 Go에서는 웹 애플리케이션을 개발할 수 있는 다양한 프레임워크 가 있습니다.\n가장 많이 사용되는 Gin, Echo, Fiber 를 소개하겠습니다.\n📌 예제: Gin 프레임워크로 간단한 웹 서버 만들기\npackage main import ( \"github.com/gin-gonic/gin\" \"net/http\" ) func main() { r := gin.Default() r.GET(\"/ping\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{\"message\": \"pong\"}) }) r.Run(\":8080\") // 8080 포트에서 실행 } ✅ Gin은 가볍고 성능이 뛰어나므로 많이 사용됩니다.\n3️⃣ 데이터베이스와의 연동 (GORM, sqlx) Go에서 데이터베이스를 사용하려면 ORM인 GORM 또는 더 유연한 sqlx 를 사용할 수 있습니다.\n📌 GORM을 이용한 데이터베이스 연결\npackage main import ( \"gorm.io/driver/sqlite\" \"gorm.io/gorm\" \"log\" ) type User struct { gorm.Model Name string Email string } func main() { db, err := gorm.Open(sqlite.Open(\"test.db\"), \u0026gorm.Config{}) if err != nil { log.Fatal(err) } db.AutoMigrate(\u0026User{}) // 테이블 자동 생성 user := User{Name: \"Alice\", Email: \"alice@example.com\"} db.Create(\u0026user) } ✅ GORM을 사용하면 Go에서 쉽게 ORM을 구현할 수 있습니다.\n4️⃣ REST API 설계 및 구현 REST API는 웹 서비스 간 데이터를 주고받을 때 사용 됩니다.\nGo의 net/http 또는 Gin 프레임워크를 활용하여 API를 개발할 수 있습니다.\n📌 예제: REST API 구현 (Gin 사용)\npackage main import ( \"github.com/gin-gonic/gin\" \"net/http\" ) type User struct { ID int `json:\"id\"` Name string `json:\"name\"` Email string `json:\"email\"` } var users = []User{ {ID: 1, Name: \"Alice\", Email: \"alice@example.com\"}, } func getUsers(c *gin.Context) { c.JSON(http.StatusOK, users) } func main() { r := gin.Default() r.GET(\"/users\", getUsers) r.Run(\":8080\") } ✅ REST API를 이용하면 다양한 클라이언트와 쉽게 데이터를 주고받을 수 있습니다.\n5️⃣ GraphQL과 Go 연동 (GQLGen, graphql-go) GraphQL은 REST보다 유연한 데이터 쿼리를 지원하는 API 입니다.\nGo에서는 GQLGen 또는 graphql-go 라이브러리를 사용할 수 있습니다.\n📌 GraphQL 서버 구축 (GQLGen 사용)\n# gqlgen 설치 go get github.com/99designs/gqlgen 📌 GraphQL 쿼리 예제\nquery { user(id: 1) { name email } } ✅ GraphQL을 사용하면 API 호출 횟수를 줄일 수 있습니다.\n6️⃣ 테스팅 및 Mocking 라이브러리 (GoMock, Testify) Go에서는 테스트 코드 작성이 필수 입니다.\n대표적인 테스팅 라이브러리는 GoMock, Testify 입니다.\n📌 예제: Testify로 단위 테스트 작성\npackage main import ( \"github.com/stretchr/testify/assert\" \"testing\" ) func Add(a, b int) int { return a + b } func TestAdd(t *testing.T) { result := Add(2, 3) assert.Equal(t, 5, result, \"Addition should be correct\") } ✅ Testify를 활용하면 테스트 코드 작성이 쉬워집니다.\n7️⃣ Go 언어의 클라우드 서비스 연동 (AWS SDK, Google Cloud SDK) Go는 AWS, GCP, Azure 등의 클라우드 서비스와 쉽게 연동 할 수 있습니다.\n예를 들어, AWS S3에 파일을 업로드할 수 있습니다.\n📌 AWS SDK를 사용하여 S3에 파일 업로드\n# AWS SDK 설치 go get github.com/aws/aws-sdk-go-v2 📌 예제: S3 파일 업로드\npackage main import ( \"context\" \"fmt\" \"github.com/aws/aws-sdk-go-v2/aws\" \"github.com/aws/aws-sdk-go-v2/config\" \"github.com/aws/aws-sdk-go-v2/service/s3\" \"log\" ) func main() { cfg, err := config.LoadDefaultConfig(context.TODO()) if err != nil { log.Fatal(err) } client := s3.NewFromConfig(cfg) bucket := \"my-bucket\" key := \"example.txt\" content := \"Hello, AWS S3!\" _, err = client.PutObject(context.TODO(), \u0026s3.PutObjectInput{ Bucket: aws.String(bucket), Key: aws.String(key), Body: aws.ReadSeekCloser(strings.NewReader(content)), }) if err != nil { log.Fatal(err) } fmt.Println(\"File uploaded successfully!\") } ✅ AWS SDK를 활용하면 쉽게 클라우드 서비스를 사용할 수 있습니다.\n🎯 정리 이번 강의에서는 Go 언어의 외부 라이브러리 및 프레임워크 활용 을 배웠습니다.\n📌 주요 내용 요약\n✔️ Go Modules 를 이용하여 패키지 관리\n✔️ Gin, Echo, Fiber 같은 웹 프레임워크 활용\n✔️ GORM, sqlx 를 활용한 데이터베이스 연동\n✔️ REST API 와 GraphQL API 개발\n✔️ Testify, GoMock 을 활용한 테스팅\n✔️ AWS SDK 를 활용한 클라우드 연동","1-외부-라이브러리-활용-go-modules로-패키지-관리#1️⃣ 외부 라이브러리 활용 (Go Modules로 패키지 관리)":"","2-웹-프레임워크-gin-echo-fiber-사용법#2️⃣ 웹 프레임워크 (Gin, Echo, Fiber) 사용법":"","3-데이터베이스와의-연동-gorm-sqlx#3️⃣ 데이터베이스와의 연동 (GORM, sqlx)":"","4-rest-api-설계-및-구현#4️⃣ REST API 설계 및 구현":"","5-graphql과-go-연동-gqlgen-graphql-go#5️⃣ GraphQL과 Go 연동 (GQLGen, graphql-go)":"","6-테스팅-및-mocking-라이브러리-gomock-testify#6️⃣ 테스팅 및 Mocking 라이브러리 (GoMock, Testify)":"","7-go-언어의-클라우드-서비스-연동-aws-sdk-google-cloud-sdk#7️⃣ Go 언어의 클라우드 서비스 연동 (AWS SDK, Google Cloud SDK)":""},"title":"Go 언어와 외부 라이브러리 및 프레임워크"},"/programing/go/go/go10/":{"data":{"":"","-정리#🎯 정리":"1️⃣ Go 성능 측정 (pprof, flamegraph, go-torch) Go 성능을 최적화하려면 먼저 성능을 측정하는 것이 중요합니다. Go에서는 다양한 도구를 제공하여 애플리케이션의 성능을 분석할 수 있습니다.\n📌 pprof를 활용한 성능 측정\npprof는 Go에서 제공하는 기본적인 프로파일링 도구로, CPU, 메모리 등의 성능을 분석할 수 있습니다.\npackage main import ( \"fmt\" \"net/http\" \"net/http/pprof\" \"log\" ) func main() { // pprof 핸들러 등록 go func() { log.Println(http.ListenAndServe(\"localhost:6060\", nil)) }() // 성능 분석을 위한 코드 fmt.Println(\"Hello, world!\") // 더 많은 코드 } 📌 Flamegraph와 go-torch 사용\ngo-torch는 pprof에서 생성한 데이터를 시각화하여 성능 분석을 보다 직관적으로 할 수 있게 돕습니다.\ngo-torch -u http://localhost:6060/debug/pprof/profile?seconds=30 ✅ pprof, flamegraph, go-torch 등을 활용하면 성능 병목을 쉽게 분석할 수 있습니다.\n2️⃣ 메모리 관리 및 최적화 (GC, 메모리 풀링) Go에서는 Garbage Collection(GC) 를 통해 메모리 관리를 자동으로 수행하지만, 때로는 성능을 더욱 최적화할 필요가 있습니다.\n메모리 풀링과 GC 튜닝을 활용하면 성능을 개선할 수 있습니다.\n📌 Garbage Collection(GC) 튜닝\nGo의 GC는 자동으로 메모리를 관리하지만, GOGC 환경 변수를 이용해 GC 빈도를 조절할 수 있습니다.\nGOGC=200 go run main.go 📌 메모리 풀링 예제 (sync.Pool 사용)\nsync.Pool은 자주 사용하는 객체를 풀에 저장하여 성능을 최적화합니다.\npackage main import ( \"fmt\" \"sync\" ) var pool = sync.Pool{ New: func() interface{} { return new(int) // 메모리 풀에서 사용할 객체 }, } func main() { p := pool.Get().(*int) // 풀에서 객체 가져오기 *p = 42 fmt.Println(*p) pool.Put(p) // 사용 후 풀에 반환 } ✅ 메모리 풀링을 활용하면 자주 생성되는 객체의 메모리 할당을 줄일 수 있습니다.\n3️⃣ CPU 및 메모리 프로파일링 Go에서는 CPU 프로파일링과 메모리 프로파일링을 통해 애플리케이션의 자원 사용 상태를 점검할 수 있습니다.\n📌 CPU 프로파일링 예제\nimport ( \"log\" \"net/http\" \"net/http/pprof\" ) func main() { go func() { log.Println(http.ListenAndServe(\"localhost:6060\", nil)) }() } 📌 메모리 프로파일링 예제\n// 실행 중에 `net/http/pprof` 패키지를 통해 메모리 사용을 추적할 수 있습니다. ✅ CPU와 메모리 프로파일링을 통해 성능의 병목을 발견할 수 있습니다.\n4️⃣ Goroutine 및 채널을 이용한 성능 최적화 Go의 Goroutine과 채널은 동시성 프로그래밍을 쉽게 구현할 수 있는 강력한 도구입니다.\n하지만 적절하게 사용하지 않으면 성능 저하가 발생할 수 있습니다.\n📌 Goroutine 최적화\nGoroutine을 사용할 때 동시성 문제를 최소화하기 위해 채널을 활용합니다.\npackage main import ( \"fmt\" \"sync\" ) func main() { var wg sync.WaitGroup jobs := make(chan int, 10) // Goroutine 생성 for i := 0; i \u003c 5; i++ { wg.Add(1) go func(i int) { defer wg.Done() fmt.Println(\"Job\", i) }(i) } // 작업 큐에 데이터 전송 for i := 0; i \u003c 5; i++ { jobs \u003c- i } wg.Wait() } ✅ Goroutine과 채널을 적절히 사용하여 성능을 최적화할 수 있습니다.\n5️⃣ 데이터 구조 최적화 (슬라이스, 맵, 링크드 리스트 등) Go에서 자주 사용하는 데이터 구조인 슬라이스, 맵, 링크드 리스트는 성능을 크게 좌우합니다.\n📌 슬라이스 최적화\n슬라이스의 용량을 미리 설정해주면 성능이 개선됩니다.\n// 미리 용량을 설정하여 슬라이스 최적화 slice := make([]int, 0, 100) for i := 0; i \u003c 100; i++ { slice = append(slice, i) } 📌 맵 최적화\n맵을 사용할 때 초기 크기를 설정해주면 성능을 높일 수 있습니다.\nm := make(map[string]int, 100) // 초기 크기 설정 ✅ 데이터 구조 최적화로 메모리 사용량을 줄이고 성능을 개선할 수 있습니다.\n6️⃣ 캐싱 및 병렬처리 최적화 Go에서는 캐시와 병렬처리를 이용해 성능을 크게 개선할 수 있습니다.\n📌 캐싱 예제\npackage main import ( \"fmt\" \"time\" \"sync\" ) var cache = make(map[string]string) var mu sync.Mutex func getFromCache(key string) string { mu.Lock() defer mu.Unlock() return cache[key] } func main() { cache[\"user1\"] = \"Alice\" fmt.Println(getFromCache(\"user1\")) } 📌 병렬처리 예제\npackage main import ( \"fmt\" \"sync\" ) func process(i int, wg *sync.WaitGroup) { defer wg.Done() fmt.Println(\"Processing\", i) } func main() { var wg sync.WaitGroup for i := 0; i \u003c 10; i++ { wg.Add(1) go process(i, \u0026wg) } wg.Wait() } ✅ 캐싱과 병렬처리를 적절히 활용하여 성능을 대폭 향상시킬 수 있습니다.\n7️⃣ Go 애플리케이션 배포 및 성능 테스트 Go 애플리케이션을 배포하고 성능 테스트를 진행하는 것도 매우 중요합니다.\n📌 배포 예시\nGo 애플리케이션을 Docker 컨테이너에 배포하면 다양한 환경에서 테스트하고 배포할 수 있습니다.\n📌 성능 테스트\n애플리케이션의 성능을 테스트하기 위해 load testing 도구를 사용할 수 있습니다.\n# 예시: `wrk`를 사용하여 성능 테스트 wrk -t4 -c100 -d30s http://localhost:8080 ✅ 성능 테스트를 통해 배포 전 애플리케이션의 성능을 검증할 수 있습니다.\n🎯 정리 이번 강의에서는 Go 언어의 고급 성능 튜닝 및 최적화 기법을 배웠습니다.\n📌 주요 내용 요약\n✔️ pprof, go-torch로 성능 분석\n✔️ sync.Pool을 이용한 메모리 최적화\n✔️ Goroutine과 채널로 동시성 최적화\n✔️ 데이터 구조 최적화 (슬라이스, 맵)\n✔️ 캐싱 및 병렬처리 활용\n✔️ 배포 및 성능 테스트","1-go-성능-측정-pprof-flamegraph-go-torch#1️⃣ Go 성능 측정 (pprof, flamegraph, go-torch)":"","2-메모리-관리-및-최적화-gc-메모리-풀링#2️⃣ 메모리 관리 및 최적화 (GC, 메모리 풀링)":"","3-cpu-및-메모리-프로파일링#3️⃣ CPU 및 메모리 프로파일링":"","4-goroutine-및-채널을-이용한-성능-최적화#4️⃣ Goroutine 및 채널을 이용한 성능 최적화":"","5-데이터-구조-최적화-슬라이스-맵-링크드-리스트-등#5️⃣ 데이터 구조 최적화 (슬라이스, 맵, 링크드 리스트 등)":"","6-캐싱-및-병렬처리-최적화#6️⃣ 캐싱 및 병렬처리 최적화":"","7-go-애플리케이션-배포-및-성능-테스트#7️⃣ Go 애플리케이션 배포 및 성능 테스트":""},"title":"Go 언어의 고급 성능 튜닝 및 최적화"},"/programing/go/go/go11/":{"data":{"":"","-정리#🎯 정리":" 1️⃣ 마이크로서비스(Microservices) 개념 이해 마이크로서비스 아키텍처는 하나의 큰 애플리케이션을 여러 개의 작은 독립적인 서비스로 분리하여 개발하는 방식입니다. 각 서비스는 독립적으로 배포되고, 서로 다른 프로그래밍 언어로 개발될 수 있습니다.\n📌 마이크로서비스의 주요 특징\n독립적인 배포 가능 서비스 간 경량화된 통신 (예: HTTP, gRPC) 각 서비스는 자체적인 데이터베이스를 가질 수 있음 장애 발생 시 서비스의 영향을 최소화 💡 예시\n마이크로서비스를 사용하는 예시로는 전자상거래 시스템을 들 수 있습니다. 예를 들어, 주문 처리 서비스, 결제 서비스, 사용자 인증 서비스 등으로 나누어져 각 서비스가 독립적으로 작동하게 됩니다.\n2️⃣ Go로 마이크로서비스 개발 (REST API, gRPC) Go 언어는 REST API와 gRPC를 사용하여 효율적인 마이크로서비스를 개발할 수 있습니다.\n2.1 REST API로 마이크로서비스 개발 RESTful API는 HTTP를 기반으로 데이터를 주고받는 서비스입니다. Go에서 net/http 패키지를 사용해 쉽게 REST API를 구축할 수 있습니다.\n📌 Go로 간단한 REST API 예시\npackage main import ( \"fmt\" \"net/http\" ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \"Hello, Microservices!\") } func main() { http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } 위의 예시는 Go로 구현한 간단한 REST API입니다. 이 API는 HTTP 요청을 받아서 “Hello, Microservices!“라는 메시지를 반환합니다.\n2.2 gRPC로 마이크로서비스 개발 gRPC는 Google에서 개발한 고성능, 언어 독립적인 원격 프로시저 호출(RPC) 시스템입니다.\nGo에서는 google.golang.org/grpc 패키지를 사용하여 gRPC 서비스를 쉽게 구현할 수 있습니다.\n📌 gRPC 서버 예시\n.proto 파일 생성 syntax = \"proto3\"; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {} } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } Go에서 gRPC 서버 구현 package main import ( \"context\" \"fmt\" \"google.golang.org/grpc\" \"log\" \"net\" pb \"path/to/your/protobuf\" ) type server struct{} func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) { return \u0026pb.HelloReply{Message: \"Hello \" + in.Name}, nil } func main() { lis, err := net.Listen(\"tcp\", \":50051\") if err != nil { log.Fatalf(\"failed to listen: %v\", err) } s := grpc.NewServer() pb.RegisterGreeterServer(s, \u0026server{}) fmt.Println(\"gRPC server running on port 50051\") if err := s.Serve(lis); err != nil { log.Fatalf(\"failed to serve: %v\", err) } } 위 코드는 gRPC 서버를 구현한 예시입니다. 클라이언트는 SayHello RPC 메서드를 호출하여 서버와 통신합니다.\n3️⃣ 서비스 간 통신 (HTTP, gRPC, RabbitMQ 등) 3.1 HTTP 기반 통신 서비스 간에 HTTP를 통해 데이터를 주고받을 수 있습니다. 위에서 설명한 REST API나 gRPC는 HTTP를 통해 통신하는 대표적인 방식입니다.\n3.2 RabbitMQ 기반 통신 RabbitMQ는 메시지 브로커로, 서비스 간 비동기식 메시지 전송을 처리하는 데 유용합니다. Go에서는 github.com/streadway/amqp 패키지를 사용하여 RabbitMQ와 통신할 수 있습니다.\n📌 RabbitMQ로 메시지 전송 예시\npackage main import ( \"log\" \"github.com/streadway/amqp\" ) func main() { conn, err := amqp.Dial(\"amqp://guest:guest@localhost:5672/\") if err != nil { log.Fatal(err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatal(err) } defer ch.Close() q, err := ch.QueueDeclare( \"task_queue\", // name true, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) if err != nil { log.Fatal(err) } body := \"Hello from Go\" err = ch.Publish( \"\", // exchange q.Name, // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType: \"text/plain\", Body: []byte(body), }) if err != nil { log.Fatal(err) } fmt.Println(\"Sent:\", body) } 위 예시는 Go에서 RabbitMQ를 사용하여 메시지를 전송하는 코드입니다.\n4️⃣ API Gateway 패턴 및 Go로 구축 API Gateway는 여러 마이크로서비스의 요청을 하나의 진입점에서 처리하는 패턴입니다. 이 패턴은 주로 인증, 라우팅, 로깅, 부하 분산 등의 기능을 처리합니다. Go로 API Gateway를 구축하려면 gorilla/mux 또는 **Gin**과 같은 라우터를 사용할 수 있습니다.\n📌 API Gateway 예시\npackage main import ( \"github.com/gin-gonic/gin\" ) func main() { r := gin.Default() r.GET(\"/service1\", func(c *gin.Context) { c.JSON(200, gin.H{ \"message\": \"Service 1\", }) }) r.GET(\"/service2\", func(c *gin.Context) { c.JSON(200, gin.H{ \"message\": \"Service 2\", }) }) r.Run(\":8080\") } 5️⃣ 트랜잭션 관리 및 CQRS 패턴 적용 5.1 트랜잭션 관리 Go에서는 SQL 트랜잭션을 통해 여러 데이터베이스 연산을 원자적으로 처리할 수 있습니다. 트랜잭션을 사용하면 여러 서비스 간 일관성을 유지할 수 있습니다.\n5.2 CQRS 패턴 CQRS (Command Query Responsibility Segregation) 패턴은 **명령(Command)**과 **쿼리(Query)**를 분리하여 처리하는 패턴입니다. 이 패턴을 통해 읽기 성능을 최적화할 수 있습니다.\n6️⃣ Docker와 Kubernetes로 Go 서비스 배포 Go로 개발한 서비스를 Docker와 Kubernetes를 사용하여 배포할 수 있습니다.\nDocker: 애플리케이션을 컨테이너로 패키징하여 배포 Kubernetes: 컨테이너화된 애플리케이션을 오케스트레이션하고 관리 📌 Dockerfile 예시\nFROM golang:1.18-alpine WORKDIR /app COPY . . RUN go build -o app . CMD [\"./app\"] 📌 Kubernetes 배포 예시\napiVersion: apps/v1 kind: Deployment metadata: name: go-app spec: replicas: 3 selector: matchLabels: app: go-app template: metadata: labels: app: go-app spec: containers: - name: go-app image: go-app:latest ports: - containerPort: 8080 7️⃣ 서비스 디스커버리 및 로드밸런싱 서비스 디스커버리는 마이크로서비스 간 동적 주소 변경을 처리하는 방법입니다. Kubernetes에서는 Service 객체를 사용하여 서비스를 디스커버리하고, 로드밸런서를 통해 트래픽을 분산할 수 있습니다.\n📌 서비스 디스커버리 및 로드밸런싱 예시\napiVersion: v1 kind: Service metadata: name: go-service spec: selector: app: go-app ports: - protocol: TCP port: 80 targetPort: 8080 🎯 정리 이번 강의에서는 Go 언어를 사용한 마이크로서비스 아키텍처 구축 방법을 배웠습니다.\n📌 주요 내용 요약\n✔️ Go로 REST API와 gRPC 서비스 개발\n✔️ RabbitMQ를 통한 서비스 간 통신\n✔️ API Gateway 패턴 구현\n✔️ Docker와 Kubernetes로 배포\n✔️ **트랜\n잭션 관리** 및 CQRS 패턴 적용\n✔️ 서비스 디스커버리 및 로드밸런싱","1-마이크로서비스microservices-개념-이해#1️⃣ 마이크로서비스(Microservices) 개념 이해":"","2-go로-마이크로서비스-개발-rest-api-grpc#2️⃣ Go로 마이크로서비스 개발 (REST API, gRPC)":"","3-서비스-간-통신-http-grpc-rabbitmq-등#3️⃣ 서비스 간 통신 (HTTP, gRPC, RabbitMQ 등)":"","4-api-gateway-패턴-및-go로-구축#4️⃣ API Gateway 패턴 및 Go로 구축":"","5-트랜잭션-관리-및-cqrs-패턴-적용#5️⃣ 트랜잭션 관리 및 CQRS 패턴 적용":"","6-docker와-kubernetes로-go-서비스-배포#6️⃣ Docker와 Kubernetes로 Go 서비스 배포":"","7-서비스-디스커버리-및-로드밸런싱#7️⃣ 서비스 디스커버리 및 로드밸런싱":""},"title":"Go 언어로 마이크로서비스 아키텍처 구현"},"/programing/go/go/go12/":{"data":{"":"","-정리#🎯 정리":" 1️⃣ Go 애플리케이션 빌드 및 배포 (Docker, Binary 등) Go 애플리케이션은 간단한 빌드 프로세스를 통해 다양한 환경에 배포할 수 있습니다.\nGo의 가장 큰 장점 중 하나는 컴파일된 바이너리가 독립적이라는 점입니다. 이를 통해, 애플리케이션을 쉽게 배포할 수 있습니다.\n1.1 Go 애플리케이션 빌드 Go는 단일 바이너리 파일로 컴파일되기 때문에, 배포가 매우 간편합니다.\n📌 Go 애플리케이션 빌드 예시\ngo build -o myapp main.go 위 명령어를 실행하면 main.go 파일을 기반으로 **myapp**이라는 실행 가능한 바이너리 파일이 생성됩니다.\n1.2 Docker를 통한 배포 Go 애플리케이션을 Docker로 컨테이너화하여 배포할 수도 있습니다. 이를 통해 애플리케이션을 다양한 환경에서 실행할 수 있게 됩니다.\n📌 Dockerfile 예시\nFROM golang:1.18-alpine WORKDIR /app COPY . . RUN go build -o myapp . CMD [\"./myapp\"] 위 Dockerfile은 Go 애플리케이션을 golang:1.18-alpine 이미지를 사용하여 빌드하고 실행하는 방법을 보여줍니다.\n2️⃣ Go 애플리케이션 컨테이너화 (Dockerfile 작성 및 최적화) Go 애플리케이션을 Docker로 컨테이너화하는 방법은 매우 직관적입니다.\nDockerfile을 사용하여 Go 애플리케이션을 최적화된 컨테이너로 빌드하고 배포할 수 있습니다.\n2.1 최적화된 Dockerfile 작성 Go 애플리케이션을 컨테이너화할 때, 불필요한 파일을 제외하고 필요한 파일만 복사하여 이미지 크기를 최소화하는 것이 중요합니다.\n📌 최적화된 Dockerfile 예시\n# 빌드 단계 FROM golang:1.18-alpine AS builder WORKDIR /app COPY . . RUN go build -o myapp . # 실행 단계 FROM alpine:latest WORKDIR /root/ COPY --from=builder /app/myapp . CMD [\"./myapp\"] 위 Dockerfile은 멀티 스테이지 빌드를 사용하여 빌드 환경과 실행 환경을 분리합니다. 이를 통해 최종 이미지 크기를 최소화하고 배포를 최적화할 수 있습니다.\n3️⃣ Kubernetes 환경에서 Go 애플리케이션 배포 Kubernetes는 애플리케이션의 컨테이너화된 인프라를 관리하는 데 유용한 도구입니다. Go 애플리케이션을 Kubernetes 클러스터에 배포하여 높은 가용성과 확장성을 얻을 수 있습니다.\n3.1 Kubernetes 배포 파일 작성 Go 애플리케이션을 Kubernetes에서 배포하려면 Deployment와 Service 객체를 사용합니다.\n📌 Deployment 예시\napiVersion: apps/v1 kind: Deployment metadata: name: go-app spec: replicas: 3 selector: matchLabels: app: go-app template: metadata: labels: app: go-app spec: containers: - name: go-app image: go-app:latest ports: - containerPort: 8080 3.2 Kubernetes Service 정의 Kubernetes에서는 Service 객체를 사용하여 클러스터 내부에서 Go 애플리케이션에 접근할 수 있도록 합니다.\n📌 Service 예시\napiVersion: v1 kind: Service metadata: name: go-app-service spec: selector: app: go-app ports: - protocol: TCP port: 80 targetPort: 8080 이렇게 배포된 Go 애플리케이션은 Kubernetes 클러스터 내에서 다수의 복제본을 통해 확장될 수 있습니다.\n4️⃣ CI/CD 파이프라인 구축 (Jenkins, GitLab CI, GitHub Actions) CI/CD(지속적 통합/지속적 배포)는 Go 애플리케이션을 자동화된 방식으로 배포하고 운영하는 데 중요한 역할을 합니다.\nGo 애플리케이션을 Jenkins, GitLab CI, GitHub Actions 등을 통해 자동화된 파이프라인을 구축할 수 있습니다.\n4.1 Jenkins를 사용한 CI/CD 파이프라인 Jenkins는 자유도 높은 CI/CD 도구로, Go 애플리케이션의 빌드 및 배포 프로세스를 자동화할 수 있습니다.\n📌 Jenkinsfile 예시\npipeline { agent any stages { stage('Build') { steps { sh 'go build -o myapp main.go' } } stage('Test') { steps { sh 'go test ./...' } } stage('Deploy') { steps { sh 'docker build -t myapp .' sh 'docker run -d -p 8080:8080 myapp' } } } } 위 Jenkinsfile은 Go 애플리케이션을 빌드, 테스트, 배포하는 간단한 CI/CD 파이프라인을 설정합니다.\n5️⃣ Go 애플리케이션 로깅 및 모니터링 Go 애플리케이션을 운영할 때 로깅과 모니터링은 필수적인 요소입니다. 이를 통해 애플리케이션의 동작 상태를 실시간으로 추적하고, 문제가 발생했을 때 신속하게 대응할 수 있습니다.\n5.1 로깅 Go에서는 log 패키지를 사용하여 로그를 출력할 수 있습니다. 또한, **logrus**와 같은 서드파티 라이브러리를 사용하여 보다 정교한 로그 시스템을 구현할 수 있습니다.\n📌 로깅 예시\npackage main import ( \"log\" ) func main() { log.Println(\"Go application started\") log.Fatal(\"An error occurred\") } 5.2 모니터링 애플리케이션을 모니터링하기 위해 Prometheus와 Grafana를 사용할 수 있습니다. 이를 통해 메트릭 수집 및 시각화가 가능합니다.\n📌 Prometheus와 Go 통합 예시\npackage main import ( \"github.com/prometheus/client_golang/prometheus\" \"github.com/prometheus/client_golang/prometheus/promhttp\" \"net/http\" ) var requestCount = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"http_requests_total\", Help: \"Total number of HTTP requests\", }, []string{\"method\", \"status\"}, ) func init() { prometheus.MustRegister(requestCount) } func handler(w http.ResponseWriter, r *http.Request) { requestCount.WithLabelValues(r.Method, \"200\").Inc() w.Write([]byte(\"Hello, Go!\")) } func main() { http.Handle(\"/metrics\", promhttp.Handler()) http.HandleFunc(\"/\", handler) http.ListenAndServe(\":8080\", nil) } 위 코드는 Prometheus 메트릭을 수집하여 /metrics 엔드포인트에서 노출하는 예시입니다.\n6️⃣ Go 애플리케이션의 보안 강화 (TLS, 인증, 인가) 애플리케이션의 보안은 매우 중요합니다. Go에서는 TLS (Transport Layer Security), 인증, 인가를 사용하여 애플리케이션을 보호할 수 있습니다.\n6.1 TLS 설정 Go에서 HTTPS를 사용하려면 http.ListenAndServeTLS 메서드를 사용하여 SSL 인증서를 설정해야 합니다.\n📌 TLS 설정 예시\npackage main import ( \"log\" \"net/http\" ) func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello, Go with TLS!\")) }) log.Fatal(http.ListenAndServeTLS(\":443\", \"cert.pem\", \"key.pem\", nil)) } 이 코드는 Go 애플리케이션을 TLS로 보호된 HTTPS 서버로 실행합니다.\n6.2 인증 및 인가 Go에서는 OAuth, JWT와 같은 인증 방식으로 사용자의 인증을 처리할 수 있습니다. 이를 통해 보안이 강화된 애플리케이션을 구축할 수 있습니다.\n🎯 정리 이번 강의에서는 Go 언어로 애플리케이션을 배포하고 운영하는 방법에 대해 배웠습니다.\n📌 주요 내용 요약\n✔️ Go 애플리케이션 빌드 및 Docker 배포\n✔️ Kubernetes 환경에서 Go 애플리케이션 배포\n✔️ CI/CD 파이프라인 구축 (Jenkins, GitLab CI, GitHub Actions)\n✔️ 로깅 및 모니터링 도구 통합\n✔️ 보안 강화 (TLS, 인증, 인가)","1-go-애플리케이션-빌드-및-배포-docker-binary-등#1️⃣ Go 애플리케이션 빌드 및 배포 (Docker, Binary 등)":"","2-go-애플리케이션-컨테이너화-dockerfile-작성-및-최적화#2️⃣ Go 애플리케이션 컨테이너화 (Dockerfile 작성 및 최적화)":"","3-kubernetes-환경에서-go-애플리케이션-배포#3️⃣ Kubernetes 환경에서 Go 애플리케이션 배포":"","4-cicd-파이프라인-구축-jenkins-gitlab-ci-github-actions#4️⃣ CI/CD 파이프라인 구축 (Jenkins, GitLab CI, GitHub Actions)":"","5-go-애플리케이션-로깅-및-모니터링#5️⃣ Go 애플리케이션 로깅 및 모니터링":"","6-go-애플리케이션의-보안-강화-tls-인증-인가#6️⃣ Go 애플리케이션의 보안 강화 (TLS, 인증, 인가)":""},"title":"Go 언어의 배포 및 운영 "},"/programing/go/go/go13/":{"data":{"":"","1-go-인터페이스-및-다형성-다형성-구현-방식#1️⃣ Go 인터페이스 및 다형성 (다형성 구현 방식)":"","2-go의-메모리-모델과-동시성-제어#2️⃣ Go의 메모리 모델과 동시성 제어":"","3-go의-garbage-collection-최적화#3️⃣ Go의 Garbage Collection 최적화":"","4-go로-개발하는-클라우드-네이티브-애플리케이션#4️⃣ Go로 개발하는 클라우드 네이티브 애플리케이션":"","5-go-애플리케이션과-외부-시스템-연동-kafka-redis-등#5️⃣ Go 애플리케이션과 외부 시스템 연동 (Kafka, Redis 등)":" 1️⃣ Go 인터페이스 및 다형성 (다형성 구현 방식) Go 언어는 인터페이스를 사용하여 다형성을 구현합니다. Go에서 인터페이스는 매우 중요한 개념으로, 객체지향 언어에서의 클래스와 유사하지만 구현을 강제하지 않으며 명시적인 선언이 필요 없습니다.\n1.1 Go 인터페이스란? Go에서 인터페이스는 메서드의 집합입니다. 객체는 특정 인터페이스를 구현하는 메서드를 가지면 자동으로 그 인터페이스를 구현한 것으로 간주됩니다. 이를 통해 다형성을 구현할 수 있습니다.\n📌 인터페이스 예시\npackage main import \"fmt\" // 인터페이스 정의 type Speaker interface { Speak() string } // 구조체 정의 type Person struct { Name string } // 메서드 정의 func (p Person) Speak() string { return \"Hello, my name is \" + p.Name } func main() { var speaker Speaker p := Person{Name: \"Alice\"} speaker = p fmt.Println(speaker.Speak()) // \"Hello, my name is Alice\" } 이 예시에서 Speaker 인터페이스는 Speak() 메서드를 정의하고 있습니다. Person 구조체는 Speak() 메서드를 구현하여 Speaker 인터페이스를 만족합니다. Go는 명시적으로 implements 키워드를 사용하지 않으며, 메서드를 구현한 객체는 자동으로 인터페이스를 구현한 것으로 간주됩니다.\n1.2 다형성의 구현 방식 Go에서는 다형성을 인터페이스를 통해 구현합니다. 동일한 인터페이스를 구현하는 다양한 타입들을 다룰 수 있습니다.\n📌 다형성 예시\npackage main import \"fmt\" // 인터페이스 정의 type Worker interface { Work() string } type Programmer struct { Name string } type Designer struct { Name string } // Worker 인터페이스 구현 func (p Programmer) Work() string { return p.Name + \" is coding.\" } func (d Designer) Work() string { return d.Name + \" is designing.\" } func main() { var worker Worker worker = Programmer{Name: \"John\"} fmt.Println(worker.Work()) // \"John is coding.\" worker = Designer{Name: \"Jane\"} fmt.Println(worker.Work()) // \"Jane is designing.\" } Programmer와 Designer는 Worker 인터페이스를 구현하여 각각 다른 동작을 정의합니다. 이를 통해 다양한 타입을 동일한 인터페이스로 다룰 수 있습니다.\n2️⃣ Go의 메모리 모델과 동시성 제어 Go는 동시성(Concurrency)과 병렬성(Parallelism)을 매우 잘 지원하는 언어입니다. Go의 메모리 모델과 동시성 제어 방식을 이해하면, 고성능 시스템을 개발하는 데 유리합니다.\n2.1 Go의 메모리 모델 Go의 메모리 모델은 병렬성과 동시성을 명확히 정의하고 있습니다. 각 고루틴은 독립적인 스택 메모리를 가지며, 공유 메모리를 안전하게 처리하는 방식에 대한 규칙을 제공합니다.\n2.2 동시성 제어 (goroutines, channels) Go의 동시성 제어는 goroutine과 channel을 사용하여 처리합니다. goroutine은 Go에서 제공하는 경량 스레드로, 여러 개의 goroutine을 동시에 실행하여 비동기적으로 작업을 수행할 수 있습니다. channel은 고루틴 간에 데이터를 안전하게 교환하는 방법입니다.\n📌 고루틴과 채널 예시\npackage main import \"fmt\" // 고루틴으로 실행할 함수 func sayHello() { fmt.Println(\"Hello from Go!\") } func main() { go sayHello() // 고루틴 실행 fmt.Println(\"Main function continues running...\") // 채널을 통한 동기화 done := make(chan bool) go func() { fmt.Println(\"Goroutine finishes execution.\") done \u003c- true }() \u003c-done // 고루틴 종료 대기 } 위 예시에서 sayHello 함수는 고루틴으로 실행되며, 메인 함수는 고루틴의 실행이 끝날 때까지 기다립니다. 채널을 사용하여 동기화를 관리합니다.\n3️⃣ Go의 Garbage Collection 최적화 Go는 자동 메모리 관리를 제공하며, **Garbage Collection (GC)**을 통해 메모리 관리의 부담을 줄여줍니다. 그러나 GC는 성능에 영향을 미칠 수 있으므로 최적화가 필요할 수 있습니다.\n3.1 Go의 Garbage Collection 동작 방식 Go의 GC는 마크-스윕(Mark-Sweep) 방식으로 동작합니다. 이를 통해 사용되지 않는 메모리를 주기적으로 회수합니다. 하지만 빈번한 GC는 성능 저하를 일으킬 수 있기 때문에 이를 최적화하는 것이 중요합니다.\n3.2 GC 최적화 방법 Go에서 GC 최적화를 위해서는 메모리 할당 최소화, 저수준 메모리 관리와 같은 방법을 사용합니다.\n📌 GC 최적화 예시\n메모리 할당 최소화\nsync.Pool을 사용하여 객체를 재사용하면 GC 부담을 줄일 수 있습니다. package main import ( \"fmt\" \"sync\" ) var pool = sync.Pool{ New: func() interface{} { return new(int) }, } func main() { item := pool.Get().(*int) *item = 42 fmt.Println(*item) pool.Put(item) // 객체 재사용 } 이 예시는 sync.Pool을 사용하여 객체를 재사용하고, GC 부담을 줄이는 방법을 보여줍니다.\n4️⃣ Go로 개발하는 클라우드 네이티브 애플리케이션 Go는 클라우드 네이티브 애플리케이션 개발에 매우 적합한 언어입니다. Go는 경량화, 빠른 실행 속도, 동시성을 통해 대규모 분산 시스템을 효율적으로 개발할 수 있게 합니다.\n4.1 클라우드 네이티브 애플리케이션의 특징 클라우드 네이티브 애플리케이션은 마이크로서비스, 컨테이너화, 자동화된 배포 등을 특징으로 합니다. Go는 이러한 시스템을 구축할 때 매우 유용한 도구입니다.\n4.2 Go로 클라우드 네이티브 애플리케이션 개발 Go로 REST API, gRPC 서비스, Kubernetes 기반의 애플리케이션을 개발할 수 있습니다.\n📌 Go로 gRPC 서비스 예시\n// gRPC 서버 설정 코드 package main import ( \"fmt\" \"net\" \"google.golang.org/grpc\" ) type server struct{} func (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloResponse, error) { return \u0026pb.HelloResponse{Message: \"Hello \" + in.GetName()}, nil } func main() { lis, err := net.Listen(\"tcp\", \":50051\") if err != nil { fmt.Println(\"Failed to listen:\", err) return } s := grpc.NewServer() pb.RegisterGreeterServer(s, \u0026server{}) if err := s.Serve(lis); err != nil { fmt.Println(\"Failed to serve:\", err) } } Go로 gRPC 서비스를 개발하여 클라우드 네이티브 애플리케이션을 구축할 수 있습니다.\n5️⃣ Go 애플리케이션과 외부 시스템 연동 (Kafka, Redis 등) Go 애플리케이션은 다양한 외부 시스템과 쉽게 연동할 수 있습니다. 예를 들어, Kafka를 사용한 메시징 시스템, Redis를 사용한 캐시 시스템 등을 통합할 수 있습니다.\n5.1 Kafka 연동 Go는 Kafka와 연동하여 비동기적인 메시지 큐 시스템을 구축할 수 있습니다.\n📌 Kafka 연동 예시\npackage main import ( \"github.com/Shopify/sarama\" \"log\" ) func main() { config := sarama.NewConfig() producer, err := sarama.NewAsyncProducer([]string{\"localhost:9092\"}, config) if err != nil { log.Fatal(err) } defer producer.AsyncClose() message := \u0026sarama.ProducerMessage{ Topic: \"test_topic\", Value: sarama.StringEncoder(\"Hello Kafka\"), } producer.Input() \u003c- message } 위 코드는 Go 애플리케이션에서 Kafka를 통해 메시지를 보내는 예시입니다.\n5.2 Redis 연동 Go는 Redis와 연동하여 빠른 데이터 캐싱 시스템을 구축할 수 있습니다.\n📌 Redis 연동 예시\npackage main import ( \"github.com/go-redis/redis\" \"fmt\" ) func main() { client := redis.NewClient(\u0026redis.Options{ Addr: \"localhost:6379\", }) err := client.Set(\"key\", \"value\", 0).Err() if err != nil { fmt.Println(err) } val, err := client.Get(\"key\").Result() if err != nil { fmt.Println(err) } fmt.Println(\"key:\", val) } ","6-go-언어의#6️⃣ Go 언어의":"원리 및 내부 동작 (Go Compiler, Go Runtime, Scheduler)\nGo는 내부적으로 Go 컴파일러, Go 런타임, 스케줄러 등을 통해 효율적인 실행을 지원합니다. 이러한 원리를 이해하면 더 효율적인 프로그램을 작성할 수 있습니다.\n6.1 Go 컴파일러 Go 컴파일러는 Go 소스 코드를 어셈블리 코드로 변환합니다. 이를 통해 Go 프로그램은 빠르게 실행될 수 있습니다.\n6.2 Go 런타임 Go 런타임은 고루틴 관리, 메모리 할당, GC 등을 처리하는 Go 프로그램의 핵심 엔진입니다.\n6.3 Go 스케줄러 Go 스케줄러는 여러 고루틴을 스레드 풀에서 효율적으로 분배하여 병렬로 실행하도록 합니다. 고루틴은 매우 경량화되어 있어 많은 수의 고루틴을 동시에 실행할 수 있습니다."},"title":"Go 언어 고급 주제"},"/programing/python/python/":{"data":{"1-python-언어의-역사-및-발전-배경#1️⃣ \u003cstrong\u003ePython 언어의 역사 및 발전 배경\u003c/strong\u003e":"","2-python의-특징과-장점#2️⃣ \u003cstrong\u003ePython의 특징과 장점\u003c/strong\u003e":"","3-python과-다른-프로그래밍-언어c-java-javascript-비교#3️⃣ \u003cstrong\u003ePython과 다른 프로그래밍 언어(C, Java, JavaScript) 비교\u003c/strong\u003e":"","4-python의-주요-사용-사례#4️⃣ \u003cstrong\u003ePython의 주요 사용 사례\u003c/strong\u003e":"","5-python의-에코시스템#5️⃣ \u003cstrong\u003ePython의 에코시스템\u003c/strong\u003e":"Python 소개 및 기본 개념 1️⃣ Python 언어의 역사 및 발전 배경 Python은 1991년 네덜란드의 프로그래머인 Guido van Rossum에 의해 처음 출시되었습니다. Python은 그 당시 존재하던 다양한 프로그래밍 언어들의 복잡성을 개선하고자 하는 목표로 만들어졌습니다. 그 이유는 많은 개발자들이 코드의 가독성을 중시하고, 배우기 쉬운 언어를 선호했기 때문입니다.\nPython의 주요 특징은 “코드를 읽는 것이 코드를 작성하는 것보다 더 중요하다\"는 철학을 따르고 있다는 점입니다. 이는 Python이 높은 가독성을 목표로 설계되었음을 의미합니다.\nPython 언어의 발전 과정\nPython은 그 당시 다른 언어들과 비교해 간결하고, 직관적인 문법을 제공하며, 다양한 분야에서 사용되기 시작했습니다. 특히, 웹 개발, 데이터 분석, 인공지능 및 자동화 분야에서 급격하게 인기를 끌었습니다.\n2️⃣ Python의 특징과 장점 간결성 Python은 코드가 간결하고 직관적입니다. 다른 언어들에 비해 문법이 간단하고, 중괄호({})나 세미콜론(;)을 사용하지 않습니다. 다음은 Python 코드의 예시입니다:\n# Hello, World! 출력 print(\"Hello, World!\") 읽기 쉬움 Python의 문법은 사람의 자연 언어처럼 읽기 쉽습니다. 예를 들어, 조건문은 if를 사용하고, 반복문은 for나 while을 사용합니다. 이는 다른 복잡한 언어들에 비해 코드가 매우 직관적입니다.\n다양한 라이브러리 Python은 다양한 라이브러리와 패키지를 제공하여, 데이터 분석, 머신러닝, 웹 개발 등 여러 분야에서 매우 강력한 도구가 됩니다. 예를 들어, NumPy, Pandas, TensorFlow, Flask, Django 등 다양한 라이브러리를 통해 복잡한 작업을 쉽게 처리할 수 있습니다.\n3️⃣ Python과 다른 프로그래밍 언어(C, Java, JavaScript) 비교 Python vs C 문법: Python은 C보다 간결하고 읽기 쉬운 문법을 가지고 있습니다. C는 저수준 언어로 메모리 관리에 대한 세부적인 제어가 필요하지만, Python은 메모리 관리를 자동으로 처리합니다. 성능: C는 Python보다 빠르지만, Python은 다양한 최적화 기법을 제공하여 성능을 개선할 수 있습니다. Python vs Java 문법: Java는 엄격한 타입 시스템과 객체 지향적인 문법을 가지고 있습니다. 반면, Python은 동적 타이핑을 지원하여 더 자유로운 코드를 작성할 수 있습니다. 배우기 쉬운 정도: Python은 Java에 비해 문법이 훨씬 단순하고 직관적입니다. Java는 보다 구조화된 접근 방식을 요구하는 경우가 많습니다. Python vs JavaScript 용도: Python은 주로 서버 사이드 개발과 데이터 분석, 인공지능 등에 사용되며, JavaScript는 클라이언트 사이드 웹 개발에 주로 사용됩니다. 문법: Python은 다른 언어에 비해 더 간결하고 읽기 쉬운 문법을 제공하지만, JavaScript는 비동기 처리와 이벤트 기반 프로그래밍에서 더 강력한 기능을 제공합니다. 4️⃣ Python의 주요 사용 사례 웹 개발 Python은 Django, Flask와 같은 강력한 웹 프레임워크를 통해 웹 애플리케이션을 쉽게 개발할 수 있습니다. 예를 들어, Flask를 사용한 간단한 웹 서버 예시는 다음과 같습니다:\nfrom flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return 'Hello, World!' if __name__ == \"__main__\": app.run() 데이터 분석 Python은 데이터 분석에 매우 강력한 언어입니다. Pandas, NumPy 라이브러리 등을 활용하여 데이터를 처리하고 분석할 수 있습니다. 예를 들어, Pandas를 사용하여 CSV 파일을 읽고 데이터를 분석할 수 있습니다:\nimport pandas as pd data = pd.read_csv(\"data.csv\") print(data.head()) 인공지능(AI) Python은 TensorFlow, Keras, PyTorch 등의 라이브러리로 인공지능 및 머신러닝 모델을 쉽게 구축할 수 있습니다.\n자동화 Python은 반복적인 작업을 자동화하는 데 유용합니다. 예를 들어, 파일 시스템 관리나 웹 스크래핑 작업을 간단한 스크립트로 자동화할 수 있습니다.\n5️⃣ Python의 에코시스템 PyPI Python Package Index(PyPI)는 Python의 패키지 저장소로, 다양한 라이브러리와 툴들을 쉽게 설치하고 사용할 수 있게 합니다. pip 명령어를 사용하여 패키지를 설치할 수 있습니다:\npip install requests Virtualenv Virtualenv는 각 프로젝트별로 독립적인 Python 환경을 관리할 수 있게 도와줍니다. 이를 통해 라이브러리 버전 충돌을 방지하고, 각 프로젝트에서 필요한 라이브러리만 설치하여 환경을 분리할 수 있습니다.\nAnaconda Anaconda는 데이터 분석, 머신러닝, 과학 컴퓨팅을 위한 Python 배포판입니다. 다양한 데이터 과학 관련 라이브러리가 포함되어 있어, 데이터 분석을 쉽게 시작할 수 있게 해줍니다.","python-소개-및-기본-개념#\u003cstrong\u003ePython 소개 및 기본 개념\u003c/strong\u003e":""},"title":"Python 소개 및 기본 개념"},"/programing/python/python/python00/":{"data":{"":"","1-python-언어의-역사-및-발전-배경#1️⃣ \u003cstrong\u003ePython 언어의 역사 및 발전 배경\u003c/strong\u003e":"","2-python의-특징과-장점#2️⃣ \u003cstrong\u003ePython의 특징과 장점\u003c/strong\u003e":"","3-python과-다른-프로그래밍-언어c-java-javascript-비교#3️⃣ \u003cstrong\u003ePython과 다른 프로그래밍 언어(C, Java, JavaScript) 비교\u003c/strong\u003e":"","4-python의-주요-사용-사례#4️⃣ \u003cstrong\u003ePython의 주요 사용 사례\u003c/strong\u003e":"","5-python의-에코시스템#5️⃣ \u003cstrong\u003ePython의 에코시스템\u003c/strong\u003e":" 1️⃣ Python 언어의 역사 및 발전 배경 Python은 1991년 네덜란드의 프로그래머인 Guido van Rossum에 의해 처음 출시되었습니다. Python은 그 당시 존재하던 다양한 프로그래밍 언어들의 복잡성을 개선하고자 하는 목표로 만들어졌습니다. 그 이유는 많은 개발자들이 코드의 가독성을 중시하고, 배우기 쉬운 언어를 선호했기 때문입니다.\nPython의 주요 특징은 “코드를 읽는 것이 코드를 작성하는 것보다 더 중요하다\"는 철학을 따르고 있다는 점입니다. 이는 Python이 높은 가독성을 목표로 설계되었음을 의미합니다.\nPython 언어의 발전 과정\nPython은 그 당시 다른 언어들과 비교해 간결하고, 직관적인 문법을 제공하며, 다양한 분야에서 사용되기 시작했습니다. 특히, 웹 개발, 데이터 분석, 인공지능 및 자동화 분야에서 급격하게 인기를 끌었습니다.\n2️⃣ Python의 특징과 장점 간결성 Python은 코드가 간결하고 직관적입니다. 다른 언어들에 비해 문법이 간단하고, 중괄호({})나 세미콜론(;)을 사용하지 않습니다. 다음은 Python 코드의 예시입니다:\n# Hello, World! 출력 print(\"Hello, World!\") 읽기 쉬움 Python의 문법은 사람의 자연 언어처럼 읽기 쉽습니다. 예를 들어, 조건문은 if를 사용하고, 반복문은 for나 while을 사용합니다. 이는 다른 복잡한 언어들에 비해 코드가 매우 직관적입니다.\n다양한 라이브러리 Python은 다양한 라이브러리와 패키지를 제공하여, 데이터 분석, 머신러닝, 웹 개발 등 여러 분야에서 매우 강력한 도구가 됩니다. 예를 들어, NumPy, Pandas, TensorFlow, Flask, Django 등 다양한 라이브러리를 통해 복잡한 작업을 쉽게 처리할 수 있습니다.\n3️⃣ Python과 다른 프로그래밍 언어(C, Java, JavaScript) 비교 Python vs C 문법: Python은 C보다 간결하고 읽기 쉬운 문법을 가지고 있습니다. C는 저수준 언어로 메모리 관리에 대한 세부적인 제어가 필요하지만, Python은 메모리 관리를 자동으로 처리합니다. 성능: C는 Python보다 빠르지만, Python은 다양한 최적화 기법을 제공하여 성능을 개선할 수 있습니다. Python vs Java 문법: Java는 엄격한 타입 시스템과 객체 지향적인 문법을 가지고 있습니다. 반면, Python은 동적 타이핑을 지원하여 더 자유로운 코드를 작성할 수 있습니다. 배우기 쉬운 정도: Python은 Java에 비해 문법이 훨씬 단순하고 직관적입니다. Java는 보다 구조화된 접근 방식을 요구하는 경우가 많습니다. Python vs JavaScript 용도: Python은 주로 서버 사이드 개발과 데이터 분석, 인공지능 등에 사용되며, JavaScript는 클라이언트 사이드 웹 개발에 주로 사용됩니다. 문법: Python은 다른 언어에 비해 더 간결하고 읽기 쉬운 문법을 제공하지만, JavaScript는 비동기 처리와 이벤트 기반 프로그래밍에서 더 강력한 기능을 제공합니다. 4️⃣ Python의 주요 사용 사례 웹 개발 Python은 Django, Flask와 같은 강력한 웹 프레임워크를 통해 웹 애플리케이션을 쉽게 개발할 수 있습니다. 예를 들어, Flask를 사용한 간단한 웹 서버 예시는 다음과 같습니다:\nfrom flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return 'Hello, World!' if __name__ == \"__main__\": app.run() 데이터 분석 Python은 데이터 분석에 매우 강력한 언어입니다. Pandas, NumPy 라이브러리 등을 활용하여 데이터를 처리하고 분석할 수 있습니다. 예를 들어, Pandas를 사용하여 CSV 파일을 읽고 데이터를 분석할 수 있습니다:\nimport pandas as pd data = pd.read_csv(\"data.csv\") print(data.head()) 인공지능(AI) Python은 TensorFlow, Keras, PyTorch 등의 라이브러리로 인공지능 및 머신러닝 모델을 쉽게 구축할 수 있습니다.\n자동화 Python은 반복적인 작업을 자동화하는 데 유용합니다. 예를 들어, 파일 시스템 관리나 웹 스크래핑 작업을 간단한 스크립트로 자동화할 수 있습니다.\n5️⃣ Python의 에코시스템 PyPI Python Package Index(PyPI)는 Python의 패키지 저장소로, 다양한 라이브러리와 툴들을 쉽게 설치하고 사용할 수 있게 합니다. pip 명령어를 사용하여 패키지를 설치할 수 있습니다:\npip install requests Virtualenv Virtualenv는 각 프로젝트별로 독립적인 Python 환경을 관리할 수 있게 도와줍니다. 이를 통해 라이브러리 버전 충돌을 방지하고, 각 프로젝트에서 필요한 라이브러리만 설치하여 환경을 분리할 수 있습니다.\nAnaconda Anaconda는 데이터 분석, 머신러닝, 과학 컴퓨팅을 위한 Python 배포판입니다. 다양한 데이터 과학 관련 라이브러리가 포함되어 있어, 데이터 분석을 쉽게 시작할 수 있게 해줍니다."},"title":"Python 소개 및 기본 개념"},"/programing/python/python/python01/":{"data":{"":"","1-python-설치-windows-macos-linux#1️⃣ \u003cstrong\u003ePython 설치 (Windows, macOS, Linux)\u003c/strong\u003e":"","2-가상환경-설정-virtualenv-venv-conda#2️⃣ \u003cstrong\u003e가상환경 설정 (virtualenv, venv, conda)\u003c/strong\u003e":"","3-ide-및-편집기-설정-vscode-pycharm-jupyter-notebook#3️⃣ \u003cstrong\u003eIDE 및 편집기 설정 (VSCode, PyCharm, Jupyter Notebook)\u003c/strong\u003e":"","4-패키지-관리자-사용-pip-conda#4️⃣ \u003cstrong\u003e패키지 관리자 사용 (pip, conda)\u003c/strong\u003e":"","5-python-인터프리터-및-버전-관리-python3-pyenv#5️⃣ \u003cstrong\u003ePython 인터프리터 및 버전 관리 (python3, pyenv)\u003c/strong\u003e":" 1️⃣ Python 설치 (Windows, macOS, Linux) Python은 다양한 운영 체제에서 설치할 수 있습니다. 각 운영 체제별로 설치 방법을 소개하겠습니다.\nWindows에서 Python 설치 Python 공식 웹사이트에 접속하여 최신 버전의 Python 설치 파일을 다운로드합니다. 설치 프로그램을 실행하고, “Add Python to PATH” 옵션을 체크한 후 “Install Now\"를 클릭합니다. 설치가 완료되면, 명령 프롬프트(또는 PowerShell)에서 python --version을 입력하여 Python이 정상적으로 설치되었는지 확인합니다. macOS에서 Python 설치 macOS에서는 기본적으로 Python 2.x가 설치되어 있지만, 최신 버전의 Python 3.x를 설치하는 것이 좋습니다.\nHomebrew를 사용하여 Python을 설치합니다. 먼저 Homebrew를 설치한 후 아래 명령어를 입력합니다: brew install python 설치가 완료되면, 터미널에서 python3 --version을 입력하여 설치된 Python 버전을 확인할 수 있습니다. Linux에서 Python 설치 Linux에서는 Python이 기본적으로 설치되어 있지만, 최신 버전을 설치하려면 패키지 관리자를 사용하여 설치할 수 있습니다.\n예를 들어, Ubuntu에서는 다음 명령어로 Python을 설치합니다:\nsudo apt update sudo apt install python3 설치 후 python3 --version 명령어로 설치된 버전을 확인할 수 있습니다.\n2️⃣ 가상환경 설정 (virtualenv, venv, conda) Python에서 가상환경은 프로젝트별로 독립적인 패키지 환경을 제공하는 도구입니다. 가상환경을 사용하면 각 프로젝트에 필요한 패키지만 설치하고, 다른 프로젝트와 충돌하지 않게 할 수 있습니다.\nvirtualenv 사용하기 virtualenv는 Python 2와 3에서 모두 사용할 수 있는 가상환경 관리 도구입니다. virtualenv를 설치하려면 다음 명령어를 실행합니다: pip install virtualenv 새로운 가상환경을 만들려면, 원하는 디렉터리로 이동 후 아래 명령어를 실행합니다: virtualenv myenv 가상환경을 활성화하려면, 아래 명령어를 사용합니다: Windows: myenv\\Scripts\\activate macOS/Linux: source myenv/bin/activate venv 사용하기 Python 3.3부터는 내장 모듈인 venv를 사용하여 가상환경을 생성할 수 있습니다. venv는 기본적으로 Python 3.x 이상에 포함되어 있으므로 별도로 설치할 필요가 없습니다.\npython3 -m venv myenv 가상환경을 활성화하는 방법은 virtualenv와 동일합니다.\nconda 사용하기 Anaconda 배포판을 사용하는 경우, conda 명령어로 가상환경을 관리할 수 있습니다. 먼저 Anaconda를 설치한 후, 새로운 가상환경을 만들려면 다음 명령어를 실행합니다:\nconda create --name myenv 가상환경을 활성화하려면:\nconda activate myenv 3️⃣ IDE 및 편집기 설정 (VSCode, PyCharm, Jupyter Notebook) Python 코드를 작성하는 데 사용할 수 있는 다양한 IDE와 편집기가 있습니다. 각 도구에 대해 간단히 소개합니다.\nVSCode 설정하기 VSCode를 다운로드하여 설치합니다. Python 확장 프로그램을 설치합니다. VSCode를 열고, Extensions 뷰에서 “Python\"을 검색한 후 설치합니다. Python 인터프리터를 선택합니다. Ctrl + Shift + P를 눌러 “Python: Select Interpreter\"를 검색하고, 설치된 Python 버전을 선택합니다. PyCharm 설정하기 PyCharm을 다운로드하여 설치합니다. PyCharm을 실행한 후, 새로운 프로젝트를 만들거나 기존 프로젝트를 엽니다. 프로젝트에 사용할 Python 인터프리터를 설정합니다. “File” -\u003e “Settings” -\u003e “Project: ” -\u003e “Python Interpreter\"에서 인터프리터를 선택합니다. Jupyter Notebook 설정하기 Jupyter Notebook은 데이터 분석 및 시각화 작업에 매우 유용한 도구입니다. Jupyter Notebook을 설치하려면, pip를 사용하여 설치합니다:\npip install notebook 설치 후, 아래 명령어로 Jupyter Notebook을 실행합니다:\njupyter notebook 브라우저에서 노트북 인터페이스가 열리며, Python 코드를 실행하고 결과를 시각적으로 확인할 수 있습니다.\n4️⃣ 패키지 관리자 사용 (pip, conda) pip 사용하기 pip는 Python의 기본 패키지 관리자입니다. Python 패키지를 설치하려면 pip를 사용합니다:\npip install \u003c패키지명\u003e 예를 들어, requests 라이브러리를 설치하려면:\npip install requests conda 사용하기 Anaconda를 사용하는 경우, conda 명령어로 패키지를 관리할 수 있습니다. pip와 비슷하게, 패키지를 설치하려면 다음 명령어를 사용합니다:\nconda install \u003c패키지명\u003e 예를 들어, numpy를 설치하려면:\nconda install numpy 5️⃣ Python 인터프리터 및 버전 관리 (python3, pyenv) python3 사용하기 Python 3을 사용하려면 python3 명령어를 사용합니다. 예를 들어, Python 3으로 스크립트를 실행하려면 다음과 같이 입력합니다:\npython3 script.py pyenv 사용하기 pyenv는 여러 버전의 Python을 쉽게 관리할 수 있는 도구입니다. pyenv를 사용하면 시스템에 여러 Python 버전을 설치하고, 프로젝트별로 사용할 Python 버전을 지정할 수 있습니다.\npyenv를 설치하려면, 아래 명령어를 사용합니다: curl https://pyenv.run | bash 원하는 Python 버전을 설치하려면: pyenv install 3.9.5 설치된 버전 중 원하는 버전을 사용하려면: pyenv global 3.9.5 "},"title":"Python 설치 및 환경 설정"},"/programing/python/python/python02/":{"data":{"":"","1-변수-선언-및-데이터-타입-int-float-string-bool-list-tuple-등#1️⃣ \u003cstrong\u003e변수 선언 및 데이터 타입 (int, float, string, bool, list, tuple 등)\u003c/strong\u003e":"","2-문자열-처리-문자열-포매팅-문자열-메서드#2️⃣ \u003cstrong\u003e문자열 처리 (문자열 포매팅, 문자열 메서드)\u003c/strong\u003e":"","3-연산자-산술-비교-논리-연산자#3️⃣ \u003cstrong\u003e연산자 (산술, 비교, 논리 연산자)\u003c/strong\u003e":"","4-조건문-if-else-elif#4️⃣ \u003cstrong\u003e조건문 (if, else, elif)\u003c/strong\u003e":"","5-반복문-for-while-break-continue#5️⃣ \u003cstrong\u003e반복문 (for, while, break, continue)\u003c/strong\u003e":"","6-함수-정의-및-호출-매개변수-반환값#6️⃣ \u003cstrong\u003e함수 정의 및 호출 (매개변수, 반환값)\u003c/strong\u003e":"","7-기본-입력-및-출력-input-print#7️⃣ \u003cstrong\u003e기본 입력 및 출력 (input, print)\u003c/strong\u003e":" 1️⃣ 변수 선언 및 데이터 타입 (int, float, string, bool, list, tuple 등) 변수 선언 Python에서 변수를 선언할 때는 자료형을 명시하지 않고 변수명에 값을 할당하는 방식으로 진행됩니다. Python은 동적 타이핑 언어이기 때문에 변수의 타입을 미리 정의할 필요가 없습니다.\nx = 5 # 정수 y = 3.14 # 실수 name = \"John\" # 문자열 is_active = True # 불리언 값 주요 데이터 타입 Python에서 자주 사용하는 데이터 타입을 소개합니다:\n정수 (int): 5, -3, 100 실수 (float): 3.14, -2.71, 0.99 문자열 (string): \"Hello\", 'Python' 불리언 (bool): True, False 리스트 (list): [1, 2, 3], [\"apple\", \"banana\", \"cherry\"] 튜플 (tuple): (1, 2, 3), (\"a\", \"b\", \"c\") x = 10 # int y = 3.14 # float name = \"Alice\" # string is_valid = False # bool fruits = [\"apple\", \"banana\", \"cherry\"] # list coordinates = (1, 2, 3) # tuple 2️⃣ 문자열 처리 (문자열 포매팅, 문자열 메서드) 문자열 포매팅 Python에서는 문자열을 다룰 때 여러 가지 방법으로 값을 삽입할 수 있습니다.\nf-string 포매팅 (Python 3.6 이상) name = \"Alice\" age = 30 greeting = f\"Hello, my name is {name} and I am {age} years old.\" print(greeting) str.format() 메서드 greeting = \"Hello, my name is {} and I am {} years old.\".format(name, age) print(greeting) % 포매팅 (구식 방법) greeting = \"Hello, my name is %s and I am %d years old.\" % (name, age) print(greeting) 문자열 메서드 Python의 문자열 객체에는 다양한 메서드가 제공됩니다. 예를 들어, 문자열의 길이를 구하거나, 대소문자를 바꿀 수 있습니다.\nmessage = \" Hello, Python! \" # 문자열 길이 print(len(message)) # 출력: 18 # 문자열 양옆 공백 제거 cleaned_message = message.strip() print(cleaned_message) # 출력: \"Hello, Python!\" # 문자열을 소문자로 변환 lowercase_message = message.lower() print(lowercase_message) # 출력: \" hello, python! \" # 문자열을 대문자로 변환 uppercase_message = message.upper() print(uppercase_message) # 출력: \" HELLO, PYTHON! \" 3️⃣ 연산자 (산술, 비교, 논리 연산자) 산술 연산자 Python에서는 기본적인 산술 연산을 지원합니다.\na = 10 b = 5 print(a + b) # 더하기 print(a - b) # 빼기 print(a * b) # 곱하기 print(a / b) # 나누기 print(a % b) # 나머지 print(a // b) # 몫 print(a ** b) # 거듭제곱 비교 연산자 비교 연산자는 두 값을 비교하는 데 사용됩니다.\nx = 10 y = 20 print(x == y) # 같음 print(x != y) # 같지 않음 print(x \u003e y) # 크기 비교 print(x \u003c y) # 크기 비교 print(x \u003e= y) # 크거나 같음 print(x \u003c= y) # 작거나 같음 논리 연산자 논리 연산자는 두 개 이상의 조건을 결합하는 데 사용됩니다.\nx = 10 y = 5 print(x \u003e 5 and y \u003c 10) # 둘 다 True일 때 True print(x \u003e 5 or y \u003e 10) # 하나만 True일 때 True print(not(x \u003e 5)) # False일 때 True 4️⃣ 조건문 (if, else, elif) Python에서는 조건문을 사용하여 코드의 흐름을 제어할 수 있습니다.\nage = 18 if age \u003e= 18: print(\"성인입니다.\") else: print(\"미성년자입니다.\") elif 사용하기 if와 else 사이에 여러 조건을 추가할 수 있는 elif를 사용할 수 있습니다.\nscore = 85 if score \u003e= 90: print(\"A\") elif score \u003e= 80: print(\"B\") elif score \u003e= 70: print(\"C\") else: print(\"F\") 5️⃣ 반복문 (for, while, break, continue) for 반복문 for 반복문은 순차적으로 데이터를 반복하는 데 사용됩니다. 주로 리스트나 튜플과 함께 사용됩니다.\nfruits = [\"apple\", \"banana\", \"cherry\"] for fruit in fruits: print(fruit) while 반복문 while 반복문은 조건이 True일 동안 계속 반복됩니다.\ncount = 0 while count \u003c 5: print(count) count += 1 break와 continue break: 반복문을 즉시 종료합니다. continue: 현재 반복을 건너뛰고 다음 반복으로 넘어갑니다. for i in range(10): if i == 5: break # i가 5일 때 반복문을 종료 print(i) for i in range(10): if i == 5: continue # i가 5일 때만 건너뛰고 계속 반복 print(i) 6️⃣ 함수 정의 및 호출 (매개변수, 반환값) 함수 정의 Python에서는 def 키워드를 사용하여 함수를 정의할 수 있습니다.\ndef greet(name): print(f\"Hello, {name}!\") greet(\"Alice\") # 함수 호출 반환값이 있는 함수 함수는 값을 반환할 수 있습니다. return 키워드를 사용하여 값을 반환합니다.\ndef add(x, y): return x + y result = add(3, 5) print(result) # 출력: 8 7️⃣ 기본 입력 및 출력 (input, print) print() 함수 print() 함수는 콘솔에 출력을 할 때 사용됩니다.\nname = \"Alice\" print(\"Hello,\", name) # 출력: Hello, Alice input() 함수 input() 함수는 사용자로부터 입력을 받을 때 사용됩니다.\nname = input(\"Enter your name: \") print(f\"Hello, {name}!\") "},"title":"Python 기본 문법"},"/programing/python/python/python03/":{"data":{"1-리스트list와-그-활용법#1️⃣ \u003cstrong\u003e리스트(List)와 그 활용법\u003c/strong\u003e":"","2-튜플tuple과-그-특성#2️⃣ \u003cstrong\u003e튜플(Tuple)과 그 특성\u003c/strong\u003e":"","3-집합set과-그-활용법#3️⃣ \u003cstrong\u003e집합(Set)과 그 활용법\u003c/strong\u003e":"","4-딕셔너리dictionary-및-키-값-쌍#4️⃣ \u003cstrong\u003e딕셔너리(Dictionary) 및 키-값 쌍\u003c/strong\u003e":"","5-list-comprehension과-dictionary-comprehension#5️⃣ \u003cstrong\u003eList Comprehension과 Dictionary Comprehension\u003c/strong\u003e":"","6-데이터-구조의-시간-복잡도-리스트-집합-딕셔너리#6️⃣ \u003cstrong\u003e데이터 구조의 시간 복잡도 (리스트, 집합, 딕셔너리)\u003c/strong\u003e":"","7-순차-자료형의-고급-기능-슬라이싱-확장-병합-등#7️⃣ \u003cstrong\u003e순차 자료형의 고급 기능 (슬라이싱, 확장, 병합 등)\u003c/strong\u003e":"Python 데이터 구조1️⃣ 리스트(List)와 그 활용법 리스트 정의 리스트는 여러 개의 값을 저장할 수 있는 순차 자료형으로, 변경이 가능한(mutable) 특징을 가집니다. 대괄호 []를 사용하여 리스트를 정의합니다.\nfruits = [\"apple\", \"banana\", \"cherry\"] 리스트 활용법 리스트는 다양한 기능을 제공하며, 인덱싱, 슬라이싱, 추가 및 삭제 등을 통해 데이터를 관리할 수 있습니다.\n리스트 접근 리스트의 각 요소는 0부터 시작하는 인덱스를 통해 접근할 수 있습니다.\nprint(fruits[0]) # apple print(fruits[1]) # banana 리스트 수정 리스트는 값의 변경이 가능하며, append()로 요소를 추가하거나 remove()로 제거할 수 있습니다.\nfruits.append(\"orange\") # 리스트 끝에 추가 print(fruits) # ['apple', 'banana', 'cherry', 'orange'] fruits.remove(\"banana\") # 특정 요소 제거 print(fruits) # ['apple', 'cherry', 'orange'] 리스트 슬라이싱 리스트에서 특정 부분을 잘라낼 때 슬라이싱을 사용합니다.\nfruits = [\"apple\", \"banana\", \"cherry\", \"orange\"] print(fruits[1:3]) # ['banana', 'cherry'] 2️⃣ 튜플(Tuple)과 그 특성 튜플 정의 튜플은 리스트와 비슷하지만, 수정이 불가능(immutable)한 자료형입니다. 소괄호 ()를 사용하여 정의합니다.\ncoordinates = (10, 20) 튜플 활용법 튜플은 변경할 수 없기 때문에, 값이 변하지 않음을 보장해야 할 때 유용합니다.\n# 튜플의 요소 접근 print(coordinates[0]) # 10 print(coordinates[1]) # 20 튜플은 주로 고정된 값을 저장할 때 사용되며, 리스트와 달리 추가, 삭제, 변경이 불가능합니다.\n3️⃣ 집합(Set)과 그 활용법 집합 정의 집합은 중복되지 않는 값을 저장하는 자료형으로, 순서가 없습니다. 중괄호 {}를 사용하여 정의합니다.\nfruits_set = {\"apple\", \"banana\", \"cherry\"} 집합 활용법 집합은 중복된 값을 자동으로 제거하며, 빠른 검색 속도를 제공합니다.\nfruits_set.add(\"orange\") # 집합에 추가 print(fruits_set) # {'banana', 'cherry', 'orange', 'apple'} fruits_set.remove(\"banana\") # 집합에서 특정 요소 제거 print(fruits_set) # {'cherry', 'orange', 'apple'} 집합의 주요 특징:\n중복되지 않는 값들만 저장 순서가 없어서 인덱싱이 불가능 4️⃣ 딕셔너리(Dictionary) 및 키-값 쌍 딕셔너리 정의 딕셔너리는 키-값 쌍을 저장하는 자료형으로, 중괄호 {}를 사용하여 정의합니다. 각 키는 고유해야 하며, 값을 통해 접근합니다.\nperson = {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"} 딕셔너리 활용법 딕셔너리에서는 키를 사용해 값을 빠르게 조회하거나 수정할 수 있습니다.\n# 키로 값 조회 print(person[\"name\"]) # John # 새로운 키-값 추가 person[\"email\"] = \"john@example.com\" print(person) # {'name': 'John', 'age': 30, 'city': 'New York', 'email': 'john@example.com'} # 키 삭제 del person[\"age\"] print(person) # {'name': 'John', 'city': 'New York', 'email': 'john@example.com'} 5️⃣ List Comprehension과 Dictionary Comprehension List Comprehension List Comprehension은 리스트를 간결하게 생성할 수 있는 문법입니다.\nsquares = [x**2 for x in range(5)] print(squares) # [0, 1, 4, 9, 16] Dictionary Comprehension Dictionary Comprehension을 사용하면 간단하게 딕셔너리를 생성할 수 있습니다.\nsquared_dict = {x: x**2 for x in range(5)} print(squared_dict) # {0: 0, 1: 1, 2: 4, 3: 9, 4: 16} 6️⃣ 데이터 구조의 시간 복잡도 (리스트, 집합, 딕셔너리) 리스트 접근: O(1) 삽입/삭제 (끝): O(1) 삽입/삭제 (중간): O(n) 집합 접근: O(1) 삽입/삭제: O(1) 딕셔너리 접근: O(1) 삽입/삭제: O(1) 따라서 집합과 딕셔너리는 리스트보다 빠른 검색 및 삽입/삭제를 지원합니다.\n7️⃣ 순차 자료형의 고급 기능 (슬라이싱, 확장, 병합 등) 슬라이싱 리스트나 튜플에서는 슬라이싱을 사용하여 데이터를 잘라낼 수 있습니다.\nfruits = [\"apple\", \"banana\", \"cherry\", \"orange\"] print(fruits[1:3]) # ['banana', 'cherry'] 리스트 확장 리스트에서는 다른 리스트를 더할 수 있습니다.\nfruits = [\"apple\", \"banana\"] more_fruits = [\"cherry\", \"orange\"] fruits.extend(more_fruits) print(fruits) # ['apple', 'banana', 'cherry', 'orange'] 리스트 병합 리스트를 합치려면 + 연산자를 사용합니다.\nfruits = [\"apple\", \"banana\"] vegetables = [\"carrot\", \"tomato\"] merged_list = fruits + vegetables print(merged_list) # ['apple', 'banana', 'carrot', 'tomato'] ","python-데이터-구조#\u003cstrong\u003ePython 데이터 구조\u003c/strong\u003e":""},"title":"Python 데이터 구조"},"/programing/python/python/python04/":{"data":{"":"","1-함수-정의와-반환값#1️⃣ \u003cstrong\u003e함수 정의와 반환값\u003c/strong\u003e":"","2-기본-인자값-및-키워드-인자#2️⃣ \u003cstrong\u003e기본 인자값 및 키워드 인자\u003c/strong\u003e":"","3-가변-인자-variadic-functions#3️⃣ \u003cstrong\u003e가변 인자 (Variadic Functions)\u003c/strong\u003e":"","4-람다-함수-lambda-functions#4️⃣ \u003cstrong\u003e람다 함수 (Lambda Functions)\u003c/strong\u003e":"","5-함수-안에서-함수-nested-functions#5️⃣ \u003cstrong\u003e함수 안에서 함수 (Nested Functions)\u003c/strong\u003e":"","6-고급-함수-개념-클로저-제너레이터-데커레이터#6️⃣ \u003cstrong\u003e고급 함수 개념 (클로저, 제너레이터, 데커레이터)\u003c/strong\u003e":"","7-고차-함수-함수-인자로-함수-전달#7️⃣ \u003cstrong\u003e고차 함수 (함수 인자로 함수 전달)\u003c/strong\u003e":"","8-재귀-함수-recursion#8️⃣ \u003cstrong\u003e재귀 함수 (Recursion)\u003c/strong\u003e":" 1️⃣ 함수 정의와 반환값 함수 정의 함수는 특정 작업을 수행하는 코드 블록으로, def 키워드를 사용하여 정의합니다. 함수를 정의할 때는 함수의 이름과 매개변수(입력값)를 지정하고, 함수 내에서 특정 작업을 처리한 후 return을 사용해 값을 반환할 수 있습니다.\n# 함수 정의 def greet(name): return f\"Hello, {name}!\" # 함수 호출 print(greet(\"Alice\")) # Hello, Alice! 반환값 함수는 return 키워드를 사용하여 결과를 반환합니다. 반환값은 함수 호출 시 사용됩니다.\ndef add(a, b): return a + b result = add(3, 4) print(result) # 7 2️⃣ 기본 인자값 및 키워드 인자 기본 인자값 (Default Arguments) 함수를 정의할 때 매개변수에 기본값을 지정할 수 있습니다. 매개변수가 전달되지 않으면 기본값이 사용됩니다.\ndef greet(name=\"Guest\"): return f\"Hello, {name}!\" print(greet()) # Hello, Guest! print(greet(\"Alice\")) # Hello, Alice! 키워드 인자 (Keyword Arguments) 함수 호출 시 인자 이름을 명시적으로 지정하는 방법입니다. 이를 사용하면 인자의 순서에 관계없이 값을 전달할 수 있습니다.\ndef greet(name, age): return f\"Hello, {name}. You are {age} years old.\" # 키워드 인자 사용 print(greet(name=\"Alice\", age=30)) # Hello, Alice. You are 30 years old. 3️⃣ 가변 인자 (Variadic Functions) 가변 인자 정의 가변 인자는 함수가 받을 수 있는 인자의 수가 정해져 있지 않은 경우에 사용합니다. *args를 사용하여 위치 인자들을 튜플로 받을 수 있으며, **kwargs를 사용하여 키워드 인자들을 딕셔너리로 받을 수 있습니다.\n# *args를 사용하여 여러 개의 위치 인자 받기 def sum_numbers(*args): return sum(args) print(sum_numbers(1, 2, 3, 4)) # 10 # **kwargs를 사용하여 여러 개의 키워드 인자 받기 def greet_multiple(**kwargs): for name, age in kwargs.items(): print(f\"Hello, {name}. You are {age} years old.\") greet_multiple(Alice=30, Bob=25) # Hello, Alice. You are 30 years old. # Hello, Bob. You are 25 years old. 4️⃣ 람다 함수 (Lambda Functions) 람다 함수 정의 람다 함수는 lambda 키워드를 사용하여 정의하며, 간단한 함수나 일회성 함수에 유용합니다. 람다 함수는 한 줄로 표현할 수 있으며, 주로 다른 함수에 인자로 전달됩니다.\n# 람다 함수 정의 multiply = lambda x, y: x * y print(multiply(2, 3)) # 6 람다 함수는 복잡한 로직을 간결하게 표현할 때 사용됩니다.\n5️⃣ 함수 안에서 함수 (Nested Functions) 함수 안에 함수 정의 함수 내부에 또 다른 함수를 정의할 수 있으며, 이를 “중첩 함수” 또는 “내부 함수\"라고 합니다. 내부 함수는 외부 함수 내에서만 호출이 가능합니다.\ndef outer_function(): def inner_function(): return \"Hello from inner function!\" return inner_function() print(outer_function()) # Hello from inner function! 6️⃣ 고급 함수 개념 (클로저, 제너레이터, 데커레이터) 클로저 (Closure) 클로저는 함수가 자신이 정의된 환경을 기억하는 특성을 의미합니다. 내부 함수가 외부 함수의 변수에 접근할 수 있는 경우, 이 변수는 클로저로 남게 됩니다.\ndef outer_function(x): def inner_function(y): return x + y return inner_function closure = outer_function(10) print(closure(5)) # 15 제너레이터 (Generator) 제너레이터는 yield 키워드를 사용하여 값을 한 번에 하나씩 반환하는 함수입니다. 제너레이터는 큰 데이터를 처리할 때 메모리 효율성을 높여줍니다.\ndef count_up_to(limit): count = 1 while count \u003c= limit: yield count count += 1 for number in count_up_to(5): print(number) # 1 # 2 # 3 # 4 # 5 데커레이터 (Decorator) 데커레이터는 다른 함수를 수정하거나 확장하는 함수입니다. 주로 함수에 추가적인 기능을 덧붙일 때 사용됩니다.\n# 데커레이터 정의 def decorator_function(original_function): def wrapper_function(): print(\"Wrapper executed this before {}\".format(original_function.__name__)) return original_function() return wrapper_function # 함수에 데커레이터 적용 @decorator_function def display(): return \"Display function executed\" print(display()) # Wrapper executed this before display # Display function executed 7️⃣ 고차 함수 (함수 인자로 함수 전달) 고차 함수 정의 고차 함수는 다른 함수를 인자로 받거나, 함수를 반환하는 함수입니다. 이를 활용하여 함수를 동적으로 처리할 수 있습니다.\ndef apply_function(f, x): return f(x) # 함수 인자로 전달 result = apply_function(lambda x: x**2, 5) print(result) # 25 8️⃣ 재귀 함수 (Recursion) 재귀 함수 정의 재귀 함수는 자기 자신을 호출하는 함수입니다. 재귀 함수는 문제를 작은 부분으로 나누어 해결할 때 유용합니다. 재귀를 사용할 때는 종료 조건을 반드시 정의해야 합니다.\n# 재귀 함수 정의 def factorial(n): if n == 1: return 1 else: return n * factorial(n-1) print(factorial(5)) # 120 "},"title":"함수와 고급 함수 기능"},"/programing/python/python/python05/":{"data":{"":"","1-객체와-클래스class#1️⃣ \u003cstrong\u003e객체와 클래스(Class)\u003c/strong\u003e":"","2-클래스의-속성-및-메서드-정의#2️⃣ \u003cstrong\u003e클래스의 속성 및 메서드 정의\u003c/strong\u003e":"","3-생성자init와-소멸자del#3️⃣ \u003cstrong\u003e생성자(init)와 소멸자(del)\u003c/strong\u003e":"","4-상속inheritance과-다형성polymorphism#4️⃣ \u003cstrong\u003e상속(Inheritance)과 다형성(Polymorphism)\u003c/strong\u003e":"","5-추상-클래스abstract-class와-인터페이스#5️⃣ \u003cstrong\u003e추상 클래스(Abstract Class)와 인터페이스\u003c/strong\u003e":"","6-클래스-상속과-메서드-오버라이딩#6️⃣ \u003cstrong\u003e클래스 상속과 메서드 오버라이딩\u003c/strong\u003e":"","7-클래스-변수와-인스턴스-변수#7️⃣ \u003cstrong\u003e클래스 변수와 인스턴스 변수\u003c/strong\u003e":"","8-예외-처리-try-except-finally#8️⃣ \u003cstrong\u003e예외 처리 (try, except, finally)\u003c/strong\u003e":"","9-python에서의-self와-cls의-차이점#9️⃣ \u003cstrong\u003ePython에서의 self와 cls의 차이점\u003c/strong\u003e":" 1️⃣ 객체와 클래스(Class) 객체(Object) 객체는 속성과 메서드를 가진 실체로, 클래스에서 정의된 구조를 실제로 메모리에 할당한 것입니다. 예를 들어, 자동차라는 클래스를 정의하면, 특정 자동차 객체는 클래스에서 정의된 속성(예: 색상, 모델)과 메서드(예: 운전, 정지)를 가질 수 있습니다.\n클래스(Class) 클래스는 객체의 설계도입니다. 클래스는 객체가 가질 속성과 메서드를 정의합니다. 클래스를 정의할 때 class 키워드를 사용합니다.\n# 클래스 정의 class Car: def __init__(self, color, model): self.color = color # 속성 self.model = model # 속성 def drive(self): return f\"The {self.color} {self.model} is driving!\" # 객체 생성 my_car = Car(\"red\", \"Toyota\") print(my_car.drive()) # The red Toyota is driving! 2️⃣ 클래스의 속성 및 메서드 정의 속성(Attributes) 속성은 클래스에서 정의된 데이터 멤버입니다. 클래스 안에서 self를 사용하여 객체의 속성에 접근하고 값을 설정할 수 있습니다.\n메서드(Methods) 메서드는 클래스 내에서 정의된 함수로, 객체의 동작을 정의합니다. 메서드는 항상 첫 번째 인자로 self를 받아 객체 자신을 참조합니다.\nclass Dog: def __init__(self, name, breed): self.name = name # 속성 self.breed = breed # 속성 def bark(self): # 메서드 return f\"{self.name} is barking!\" my_dog = Dog(\"Buddy\", \"Golden Retriever\") print(my_dog.bark()) # Buddy is barking! 3️⃣ 생성자(init)와 소멸자(del) 생성자 (Constructor, __init__) 생성자는 객체가 생성될 때 호출되는 특별한 메서드로, 객체를 초기화합니다. __init__ 메서드를 사용하여 객체의 속성 값을 초기화할 수 있습니다.\nclass Person: def __init__(self, name, age): self.name = name self.age = age person = Person(\"Alice\", 30) print(person.name) # Alice 소멸자 (Destructor, __del__) 소멸자는 객체가 메모리에서 삭제될 때 호출되는 특별한 메서드입니다. __del__ 메서드를 사용하여 객체가 소멸되기 전에 필요한 작업을 할 수 있습니다.\nclass Car: def __del__(self): print(\"Car object is being deleted.\") my_car = Car() del my_car # Car object is being deleted. 4️⃣ 상속(Inheritance)과 다형성(Polymorphism) 상속 (Inheritance) 상속은 기존 클래스의 속성과 메서드를 새로운 클래스에서 재사용하는 방식입니다. 상속을 통해 코드 재사용성과 유지보수성을 높일 수 있습니다. 자식 클래스는 부모 클래스의 모든 속성 및 메서드를 상속받습니다.\nclass Animal: def speak(self): return \"Animal speaking\" class Dog(Animal): # Animal 클래스를 상속 def bark(self): return \"Woof!\" my_dog = Dog() print(my_dog.speak()) # Animal speaking print(my_dog.bark()) # Woof! 다형성 (Polymorphism) 다형성은 여러 클래스에서 동일한 메서드 이름을 사용하지만, 각 클래스에서 다르게 동작하게 만드는 특성입니다. 이는 상속을 통해 구현할 수 있습니다.\nclass Cat(Animal): def speak(self): return \"Meow!\" def make_sound(animal): print(animal.speak()) make_sound(Dog()) # Animal speaking make_sound(Cat()) # Meow! 5️⃣ 추상 클래스(Abstract Class)와 인터페이스 추상 클래스 (Abstract Class) 추상 클래스는 인스턴스를 생성할 수 없는 클래스입니다. abc 모듈을 사용하여 추상 클래스를 정의할 수 있으며, 추상 메서드를 포함할 수 있습니다. 추상 메서드는 자식 클래스에서 반드시 구현해야 하는 메서드입니다.\nfrom abc import ABC, abstractmethod class Animal(ABC): @abstractmethod def speak(self): pass class Dog(Animal): def speak(self): return \"Woof!\" my_dog = Dog() print(my_dog.speak()) # Woof! 인터페이스 (Interface) Python은 Java와 같은 전통적인 인터페이스 개념을 직접 지원하지 않지만, 추상 클래스와 다형성을 활용하여 인터페이스와 유사한 기능을 구현할 수 있습니다.\n6️⃣ 클래스 상속과 메서드 오버라이딩 메서드 오버라이딩 (Method Overriding) 메서드 오버라이딩은 자식 클래스에서 부모 클래스의 메서드를 재정의하는 것입니다. 이를 통해 자식 클래스에서 부모 클래스의 동작을 변경할 수 있습니다.\nclass Animal: def speak(self): return \"Animal speaking\" class Dog(Animal): def speak(self): return \"Woof!\" my_dog = Dog() print(my_dog.speak()) # Woof! 7️⃣ 클래스 변수와 인스턴스 변수 클래스 변수(Class Variable) 클래스 변수는 클래스에 속하는 변수로, 모든 객체가 공유합니다. 클래스 변수는 클래스 정의 내에서 직접 정의됩니다.\nclass Dog: species = \"Canine\" # 클래스 변수 dog1 = Dog() dog2 = Dog() print(dog1.species) # Canine print(dog2.species) # Canine 인스턴스 변수(Instance Variable) 인스턴스 변수는 객체마다 별도로 존재하는 변수입니다. 인스턴스 변수는 생성자 내에서 self를 사용하여 정의됩니다.\nclass Dog: def __init__(self, name): self.name = name # 인스턴스 변수 dog1 = Dog(\"Buddy\") dog2 = Dog(\"Max\") print(dog1.name) # Buddy print(dog2.name) # Max 8️⃣ 예외 처리 (try, except, finally) 예외 처리 예외 처리는 프로그램 실행 중 발생할 수 있는 오류를 처리하기 위해 사용됩니다. try, except, finally 블록을 사용하여 예외를 처리할 수 있습니다.\ntry: x = 10 / 0 except ZeroDivisionError: print(\"Cannot divide by zero!\") finally: print(\"This block is always executed.\") 출력:\nCannot divide by zero! This block is always executed. 9️⃣ Python에서의 self와 cls의 차이점 self와 cls의 차이점 self: 객체 인스턴스를 참조하는 변수로, 인스턴스 메서드에서 사용됩니다. cls: 클래스 자체를 참조하는 변수로, 클래스 메서드에서 사용됩니다. cls는 클래스에서 호출되며, 인스턴스를 생성하지 않아도 접근할 수 있습니다. class MyClass: class_var = \"Class Variable\" def instance_method(self): print(self.class_var) # self를 사용하여 클래스 변수에 접근 @classmethod def class_method(cls): print(cls.class_var) # cls를 사용하여 클래스 변수에 접근 obj = MyClass() obj.instance_method() # Class Variable obj.class_method() # Class Variable "},"title":"Python의 객체지향 프로그래밍(OOP)"},"/programing/python/python/python06/":{"data":{"":"","1-예외-처리-및-사용자-정의-예외-클래스#1️⃣ \u003cstrong\u003e예외 처리 및 사용자 정의 예외 클래스\u003c/strong\u003e":"","2-python-모듈module과-패키지package#2️⃣ \u003cstrong\u003ePython 모듈(Module)과 패키지(Package)\u003c/strong\u003e":"","3-__init__#3️⃣ \u003cstrong\u003e\u003ccode\u003e__init__\u003c/code\u003e, \u003ccode\u003e__call__\u003c/code\u003e, \u003ccode\u003e__str__\u003c/code\u003e, \u003ccode\u003e__repr__\u003c/code\u003e 등의 특수 메서드\u003c/strong\u003e":"","4-프로퍼티와-접근자gettersetter#4️⃣ \u003cstrong\u003e프로퍼티와 접근자(getter/setter)\u003c/strong\u003e":"","5-파이썬의#5️⃣ \u003cstrong\u003e파이썬의 \u003ccode\u003ewith\u003c/code\u003e문과 \u003ccode\u003econtextlib\u003c/code\u003e\u003c/strong\u003e":"","6-파이썬의#6️⃣ \u003cstrong\u003e파이썬의 \u003ccode\u003eglobal\u003c/code\u003e과 \u003ccode\u003enonlocal\u003c/code\u003e 키워드\u003c/strong\u003e":"","7-메타클래스metaclasses-이해#7️⃣ \u003cstrong\u003e메타클래스(Metaclasses) 이해\u003c/strong\u003e":"","8-itertools-functools-등-유용한-표준-라이브러리-활용#8️⃣ \u003cstrong\u003eitertools, functools 등 유용한 표준 라이브러리 활용\u003c/strong\u003e":"1️⃣ 예외 처리 및 사용자 정의 예외 클래스 예외 처리 Python에서 예외 처리는 try, except, else, finally 블록을 사용하여 발생할 수 있는 오류를 관리합니다. 예외 처리는 프로그램 실행 중 예상치 못한 상황이 발생했을 때 프로그램이 중단되지 않고 계속 실행되도록 도와줍니다.\ntry: x = 1 / 0 # ZeroDivisionError 발생 except ZeroDivisionError as e: print(\"Cannot divide by zero:\", e) finally: print(\"This will always be executed.\") 사용자 정의 예외 클래스 Python에서는 Exception 클래스를 상속하여 사용자 정의 예외를 만들 수 있습니다. 이를 통해 프로그램 내에서 특정 오류를 더 구체적으로 처리할 수 있습니다.\nclass MyCustomError(Exception): pass def divide(a, b): if b == 0: raise MyCustomError(\"Division by zero is not allowed!\") return a / b try: result = divide(10, 0) except MyCustomError as e: print(e) # Division by zero is not allowed! 2️⃣ Python 모듈(Module)과 패키지(Package) 모듈(Module) 모듈은 Python 코드가 들어있는 파일입니다. 함수, 클래스, 변수 등을 포함할 수 있으며, 다른 Python 파일에서 import를 통해 재사용할 수 있습니다.\n# my_module.py def greet(name): return f\"Hello, {name}!\" # main.py import my_module print(my_module.greet(\"Alice\")) # Hello, Alice! 패키지(Package) 패키지는 여러 모듈을 포함하는 디렉터리입니다. 패키지 내에는 __init__.py 파일이 있어야 패키지로 인식됩니다.\nmy_package/ __init__.py module1.py module2.py # main.py from my_package import module1 module1.function() # 모듈 내의 함수 사용 3️⃣ __init__, __call__, __str__, __repr__ 등의 특수 메서드 __init__ 메서드 __init__ 메서드는 객체가 생성될 때 호출되는 생성자입니다. 객체의 초기화를 담당합니다.\nclass Person: def __init__(self, name, age): self.name = name self.age = age p = Person(\"John\", 25) __call__ 메서드 __call__ 메서드는 객체를 함수처럼 호출할 수 있도록 합니다. 이를 통해 객체를 함수처럼 사용할 수 있습니다.\nclass Adder: def __init__(self, x): self.x = x def __call__(self, y): return self.x + y add_five = Adder(5) print(add_five(10)) # 15 __str__과 __repr__ 메서드 __str__: 객체를 사람이 읽을 수 있는 형태로 반환합니다. __repr__: 객체를 개발자가 읽을 수 있는 형태로 반환합니다. 디버깅 용도로 사용됩니다. class Person: def __init__(self, name, age): self.name = name self.age = age def __str__(self): return f\"Person({self.name}, {self.age})\" def __repr__(self): return f\"Person('{self.name}', {self.age})\" p = Person(\"Alice\", 30) print(str(p)) # Person(Alice, 30) print(repr(p)) # Person('Alice', 30) 4️⃣ 프로퍼티와 접근자(getter/setter) 프로퍼티 (Property) 프로퍼티는 클래스의 속성에 대한 getter, setter 메서드를 간단하게 정의하는 방법입니다. @property 데코레이터를 사용하여 속성에 접근할 때 추가적인 로직을 실행할 수 있습니다.\nclass Person: def __init__(self, name, age): self._name = name self._age = age @property def name(self): return self._name @name.setter def name(self, value): self._name = value p = Person(\"Alice\", 30) print(p.name) # Alice p.name = \"Bob\" print(p.name) # Bob 5️⃣ 파이썬의 with문과 contextlib with문 with문은 파일 열기, 데이터베이스 연결 등과 같이 자원을 자동으로 정리할 수 있게 도와줍니다. with문은 __enter__와 __exit__ 메서드를 정의하는 객체와 함께 사용됩니다.\nwith open('example.txt', 'w') as file: file.write(\"Hello, world!\") contextlib 모듈 contextlib는 with문에서 사용하는 컨텍스트 관리자를 쉽게 만들 수 있는 유틸리티를 제공합니다.\nfrom contextlib import contextmanager @contextmanager def my_context_manager(): print(\"Entering the context\") yield print(\"Exiting the context\") with my_context_manager(): print(\"Inside the context\") 출력:\nEntering the context Inside the context Exiting the context 6️⃣ 파이썬의 global과 nonlocal 키워드 global global 키워드는 함수 내에서 전역 변수를 사용하거나 수정할 때 사용됩니다.\nx = 10 def modify_global(): global x x = 20 modify_global() print(x) # 20 nonlocal nonlocal 키워드는 함수 내에서 상위 함수의 변수에 접근하거나 수정할 때 사용됩니다.\ndef outer(): x = 10 def inner(): nonlocal x x = 20 inner() print(x) outer() # 20 7️⃣ 메타클래스(Metaclasses) 이해 메타클래스는 클래스의 클래스로, 클래스가 생성되는 방식을 제어할 수 있습니다. 기본적으로 Python의 모든 클래스는 type 메타클래스를 사용하여 생성됩니다.\nclass MyMeta(type): def __new__(cls, name, bases, dct): print(f\"Creating class {name}\") return super().__new__(cls, name, bases, dct) class MyClass(metaclass=MyMeta): pass 출력:\nCreating class MyClass 8️⃣ itertools, functools 등 유용한 표준 라이브러리 활용 itertools 라이브러리 itertools는 반복 가능한 데이터를 다루는 다양한 기능을 제공합니다. 예를 들어, count, cycle, permutations 등이 있습니다.\nimport itertools for i in itertools.count(10, 2): if i \u003e 20: break print(i) functools 라이브러리 functools는 함수형 프로그래밍에서 유용한 함수들을 제공합니다. 예를 들어, reduce, partial 등이 있습니다.\nfrom functools import reduce numbers = [1, 2, 3, 4] result = reduce(lambda x, y: x + y, numbers) print(result) # 10 "},"title":"Python의 고급 기능"},"/programing/python/python/python07/":{"data":{"":"","1-파일-읽기-및-쓰기-텍스트-파일-이진-파일#1️⃣ \u003cstrong\u003e파일 읽기 및 쓰기 (텍스트 파일, 이진 파일)\u003c/strong\u003e":"","2-파일-및-디렉터리-관리-os-shutil#2️⃣ \u003cstrong\u003e파일 및 디렉터리 관리 (os, shutil)\u003c/strong\u003e":"","3-csv-파일-처리-csv-pandas#3️⃣ \u003cstrong\u003eCSV 파일 처리 (csv, pandas)\u003c/strong\u003e":"","4-json-파일-처리-json-모듈#4️⃣ \u003cstrong\u003eJSON 파일 처리 (json 모듈)\u003c/strong\u003e":"","5-xml과-html-처리-xmletreeelementtree-beautifulsoup#5️⃣ \u003cstrong\u003eXML과 HTML 처리 (xml.etree.ElementTree, BeautifulSoup)\u003c/strong\u003e":"","6-정규-표현식regex-re-모듈#6️⃣ \u003cstrong\u003e정규 표현식(Regex) (re 모듈)\u003c/strong\u003e":"","7-excel-파일-처리-openpyxl-xlrd#7️⃣ \u003cstrong\u003eExcel 파일 처리 (openpyxl, xlrd)\u003c/strong\u003e":" 1️⃣ 파일 읽기 및 쓰기 (텍스트 파일, 이진 파일) 텍스트 파일 읽기 및 쓰기 Python에서는 open() 함수를 사용하여 파일을 열고, 파일을 읽거나 쓸 수 있습니다. 텍스트 파일을 읽고 쓰는 기본적인 예제입니다.\n# 파일 쓰기 with open('example.txt', 'w') as file: file.write(\"Hello, Python!\\nThis is a test.\") # 파일 읽기 with open('example.txt', 'r') as file: content = file.read() print(content) 출력:\nHello, Python! This is a test. 이진 파일 읽기 및 쓰기 이진 파일을 다룰 때는 'rb'(읽기) 또는 'wb'(쓰기) 모드를 사용합니다.\n# 이진 파일 쓰기 with open('example.bin', 'wb') as file: data = b'\\x01\\x02\\x03\\x04' file.write(data) # 이진 파일 읽기 with open('example.bin', 'rb') as file: data = file.read() print(data) # b'\\x01\\x02\\x03\\x04' 2️⃣ 파일 및 디렉터리 관리 (os, shutil) os 모듈 os 모듈을 사용하면 파일과 디렉터리의 경로를 관리하고, 파일의 존재 여부를 확인할 수 있습니다.\nimport os # 파일 존재 여부 확인 if os.path.exists('example.txt'): print(\"File exists!\") else: print(\"File not found!\") # 디렉터리 생성 if not os.path.exists('my_directory'): os.makedirs('my_directory') shutil 모듈 shutil 모듈은 파일 복사, 이동, 삭제 등 다양한 고급 파일 작업을 제공합니다.\nimport shutil # 파일 복사 shutil.copy('example.txt', 'example_copy.txt') # 디렉터리 이동 shutil.move('my_directory', 'new_directory') # 파일 삭제 os.remove('example_copy.txt') 3️⃣ CSV 파일 처리 (csv, pandas) csv 모듈을 이용한 CSV 파일 처리 CSV 파일을 처리하는 가장 기본적인 방법은 csv 모듈을 사용하는 것입니다.\nimport csv # CSV 파일 쓰기 with open('example.csv', 'w', newline='') as file: writer = csv.writer(file) writer.writerow([\"Name\", \"Age\"]) writer.writerow([\"Alice\", 30]) writer.writerow([\"Bob\", 25]) # CSV 파일 읽기 with open('example.csv', 'r') as file: reader = csv.reader(file) for row in reader: print(row) 출력:\n['Name', 'Age'] ['Alice', '30'] ['Bob', '25'] pandas를 이용한 CSV 파일 처리 pandas는 CSV 파일을 효율적으로 처리할 수 있는 라이브러리입니다.\nimport pandas as pd # CSV 파일 읽기 df = pd.read_csv('example.csv') print(df) # CSV 파일 쓰기 df.to_csv('output.csv', index=False) 4️⃣ JSON 파일 처리 (json 모듈) JSON 파일 읽기 및 쓰기 Python의 json 모듈을 사용하여 JSON 데이터를 쉽게 읽고 쓸 수 있습니다.\nimport json # JSON 파일 쓰기 data = {\"name\": \"Alice\", \"age\": 30} with open('data.json', 'w') as file: json.dump(data, file) # JSON 파일 읽기 with open('data.json', 'r') as file: data = json.load(file) print(data) 출력:\n{'name': 'Alice', 'age': 30} 5️⃣ XML과 HTML 처리 (xml.etree.ElementTree, BeautifulSoup) xml.etree.ElementTree로 XML 처리 xml.etree.ElementTree 모듈을 사용하여 XML 파일을 읽고 쓸 수 있습니다.\nimport xml.etree.ElementTree as ET # XML 데이터 생성 root = ET.Element(\"root\") child1 = ET.SubElement(root, \"child1\") child1.text = \"This is child 1\" tree = ET.ElementTree(root) tree.write(\"example.xml\") # XML 파일 읽기 tree = ET.parse('example.xml') root = tree.getroot() for child in root: print(child.tag, child.text) BeautifulSoup으로 HTML 처리 BeautifulSoup은 HTML 파싱을 위한 강력한 도구입니다.\nfrom bs4 import BeautifulSoup html = \"\u003chtml\u003e\u003cbody\u003e\u003ch1\u003eWelcome to Python!\u003c/h1\u003e\u003c/body\u003e\u003c/html\u003e\" soup = BeautifulSoup(html, 'html.parser') print(soup.h1.text) # 출력: Welcome to Python! 6️⃣ 정규 표현식(Regex) (re 모듈) re 모듈을 이용한 정규 표현식 처리 정규 표현식은 텍스트에서 특정 패턴을 찾거나 대체하는 데 사용됩니다.\nimport re # 문자열에서 숫자 추출 text = \"The price is 100 dollars\" result = re.findall(r'\\d+', text) print(result) # ['100'] 정규 표현식 대체 # 숫자 대체 new_text = re.sub(r'\\d+', 'XXX', text) print(new_text) # The price is XXX dollars 7️⃣ Excel 파일 처리 (openpyxl, xlrd) openpyxl로 Excel 파일 처리 openpyxl을 사용하여 Excel 파일을 읽고 쓸 수 있습니다.\nfrom openpyxl import Workbook # Excel 파일 쓰기 wb = Workbook() ws = wb.active ws['A1'] = \"Hello\" ws['A2'] = \"World\" wb.save('example.xlsx') # Excel 파일 읽기 from openpyxl import load_workbook wb = load_workbook('example.xlsx') ws = wb.active print(ws['A1'].value) # Hello xlrd로 Excel 파일 처리 xlrd는 주로 Excel 파일을 읽는 데 사용됩니다.\nimport xlrd # Excel 파일 읽기 workbook = xlrd.open_workbook('example.xls') sheet = workbook.sheet_by_index(0) print(sheet.cell_value(0, 0)) # 첫 번째 셀의 값 출력 "},"title":"파일 입출력 및 데이터 처리"},"/programing/python/python/python08/":{"data":{"":"","1-멀티스레딩multithreading-개념과-활용#1️⃣ \u003cstrong\u003e멀티스레딩(Multithreading) 개념과 활용\u003c/strong\u003e":"","2-멀티프로세싱multiprocessing#2️⃣ \u003cstrong\u003e멀티프로세싱(Multiprocessing)\u003c/strong\u003e":"","3-python의-asyncio로-비동기-프로그래밍#3️⃣ \u003cstrong\u003ePython의 asyncio로 비동기 프로그래밍\u003c/strong\u003e":"","4-threading-vs-multiprocessing-vs-asyncio-차이점#4️⃣ \u003cstrong\u003ethreading vs multiprocessing vs asyncio 차이점\u003c/strong\u003e":"","5-비동기-함수-작성-및-이벤트-루프event-loop#5️⃣ \u003cstrong\u003e비동기 함수 작성 및 이벤트 루프(Event Loop)\u003c/strong\u003e":"","6-queue-모듈을-이용한-스레드-간-통신#6️⃣ \u003cstrong\u003equeue 모듈을 이용한 스레드 간 통신\u003c/strong\u003e":"","7-동기화-및-lock-semaphore-사용법#7️⃣ \u003cstrong\u003e동기화 및 Lock, Semaphore 사용법\u003c/strong\u003e":" 1️⃣ 멀티스레딩(Multithreading) 개념과 활용 멀티스레딩의 개념 멀티스레딩은 하나의 프로세스 내에서 여러 스레드를 실행하여 CPU를 효율적으로 사용하는 방법입니다. 여러 스레드를 동시에 실행하는 것처럼 보이지만, 실제로는 하나의 CPU 코어에서 빠르게 전환되는 방식으로 동작합니다. 멀티스레딩을 사용하면 I/O 작업과 같은 병목 현상을 줄일 수 있습니다.\nimport threading # 스레드로 실행할 함수 정의 def print_numbers(): for i in range(5): print(i) # 스레드 생성 thread1 = threading.Thread(target=print_numbers) thread2 = threading.Thread(target=print_numbers) # 스레드 시작 thread1.start() thread2.start() # 스레드가 끝날 때까지 기다림 thread1.join() thread2.join() 출력:\n0 1 2 3 4 0 1 2 3 4 멀티스레딩 활용 멀티스레딩은 주로 I/O 바운드 작업에서 효율성을 발휘합니다. 예를 들어, 웹 스크래핑, 파일 다운로드와 같은 작업에서 유용합니다.\n2️⃣ 멀티프로세싱(Multiprocessing) 멀티프로세싱의 개념 멀티프로세싱은 여러 개의 독립적인 프로세스를 동시에 실행하는 방식입니다. 각 프로세스는 독립적인 메모리 공간을 가지므로 멀티스레딩과 달리 GIL(Global Interpreter Lock)의 영향을 받지 않으며 CPU 집약적인 작업에 유리합니다.\nimport multiprocessing # 프로세스로 실행할 함수 정의 def square(number): print(number * number) # 프로세스 생성 process1 = multiprocessing.Process(target=square, args=(10,)) process2 = multiprocessing.Process(target=square, args=(20,)) # 프로세스 시작 process1.start() process2.start() # 프로세스가 끝날 때까지 기다림 process1.join() process2.join() 출력:\n100 400 멀티프로세싱 활용 멀티프로세싱은 CPU 집약적인 작업에서 효과적입니다. 예를 들어, 이미지 처리, 데이터 분석, 계산 등의 작업에서 사용됩니다.\n3️⃣ Python의 asyncio로 비동기 프로그래밍 asyncio의 개념 asyncio는 Python에서 비동기 프로그래밍을 위한 라이브러리로, 이벤트 루프와 비동기 함수(async function)를 사용하여 효율적으로 비동기 작업을 처리합니다. 주로 I/O 바운드 작업에 유용합니다.\nimport asyncio # 비동기 함수 정의 async def print_numbers(): for i in range(5): print(i) await asyncio.sleep(1) # 1초 대기 # 이벤트 루프 실행 asyncio.run(print_numbers()) 출력:\n0 1 2 3 4 비동기 함수와 이벤트 루프 await 키워드는 비동기 함수에서 다른 비동기 작업이 완료될 때까지 기다리도록 합니다. 이벤트 루프는 이러한 비동기 작업들을 관리하며, 동시에 여러 작업을 효율적으로 처리할 수 있게 도와줍니다.\n4️⃣ threading vs multiprocessing vs asyncio 차이점 threading 여러 스레드를 통해 작업을 동시에 실행할 수 있음. I/O 바운드 작업에서 효율적. GIL(Global Interpreter Lock)로 인해 CPU 집약적인 작업에는 적합하지 않음. multiprocessing 독립적인 프로세스를 통해 작업을 동시에 실행. CPU 집약적인 작업에서 효율적. GIL의 영향을 받지 않음. asyncio 비동기 함수와 이벤트 루프를 사용하여 효율적인 비동기 작업 처리. I/O 바운드 작업에서 뛰어난 성능을 발휘. 코드가 비동기적이므로, 작업의 흐름을 이해하는 데 주의가 필요. 5️⃣ 비동기 함수 작성 및 이벤트 루프(Event Loop) 비동기 함수 작성 비동기 함수는 async 키워드로 정의하며, await 키워드를 사용하여 다른 비동기 작업을 대기할 수 있습니다.\nimport asyncio # 비동기 함수 정의 async def fetch_data(): print(\"Fetching data...\") await asyncio.sleep(2) print(\"Data fetched\") # 이벤트 루프 실행 asyncio.run(fetch_data()) 출력:\nFetching data... (Data fetching 2초 대기) Data fetched 이벤트 루프 이벤트 루프는 비동기 함수들이 실행되는 환경을 관리하며, 비동기 작업들이 완료될 때까지 기다립니다.\n6️⃣ queue 모듈을 이용한 스레드 간 통신 Queue를 이용한 스레드 간 통신 queue 모듈을 사용하면 여러 스레드 간에 데이터를 안전하게 교환할 수 있습니다. 특히 멀티스레딩에서 데이터를 처리할 때 유용합니다.\nimport threading import queue # 스레드로 실행할 함수 정의 def producer(q): for i in range(5): q.put(i) def consumer(q): while True: item = q.get() if item is None: break print(f\"Consumed: {item}\") # 큐 객체 생성 q = queue.Queue() # 프로듀서와 컨슈머 스레드 생성 thread1 = threading.Thread(target=producer, args=(q,)) thread2 = threading.Thread(target=consumer, args=(q,)) # 스레드 시작 thread1.start() thread2.start() # 스레드가 끝날 때까지 기다림 thread1.join() q.put(None) # 작업 종료 신호 thread2.join() 출력:\nConsumed: 0 Consumed: 1 Consumed: 2 Consumed: 3 Consumed: 4 7️⃣ 동기화 및 Lock, Semaphore 사용법 Lock을 이용한 동기화 멀티스레딩에서는 여러 스레드가 동시에 공유 자원에 접근하는 경우 동기화 문제가 발생할 수 있습니다. 이를 해결하기 위해 Lock을 사용합니다.\nimport threading lock = threading.Lock() def thread_function(): with lock: print(\"Critical section\") # 스레드 생성 threads = [threading.Thread(target=thread_function) for _ in range(5)] # 스레드 시작 for thread in threads: thread.start() # 스레드가 끝날 때까지 기다림 for thread in threads: thread.join() Semaphore 사용법 Semaphore는 한 번에 지정된 수의 스레드만 자원에 접근하도록 제한합니다. 자원에 대한 경쟁 상태를 제어할 때 유용합니다.\nimport threading semaphore = threading.Semaphore(3) # 동시에 3개 스레드만 자원 접근 가능 def thread_function(): with semaphore: print(\"Accessing shared resource\") # 스레드 생성 threads = [threading.Thread(target=thread_function) for _ in range(5)] # 스레드 시작 for thread in threads: thread.start() # 스레드가 끝날 때까지 기다림 for thread in threads: thread.join() "},"title":"Python의 동시성 및 병렬성"},"/programing/python/python/python09/":{"data":{"":"","1-flask로-웹-애플리케이션-개발#1️⃣ \u003cstrong\u003eFlask로 웹 애플리케이션 개발\u003c/strong\u003e":"","2-django로-웹-애플리케이션-개발#2️⃣ \u003cstrong\u003eDjango로 웹 애플리케이션 개발\u003c/strong\u003e":"","3-rest-api-설계-및-구현#3️⃣ \u003cstrong\u003eREST API 설계 및 구현\u003c/strong\u003e":"","4-http-요청-및-응답-처리-requests-httpclient-모듈#4️⃣ \u003cstrong\u003eHTTP 요청 및 응답 처리 (requests, http.client 모듈)\u003c/strong\u003e":"","5-웹-서버와-클라이언트-간-데이터-전송-json-xml#5️⃣ \u003cstrong\u003e웹 서버와 클라이언트 간 데이터 전송 (JSON, XML)\u003c/strong\u003e":"","6-flask와-django에서의-템플릿-엔진-사용-jinja2#6️⃣ \u003cstrong\u003eFlask와 Django에서의 템플릿 엔진 사용 (Jinja2)\u003c/strong\u003e":"","7-세션-및-쿠키-관리#7️⃣ \u003cstrong\u003e세션 및 쿠키 관리\u003c/strong\u003e":"","8-url-라우팅-및-미들웨어#8️⃣ \u003cstrong\u003eURL 라우팅 및 미들웨어\u003c/strong\u003e":"","9-oauth-및-jwt-인증#9️⃣ \u003cstrong\u003eOAuth 및 JWT 인증\u003c/strong\u003e":" 1️⃣ Flask로 웹 애플리케이션 개발 Flask란? Flask는 Python을 이용한 경량 웹 프레임워크입니다. 간단한 웹 애플리케이션을 빠르게 만들 수 있도록 도와주며, 확장성도 뛰어나서 복잡한 애플리케이션을 구축할 때도 사용됩니다.\nFlask 애플리케이션 기본 구조 my_flask_app/ │ ├── app.py # 애플리케이션의 기본 파일 └── templates/ # HTML 템플릿 파일 └── index.html Flask 기본 예제 from flask import Flask app = Flask(__name__) @app.route('/') def hello_world(): return 'Hello, World!' if __name__ == \"__main__\": app.run(debug=True) Flask 실행 python app.py 웹 브라우저에서 http://127.0.0.1:5000/를 열면 “Hello, World!“가 출력됩니다.\n2️⃣ Django로 웹 애플리케이션 개발 Django란? Django는 Python을 위한 풀스택 웹 프레임워크로, 강력한 기능을 제공하며 빠르게 웹 애플리케이션을 개발할 수 있도록 도와줍니다. 특히, 관리 시스템이 내장되어 있어, CRUD 작업을 손쉽게 처리할 수 있습니다.\nDjango 애플리케이션 기본 구조 my_django_app/ │ ├── manage.py # Django 관리 명령어를 실행하는 파일 ├── myapp/ # 애플리케이션 디렉토리 │ ├── models.py # 데이터 모델 정의 │ ├── views.py # 뷰 함수 정의 │ └── urls.py # URL 라우팅 설정 └── db.sqlite3 # SQLite 데이터베이스 파일 Django 기본 예제 # 가상환경 활성화 및 Django 설치 pip install django # Django 프로젝트 생성 django-admin startproject myproject # Django 애플리케이션 생성 python manage.py startapp myapp views.py 파일에서 기본 뷰를 작성하고, urls.py에서 URL을 라우팅합니다.\n# myapp/views.py from django.http import HttpResponse def hello_world(request): return HttpResponse(\"Hello, Django World!\") urls.py 파일에 URL 라우팅 설정 추가:\n# myproject/urls.py from django.contrib import admin from django.urls import path from myapp import views urlpatterns = [ path('admin/', admin.site.urls), path('hello/', views.hello_world), ] Django 실행 python manage.py runserver 웹 브라우저에서 http://127.0.0.1:8000/hello/를 열면 “Hello, Django World!“가 출력됩니다.\n3️⃣ REST API 설계 및 구현 REST API란? REST (Representational State Transfer)는 웹 서비스 아키텍처 스타일 중 하나로, HTTP 프로토콜을 기반으로 클라이언트와 서버 간의 통신을 다룹니다. JSON 형식을 사용하여 데이터 전송을 효율적으로 처리합니다.\nFlask로 REST API 예제 from flask import Flask, jsonify, request app = Flask(__name__) # 데이터 저장용 변수 tasks = [] @app.route('/tasks', methods=['GET']) def get_tasks(): return jsonify(tasks) @app.route('/tasks', methods=['POST']) def create_task(): task = request.get_json() tasks.append(task) return jsonify(task), 201 if __name__ == '__main__': app.run(debug=True) Django로 REST API 예제 Django에서 REST API를 구축하려면 Django REST Framework를 사용하는 것이 일반적입니다.\npip install djangorestframework views.py에 REST API를 구현합니다.\nfrom rest_framework.views import APIView from rest_framework.response import Response from rest_framework import status class TaskView(APIView): def get(self, request): tasks = [\"Task 1\", \"Task 2\"] return Response(tasks, status=status.HTTP_200_OK) def post(self, request): task = request.data return Response(task, status=status.HTTP_201_CREATED) API 요청 GET 요청: /tasks로 요청하면 모든 작업을 가져옵니다. POST 요청: /tasks로 새로운 작업을 추가할 수 있습니다. 4️⃣ HTTP 요청 및 응답 처리 (requests, http.client 모듈) requests 모듈 requests는 HTTP 요청을 쉽게 보낼 수 있는 라이브러리입니다.\npip install requests GET 요청 예제 import requests response = requests.get('https://jsonplaceholder.typicode.com/posts') print(response.json()) # JSON 데이터 출력 POST 요청 예제 import requests data = {'title': 'foo', 'body': 'bar', 'userId': 1} response = requests.post('https://jsonplaceholder.typicode.com/posts', json=data) print(response.json()) # 서버에서 반환한 JSON 데이터 출력 5️⃣ 웹 서버와 클라이언트 간 데이터 전송 (JSON, XML) JSON JSON은 경량 데이터 교환 형식으로, 웹 애플리케이션에서 클라이언트와 서버 간의 데이터 전송에 널리 사용됩니다. Python에서는 json 모듈을 사용하여 JSON을 처리할 수 있습니다.\nimport json # Python 객체를 JSON 문자열로 변환 data = {'name': 'John', 'age': 30} json_data = json.dumps(data) print(json_data) XML XML은 데이터 저장과 전송을 위한 형식으로 사용됩니다. Python에서는 xml.etree.ElementTree 모듈을 사용하여 XML을 처리할 수 있습니다.\nimport xml.etree.ElementTree as ET # XML 데이터 생성 data = ET.Element(\"data\") name = ET.SubElement(data, \"name\") name.text = \"John\" age = ET.SubElement(data, \"age\") age.text = \"30\" tree = ET.ElementTree(data) tree.write(\"output.xml\") 6️⃣ Flask와 Django에서의 템플릿 엔진 사용 (Jinja2) Jinja2란? Jinja2는 Flask와 Django에서 HTML 템플릿을 렌더링할 때 사용하는 템플릿 엔진입니다. 동적인 HTML 페이지를 생성할 수 있도록 도와줍니다.\nFlask에서의 Jinja2 사용 from flask import render_template @app.route('/') def home(): return render_template('index.html', title=\"Home Page\") index.html 템플릿 파일:\n\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003e{{ title }}\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to the {{ title }}\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e Django에서의 Jinja2 사용 Django는 기본적으로 Django Templates를 사용하지만, Jinja2를 사용하려면 설정을 추가해야 합니다.\npip install jinja2 7️⃣ 세션 및 쿠키 관리 세션 관리 Flask에서는 session 객체를 사용하여 세션 데이터를 관리할 수 있습니다.\nfrom flask import session @app.route('/set_session') def set_session(): session['username'] = 'John' return 'Session is set' @app.route('/get_session') def get_session(): username = session.get('username', 'Not set') return f'Session Username: {username}' 쿠키 관리 Flask에서 cookies는 request와 response 객체를 통해 관리합니다.\nfrom flask import request, make_response @app.route('/set_cookie') def set_cookie(): resp = make_response(\"Cookie is set\") resp.set_cookie('username', 'John') return resp @app.route('/get_cookie') def get_cookie(): username = request.cookies.get('username') return f'Cookie Username: {username}' 8️⃣ URL 라우팅 및 미들웨어 URL 라우팅 Flask와 Django 모두 URL 라우팅을 지원하며, 이를 통해 웹 애플리케이션의 경로를 정의합니다.\nFlask에서의 URL 라우팅 @app.route('/hello/\u003cname\u003e') def hello_name(name): return f'Hello {name}!' 미들웨어 Flask와 Django에서 미들웨어는 요청과 응답을 처리하는 중간 처리기입니다. Flask에서는 before_request, after_request 데코레이터를 사용하고, Django에서는 Middleware 클래스를 사용하여 미들웨어를 정의합니다.\n9️⃣ OAuth 및 JWT 인증 OAuth 인증 OAuth는 타사 서비스와 안전하게 연결할 수 있도록 도와주는 인증 프로토콜입니다. Flask나 Django에서 이를 구현하려면 Flask-OAuthlib, Django-OAuth-toolkit을 사용합니다.\nJWT 인증 JWT(JSON Web Token)는 클라이언트와 서버 간에 인증 정보를 안전하게 전달하기 위한 방법입니다. Python에서는 PyJWT를 사용하여 JWT를 생성하고 검증할 수 있습니다.\npip install pyjwt import jwt import datetime # JWT 생성 payload = {'user_id': 1, 'exp': datetime.datetime.utcnow() + datetime.timedelta(hours=1)} token = jwt.encode(payload, 'secret_key', algorithm='HS256') # JWT 검증 decoded = jwt.decode(token, 'secret_key', algorithms=['HS256']) print(decoded) "},"title":"Python의 웹 개발"},"/programing/python/python/python10/":{"data":{"":"","1-sqlalchemy를-이용한-관계형-데이터베이스-처리#1️⃣ \u003cstrong\u003eSQLAlchemy를 이용한 관계형 데이터베이스 처리\u003c/strong\u003e":"","2-sqlite-mysql-postgresql-등과-연동#2️⃣ \u003cstrong\u003eSQLite, MySQL, PostgreSQL 등과 연동\u003c/strong\u003e":"","3-데이터베이스-쿼리-및-orm-object-relational-mapping#3️⃣ \u003cstrong\u003e데이터베이스 쿼리 및 ORM (Object-Relational Mapping)\u003c/strong\u003e":"","4-django-orm과-flask-orm-사용법#4️⃣ \u003cstrong\u003eDjango ORM과 Flask ORM 사용법\u003c/strong\u003e":"","5-데이터베이스-트랜잭션-관리#5️⃣ \u003cstrong\u003e데이터베이스 트랜잭션 관리\u003c/strong\u003e":"","6-mongodb와-python-연동-pymongo-mongoengine#6️⃣ \u003cstrong\u003eMongoDB와 Python 연동 (PyMongo, MongoEngine)\u003c/strong\u003e":"","7-redis와-python-연동-redis-py#7️⃣ \u003cstrong\u003eRedis와 Python 연동 (redis-py)\u003c/strong\u003e":"","8-데이터베이스-마이그레이션-alembic-django-migrations#8️⃣ \u003cstrong\u003e데이터베이스 마이그레이션 (Alembic, Django Migrations)\u003c/strong\u003e":" 1️⃣ SQLAlchemy를 이용한 관계형 데이터베이스 처리 SQLAlchemy란? SQLAlchemy는 Python에서 데이터베이스를 다룰 때 사용되는 대표적인 ORM(Object-Relational Mapping) 라이브러리입니다. 이를 사용하면 SQL문을 직접 작성하지 않고도 Python 객체와 데이터베이스 테이블 간의 매핑을 할 수 있습니다.\nSQLAlchemy 기본 사용법 설치 pip install sqlalchemy 기본 설정 예제 from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy import Column, Integer, String # SQLAlchemy 엔진과 세션 설정 engine = create_engine('sqlite:///example.db', echo=True) Base = declarative_base() # 테이블 클래스 정의 class User(Base): __tablename__ = 'users' id = Column(Integer, primary_key=True) name = Column(String) # 데이터베이스 생성 Base.metadata.create_all(engine) 세션 사용 예제 from sqlalchemy.orm import sessionmaker # 세션 만들기 Session = sessionmaker(bind=engine) session = Session() # 데이터 삽입 new_user = User(name=\"John\") session.add(new_user) session.commit() # 데이터 조회 user = session.query(User).filter_by(name=\"John\").first() print(user.name) 2️⃣ SQLite, MySQL, PostgreSQL 등과 연동 SQLite와 SQLAlchemy 연동 SQLite는 가벼운 파일 기반 데이터베이스로, SQLAlchemy에서 쉽게 연동할 수 있습니다.\nengine = create_engine('sqlite:///example.db', echo=True) MySQL과 SQLAlchemy 연동 MySQL은 더 강력한 관계형 데이터베이스로, SQLAlchemy를 사용하여 쉽게 연결할 수 있습니다. mysqlclient를 설치해야 합니다.\npip install mysqlclient engine = create_engine('mysql://username:password@localhost/dbname') PostgreSQL과 SQLAlchemy 연동 PostgreSQL은 SQLAlchemy와 잘 호환되며, psycopg2를 설치해야 합니다.\npip install psycopg2 engine = create_engine('postgresql://username:password@localhost/dbname') 3️⃣ 데이터베이스 쿼리 및 ORM (Object-Relational Mapping) ORM을 이용한 데이터베이스 쿼리 SQLAlchemy의 ORM을 사용하면 SQL 쿼리를 Python 객체로 변환할 수 있습니다. 이를 통해 데이터베이스를 직관적으로 다룰 수 있습니다.\n# 데이터를 필터링하여 조회 users = session.query(User).filter(User.name == \"John\").all() for user in users: print(user.name) 쿼리 조합 # 데이터를 조건에 맞게 정렬 users = session.query(User).order_by(User.name.desc()).all() 4️⃣ Django ORM과 Flask ORM 사용법 Django ORM 사용법 Django는 자체적으로 ORM을 제공하여, Python 코드로 데이터베이스 모델을 정의하고, 쿼리를 작성할 수 있습니다. 기본적인 Django 모델은 models.Model을 상속받아 작성합니다.\n# Django 모델 예시 from django.db import models class User(models.Model): name = models.CharField(max_length=100) # 객체 저장 new_user = User(name=\"John\") new_user.save() # 객체 조회 user = User.objects.filter(name=\"John\").first() print(user.name) Flask ORM 사용법 Flask는 SQLAlchemy를 사용하여 ORM을 설정합니다.\nfrom flask import Flask from flask_sqlalchemy import SQLAlchemy app = Flask(__name__) app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///example.db' db = SQLAlchemy(app) class User(db.Model): id = db.Column(db.Integer, primary_key=True) name = db.Column(db.String(100)) @app.route('/') def index(): user = User.query.first() return f'User: {user.name}' 5️⃣ 데이터베이스 트랜잭션 관리 트랜잭션이란? 트랜잭션은 데이터베이스에서 일련의 작업을 하나의 단위로 처리하는 방식입니다. 모든 작업이 성공하면 커밋하고, 하나라도 실패하면 롤백합니다. 이를 통해 데이터의 일관성을 유지할 수 있습니다.\nSQLAlchemy에서 트랜잭션 관리 # 세션 시작 session = Session() try: # 데이터 삽입 new_user = User(name=\"Alice\") session.add(new_user) # 트랜잭션 커밋 session.commit() except Exception as e: # 예외 발생 시 롤백 session.rollback() print(f\"Error: {e}\") finally: session.close() 6️⃣ MongoDB와 Python 연동 (PyMongo, MongoEngine) PyMongo 설치 pip install pymongo PyMongo 사용 예제 from pymongo import MongoClient # MongoDB 연결 client = MongoClient('mongodb://localhost:27017/') db = client['mydatabase'] collection = db['users'] # 데이터 삽입 collection.insert_one({'name': 'John', 'age': 30}) # 데이터 조회 user = collection.find_one({'name': 'John'}) print(user) MongoEngine 사용 예제 pip install mongoengine from mongoengine import Document, StringField, IntField, connect # MongoDB 연결 connect('mydatabase') # MongoEngine 모델 정의 class User(Document): name = StringField() age = IntField() # 객체 저장 user = User(name=\"John\", age=30) user.save() # 객체 조회 user = User.objects(name=\"John\").first() print(user.name) 7️⃣ Redis와 Python 연동 (redis-py) redis-py 설치 pip install redis Redis와 Python 연동 예제 import redis # Redis 연결 r = redis.Redis(host='localhost', port=6379, db=0) # 데이터 삽입 r.set('username', 'John') # 데이터 조회 username = r.get('username') print(username.decode('utf-8')) 8️⃣ 데이터베이스 마이그레이션 (Alembic, Django Migrations) Alembic 설치 및 사용 Alembic은 SQLAlchemy와 함께 사용할 수 있는 데이터베이스 마이그레이션 도구입니다.\npip install alembic Alembic 예제 Alembic 초기화 alembic init alembic 마이그레이션 생성 alembic revision --autogenerate -m \"create user table\" 마이그레이션 적용 alembic upgrade head Django Migrations Django는 내장된 마이그레이션 시스템을 사용하여 데이터베이스 스키마를 관리합니다.\n마이그레이션 생성 python manage.py makemigrations 마이그레이션 적용 python manage.py migrate "},"title":"데이터베이스와 Python 연동"},"/programing/python/python/python11/":{"data":{"":"","1-python의-unittest-모듈을-이용한-단위-테스트#1️⃣ \u003cstrong\u003ePython의 unittest 모듈을 이용한 단위 테스트\u003c/strong\u003e":"","2-pytest를-활용한-테스트-자동화#2️⃣ \u003cstrong\u003epytest를 활용한 테스트 자동화\u003c/strong\u003e":"","3-테스트-커버리지-분석-coverage#3️⃣ \u003cstrong\u003e테스트 커버리지 분석 (coverage)\u003c/strong\u003e":"","4-모의-객체mock-및-스텁stub-테스트#4️⃣ \u003cstrong\u003e모의 객체(Mock) 및 스텁(Stub) 테스트\u003c/strong\u003e":"","5-디버깅-기법-pdb-logging-breakpoint#5️⃣ \u003cstrong\u003e디버깅 기법 (pdb, logging, breakpoint())\u003c/strong\u003e":"","6-통합-테스트-및-시스템-테스트-작성#6️⃣ \u003cstrong\u003e통합 테스트 및 시스템 테스트 작성\u003c/strong\u003e":"","7-cicd-도구와-연동한-테스트-파이프라인#7️⃣ \u003cstrong\u003eCI/CD 도구와 연동한 테스트 파이프라인\u003c/strong\u003e":" 1️⃣ Python의 unittest 모듈을 이용한 단위 테스트 unittest란? unittest는 Python에서 제공하는 기본적인 단위 테스트 프레임워크입니다. 이 모듈을 사용하면, 코드의 각 기능을 테스트할 수 있는 테스트 케이스를 작성할 수 있습니다.\n기본 사용법 import unittest # 테스트할 함수 def add(a, b): return a + b # 테스트 클래스 정의 class TestAddFunction(unittest.TestCase): def test_add(self): self.assertEqual(add(2, 3), 5) self.assertEqual(add(-1, 1), 0) if __name__ == '__main__': unittest.main() 실행 방법 python -m unittest test_file.py 2️⃣ pytest를 활용한 테스트 자동화 pytest란? pytest는 Python에서 매우 널리 사용되는 테스트 프레임워크로, unittest보다 간결하고 더 많은 기능을 제공합니다. 특히, 테스트 자동화 및 반복적인 테스트 실행에 매우 유용합니다.\n기본 사용법 # test_add.py def add(a, b): return a + b def test_add(): assert add(2, 3) == 5 assert add(-1, 1) == 0 pytest 실행 pytest test_add.py pytest 특징 자동으로 모든 test_로 시작하는 함수 찾기 다양한 플러그인 지원 더 유연하고 풍부한 출력 3️⃣ 테스트 커버리지 분석 (coverage) coverage란? coverage는 코드가 얼마나 테스트되었는지 확인할 수 있는 도구입니다. 이를 사용하면, 테스트가 어떤 부분을 커버하고 어떤 부분은 커버하지 못했는지 알 수 있습니다.\n설치 및 사용법 pip install coverage coverage run -m pytest coverage report 결과 Name Stmts Miss Cover --------------------------------------- test_add.py 5 0 100% 4️⃣ 모의 객체(Mock) 및 스텁(Stub) 테스트 모의 객체(Mock) 모의 객체는 테스트 대상 객체와 의존 관계에 있는 다른 객체를 대체하는 가짜 객체입니다. 이를 통해 테스트가 의존하는 외부 요소를 최소화할 수 있습니다.\nMock 예제 from unittest.mock import Mock # 외부 API를 호출하는 함수 def fetch_data(api_client): return api_client.get_data() # Mock 객체 생성 mock_api = Mock() mock_api.get_data.return_value = \"Test Data\" # Mock 객체로 테스트 실행 result = fetch_data(mock_api) print(result) # Test Data 스텁(Stub) 스텁은 특정 동작을 미리 설정하여 테스트할 때 반환되는 값을 설정하는 객체입니다.\ndef fetch_data(api_client): return api_client.get_data() class ApiClientStub: def get_data(self): return \"Test Data\" # 스텁 객체로 테스트 실행 api_client = ApiClientStub() result = fetch_data(api_client) print(result) # Test Data 5️⃣ 디버깅 기법 (pdb, logging, breakpoint()) pdb (Python Debugger) pdb는 Python에서 제공하는 디버거로, 코드의 실행을 중단하고 변수를 검사하거나 코드 흐름을 제어할 수 있습니다.\nimport pdb def add(a, b): pdb.set_trace() # 디버거 시작 return a + b add(2, 3) logging 모듈 logging은 애플리케이션에서 발생하는 다양한 이벤트를 기록하는 데 유용한 모듈입니다. 디버깅뿐만 아니라, 애플리케이션의 로그를 관리하는 데 유용합니다.\nimport logging logging.basicConfig(level=logging.DEBUG) logging.debug(\"디버깅 메시지\") logging.info(\"정보 메시지\") logging.warning(\"경고 메시지\") logging.error(\"오류 메시지\") logging.critical(\"치명적인 오류 메시지\") breakpoint() Python 3.7부터 추가된 breakpoint() 함수는 디버깅을 쉽게 시작할 수 있게 해줍니다. pdb.set_trace()와 동일하게 작동합니다.\ndef add(a, b): breakpoint() # 디버깅 시작 return a + b add(2, 3) 6️⃣ 통합 테스트 및 시스템 테스트 작성 통합 테스트 통합 테스트는 여러 개의 컴포넌트가 함께 동작하는지 확인하는 테스트입니다. 예를 들어, 데이터베이스와 애플리케이션의 상호작용을 테스트할 수 있습니다.\nimport requests def test_integration(): response = requests.get(\"http://localhost:5000/api/data\") assert response.status_code == 200 assert response.json() == {\"key\": \"value\"} 시스템 테스트 시스템 테스트는 전체 애플리케이션이 기대대로 동작하는지 검증하는 테스트입니다. 다양한 기능을 통합하여 전체 시스템이 문제 없이 동작하는지 확인합니다.\ndef test_system(): # 전체 시스템 테스트 예시 response = requests.get(\"http://localhost:5000/\") assert response.status_code == 200 7️⃣ CI/CD 도구와 연동한 테스트 파이프라인 CI/CD란? CI/CD는 Continuous Integration(지속적인 통합)과 Continuous Delivery(지속적인 배포)를 의미합니다. CI/CD 파이프라인에서 테스트는 코드가 배포되기 전에 자동으로 실행되어야 합니다.\nCI/CD 파이프라인 예시 GitHub Actions 예시 name: Python application on: [push] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.x' - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Run tests run: | pytest GitLab CI/CD 예시 stages: - test test: script: - pip install -r requirements.txt - pytest "},"title":"Python에서의 테스트 및 디버깅"},"/programing/python/python/python12/":{"data":{"":"","1-python의-unittest-모듈을-이용한-단위-테스트#1️⃣ \u003cstrong\u003ePython의 unittest 모듈을 이용한 단위 테스트\u003c/strong\u003e":"","2-pytest를-활용한-테스트-자동화#2️⃣ \u003cstrong\u003epytest를 활용한 테스트 자동화\u003c/strong\u003e":"","3-테스트-커버리지-분석-coverage#3️⃣ \u003cstrong\u003e테스트 커버리지 분석 (coverage)\u003c/strong\u003e":"","4-모의-객체mock-및-스텁stub-테스트#4️⃣ \u003cstrong\u003e모의 객체(Mock) 및 스텁(Stub) 테스트\u003c/strong\u003e":"","5-디버깅-기법-pdb-logging-breakpoint#5️⃣ \u003cstrong\u003e디버깅 기법 (pdb, logging, breakpoint())\u003c/strong\u003e":"","6-통합-테스트-및-시스템-테스트-작성#6️⃣ \u003cstrong\u003e통합 테스트 및 시스템 테스트 작성\u003c/strong\u003e":"","7-cicd-도구와-연동한-테스트-파이프라인#7️⃣ \u003cstrong\u003eCI/CD 도구와 연동한 테스트 파이프라인\u003c/strong\u003e":" 1️⃣ Python의 unittest 모듈을 이용한 단위 테스트 unittest란? unittest는 Python에서 제공하는 기본적인 단위 테스트 프레임워크입니다. 이 모듈을 사용하면 함수나 클래스가 예상대로 동작하는지 확인할 수 있습니다.\n기본 사용법 unittest는 TestCase 클래스를 상속받아 테스트를 작성하고 실행할 수 있게 합니다. 기본적인 테스트는 assertEqual(), assertTrue()와 같은 메서드를 활용하여 실제 결과와 예상 결과를 비교합니다.\nimport unittest # 테스트할 함수 def add(a, b): return a + b # 테스트 클래스 정의 class TestAddFunction(unittest.TestCase): def test_add(self): self.assertEqual(add(2, 3), 5) self.assertEqual(add(-1, 1), 0) if __name__ == '__main__': unittest.main() 실행 방법 unittest를 통해 테스트를 실행하려면, 터미널에서 다음 명령어를 입력합니다.\npython -m unittest test_file.py 2️⃣ pytest를 활용한 테스트 자동화 pytest란? pytest는 Python에서 매우 인기 있는 테스트 프레임워크로, unittest보다 간결하고 유용한 기능들을 제공합니다. pytest는 자동화된 테스트를 쉽게 실행할 수 있으며, 파라미터화된 테스트나 다양한 플러그인도 지원합니다.\n기본 사용법 pytest는 테스트 함수를 test_로 시작하는 함수로 작성하고, assert 문을 사용하여 결과를 비교합니다.\n# test_add.py def add(a, b): return a + b def test_add(): assert add(2, 3) == 5 assert add(-1, 1) == 0 pytest 실행 방법 pytest test_add.py pytest 장점 코드가 간결하고 사용하기 쉬움 자동으로 test_로 시작하는 모든 테스트 함수를 찾음 다양한 플러그인과 확장 가능 3️⃣ 테스트 커버리지 분석 (coverage) coverage란? coverage는 테스트가 코드의 어느 부분을 커버했는지, 즉 테스트되지 않은 코드를 식별할 수 있도록 도와주는 도구입니다. 이를 통해 누락된 테스트를 찾아 추가할 수 있습니다.\n설치 및 사용법 coverage를 설치한 후, pytest와 함께 실행하여 테스트 커버리지를 분석할 수 있습니다.\npip install coverage coverage run -m pytest coverage report 결과 예시 Name Stmts Miss Cover --------------------------------------- test_add.py 5 0 100% 4️⃣ 모의 객체(Mock) 및 스텁(Stub) 테스트 모의 객체(Mock) 모의 객체는 실제 객체를 대신해 특정 동작을 테스트하는 가짜 객체입니다. 주로 외부 API나 데이터베이스와의 의존을 테스트할 때 사용합니다.\nfrom unittest.mock import Mock # 외부 API를 호출하는 함수 def fetch_data(api_client): return api_client.get_data() # Mock 객체 생성 mock_api = Mock() mock_api.get_data.return_value = \"Test Data\" # Mock 객체로 테스트 실행 result = fetch_data(mock_api) print(result) # Test Data 스텁(Stub) 테스트 스텁은 특정 데이터를 반환하도록 설정된 간단한 객체입니다. 모의 객체와 달리 실제 동작을 대체하는 것이 아니라, 특정 동작을 흉내 낼 때 사용합니다.\nclass ApiClientStub: def get_data(self): return \"Test Data\" # 스텁 객체로 테스트 실행 api_client = ApiClientStub() result = fetch_data(api_client) print(result) # Test Data 5️⃣ 디버깅 기법 (pdb, logging, breakpoint()) pdb (Python Debugger) pdb는 Python의 기본 디버거입니다. 코드 실행을 중단하고, 변수 값을 확인하거나 코드 흐름을 제어할 수 있습니다.\nimport pdb def add(a, b): pdb.set_trace() # 디버깅 시작 return a + b add(2, 3) logging 모듈 logging 모듈을 사용하면 애플리케이션의 실행 중 발생한 이벤트를 기록할 수 있습니다. 이는 디버깅 외에도 애플리케이션의 로그를 관리하는 데 유용합니다.\nimport logging logging.basicConfig(level=logging.DEBUG) logging.debug(\"디버깅 메시지\") logging.info(\"정보 메시지\") logging.warning(\"경고 메시지\") logging.error(\"오류 메시지\") logging.critical(\"치명적인 오류 메시지\") breakpoint() breakpoint()는 Python 3.7부터 추가된 기능으로, 코드 중간에 디버거를 쉽게 삽입할 수 있게 해줍니다. pdb.set_trace()와 동일하게 작동합니다.\ndef add(a, b): breakpoint() # 디버깅 시작 return a + b add(2, 3) 6️⃣ 통합 테스트 및 시스템 테스트 작성 통합 테스트 통합 테스트는 여러 개의 컴포넌트가 결합되어 동작하는지 확인하는 테스트입니다. 예를 들어, 데이터베이스와 애플리케이션 간의 상호작용을 검증할 수 있습니다.\nimport requests def test_integration(): response = requests.get(\"http://localhost:5000/api/data\") assert response.status_code == 200 assert response.json() == {\"key\": \"value\"} 시스템 테스트 시스템 테스트는 전체 애플리케이션이 기대하는 대로 동작하는지 테스트하는 단계입니다. 다양한 기능을 통합하여 시스템을 테스트합니다.\ndef test_system(): # 전체 시스템 테스트 예시 response = requests.get(\"http://localhost:5000/\") assert response.status_code == 200 7️⃣ CI/CD 도구와 연동한 테스트 파이프라인 CI/CD란? CI/CD는 Continuous Integration(지속적인 통합)과 Continuous Delivery(지속적인 배포)의 약자로, 자동화된 빌드와 테스트, 배포를 위한 프로세스를 포함합니다. CI/CD 파이프라인에서 테스트는 필수적으로 자동화되어야 합니다.\nGitHub Actions 예시 name: Python application on: [push] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Set up Python uses: actions/setup-python@v2 with: python-version: '3.x' - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Run tests run: | pytest GitLab CI/CD 예시 stages: - test test: script: - pip install -r requirements.txt - pytest "},"title":"Python에서의 테스트 및 디버깅"},"/programing/python/python/python13/":{"data":{"":"","1-numpy로-고성능-배열-및-수학-연산#1️⃣ \u003cstrong\u003eNumPy로 고성능 배열 및 수학 연산\u003c/strong\u003e":"","2-pandas로-데이터프레임-처리-및-분석#2️⃣ \u003cstrong\u003ePandas로 데이터프레임 처리 및 분석\u003c/strong\u003e":"","3-matplotlib-seaborn을-활용한-데이터-시각화#3️⃣ \u003cstrong\u003eMatplotlib, Seaborn을 활용한 데이터 시각화\u003c/strong\u003e":"","4-plotly-bokeh를-이용한-대화형-시각화#4️⃣ \u003cstrong\u003ePlotly, Bokeh를 이용한 대화형 시각화\u003c/strong\u003e":"","5-데이터-클렌징-및-전처리-결측값-처리-스케일링#5️⃣ \u003cstrong\u003e데이터 클렌징 및 전처리 (결측값 처리, 스케일링)\u003c/strong\u003e":"","6-데이터-분석-워크플로우-eda-통계-분석-모델링#6️⃣ \u003cstrong\u003e데이터 분석 워크플로우 (EDA, 통계 분석, 모델링)\u003c/strong\u003e":"","7-시계열-데이터-분석#7️⃣ \u003cstrong\u003e시계열 데이터 분석\u003c/strong\u003e":"","8-머신러닝과-python-scikit-learn-tensorflow-pytorch#8️⃣ \u003cstrong\u003e머신러닝과 Python (Scikit-Learn, TensorFlow, PyTorch)\u003c/strong\u003e":" 1️⃣ NumPy로 고성능 배열 및 수학 연산 NumPy란? NumPy는 고성능 배열 객체와 수학 연산을 제공하는 Python 라이브러리로, 데이터 분석 및 과학적 계산에서 매우 중요한 역할을 합니다. NumPy 배열은 Python의 기본 리스트보다 훨씬 빠르고 효율적입니다.\n기본 사용법 NumPy는 ndarray라는 다차원 배열 객체를 사용하며, 배열 간의 연산을 매우 효율적으로 처리합니다.\nimport numpy as np # NumPy 배열 생성 arr = np.array([1, 2, 3, 4, 5]) # 배열 연산 arr_sum = np.sum(arr) arr_mean = np.mean(arr) arr_max = np.max(arr) print(f\"Sum: {arr_sum}, Mean: {arr_mean}, Max: {arr_max}\") 배열 슬라이싱 배열에서 특정 부분만 추출하는 슬라이싱도 가능합니다.\narr = np.array([1, 2, 3, 4, 5]) sub_arr = arr[1:4] # 2, 3, 4 추출 print(sub_arr) 2️⃣ Pandas로 데이터프레임 처리 및 분석 Pandas란? Pandas는 데이터 분석에 특화된 라이브러리로, DataFrame을 사용하여 2차원 데이터를 쉽게 다룰 수 있습니다. Pandas는 데이터 필터링, 정렬, 그룹화 등 다양한 기능을 제공합니다.\n기본 사용법 Pandas를 사용하여 데이터를 불러오고 처리하는 기본적인 예시입니다.\nimport pandas as pd # 데이터프레임 생성 data = {'Name': ['John', 'Anna', 'Peter'], 'Age': [28, 24, 35], 'City': ['New York', 'Paris', 'Berlin']} df = pd.DataFrame(data) # 데이터프레임 출력 print(df) # 특정 열 추출 ages = df['Age'] print(ages) CSV 파일 읽기/쓰기 Pandas를 사용하여 CSV 파일을 읽고 쓸 수 있습니다.\n# CSV 파일 읽기 df = pd.read_csv('data.csv') # CSV 파일로 저장 df.to_csv('output.csv', index=False) 3️⃣ Matplotlib, Seaborn을 활용한 데이터 시각화 Matplotlib란? Matplotlib는 Python에서 가장 널리 사용되는 데이터 시각화 라이브러리로, 다양한 차트와 그래프를 만들 수 있습니다.\n기본 사용법 Matplotlib를 사용한 간단한 라인 플롯 예시입니다.\nimport matplotlib.pyplot as plt # 데이터 x = [1, 2, 3, 4, 5] y = [10, 20, 25, 30, 35] # 플롯 생성 plt.plot(x, y) # 제목 및 레이블 추가 plt.title('Simple Plot') plt.xlabel('X Axis') plt.ylabel('Y Axis') # 플롯 보여주기 plt.show() Seaborn을 이용한 고급 시각화 Seaborn은 Matplotlib 위에 구축된 고급 시각화 라이브러리로, 아름답고 직관적인 차트를 만들 수 있습니다.\nimport seaborn as sns import matplotlib.pyplot as plt # 데이터셋 로드 tips = sns.load_dataset(\"tips\") # 상관 관계 시각화 sns.heatmap(tips.corr(), annot=True, cmap=\"coolwarm\") plt.show() 4️⃣ Plotly, Bokeh를 이용한 대화형 시각화 Plotly란? Plotly는 대화형 시각화를 만들 수 있는 라이브러리로, 웹 브라우저에서 동적으로 데이터를 탐색할 수 있도록 합니다.\nimport plotly.express as px # 샘플 데이터셋 df = px.data.iris() # 산점도 그래프 fig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\") fig.show() Bokeh란? Bokeh는 Python으로 대화형 시각화를 만들기 위한 라이브러리로, 특히 실시간 데이터 시각화에 유용합니다.\nfrom bokeh.plotting import figure, show # 플롯 생성 p = figure(title=\"Simple Line Plot\", x_axis_label='X', y_axis_label='Y') # 데이터 추가 p.line([1, 2, 3, 4], [10, 20, 30, 40], legend_label=\"Temp.\", line_width=2) # 시각화 출력 show(p) 5️⃣ 데이터 클렌징 및 전처리 (결측값 처리, 스케일링) 결측값 처리 데이터에 결측값이 있을 경우, 이를 처리하는 방법이 필요합니다. Pandas는 결측값을 다루는 다양한 방법을 제공합니다.\nimport pandas as pd # 결측값 있는 데이터프레임 data = {'Name': ['John', 'Anna', None, 'Peter'], 'Age': [28, None, 35, 32]} df = pd.DataFrame(data) # 결측값 채우기 df['Age'] = df['Age'].fillna(df['Age'].mean()) print(df) 스케일링 스케일링은 데이터의 크기 차이를 조정하는 과정입니다. sklearn의 StandardScaler를 사용하여 데이터를 정규화할 수 있습니다.\nfrom sklearn.preprocessing import StandardScaler # 데이터 data = [[1, 2], [3, 4], [5, 6]] # 스케일링 scaler = StandardScaler() scaled_data = scaler.fit_transform(data) print(scaled_data) 6️⃣ 데이터 분석 워크플로우 (EDA, 통계 분석, 모델링) EDA (탐색적 데이터 분석) 탐색적 데이터 분석(EDA)은 데이터를 시각화하고 통계적 분석을 통해 데이터의 패턴과 관계를 이해하는 과정입니다.\nimport seaborn as sns # 데이터셋 로드 tips = sns.load_dataset(\"tips\") # 상자 그림(Boxplot)으로 EDA sns.boxplot(x='day', y='total_bill', data=tips) 모델링 모델링은 데이터를 분석하여 예측 모델을 만드는 과정입니다. 예를 들어, Scikit-Learn을 사용하여 선형 회귀 모델을 만들 수 있습니다.\nfrom sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split # 예시 데이터 X = tips[['total_bill']] y = tips['tip'] # 훈련 세트와 테스트 세트로 분리 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 모델 학습 model = LinearRegression() model.fit(X_train, y_train) # 예측 predictions = model.predict(X_test) print(predictions) 7️⃣ 시계열 데이터 분석 시계열 데이터란? 시계열 데이터는 시간에 따라 수집된 데이터를 의미하며, 이를 분석하여 패턴이나 추세를 식별합니다. Pandas를 사용하여 시계열 데이터를 쉽게 처리할 수 있습니다.\nimport pandas as pd # 날짜 범위 생성 dates = pd.date_range('2023-01-01', periods=6, freq='D') # 시계열 데이터프레임 생성 data = {'Date': dates, 'Value': [10, 12, 14, 13, 15, 18]} df = pd.DataFrame(data) # 날짜를 인덱스로 설정 df.set_index('Date', inplace=True) print(df) 시계열 데이터 시각화 import matplotlib.pyplot as plt df['Value'].plot() plt.title('Time Series Data') plt.xlabel('Date') plt.ylabel('Value') plt.show() 8️⃣ 머신러닝과 Python (Scikit-Learn, TensorFlow, PyTorch) Scikit-Learn을 이용한 머신러닝 Scikit-Learn은 머신러닝을 위한 강력한 라이브러리로, 분류, 회귀, 클러스터링 등의 알고리즘을 쉽게 사용할 수 있습니다.\nfrom sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # 데이터 로드 iris = load_iris() X = iris.data y = iris.target # 데이터 분할 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 모델 학습 model = LogisticRegression(max_iter=200) model.fit(X_train, y_train) # 예측 및 정확도 평가 y_pred = model.predict(X_test) print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\") "},"title":"Python의 데이터 분석 및 시각화"},"/programing/python/python/python14/":{"data":{"":"","1-python의-고급-메모리-관리-garbage-collection-메모리-프로파일링#1️⃣ \u003cstrong\u003ePython의 고급 메모리 관리 (Garbage Collection, 메모리 프로파일링)\u003c/strong\u003e":"","2-python의-성능-최적화-cython-numba-pypy#2️⃣ \u003cstrong\u003ePython의 성능 최적화 (Cython, Numba, PyPy)\u003c/strong\u003e":"","3-python의-고급-병렬-처리-joblib-celery#3️⃣ \u003cstrong\u003ePython의 고급 병렬 처리 (Joblib, Celery)\u003c/strong\u003e":"","4-python의-인터프리터-동작-이해#4️⃣ \u003cstrong\u003ePython의 인터프리터 동작 이해\u003c/strong\u003e":"","5-python의-메타프로그래밍-reflection-dynamic-importing#5️⃣ \u003cstrong\u003ePython의 메타프로그래밍 (Reflection, Dynamic Importing)\u003c/strong\u003e":"","6-파이썬으로-iot-및-하드웨어-제어-gpio-raspberry-pi#6️⃣ \u003cstrong\u003e파이썬으로 IoT 및 하드웨어 제어 (GPIO, Raspberry Pi)\u003c/strong\u003e":"","7-python과-블록체인-bitcoin-ethereum#7️⃣ \u003cstrong\u003ePython과 블록체인 (Bitcoin, Ethereum)\u003c/strong\u003e":" 1️⃣ Python의 고급 메모리 관리 (Garbage Collection, 메모리 프로파일링) Garbage Collection (GC) Python은 자동 메모리 관리를 제공하며, 그 중에서 가장 중요한 개념은 “Garbage Collection (GC)“입니다. GC는 더 이상 참조되지 않는 객체를 메모리에서 자동으로 제거하는 기능입니다. Python은 reference counting과 generational garbage collection을 통해 메모리를 관리합니다.\nGC 예시 import gc # 객체 생성 a = [1, 2, 3] b = a del a # a를 삭제해도 b는 여전히 참조하고 있음 # GC 강제 실행 gc.collect() 메모리 프로파일링 Python에서 메모리 사용량을 확인하려면 memory_profiler와 같은 도구를 사용할 수 있습니다.\npip install memory-profiler from memory_profiler import profile @profile def my_func(): a = [i for i in range(10000)] return a my_func() 2️⃣ Python의 성능 최적화 (Cython, Numba, PyPy) Cython을 이용한 성능 최적화 Cython은 Python 코드를 C로 컴파일하여 성능을 극대화하는 방법입니다. Cython을 사용하면 C 언어의 속도를 얻을 수 있습니다.\npip install cython Cython 코드를 .pyx 파일로 작성하고 C 컴파일러를 사용해 컴파일합니다.\n# hello.pyx def say_hello(): print(\"Hello, World!\") Numba Numba는 Python 코드의 성능을 최적화할 수 있는 라이브러리로, 수학적 계산이나 배열 연산을 최적화하는 데 유용합니다.\npip install numba from numba import jit @jit(nopython=True) def add(a, b): return a + b print(add(5, 6)) PyPy PyPy는 Python의 JIT(Just-In-Time) 컴파일러로, 표준 CPython보다 훨씬 빠른 성능을 제공합니다. PyPy를 사용하면 Python 코드 실행 속도를 크게 향상시킬 수 있습니다.\n# PyPy 설치 sudo apt-get install pypy 3️⃣ Python의 고급 병렬 처리 (Joblib, Celery) Joblib을 이용한 병렬 처리 Joblib은 Python의 다중 프로세싱을 보다 쉽게 활용할 수 있는 라이브러리입니다. 특히, 대규모 계산에 효과적입니다.\npip install joblib from joblib import Parallel, delayed def process_item(item): return item * item items = [1, 2, 3, 4, 5] results = Parallel(n_jobs=2)(delayed(process_item)(item) for item in items) print(results) Celery를 이용한 비동기 작업 Celery는 분산 작업 큐를 관리하고 비동기 작업을 처리하는 데 사용됩니다. 웹 애플리케이션에서 비동기적인 작업을 처리하는 데 매우 유용합니다.\npip install celery from celery import Celery app = Celery('tasks', broker='pyamqp://guest@localhost//') @app.task def add(x, y): return x + y 4️⃣ Python의 인터프리터 동작 이해 Python 인터프리터 Python 코드는 인터프리터를 통해 실행됩니다. CPython은 가장 널리 사용되는 Python 인터프리터로, Python 코드를 바이트 코드로 변환한 후 실행합니다.\n소스 코드: Python 파일(.py) 바이트 코드: .pyc 파일 인터프리터: 바이트 코드를 실행하여 결과를 출력 인터프리터 동작 예시 python -m dis example.py 위 명령은 Python 코드의 바이트 코드로 변환된 결과를 보여줍니다.\n5️⃣ Python의 메타프로그래밍 (Reflection, Dynamic Importing) Reflection Python의 reflection은 객체의 속성이나 메서드를 런타임에 동적으로 확인하거나 수정할 수 있는 기능을 제공합니다.\nclass MyClass: def greet(self): print(\"Hello, World!\") obj = MyClass() method = getattr(obj, 'greet') method() Dynamic Importing Python은 런타임에 모듈을 동적으로 임포트할 수 있습니다. importlib 모듈을 사용하여 동적 임포트를 할 수 있습니다.\nimport importlib math = importlib.import_module('math') print(math.sqrt(16)) 6️⃣ 파이썬으로 IoT 및 하드웨어 제어 (GPIO, Raspberry Pi) Raspberry Pi에서 GPIO 제어 RPi.GPIO 라이브러리는 Raspberry Pi의 GPIO 핀을 제어하는 데 사용됩니다. LED를 제어하는 예시입니다.\npip install RPi.GPIO import RPi.GPIO as GPIO import time GPIO.setmode(GPIO.BOARD) GPIO.setup(12, GPIO.OUT) # LED 켜기 GPIO.output(12, GPIO.HIGH) time.sleep(1) # LED 끄기 GPIO.output(12, GPIO.LOW) GPIO.cleanup() IoT 장치 제어 Python을 사용하여 온도 센서나 모터와 같은 IoT 장치를 제어할 수 있습니다. 예를 들어, Adafruit_DHT 라이브러리를 사용하여 온도 센서를 읽을 수 있습니다.\npip install Adafruit-DHT import Adafruit_DHT sensor = Adafruit_DHT.DHT11 pin = 4 humidity, temperature = Adafruit_DHT.read_retry(sensor, pin) print(f\"Temperature: {temperature}C, Humidity: {humidity}%\") 7️⃣ Python과 블록체인 (Bitcoin, Ethereum) Bitcoin 블록체인 Python을 사용하여 Bitcoin의 블록체인을 분석하거나 데이터를 가져올 수 있습니다. bitcoin 라이브러리를 사용하여 Bitcoin 네트워크와 상호작용할 수 있습니다.\npip install bitcoin import bitcoin # Bitcoin 주소 생성 private_key = bitcoin.random_key() public_key = bitcoin.privtopub(private_key) address = bitcoin.pubtoaddr(public_key) print(\"Bitcoin Address:\", address) Ethereum 블록체인 web3.py 라이브러리를 사용하여 Ethereum 네트워크와 상호작용할 수 있습니다. 스마트 계약을 배포하거나 트랜잭션을 보내는 기능을 제공합니다.\npip install web3 from web3 import Web3 # Ethereum 네트워크 연결 w3 = Web3(Web3.HTTPProvider('https://mainnet.infura.io/v3/YOUR_INFURA_PROJECT_ID')) # 이더리움 주소 확인 address = w3.toChecksumAddress('0x742d35Cc6634C0532925a3b844Bc454e4438f44e') balance = w3.eth.get_balance(address) print(f\"Balance: {w3.fromWei(balance, 'ether')} ETH\") "},"title":"Python의 고급 주제 및 기타 활용"},"/system/":{"data":{"":" RSS Feed "},"title":"System"},"/system/aws/amazonwebservice/":{"data":{"":"","#":"Amazon Web Service\nAws와 CloudComputing Amazon Web Services ( AWS ) AWS Cloud Computing 와 aws Cloud Computing의 종류 IaaS PaaS SaaS On Premise 서버와 Cloud 서버의 차이 소유자 ( Owner ) 용량 ( Capacity ) 렌탈 서버 ( 공유 서버 )와 Public의 차이 렌탈 서버 전용 서버와 가상 전용 서버 렌탈 서버와 AWS ( Public )의 차이 Private Cloud와 Public Cloud AWS에서의 Private Cloud의 정의 AWS 서비스의 구성 AWS Computing EC2 EC2 상태의 종류 EC2 구매옵션 Lightsail Lightsail의 유료 Plan Lightsail\u0026 EC2 ECS Linux Container Kernel Docker Lambda Lambda EC2 vs Lambda Batch amazonwebservice Batch의 구성요소 Batch Group Elastic Beanstalk Elastic Beanstalk의 장점 AWS Database Amazon RDS ( Relational Database Service ) DB Instance DB Instance Storage Multi-AZ Read Replica Automated Backup Enhanced Monitoring RDS vs DB in EC2 Amazon DynamoDB DynamoDB의 특징 Amazon ElastiCache Cache In Memory Cache ( In Memory DataBase ) Memcache ElastiCache Amazon Redshift Redshift Redshift의 구성 Data Warehouse(DW) ETL(Extract, Tranform, Load) BI(Business Intelligence) Redshift vs RDS Amazon Aurora AWS Storage AWS Network AWS Migrate AWS Developer AWS Management AWS Security AWS Analysis ","amazon-web-service#\u003cstrong\u003eAmazon Web Service\u003c/strong\u003e":""},"title":"AWS docs"},"/system/aws/amazonwebservice/aws%EB%9E%80/":{"data":{"":"AWS 란? Amazon Web Services ( AWS ) AWS AWS는 Amazon에서 제공하는 클라우드 서비스로, 네트워크를 기반으로 가상 컴퓨터와 스토리지를 비롯한 다양한 서비스를 제공 합니다. Cloud Computing 와 AWS AWS에 대해 공부하기 앞서, 우리는 Cloud Computing이 무엇이고, 어떠한 개념에 대해 알고 있어야 합니다. 그 이유는 AWS가 클라우드 컴퓨티 그 자체이기 때문이죠.\n클라우드 컴퓨팅 ( Cloud Computing : 이하 클라우드 )은 컴퓨터 리소스의 이용 형태로, 클라우드는 컴퓨터의 계산 리소스, 스토리지, 애플리케이션 처리를 네트워크 기반 서비스로 제공하는 것을 뜻 합니다.\n클라우드 컴퓨팅의 클라우드는 “구름 ( Cloud )“를 의미하는 것으로, Cloud는 Google의 최고 경영 책임자인 에릭 슈미트가 2006년 8월 “인터넷에 접속해서 다양한 리소스를 사용할 수 있게 하는 구조\"를 구름으로 예를 들면서 널리 사용되게 되었으며, 현재는 대표적으로 Google의 GCP ( Google Cloud Platform ), Microsoft의 Azure, Amazon의 AWS가 널리 사용되어지고 있습니다.\n예전부터 네트워크를 이용한 컴퓨터 리소스를 공유하는 개념은 존재해왔지만, 클라우드란 용어가 정착하게 된 결정적인 이유는, 브로드 밴드 네트워크의 일반화, 하드웨어 및 소프트웨어의 진화와 구글과 같은 플랫폼을 제공하는 기업 등의 여러 상호작용의 결과라고 할 수 있습니다.\nCloud Computing의 종류 클라우드 컴퓨팅에도 여러 서비스의 종류가 있고, 이들 중 위의 그림에 나타난 클라우드 컴퓨팅을 대표하는 서비스에 대해 알아보도록 하겠습니다. Infratructure as a Service : IaaS IaaS는 가상 서버 또는 스토리지 등의 리소스를 인터넷을 기반으로 제공하는 서비스를 의미하며, 추가적으로 네트워크 서비스 자체를 포함하기도 합니다.\nIaaS의 가장 큰 장점은 물리적인 하드웨어를 관리할 필요가 없음에도, 직접적으로 컴퓨터 리소르를 사용할 수 있다는 점입니다.\nIaaS는 위의 그림에서처럼 가장 하단에 위치하며 클라우드 레이어로는 갖아 아래의 기초적인 부분을 담당합니다. 즉, IaaS는 물리 장치에 가장 가까운 서비스라 할 수 있습니다.\nPlatform as a Service : PaaS PaaS는 데이터베이스 또는 애플리케이션 서버 등의 미들웨어를 제공하는 서비스입니다.\nOS와 미들웨어의 관리는 서비스 제공자가 하며, 사용자는 미들웨어만을 직접 사용할 수 있습니다.\nSoftware as as Service : SaaS SaaS는 소프트웨어 또는 애플리케이션의 기능을 인터넷을 통해 제공합니다.\nSaaS는 내부적으로 메일 서비스, 큐 서비스, 업무 관리 시스템 등으로 다양하게 분류되어 있습니다.\nSaaS를 제공하는 것을 SaaS제공자 ( Provider )라고 부릅니다. 이는 ASP ( Application Service Provider )와 동일한 것으로, 다만 SaaS의 제공자는 클라우드라는 것에 조금 더 비중을 두어 말하는 것이 차이점이라 할 수 있습니다.\nOn Premise 서버와 Cloud 서버의 차이 On Premise ( 물리 서버 )라고하면 일반적으로 물리 머신을 한정해서 말하는 것이므로, 네트워크 장치 또는 전력 설비 등을 포함하는 의미로 On Premise라는 용어가 되었습니다.\nOn Premise는 조직 내부에서 사용할 목적으로 준비한 설비를 나타내며, 기업 내부에서 일반적으로 사용하는 형태라서 이전에는따로 명칭이 없었지만, 클라우드가 등장하면서 기존에 사용하던 형태를 나타내는 용어로 사용도기 시작했습니다.\n그러면, On premise와 Cloud의 가장 큰 차이는 무엇일까요?, 그것은 크게 2가지로 소유자 ( Owner )와 용량 ( Capacity ) 입니다.\n소유자 ( Owner ) **On Premise와 Public의 첫 번째 차이는 소유자로, On Premise의 경우 리소스 등의 예외가 있을 수는 있지만, 일반적으로 설비를 준비한 기업이 소유하고 있습니다. 반면, Public은 해당 깅버이 모든 리소스를 소유하고, 해당 리소스를 서비스로 만든 것을 사용하는 형태로, **소유자와 사용자가 다르다고 할 수 있습니다.****\n소유자와 사용자가 다르다는 차이점은 다방면에서 영향을 끼칠 수 있습니다.\n먼저 초기 비용의 차이입니다. On Premise는 서버 등을 이용할 때, 초기에 물리 장치를 구매해서 도입해야 하며, 여러 비용이 발생할 수 있습니다. 반면, AWS는 사용자가 물리 잧이를 구매할 필요가 없어 초기 비용이 거의 들지 않습니다. 이는 Public 차원에서 미리 물리 장치에 투자한 자산을 서비스 제공이라는 형태로 분산해서 회수하는 형태이기 때문입니다. 이어서 서버 등의 조달 기간입니다. On Premise의 경우는 견적을 받고 발주 및 배송에 몇 주에서 몇 달의 시간이 걸리는 것이 일반적이지만, 반면 Public 환경에서는 웹 브라우저, 콘솔, 프로그램에서 호출하면 몇 분 내로 조달이 완료됩니다. 이와 마찬가지로 사용하고 있는 서버를 추가하거나, 크기를 변경할 때도 동일합니다. On Premise의 경우는 시간과 비용이 들어가지만, 서버의 성능을 Scale Up하거나 이와 반대되는 경우, 혹은 서버자체를 새로 구매해야할 때, Public 상에서는 버튼 하나로 변경 및 추가 구매가 가능합니다. Option On Premises Public 비용 초기에 모두 필요함 초기 비용은 따로 필요 없으며, 종량제 과금에 따라 비용이 분산되어 발생 서버 조달 기간 몇 주- 몇달 몇 분 서버 추가/ 변경 시간과 비용이 들어감 추가/ 변경과 관련된 비용이 필요하지 않음 용량 ( Capacity ) On Premise와 AWS에서는 소유와 사용에 따라 비용이 발생하는 방식이 다릅니다. 추가로 서버 조달 기간 또는 조달 비용도 다릅니다. 따라서 용량 ( Capacity ) 설계도 전혀 다르게 해야합니다.\nOn Premise는 서버 조달, 추가/변경으로 인한 기간이 길고, 비용이 크기 때문에 자원을 많이 사용할 때의 필요 자원에 맞춰서 모든 것을 준비해야 합니다. 반면 Public은 자원의 추가/ 변경이 쉬우며, 따라서 실제 수요에 맞춰 자원을 크게 만들 수도 있고, 작게 만들 수도 있습니다. 또한 대부분의 Public 플랫폼은 종량제 비용이므로 작게 만들면 비용을 줄일 수 있습니다.\n즉, Public 인프라를 효율적으로 활용하려면, On Premise에서와 다르게 해야한다는 점 을 확실하게 이해해야 합니다.\n렌탈 서버 ( 공유 서버 )와 Public의 차이 렌탈 서버 위에서 Public 인프라가 다른 소유자의 자원을 사용한다는 점을 말씀드렸습니다. 그렇다면 우리가 흔히 알고 있는 호스팅 서버 혹은 공용 서버라 불리는 렌탈서버와 다른 점을 무엇일까요?.\n먼저 렌탈서버란 1대의 서버를 여러 사용자가 공용으로 사용 하는 형태로, 주로 웹 서버나 메일 서버를 사용하는 것이 일반적이었습니다.\n즉, 1대의 물리서버를 모두 점유하는 전용 서버의 위에 가상 서버를 여러 개를 만들어, 해당 가상 서버를 점유하는 가상 전용 서버( VPS )라는 형태를 취하는 것이 렌탈 서버입니다.\n그렇다면, 이러한 렌탈 서버에 문제점은 무엇일까요? 그것은 크게 3가지로 말씀드릴수 있는데, 낮은 자유도, 보안문제, 다른 사용자로 부터의 영향으로 정리할 수 있습니다.\n먼저, 낮은 자유도라는 것은 공용 서버를 이용할 때에는 root 계정이 아닌, 사용자 권한의 계정만 부여되므로, 이는 애플리케이션이나 미들웨어를 자신이 원할 때 변경 등이 불가능하며, 자신이 원하는 대로 환경을 바꿀 수 없습니다.\n이어 보안 문제또한 위에 이어지는 문제로, 기본적으로 자신이 원하는 환경을 구축할 수 없으므로, 보안 대책도 업자에게 맡기게됩니다. 이에 따라 취약성이 있는 미들웨어를 사용하고 있다는 것을 파악하여도, 사용자는 이를 해결하기 어려우며, 또한 만약 다른 사람이 만든 애플리케이션에 취약점이 발견되면, 그 취약점에 영향을 받을 수도 있습니다.\n마지막으로는 다른 사용자로부터의 영향입니다. 만약 Apache를 사용하는 웹 서버를 이용할 때 공용 서버를 사용하면 유저마다 프로세스를 사용하는 것이 아닌, 모두 동일한 프로세스를 분할해서 사용하게 되는 데, 만약 1명의 사용자가 부하처리, CGI 등을 사용한 프로그램 처리가 폭주하면 모든 사용자는 영향을 받게 되어, 서비스가 중단될 수 있어, 공용 서버는 다른 사용자에게 영향을 받기 쉬운 형태라 할 수 있습니다.\n전용 서버와 가상 전용 서버 위와 같은 렌탈 서버의 문제를 해결하기위해 전용서버와 가상 전용 서버라는 형태가 등장하게 되었습니다.\n전용 서버와 가상 전용 서버는 관리자의 권한이 부여되어 있는 사용자 계정이 생성이 가능하여, 자유도가 높으며 스스로 관리가 가능합니다.\n한편 전용서버는 1대의 물리 서버를 1명의 사용자에게 주어야 하기에, 비용적으로 부담이 크며, 이 때문에 가상화 기술을 사용해 1대의 물리 서버를 여러 대의 가상 서버로 분할해 비용을 줄인 것을 가상 전용 서버입니다.\n또한 전용서버는 한 대의 물리서버이므로 다른 사용자의 영향을 전혀 받지 않으며, 반면 가상 전용 서버는 어느 정도의 영향을 받을 수 있지만, 렌탈 서버, 즉 공용 서버에 비해서는 거의 영향을 받지 않는 다고 할 수 있습니다.\n옵션 공용 서버 전용 서버 가상 전용 서버 사용 형태 1대의 물리 서버를 분할해서 사용 1대의 물리 서버 점유 1대의 물리 서버 위에 있는 가상 서버를 점유 비용 적음 높음 중간 자유도 거의 없음 높음 높음 보안 관리 불가능 관리 가능 관리 가능 다른 사용자의 영향 높음 없음 거의 없음 렌탈 서버와 AWS ( Public )의 차이 EC2라는 AWS의 가상 컴퓨트 서비스는 가상화 기술을 사용해 1대의 물리 컴퓨터 위에 여러 개의 가상 컴퓨터를 만들어서 사용합니다.\n**여기에서 사용자는 관리자 권한을 가진 계정을 사용할 수 있으며, 해당 가상 컴퓨터 내부의 모든 것을 관리할 수 있습니다. 따라서 이러하 면에서 **EC2는 가상 전용 서버와 비슷하다 할 수 있습니다.**\n그러나 EC2는 디스크를 동적으로 추가하거나, CPU와 메모리를 다른 인스턴스 유형으로 쉽게 변경하는 등의 기존의 렌탈 서버에 없는 기능이 많습니다. 또한, 가상 머신 이미지를 생성해서 백업하고, 백업한 이미지를 사용하여 여러 서버로 복제하는 등의 서비스도 이용이 가능합니다. 이와 같이 AWS와 같은 대부분의 클라우드 컴퓨팅을 서비스를 하는 기업의 대부분은 위에 렌탈 서버가 제공하는 서비스 뿐만아닌 추가적인 서비스를 더 제공하는 형태라고 할 수 있습니다.\nPrivate Cloud와 Public Cloud 크게 클라우드의 형태는 Private Cloud와 Public Cloud가 있습니다. 이는 말을 정의하는 사람에 따라 의미가 조금씩 다를 수 있으며, 일반적인 의미에서는 누구에게 서비스를 제공하는 가에 따라 정의됩니다.\n크게 Public Cloud는 GCP, Azure, AWS와 같이 누구에게나 서비스를 제공하는 형태의 서비스를 의미하고 Priavte Cloud는 기업 사내망, 즉 기업 전용서버로 해석되기도 하며, Public과는 반대로 특정 기업/ 조직 전용으로 제공되는 서비스를 의미합니다.\n이 뿐만 아니라 현재는 이 둘을 혼용으로 사용하는 Hybrid Cloud와 특정 업종의 기업들이 함께 운영해나가는 Community Cloud라는 용어가 있습니다.\nAWS에서의 Private Cloud의 정의 AWS를 제공하는 Amazon은 Public과 Private라는 용어를 따로 사용하고 있지 않습니다. 이는 클라우드라는 용어가 없었던 때부터 서비스를 시작한 Amazon의 자부심이라 할 수 있겠으며, 일반적으로 AWS를 대표적인 Public Cloud Service로 분류합니다.\nAWS 내에는 Virtual Private Cloud ( VPC )라는 서비스가 있는 데, 이는 가상 네트워크를 생성하여 IP 주소 범위, 라우트 테이블, 네트워크 게이트웨이 등을 자유롭게 설정할 수 있게 해주는 서비스로, VPC를 사용하면 기존 데이터 센터와 회사 내부에 만들던 것과 같은 방식으로 네트워크를 만들 수 있습니다. 경우에 따라서는 이를 Private 클라우드라 표현하기도 합니다.\nAWS 서비스의 구성 AWS는 이미 30개가 넘는 서비스가 있으며, 해마다 새로운 서비스와 기능이 추가되므로 서비스의 전체적인 구성을 파악하는 것은 굉장히 힘듭니다.\n하지만 AWS를 사용할 때에 대한 기본적인 개념, 사고방식 등은 베이스로 학습한 후에 진행하는 것이 보다 빠른 이해를 도울 것입니다.","#":"","amazon-web-services--aws-#\u003cstrong\u003eAmazon Web Services ( AWS )\u003c/strong\u003e":"","aws-란#\u003cstrong\u003eAWS 란?\u003c/strong\u003e":"","cloud-computing의-종류#\u003cstrong\u003eCloud Computing의 종류\u003c/strong\u003e":"","on-premise-서버와-cloud-서버의-차이#\u003cstrong\u003eOn Premise 서버와 Cloud 서버의 차이\u003c/strong\u003e":"","private-cloud와-public-cloud#\u003cstrong\u003ePrivate Cloud와 Public Cloud\u003c/strong\u003e":"","렌탈-서버--공유-서버-와-public의-차이#\u003cstrong\u003e렌탈 서버 ( 공유 서버 )와 Public의 차이\u003c/strong\u003e":""},"title":"CloudComputing과 AWS"},"/system/aws/amazonwebservice/aws_analysis/":{"data":{"":"AWS Analysis Amazon Athena Amazon CloudSearch Amazon EMR Amazon ES Amazon Kinesis Amazon QuickSight AWS DataPipeline ","amazon-athena#\u003cstrong\u003eAmazon Athena\u003c/strong\u003e":"","amazon-cloudsearch#\u003cstrong\u003eAmazon CloudSearch\u003c/strong\u003e":"","amazon-emr#\u003cstrong\u003eAmazon EMR\u003c/strong\u003e":"","amazon-es#\u003cstrong\u003eAmazon ES\u003c/strong\u003e":"","amazon-kinesis#\u003cstrong\u003eAmazon Kinesis\u003c/strong\u003e":"","amazon-quicksight#\u003cstrong\u003eAmazon QuickSight\u003c/strong\u003e":"","aws-analysis#\u003cstrong\u003eAWS Analysis\u003c/strong\u003e":"","aws-datapipeline#\u003cstrong\u003eAWS DataPipeline\u003c/strong\u003e":""},"title":"AWS Analysis"},"/system/aws/amazonwebservice/aws_computing/":{"data":{"":"AWS 컴퓨팅 서비스 EC2 ( Elastic Compute Cloud ) EC2 공식 홈페이지 가상 컴퓨팅 서비스를 제공해주는 서버로 실제 물리서버와 똑같은 형태의 서비스를 제공 AMI를 통해 필요한 운영체제와 여러 소프트웨어를 쉽게 생성 가능 키 페어를 사용하여 로그린 정보 보호 SSH로 원격 연결이 가능 중지가 가능한 EBS 기반 인스턴스와 임시 스토리지를 제공하여 중지가 불가능한 Instance Store 기반 EC2로 분류 됨 ( 재부팅은 모두 가능 ) 인스턴스의 유형으로는 범용, 컴퓨팅 최적화, 메모리 최적화, 스토리지 최적화 등이 존재 EC2 상태의 종류 Pending : 인스턴스가 구동하기 위해 준비중인 상태, 요금 미청구 Runnung : 인스턴스가 실행하고 사용할 준비가 된 상태, 요금 청구 Stopping : 인스턴스가 중지 모드로 전환되려는 상태, 요금 미청구 Shutting-down : 인스턴스가 종료를 위해 준비중인 상태, 요금 미청구 Terminated : 인스턴스가 종료된 상태, 요금 미청구 EC2 구매옵션 On demand : 필요할 때 바로 생성하는 방식으로 1시간 단위로 과금이 이루어짐 ( 1분 사용시도 1시간 ) Spot : 경매방식의 인스턴스 기준가격보다 높은 가격 제시시 사용가능하며, 타인에 의해 불시로 종료되거나 정지될 수 있어 각종 테스트에 적합 Reserved : 12개월- 36개월 단위로 예약하여 사용하는 인스턴스로 On demandq비해 가격이 대폭 할인되며, 장기적으로 사용할 경우 효율이 좋지만, 예약된 인스턴스이기에 사용하지 않아도 과금이 부과되어짐 Lightsail Lightsail Site AWS에서 VPS ( Virtual Private Server : 가상 사설 서버 ) 를 시작하는 가장 쉽고 빠른 방법 가상 사설 서버, 영구적인 스토리지, 네트워킹을 포함 클릭 한 번으로 모든 과정을 생략, 쉽게 VPS를 생성 및 관리 확장가능 및 타 AWS Services에 접근 가능 고가용성 어플리케이션 생성 가능 저렴하고, 비용의 예측이 보다 쉬움 완전히 사전구성되어 있는 서버 ( BluePrint ) Lightsail의 유료 Plan 월간 무료 데이터 허용량 초과시, 퍼블릭 IP 주소를 사용ㅎ여 요금 청구 Lightsail 스냅샷 비용 : 인스턴스 스냅샷 + 디스크 스냅샷 1시간 이상 인스턴스에 연결되어 있지 않은 고정 IP ( Elastic IP ) 무료로 주어지는 3백만 개의 쿼리를 초과하는 경우 Lightsail\u0026 EC2 Lightsail의 주 사용용도 웹 사이트 및 블로그 단순 앱 개발 및 테스트 환경 소수의 서버로 구성된 비즈니스 소프트웨어 EC2 빅데이터 분석 고성능 컴퓨팅 과학 분야 컴퓨팅 멀티’티어 애플리케이션 ECS ( Elastic Container Service ) AWS의 Virtual Machine, VM (가상 머신) 가상의 컴퓨터, 하나의 호스트에 안에 또다른 호스트를 만들어 사용하는 것 CPU, Memory와 같은 주요 하드웨어 부품을 소프트웨어로 완전 재현해내어 기능을 흉내내게 하고(에뮬레이션), 격리된 실행 환경(OS)를 만듬 즉 하드웨어를 직접 가상화 클러스터에서 도커 컨테이너를 손쉽게 관리하기 위해 컨테이너 관리 서비스 클러스터는 Task(작업) 또는 서비스로 일컬어지는 컨테이너들의 집합 2가지 구성 요소로 시작 가능 EC2(Container Instance) : EC2를 생성하여 EC2 내에 Task(컨테이너가 수행할 작업) Fargate : EC2를 생성하거나 컨테이너를 실행하기 위한 Orchestration을 AWS가 맡아 하는 서비스로, 관리가 용이함 하나의 클러스터 내에 다수의 Task 혹은 컨테이너 인스턴스로 구성됨 또한 ELB, EBS 볼륨, VPC, IAM과 같은 기능을 사용하여 구성 가능 즉 ECS 각 작업의 권한, ECS 액세스를 IAM으로 조절하거나, EC2 유형의 컨테이너 인스턴스만이 OS에 액세스 가능한 특징 등을 갖게 됨 호스트의 OS(Operating System) 내에 또다른 실행환경의 OS가 존재함 윈도우 OS의 호스트 내에 리눅스, 우분투 등의 다양한 OS를 올릴 수 있음 다만 OS를 포함하기 때문에 용량을 많이 차지할 뿐더러, 사용자가 필요치 않는 기능까지 포함할 수 있으며 느림 Linux Container ECS를 사용하는 목적이자 관리 대상 하드웨어가 아닌 OS를 가상화하여 커널을 공유하며 프로세스(컨테이너와 비슷)를 격리된 환경에서 실행하는 것 VM와 달리 달리 호스트의 OS에서 가상화를 실시하여, 이 OS 위에 프로세스들이 ‘컨테이너’로서 격리된 환경에서 실행됨 호스트의 입장에선 컨테이너는 프로세스에 불과하지만, 컨테이너 입장에서는 독립된 실행환경임 OS를 포함하지 않는만큼 가볍고, 하드웨어를 가상화하지 않기 때문에 빠름 Kernel Operaintg System에서 가장 중요한 역할을 맡고 있는 핵심(核心) 커널이 각 프로세스(실행환경)에 하드웨어 자원(CPU 등)을 할당하고, 작업 스케쥴링(처리순서)를 담당하며, 프로세스 간 접근과 보안을 책임짐 과거에 커널이 없던 시절에도 컴퓨터는 존재할 수 있었으나 메모리를 초기화하기 위해서는 컴퓨터를 리부팅해야 하는 등, 자원관리/제어 주체의 필요성에 의해 탄생 Docker 앞서 설명한 Linux Container 기술에 근간을 두는 오픈소스 컨테이너 프로젝트 ‘Docker’라는 단어 자체가 ‘부두에서 일하는 노동자’, 즉 컨테이너를 관리하는 존재임을 뜻함 Linux Container 기술을 사용하는 솔루션이므로 별도의 OS를 설치하지 않고 컨테이너를 생성하여 애플리케이션을 실행함 컨테이너를 생성할 이미지(서비스에 필요한 리소스를 모아둔 최소한의 단위)를 기반으로 운영함 이미지만 가지고 있다면 어느 시점에서든 동일한 리소스의 컨테이너를 생성할 수 있음 그 밖에 컨테이너간의 연결, 다양한 API 제공 등의 기능을 보유 Lambda Serverless Service 서버를 구축, 프로비져닝하고 필요한 패키지를 설치하는 등의 과정을 거치지 않고, 코드를 실행하는 서비스 사용자는 애플리케이션이나 백엔드 서비스를 관리할 필요 없이 코드를 실행할 수 있음 CloudWatch, ALB, DynamoDB 등을 트리거로 이용하여 특정 상황에서 코드를 실행시키고 것이 가능 API Gateway와 Lambda를 조합하여 요청별로 특정 코드를 수행하도록 구성 가능 15분을 초과하는 작업에 대해서는 Lambda 비적합 Lamda Function의 정의와 구성 코드를 실행하기 위해 호출할 수 있는 리소스 이벤트를 처리하는 코드, 계층, 트리거, 전달 대상 등으로 구성됨 함수코드 : 실제 호출되기 실행되는 코드, Runtime(코드 실행지원), IAM, VPC, Memory 등으로 구성됨 트리거 : 함수코드를 발동시키는 서비스(S3, SNS, SQS, DynamoDB, CloudWatch Event, Cloudwatch Log 등) SNS의 메시지 구독 대상에 Lambda를 포함시키면, 메시지 발송시 Lambda가 이를 전달받고 함수코드 실행 전달대상 : 함수가 비동기식으로 호출되거나, 레코드를 처리한 경우 전달될 대상 SNS, SQS, 또다른 Lambda, EventBridge 이벤트 버스로 전달가능 NS로부터 메시지를 전달받아 코드를 처리하고 이를 SQS로 보내 메시지 대기열에 적재할 수 있음 EC2 vs Lambda EC2 사용시 프로비져닝, 운영 체제, 네트워크 세부 설정, 보안 설정 등을 사용자가 원하는 방향으로 지정 가능 Lambda 사용시 프로비져닝 필요없이 AWS가 모니터링, 프로비져닝, 보안패치 적용, 코드 배포를 모두 수행함 Batch 종합 관리형 서비스 한 리전 내의 여러 가용 영역에 걸쳐 배치 작업을 실행하는 과정을 간소화하는 리전 서비스 새 VPC 또는 기존 VPC에서 컴퓨팅 환경을 생성할 수 있음 AWS Batch를 사용하면 AWS 클라우드에서 배치 컴퓨팅 워크로드를 실행이 가능 배치 컴퓨팅 : 다수의 사람들이 수 많은 컴퓨터 리소스에 엑세스 할 때 일반적으로 사용하는 방법 AWS Batch의 구성요소 작업 AWS Batch에 제출한 작업 단위 ( 쉘 스크립ㅌ, Linux 실행 파일, Docker 컨테이너 이미지 ) 작업에는 이름이 있으며, 파라미터를 사용하여 컴퓨팅 환경의 인스턴스에서 컨테이너화된 애플리케이션으로 실행 작업 정의 작업이 실행되는 방식을 지정하며 작업에 있는 리소스에 대한 블루프린트를 의미 IAM 역할을 제공하여 다른 AWS 리소스에 프로그래밍 방식으로 엑세스할 수 있으며 메모리 및 CPU 요구 사항을 모두 지정가능 작업 대기열 AWS Batch 작업이 대기열 예약되는 환경 우선 순위 갑 및 작업 대기열 전체에 우선 순위 할당 가능 컴퓨팅 환경 작업을 싱해하는 데 사용되는 관리형 또는 비관리형 컴퓨팅 리소스 세트 여러 세부 수준에서의 인스턴스 유형의 설정이 가능 Batch Group 클러스터 : 인스턴스를 AZ 내에서 근접하게 배치, 결합된 노드 간 낮은 지연 시간의 네트워크 달성 가능 파티션 : 인스턴스가 담긴 그룹을 논리 세그먼트로 나누어 각 파티션에 배치, 최대 7개의 파티션을 가질 수 있으며, 각 파티션은 자체 랙 세트를 보유하고 자체 네트워크 전원을 보유 분산 : 파티션이 논리 세그먼트로 분리된 인스턴스 그룹인 것과 달리 분산은 인스턴스 개체 하나가 자체 랙에 분산 배치됨, AZ당 최대 7개의 인스턴스 배치 가능 Elastic Beanstalk Java, NET, PHP, Node js, Python, Ruby, Go, Docker을 사용하여 Apache, Nginx, Passenger, llS와 같은 친숙한 서버에서 개발된 웹 어플리케이션 및 서비스를 간편하게 배포하고 조정 할 수 있는 서비스 EC2, ASG, RDS 등 AWS 리소스들을 조합하여 완성된 어플리케이션 플랫폼으로 PaaS의 일종 오토 스케일링, 로브 밸런싱, 버전 관리 등의 기능을 콘솔에서 몇 번의 클릭으로 생성 가능 실제 서비스가 아니기에 사용 요금이 없음 Elastic Beanstalk의 장점 간단한 서버 세팅 환경변수들을 쉽게 변경/ 관리가 가능 오토 스케일링이 용이 로그의 자동화 추가요금이 없음 ","#":"","aws-컴퓨팅-서비스#\u003cstrong\u003eAWS 컴퓨팅 서비스\u003c/strong\u003e":"","batch#\u003cstrong\u003eBatch\u003c/strong\u003e":"","ec2--elastic-compute-cloud-#\u003cstrong\u003eEC2 ( Elastic Compute Cloud )\u003c/strong\u003e":"","ecs--elastic-container-service-#\u003cstrong\u003eECS ( Elastic Container Service )\u003c/strong\u003e":"","elastic-beanstalk#\u003cstrong\u003eElastic Beanstalk\u003c/strong\u003e":"","lambda#\u003cstrong\u003eLambda\u003c/strong\u003e":"","lightsail#\u003cstrong\u003eLightsail\u003c/strong\u003e":""},"title":"AWS Computing"},"/system/aws/amazonwebservice/aws_database/":{"data":{"":"AWS DataBase Amazon RDS ( Relational Database Service ) 분산 관계형 데이터베이스 MariaDB, MySQL, PostgreSQL, Oracle 등을 AWS에서 제공해주는 것 애플리케이션 내에서 관계형 데이터베이스의 설정, 운영, 스케일링을 단순케 하도록 설계된 클라우드 내에서 동작하는 웹 서비스 데이터베이스 소프트웨어 패치하거나 데이터베이스를 백업하거나 시점 복구를 활성화하는 것과 같은 복잡한 관리 프로세스들은 자동으로 관리 스토리지와 연산 자원들을 스케일링 하는 것은 하나의 API 호출로 수행이 가능 관계형 데이터베이스를 AWS 상에서 사용할 수 있도록 지원하는 서비스 생성 후 서비스를 이용하기만 되므로 SaaS에 해당 MySQL, MariaDB, Postgre SQL, Oracle, MS SQL, Aurora 사용 가능 DB 인스턴스에 대한 shell 지원 불가 및 OS 제어 불가능 ( AWS 관리 ) 백업, 소프트웨어 패치, 장애 감지 및 복구를 AWS가 관리 Storage 용량에 대하여 Auto Scaling MariaDB, MySQL, Aurora는 서로 호환이 가능 DB Instance RDS의 기본 구성요소로서 클라우드에서 실행하는 격리된 데이터베이스 환경을 의미, 인스턴스 내에서는 여러 사용자가 만든 데이터베이스가 포함되며 엑세스할 여러 도구와 앱 사용 가능 DB 인스턴스도 EC2처럼 다양한 클래스를 가지고 있음 ( db.m5, db.r5 등 ) RDS도 클라우드에서 실행되기 때문에 하나의 AZ에서 격리되어 인스턴스로서 실행 DB Instance Storage 데이터베이스의 유지를 위패 EBS를 사용하며 필요한 스토리지 용량에 맞춰 자동으로 데이터를 여러 EBS 볼륨에 나누어 저장 스토리지의 유형 범용 SSD: 대부분의 워크로드에서 사용하는 기본적인 스토리지 프로비져닝 IOPS: 빠르고 일관적인 I/O 성능이 필요하고 일관적으로 낮은 지연시간이 요구될 경우 사용하는 스토리지 ( I/O input/ Output ) 마그네틱: 접속 빈도가 적은 워크로드에 적합한 스토리지 Multi-AZ RDS는 Multi-AZ라는 기능을 통해 고가용성을 지원 ( 다수의 AZ에 DB 인스턴스를 둠으로써 하나 혹은 그 이상의 AZ가 파괴되어 서브시가 불가능 할 때를 대비 ) 기본 인스턴스가 수행해야할 작업( 백업, 스냅샷 생성 ) 등을 대신하여 수행함으로서 기본 인스턴스의 부담을 줄임 RDS도 클라우드에서 실행되기 때문에 하나의 AZ에서 격리되어 인스턴스로서 실행 기본 인스턴스에서 스냅샷을 캡쳐한 후 다른 AZ에 복원하여 ‘동기식’ 예비 복제본을 생성 Active( AZ A )-Standby ( AZ B, C ) 구조를 형셩한 후 지속적으로 동기화 ‘ 예비 ‘ 복제본이기 때문에 읽기 및 쓰기 작업을 수행할 수 없음 Multi-AZ를 사용하는 경우, 단일 AZ 배포에 비해 쓰기 및 저장 지연 시간이 길어질 수 있음 ( 동기화 문제 ) Multi-AZ Multi-AZ를 활성화한 상태에서 DB 인스턴스에 문제가 발생하면 자동으로 다른 AZ의 예비 복제본 ( Standby )로 전환하며 서비스를 이어나감 전환에 사용되는 시간은 60- 120초 전환되는 상황 가용 영역( AZ ) 중단 기본 DB 인스턴스 오류 DB 인스턴스 서버 유형 변경 기본 DB 인스턴스 OS에서 소프트웨어 패치 실시 장애 조치 재부팅( Failover ) 실시 DB Instance Storage 데이터베이스의 유지를 위패 EBS를 사용하며 필요한 스토리지 용량에 맞춰 자동으로 데이터를 여러 EBS 볼륨에 나누어 저장 Read Replica 읽기 전용의 복제본, 기본 DB 인스턴스가 읽기와 쓰기를 담당한다면 Read Replica는 읽기 작업만을 담당하여 마스터 DB 인스턴스의 부하를 줄임 우선 DB 마그네틱: 접속 빈도가 적은 워크로드에 적합한 스토리지 Automated Backup RDS의 자동백업으로 개별 데이터베이스를 백업하는 것이 아닌 DB 인스턴스 전체를 백업하는 것 매일매일 백업이 이루어지며, 기본 보존기간은 CLI로 생성시 1일\u0026 콘솔로 생성시 7일이며 최저 1일부터 35일 까지 가능 특정시점을 지정하여 복원가능하며 복원 기간내로부터 최근 5분까지 특정시점을 지정하여 복원 가능 사용자가 백업시간에 자동적으로 백업되며, 백업 중에는 스토리지 I/O가 일시적으로 중단될 수 있음 ( Multi-AZ 사용시 Standby에서 백업 실시 ) 전환되는 상황 가용 영역( AZ ) 중단 기본 DB 인스턴스 오류 DB 인스턴스 서버 유형 변경 기본 DB 인스턴스 OS에서 소프트웨어 패치 실시 장애 조치 재부팅( Failover ) 실시 Enhanced Monitoring RDS의 지표를 실시간으로 모니터링하는 ‘ 강화된＇ 모니터링 모니터링 지표는 CloudWatchs Logs에 30일간 저장됨 일반 모니터링과의 차이점은 Enhanced Monitoring은 인스턴스 내 에이전트를 통해 지표를 수집하는 반면, 일반 모니터링은 하이퍼바이저에서 수집 ( 최대 1초 단위 ) RDS vs DB in EC2 EC2 위에 데이터베이스를 직접 올리는 만큼 설정을 마음대로 변경할 수 있고, 커스터마이징 또한 가능 RDS와는 반대로 백업과 패치 등 관리를 직접해야 함 EC2에 설치하는 것이기에 SSH 접속 가능 Amazon DynamoDB 종합 관리형 NoSQL 데이터베이스 서비스로, 원할환 확장성과 예측 가능한 성능을 제공 데이터 규모에 관계없이 데이터를 저장 및 검색하고, 어떤 수준의 요청 트래픽이라도 처리할 수 있는 데이터베이스 테이블의 생성이 가능 배포가 단순하고 신속, 설계를 해서 데이터베이스의 적용까지 많은 시간이 소요되지 않음 확장이 단순하고 신속, 단순한 인터페이스의 유리 온 디맨드 백업기능 제공 DynamoDB의 특징 배포가 단순하고 신속 확장이 단순하고 신속, 수백만 IOPS 데이터는 자동으로 복제되어 있음 빠르고 일관된 응답시간, SSD, 10밀리초 미만 보조 인덱스를 통한 빠른 조회 사용한만큼 지불, 저장소 및 프로비저닝된 처리용량 Amazon ElastiCache Cache Cache는 CPU 칩 안에 들어가 있는 작은 메모리 ( 물리적 실체 ) 프로세서가 필요한 데이터가 있을 때마다 메인 메모리에 일일이 접근하여 속도가 지연되는 것을 막기 위해 자주 사용하는 데이터를 담아두는 곳 즉 처리 속도 향상을 위해 존재하는 작은 칩이자 메모리 L1,L2,L3로 나뉘며 숫자가 적을 수록 도달하는 속도가 빠름 Cache는 CPU와 메모리 사이 뿐만 아니라, 메모리와 디스크 사이에서도 발생함 후술할 In Memory Cache는 메모리와 디스크 사이의 Caching을 의미 In Memory Cache ( In Memory DataBase ) 데이터 처리 속도를 향상시키기 위한 메모리 기반의 DBMS 메모리 위에 모든 데이터를 올려두고 사용하는 데이터베이스의 일종( ElastiCache가 AWS 카테고리에서 DB 부분에 있는 이유 ) 디스크에 최적화된 Database ( RDS 등 ) 에서 저장된 쿼리 결과나 자주 사용하는 데이터를 메모리에 적재하여 사용하는 것은 비효율적 즉 모든 데이터를 메모리 위에 올려두어 굳이 디스크 기반의 데이터베이스에까지 이동하여 데이터를 가져와 속도가 저하되는 것을 막음 데이터베이스의 데이터뿐만 아니라, 디스크, 세션, 기타 동적으로 생성된 데이터를 저장할 수 있음 메모리 기반의 데이터베이스이기 때문에, 휘발성 메모리라는 단점이 존재하며 전원 공급 차단시 모든 데이터가 유실되고 할당된 메모리에 한해 저장 가능 ElastiCache AWS의 In Memory Cache Service Memcached와 Redis로 나뉨 Memached, Redis 모두 비관계데이터베이스형(NosQL) 서비스이며, Key-value 기반임 Memached, Redis 모두 이미 존재하는 서비스이며 AWS에서 사용가능하도록 구현한 것 ElastiCache는 Node로 구성되어 서비스를 제공하며, Node는 EC2처럼 다양한 Type을 가지고 유형에 따라 다양한 메모리 크기를 가짐 다양한 Type을 갖는 이유는 적은 양의 메모리가 필요할 경우, 작은 Type의 Node를 사용하여 비용을 적게 들게 하기 위함 유형이 결정된 Node들은 ‘고정된’ 메모리 크기를 가지며, 각자의 DNS로 이루어진 엔드포인트를 보유함 Memcache Cluster로 구성되어 있으며, Cluster 내에는 Node들이 존재하여 인 메모리 캐시로서의 역할을 담당함 각 Node는 Type별로 메모리를 보유하며 서비스를 제공하며, 필요시 Node를 늘려 서비스 용량을 향상시킬 수 있음 각 Node별로 AZ를 따로 둘 수 있지만, 장애 조치(Failover)가 불가능하고 복제본을 둘 수 없음 Redis의 특징 기본적으로 Cluster로 구성되지는 않지만, Cluster로 구성이 가능하며 Shard와 Node를 가지고 있음 Shard는 여러 Node로 구성되며, 하나의 Node가 읽기/쓰기를 담당하고 나머지 Node는 복제본 역할을 함 Cluster로 구성되지 않은 Redis는 하나의 Shard만을 가지지만, Cluster로 구성될 경우 다수의 Shard를 갖게 됨 복제본을 가지므로, 장애조치(복제본을 기본 Node로 승격)가 가능하며 Multi-AZ 기능을 지원함 Amazon Redshift Redshift PostgreSQL를 기반으로 하는 AWS의 Data Warehouse Service 모든 데이터를 표준 SQL 혹은 BI 도구를 사용하여 효율적으로 분석할 수 있도록 지원 대량 병렬처리(MPP)를 통해 복잡한 쿼리라도 빠른 속도로 실행하여 대용량 처리 가능 열(Column) 단위 데이터 저장방식 COPY 명령어를 통해 Amazon EMR, Amazon DynamoDB, S3로부터 데이터를 병렬 로드 가능 Enhanced VPC Routing을 통해 클러스터와 VPC 외부의 COPY, UNLOAD 트래픽을 모니터링할 수 있음 WLM(Workload Management)를 통해 사용자가 작업 부하 내 우선 순위를 유연하게 관리하도록 지원 보존기간이 1일인 자동 백업을 지원하며, 최대 35일까지 설정 가능 단일 AZ 배포만을 지원함 Redshift의 구성 클러스터 : Redshift의 핵심 요소로, 하나의 리더 노드와 다수의 컴퓨팅 노드를 가지고 있는 구성 요소 리더 노드 : 클라이언트 프로그램과 일어나는 통신을 비롯해 컴퓨팅 노드간의 모든 통신/작업 관리 컴퓨팅 노드 : 실제 작업을 수행하는 노드로, 각 노드마다 전용 CPU와 메모리 내장 디스크 스토리지를 따로 보유함 Data Warehouse(DW) 하나의 통합된 데이터 저장공간으로서, 다양한 운영 환경의 시스템들로부터 데이터를 추출, 변환, 통합해서 요약한 데이터베이스 데이터베이스가 관련 있는 업무 데이터는 잘 저장하나, 저장된 데이터들을 제대로 활용하지 못 하는 것에서 착안 기본적으로 관계형 데이터베이스가 있는 상태를 가정하여 DW를 구성하며, 동영상이나 음악처럼 DB에 저장할 수 없는 파일도 필요한 부분을 추출하여 보여주어야 함 ETL(Extract, Tranform, Load) 데이터를 추출하고, 변형하여, (Data Warehouse에) 적재하는 과정을 일컫는 말 BI(Business Intelligence) 데이터 추출/통합/리포팅을 위한 기본도구 집합, DW에서 분석된 데이터를 통해 숨겨진 패턴을 찾아냄 == \u003e ETL을 통해 뽑아낸 데이터를 DW에 적재하고, BI를 이용하여 분석하는 기본 과정을 거침 Redshift vs RDS Redshift는 보고 및 분석에 사용되지만, RDS는 OLTP(온라인 트랜잭션) 워크로드에 사용 Redshfit는 대용량 데이터 세트를 대상을 복합적인 분석 쿼리를 빠르게 실행하는 것에 목표를, RDS는 단일 행 트랜잭션에 목표를 둠 Amazon Aurora 클라우드에서 데이터베이스를 처음부터 설계하자는 생각에서 출발한 DB 서비스 MySQL과 PostgreSQl과 호환이 가능 각 AZ마다 2개의 데이터 복사본을 자동으로 유지하며, 에러를 스스로 찾아내고 복구 Read Replica는 다른 DB 서비스와 달리 최대 15개 까지 가능하며, 백업과 스냅샷이 퍼포먼스에 영향을 주지 않음 ","#":"","amazon-aurora#\u003cstrong\u003eAmazon Aurora\u003c/strong\u003e":"","amazon-dynamodb#\u003cstrong\u003eAmazon DynamoDB\u003c/strong\u003e":"","amazon-elasticache#\u003cstrong\u003eAmazon ElastiCache\u003c/strong\u003e":"","amazon-rds--relational-database-service-#\u003cstrong\u003eAmazon RDS ( Relational Database Service )\u003c/strong\u003e":"","amazon-redshift#\u003cstrong\u003eAmazon Redshift\u003c/strong\u003e":"","aws-database#\u003cstrong\u003eAWS DataBase\u003c/strong\u003e":""},"title":"AWS Database"},"/system/aws/amazonwebservice/aws_developer/":{"data":{"":"AWS_Developer AWS CodeBuild AWS CodeBuild는 소스 코드를 컴파일하는 단계부터 테스트 실행 후 소프트웨어 패키지를 개발하여 배포하는 단계까지 마칠 수 있는 완전관리형의 지속적 통합 서비스 CodeBuild는 지속적으로 확장되며 여러 빌드를 동시에 처리 사전 패키징된 빌드 환경을 사용하면 신속하게 시작할 수 있으며 혹은 자체 빌드 도구를 사용하는 사용자 지정 빌드 환경제작 가능 AWS CodeBuild는 코드를 실행하고 아티팩트를 Amazon S3 버킷에 저장 CodeBuild에서는 사용한 컴퓨팅 리소스에 대한 분당 요금이 청구 AWS CodeCommit AWS CodeCommit은 안전한 Git 기반 리포지토리를 호스팅하는 완전관리형 소스 제어 서비스 뛰어난 확장성의 안전한 에코시스템에서 여러 팀이 협업하여 코드 작업을 수행가능 CodeCommit을 사용하면 소스 코드에서 바이너리까지 모든 항목을 안전하게 저장할 수 있고 기존 Git 도구와 원활하게 연동 에코시스템\n자연계의 생태계처럼 관련 기업이 협력하여 공생하는 시스템을 의미 IT 분야의 여러 기업이 몇몇 리더 기업을 중심으로 경쟁과 협력을 통해 공생하고 함께 발전해 나가는 모습을 지칭 AWS CodeDeploy AWS에서 제공하는 배포 자동화 서비스 EC2 인스턴스들에 코드를 배포하는 과정을 자동으로 진행시켜 줌 카피스트라노 ( Capistrano )나 젠킨스 ( Jenkins ) 같은 서드파티 배포 자동화도구 보다 AWS 내 다양한 서비스와 손쉽게 연동이 가능하다. CodeDeploy는 무중단 배포 기법들인 IDP/ BGD를 둘다 지원한다. CodeDeploy란 단순히 명령어를 적어두고 프로그램이 그 명령을 순차적으로 실행하는 것 뿐이다. 단순히 우리가 해주는 일을 대신 해주는 Auto Scaling과 같은 개념 CodeDeploy로 배포하고자 한다면 EC2 인스턴스에 반드시 설치되어 있어야 하며 *.yml파일에 있는 절차를 따라서 배포를 진행한다. CodeDeploy 구성요소 vesion: 0.0 os: linux # 윈도우, 리눅스 등 어떤 OS를 위한 배포 파일인지를 명시한다. # CodeDeploy Agent는 배포 명령을 받으면 코드 저장소에 있는 프로젝트 전체를 서버의 임시 결로로 내려 받는다. # 내려받은 프로젝트를 서버 내 어느 경로로 이동시킬지 명시할 수 있다. files: - source: / destination: /var/www # AppSpec.yml에서는 배포 시 발생하는 다양한 생명주기마다 원하는 스크립트를 실행할 수 있게 후크를 제공해준다. # 배포 시 사용하는 스크립트들은 훤하는 곳에 둬도 되며, 보통은 프로젝트에 AppSpec.yml 파일을 포함하듯이 함께 포함한다. # 이 예시에서는 프로젝트 최상단에 scripts라는 디렉터리를 만들어 그 안에 스크립트들을 보관해 둔다. hooks: # 코드 저장소에서 프로젝트를 낼받은 뒤 인스턴스 내 배포를 원하느 경로에 파일들을 옮기기 전이며, 예시에서 사용한 스크립트의 이름을 보면 리소스 데이터 번들을 압축 해제하는 것으로 추축할 수 있다. BeforeInstall: - location: scripts/UnzipResourceBundle.sh - location: scripts/UnzipDataBundle.sh # 파일을 모두 이동한 후 실행되는 스크립트들이다. # 파일 이름을 봐서 리소스 파일들이 제대로 존재하는 지 테스트하는 것으로 추측할 수 있다. # 또한 Timeout 옵션을 두어 180초 이내에 스크립트가 완료되지 않으면 배포에 실패한 것으로 간주한다. AfterInstall: - location: scripts/RunResourceTests.sh timeout: 180 # 애플리케이션을 시작할 때 사용하는 스크립트들이다. # 예시에서는 서버를 재시작하고 최대 240초 동안 기다리는 것을 알 수 있다. ApplicationStart: - location: scripts/RestartServer.sh timeout: 240 # 서비스를 재시작한 후 실제로 서비스가 올바르게 실행됐는 지 확인 할 때 사용한느 스크립트들이다. # runas 옵션을 주어 기본 사용자인 ec2-user가 아닌 codedeployuser라는 다른 user로 실행하게 했다. ValidateService: - location: scripts/ValidateService.sh timeout: 30 runas: codedeployuser 스크립트 파일들에 실행 권항을 추가해서 Git에 올리고 싶다면 다음과 같은 명령어를 이용하면 된다. git update-index --chmod=+x \u003c스크립트 파일 이름\u003e CodeDeploy 작동절차 AppSpec.yml 파일을 추가한 후, 프로젝트를 코드 저장소인 GitHub 혹은 AWS S3에 업로드한다. CodeDeploy에 프로젝트의 특정 버전을 배포해 달라 요청한다. CodeDeploy는 배포를 진행할 EC2 인스턴스들에 설치되어 있는 CodeDeploy Agent들과 통신하며 Agent들에게 요청받은 버전을 배포해 달라고 요청한다. 요청을 받은 CodeDeploy Agent들은 코드 저장소에서 프로젝트 전체를 서버로 내려받는다. 그리고 내려받은 프로젝트에 있는 AppSpec.yml 파일을 읽고 해당 파일에 적힌 절차대로 배포를 진행한다. CodeDeploy Agent를 배포를 진행할 후 성공/ 실패 등 결과를 CodeDeploy에게 전달한다. AWS CodePipeling AWS CodePipeline은 빠르고 안정적인 애플리케이션 및 인프라 업데이트를 위해 릴리스 파이프라인을 자동화하는 데 도움이 되는 완전관리형 지속적 전달 서비스 코드 변경이 발생할 때마다 사용자가 정의한 릴리스 모델을 기반으로 릴리스 프로세스의 빌드, 테스트 및 배포 단계를 자동화 AWS CodePipeline을 GitHub 또는 자체 사용자 지정 플러그인과 같은 타사 서비스와 손쉽게 통합가능 사용한 만큼만 비용을 지불합니다. 선결제 금액이나 장기 약정이 존재하지 않음 AWS X-Ray AWS X-Ray는 개발자가 마이크로 서비스 아키텍처를 사용해 구축된 애플리케이션과 같은 프로덕션 분산 애플리케이션을 분석하고 디버그하는 데 도움을 주는 서비스 X-Ray를 사용해 자신이 개발한 애플리케이션과 기본 서비스가 성능 문제와 오류의 근본 원인 식별과 문제 해결을 올바로 수행하는지 파악가능 X-Ray는 요청이 애플리케이션을 통과함에 따라 요청에 대한 엔드 투 엔드 뷰를 제공하고 애플리케이션의 기본 구성 요소를 맵으로 제시 3-티어 애플리케이션에서부터 수천 개의 서비스로 구성된 복잡한 마이크로 서비스 애플리케이션에 이르기까지 개발 중인 애플리케이션과 프로덕션에 적용된 애플리케이션 모두 분석가능 ","#":"","aws-codebuild#\u003cstrong\u003eAWS CodeBuild\u003c/strong\u003e":"","aws-codecommit#\u003cstrong\u003eAWS CodeCommit\u003c/strong\u003e":"","aws-codedeploy#\u003cstrong\u003eAWS CodeDeploy\u003c/strong\u003e":"","aws-codepipeling#\u003cstrong\u003eAWS CodePipeling\u003c/strong\u003e":"","aws-x-ray#\u003cstrong\u003eAWS X-Ray\u003c/strong\u003e":"","aws_#\u003cstrong\u003eAWS_Developer\u003c/strong\u003e":"","codedeploy-작동절차#\u003cstrong\u003eCodeDeploy 작동절차\u003c/strong\u003e":""},"title":"AWS Developer"},"/system/aws/amazonwebservice/aws_management/":{"data":{"":"","#":"AWS Management Amazon CloudWatch AWS 클라우드 리소스와 AWS에서 실행되는 애플리케이션을 위한 모니터링 서비스 리소스 및 애플리케이션에 대해 측정할 수 있는 변수인 지표를 수집하고 추적 가능 사용중인 모든 AWS 서비스에 대한 지표가 자동적으로 표시디며, 사용자 지정 대시보드를 통해 사용자 지정 애플리케이션에 대한 지표를 표시하고 지정 집합 표시 가능 지표는 Cloudwatch에 게시된 시간 순서별 데이터 요소 세트이며, 모니터링할 변수 ( 가령 EC2의 CPU 사용량은 EC2가 제공하는 하나의 지표 ) 기본 모니터링과 세부 모니터링으로 나뉘며, 가각 5분과 1분 주기로 수집함 기본 모니터링은 자동활성화이지만, 세부 모니터링은 선택사항 기본적으로 CPU, Network, Disk, Status Check 등을 수집 ( Memory 항목이 없음 ) 지표 데이터의 보존기간 기간 60초 미만의 경우, 3시간 기간 60초의 경우, 15일 기간 300초의 경우 63일 기간 3600초의 경우, 455일 AWS CLI 혹은 API를 이용하여, Cloudwatch에 사용자 정의 지표 게시 가능 경보기능을 사용하여 어떤 지표가 일정기간동안 일정값에 도달할 경우 각 서비스가 취해야할 행동을 정의할 수 있음 모니터링하기로 선택한 측정치가 정의한 임계값을 초과할 경우 하나 이상의자동화 작업을 수행하도록 구성 EC2의 경우, 경보에 따라, 인스턴스 중비, 복구, 종료, 재부팅 가능 Cloudwatch Agent EC2에 Agent를 설치하게 되면 더 많은 시스템 수준 지표를 수집할 수 있음 온프레미스 서버 또한 Cloudwatch Agent 사용 가능 Memory 항목 포함 Cloudwatch Agent는 로그를 수집할 수 있으며, 후술할 Cloudwatch Logs 기능 사용 가능 Cloudwatch Logs EC2( Agent에서 수집된 ), CloudTrail, Route 53, Route 53, VPC flow Log 등 기타 소스에서 발생한 로그 파일을 모니터링, 저장 및 엑세스하는 기능 Cloudwatch Agent를 사용하여 로그를 수집 Cloudwatch Log Insights를 사용하여 CloudWatch Logs에서 로그 데이터를 대화식으로 검색해 분석할 수 있음 Agent는 기본적으로 5초마다 로그 데이터를 전송 Cloudwatch Events AWS 각 서비스의 이벤트가 사용자가 지정한 이벤트패턴과 일치하거나 일정이 트리거될 경우, 사용자가 월하는 기능을 발동시키도록 하는 기능 이번트 소스와 대상으로 나뉨 이벤트 소스: AWS 환경에서 발생하는 이벤트이며, 가령 S3의 경우 오브젝트 등록, 삭제 등을 들 수 있음 대상: 이벤트 발생시 해야할 행동을 정의하는 것이며, SNS 전송 혹은 람다, SQS 게시 등을 설정할 수 있음 이벤트 소스에 해당하는 규칙이 트리거될 경우 대상에 해당하는 서비스를 실행시킴 이벤트가 시스템에 생성해 둔 규칙과 일치하는 경우, AWS Lambda 함수를 자동으로 호출하고, 해당 이벤트를 Amazon Kinesis 스트림에 전달하고, Amazon SNS 주제를 알림’ having 1=1## AWS CloudFormation 인프라를 관리 간소화를 목적으로 하는 서비스 AWS의 리소스를 일일이 설정하지 않고 해당 서비스의 프로비져닝과 설정을 미리 구성하여 반복작업을 줄이도록 도와주는 역할 EC2, Auto Scaling Group으로부터 ELB, RDS, S3 등을 사전에 구성하여 한 번의 클릭으로 다수의 서비스를 빠르게 생성할 수 있음 생성된 리소스 모음은 다른 계정 혹은 다른 리전에 옮겨 사용 가능 Stack 하나의 단위로 관리할 수 있는 AWS 리소스들의 모음 스택을 생성, 업데이트 또는 삭제하여 리소스 모음을 생성, 업데이트, 삭제가 가능 스택에서 실행중인 리소스를 변경해야 하는 경우 스택을 업데이트할 수 있는 데, 이 업데이트된 세트를 변경세트라 칭함 스택을 삭제하는 경우 삭제할 스택을 지정하면 해당 스택과 스택 내 모든 리소스를 삭제 AWS에서 리소스를 삭제할 수 없는 경우 스택이 삭제 스택의 리소스 중 하나라도 성공적으로 생성되지 않은 경우 성공적으로 생성한 모든 리소스를 모두 삭제함 ( Automatic rollback on error ) Template 스택을 구성하는 AWS 리소스를 JSON 혹은 YAML 형식으로 선언한 텍스트 파일\n템플릿은 로컬 혹은 S3에 저장되며, 템플릿을 불러올 때 S3 bucket을 지정할 수 있음\n템플릿을 “Designer\"을 통해 생성할 수도 있으며, S3 bucket에 저장된 것을 불러와 생성이 가능\nTemplate의 여러 요소 Parameter : 선택 섹션, 스택 생성 및 업데이트 시 템플릿에 전달하는 값, 사용자가 선택하는 여러 요소들 ( EC2 유형 - t2.micro 등 )\nConditions : 선택 섹션, 조건문, 리소스가 생성되는 조건을 만들어 조건 충족시에만 리소스를 만들 수 있또록 하는 요소\nResources : 필수 섹션, CLoudformation에 포함될 리소스\nMetadata : 선택 섹션, 템플릿에 대한 세부 정보를 제공하는 임의의 JSON, YAML 객체\nMappings : 선택 센셕, 프로그래밍 언어로 따지만 ‘Switch’ 조건문에 해당하며, ‘키’에 해당하는 값 세트를 생성하고 해당하는 키가 있으면 값 세트에 맞춰 리소를 생성\nAWS CloudTrail CloudTrail의 이벤트는 AWS 계정에서의 활동 기록을 의미 용자, 역할 또는 CloudTrail에서 모니터링이 가능한 서비스에 의해 수행되는 작업, AWS Management 콘솔, AWS SDK, 명령줄 도구 및 기타 AWS 서비스를 통해 수행되는 API 계정 활동과 비 API 계정 활동 모두에 대한 기록을 제공 CloudTrail에는 로깅할 수 있는 두 가지 유형의 이벤트가 존재 관리 이벤트 : 기본적으로 로깅 데이터 이벤트 : 기본적으로 로깅을 하지 않음 관리 이벤트와 데이터 이벤트 모두 동일한 CloudTrail JSON 로그 형식을 사용 관리 이벤트 AWS 계정의 리소스에 대해 수행되는 관리 작업에 대한 정보를 제공하며, 이를 제어 영역 작업이라 함 보안 구성 디바이스 등록 데이터 라우팅 규칙 구성 로깅 설정 데이터 이벤트 데이터 이벤트는 리소스 상에서, 또는 리소스 내에서 수행되는 리소스 작업에 대한 정보를 제공하며, 이를 데이터 영역 작업이라 함 Amazon S3 객체 수준 API활동 AWS Lambda 함수 실행 활동 Insights events CloudTrail Insights 이벤트는 AWS 계정에서 비정상적인 활동을 캡처 Insights events을 활성화하고 CloudTrail가 비정상적인 활동을 감지한 경우, Insights events는 다른 폴더나 트레일에 대한 대상 S3 버킷의 접두사에 로깅 Insights events은 계정 API 사용량 변화가 계정의 일반적인 사용 패턴과 크게 다르다는 것을 CloudTrail가 감지한 경우에만 로깅 CloudTrail 이벤트 기록 CloudTrail 이벤트 기록은 CloudTrail 이벤트에 대한 지난 90일간의 기록으로 확인, 검색 및 다운로드가 가능 AWS Management 콘솔, AWS SDK, 명령줄 도구 및 기타 AWS 서비스를 통해 수행되는 AWS 계정 활동에 대한 가시성을 확보가능 CloudTrail 콘솔에서 어떤 열이 표시되는지를 선택하여 이벤트 기록 보기를 사용자 지정가능 추적 추적은 Amazon S3 버킷, CloudWatch Logs 및 CloudWatch 이벤트에 CloudTrail 이벤트를 제공할 수 있는 구성 추적을 사용하여 제공하고자 하는 CloudTrail 이벤트를 필터링하고, AWS KMS 키로 CloudTrail 이벤트 로그 파일을 암호화하며, 로그 파일 제공을 위해 Amazon SNS 알림을 설정이 가능 조직 추적 조직 추적은 조직의 마스터 계정과 모든 멤버 계정의 CloudTrail 이벤트를 동일한 Amazon S3 버킷, CloudWatch Logs 및 CloudWatch 이벤트에 전달할 수 있도록 하는 구성을 의미 AWS Config AWS Config는 AWS 리소스 구성을 측정, 감사 및 평가할 수 있는 서비스 Config는 AWS 리소스 구성을 지속적으로 모니터링 및 기록하고, 원하는 구성을 기준으로 기록된 구성을 자동으로 평가 AWS OpsWorks AWS OpsWorks는 Chef 및 Puppet의 관리형 인스턴스를 제공하는 구성 관리 서비스 Chef 및 Puppet은 코드를 사용해 서버 구성을 자동화할 수 있게 해주는 자동화 플랫폼 OpsWorks를 사용하면 Chef 및 Puppet을 통해 Amazon EC2 인스턴스 또는 온프레미스 컴퓨팅 환경 전체에서 서버가 구성, 배포 및 관리되는 방법을 자동가 가능 AWS Managed Services AWS Managed Services(AMS)는 안전하고 규정을 준수하는 AWS Landing Zone을 제공하고 고객 대신 AWS를 운영하는 서비스 AWS Managed Services는 인프라를 유지 관리하기 위한 모범 사례를 구현하여 운영 오버헤드와 위험을 줄일 수 있도록 지원 AWS Managed Services는 변경 요청, 모니터링, 패치 관리, 보안, 백업 서비스 등과 같은 일반적인 활동을 자동화하고 인프라를 프로비저닝, 운영 및 지원하기 위한 전체 수명 주기 서비스를 제공 Landing Zone 검증된 엔터프라이즈 운영 모델이자 지속적인 비용 최적화 및 일상적인 인프라 관리 수단 AWS Service Catalog AWS Service Catalog를 사용하는 조직은 AWS에서 사용이 승인된 IT 서비스 카탈로그를 생성하고 관리하는 서비스 서비스에는 가상 머신 이미지, 서버, 소프트웨어 및 데이터베이스에서 멀티 티어 애플리케이션 아키텍처를 완성하는 모든 서비스가 포함 AWS Service Catalog를 사용하면 일반적으로 배포된 IT 서비스를 중앙에서 관리할 수 있고 일관된 거버넌스를 달성하고 규정 준수 요건을 충족하는 데 도움이 되는 동시에 사용자가 필요로 하는 승인된 IT 서비스만을 신속하게 배포가능 AWS Service Catalog의 핵심개념 Service Catalog 서비스 카탈로그는 하나의 AWS account에 종속됩니다. 관리자가 관리하며 하나 이상의 포트폴리오(Portfolios)를 포함 Administrtor 관리자는 서비스 카탈로그 안에 있는 프로덕트 포트폴리오(Portfolios of Products)를 업로드하고 관리 User 사용자는 서비스 카탈로그의 포털페이지를 접속하여 여러 포트폴리오를 찾아보고, 관심있는 프로덕트를 선택 Portal 포탈은 서비스 카탈로그의 창문(View)인데요, 특정 사용자가 접속할 수 있는 포트폴리오와 제품만 보여주도록 맞춤제작 가능 Portfolio 포트폴리오란 서비스 카탈로그 아래 버전관리 중인 프로덕트들의 모음 Product 프로덕트는 AWS리소스들의 모음 ( EC2 인스턴스, 애플리케이션 서버, 데이타베이스 )들로 이 단위 별로 프로덕트를 런치하고 관리 AWS TrustedAdvisor AWS Trusted Advisor는 AWS 모범 사례에 따라 리소스를 프로비저닝하는 데 도움이 되도록 실시간 지침을 제공하는 온라인 도구 AWS TrustedAdvisro의 분석 카테고리 ","amazon-cloudwatch#\u003cstrong\u003eAmazon CloudWatch\u003c/strong\u003e":"","aws-cloudformation#\u003cstrong\u003eAWS CloudFormation\u003c/strong\u003e":"","aws-cloudtrail#\u003cstrong\u003eAWS CloudTrail\u003c/strong\u003e":"","aws-config#\u003cstrong\u003eAWS Config\u003c/strong\u003e":"","aws-managed-services#\u003cstrong\u003eAWS Managed Services\u003c/strong\u003e":"","aws-management#\u003cstrong\u003eAWS Management\u003c/strong\u003e":"","aws-opsworks#\u003cstrong\u003eAWS OpsWorks\u003c/strong\u003e":"","aws-service-catalog#\u003cstrong\u003eAWS Service Catalog\u003c/strong\u003e":"","aws-trustedadvisor#\u003cstrong\u003eAWS TrustedAdvisor\u003c/strong\u003e":""},"title":"AWS Management"},"/system/aws/amazonwebservice/aws_migrate/":{"data":{"":"","#":"AWS Migrate AWS Application Discovery Service AWS Application Discovery Service는 서버로부터 구성, 사용 및 동작 데이터를 수집하여 제공함으로써 워크로를 효율적 관리를 도와주는 서비스 기업의 고객이 사내 데이터 센터에 대한 정보를 수집하여 마이그레이션 프로젝트를 계획하는 데 도움을 줌 데이터 센터 마이그레이션을 계획하는 작업에는 상호 의존성이 높은 수천 개의 워크로드가 수반 되어지는 짐 수집된 데이터는 AWS Application Discovey Service 데이터 스토어에 암호화된 형태로 보관되어짐 AWS Application Discovery Service의 이점 마이그레이션 계획 수립을 위한 신뢰할 수 있는 검색 Application Discovey Service는 서버 사양 정보, 성능 데이터, 실행 프로세스 및 네트워크 연결 세부 정보를 수집, 이러한 데이터는 AWS로 마이그레이션하기 전에 상세한 비용 추정을 수행하거나 계획을 위해 서버를 애플리케이션으로 그룹화하는 데 사용될 수 있음 Migration Hub와 통합 AWS Application Discovery Service는 AWS Migration Hub와 통합되므로 마이그레이션 추적이 간소화 및 Hub를 통한 마이그레이션 상태 추적이 가능 암호화로 데이터 보호 AWS Application Discovery Service는 수집한 데이터를 AWS로 전송할 때와 Application Discovery Service 데이터 스토어에 저장할 때 모두 암호화 마이그레이션 전문가의 지원 AWS Professional Services와 APN 마이그레이션 파트너는 수많은 엔터프라이즈 고객이 클라우드로의 마이그레이션을 성공적으로 완료하도록 지원 AWS DMS ( Database Migration Service ) AWS Database Migration Service는 데이터베이스를 AWS로 빠르고 안전하게 마이그레이션할 수 있도록 지원하는 서비스 이그레이션하는 동안 소스 데이터베이스가 변함없이 운영되어 해당 데이터베이스를 사용하는 애플리케이션의 가동 중지 시간을 최소화 AWS Database Migration Service는 Oracle에서 Oracle로의 마이그레이션과 같은 동종 마이그레이션뿐 아니라 Oracle 또는 Microsoft SQL Server에서 Amazon Aurora로의 마이그레이션과 같은 이기종 데이터베이스 플랫폼 간의 마이그레이션도 지원 데이터베이스를 Amazon Aurora, Amazon Redshift, Amazon DynamoDB 또는 Amazon DocumentDB(MongoDB 호환 가능)로 마이그레이션하는 경우 6개월 동안 DMS를 무료로 제공 AWS DMS의 이점 간편한 사용 AWS Management Console에서 클릭 몇 번으로 데이터베이스 마이그레이션을 시작 마이그레이션이 시작되면, 마이그레이션 프로세스 도중에 소스 데이터베이스에 발생한 데이터 변경을 자동으로 복제하는 것을 비롯하여 마이그레이션 프로세스의 모든 복잡성을 DMS에서 관리 최소한의 가동 중단 AWS Database Migration Service는 사실상 가동 중단 시간 없이 데이터베이스를 AWS로 마이그레이션하도록 지원 마이그레이션하는 동안 소스 데이터베이스에 발생한 모든 데이터 변경 사항은 지속적으로 대상 데이터베이스에 복제되므로, 마이그레이션하는 동안 소스 데이터베이스가 변함없이 운영 널리 사용되는 데이터베이스 지원 AWS Database Migration Service를 사용하면 가장 널리 사용되는 상용 및 오픈 소스 데이터베이스 플랫폼에서 또는 이를 대상으로 데이터를 마이그레이션 가능 저렴한 비용 마이그레이션 프로세스 중에 사용한 컴퓨팅 리소스와 추가 로그 스토리지에 대한 비용만 지불 라바이트 규모의 데이터베이스를 3 USD라는 저렴한 비용 빠르고 쉬운 설정 마이그레이션 태스크는 AWS Database Migration Service가 마이그레이션을 실행하는 데 사용할 파라미터를 정의하는 곳 마이그레이션 태스크에는 소스 및 대상 데이터베이스에 대한 연결 설정과 더불어 마이그레이션 프로세스를 실행하는 데 사용할 복제 인스턴스 선택이 포함 동일한 태스크를 사용하여 실제로 마이그레이션을 수행하기 전에 테스트를 실행가능 안정성 AWS Database Migration Service는 복원력과 자가 복구 기능 존재 소스 및 대상 데이터베이스, 네트워크 연결성 및 복제 인스턴스를 지속적으로 모니터링 AWS SMS ( Server Migration Service ) AWS Server Migration Service는 온프레미스 VMware vSphere, Microsoft Hyper-V/SCVMM 및 Azure 가상 머신을 AWS 클라우드로 자동으로 마이그레이션하는 서비스 AWS SMS는 서버 VM을 Amazon EC2에 바로 배포할 수 있는 클라우드 호스팅된 Amazon 머신 이미지(AMI)를 증분 방식으로 복제하는 서비스 AWS SMS의 이점 클라우드 마이그레이션 프로세스가 단순화 마이그레이션이 시작되면 AWS SMS은(는) 복잡한 마이그레이션 프로세스를 관리하여 라이브 서버 볼륨의 AWS로 복제하고 새로운 AMI를 정기적으로 생성하는 작업 등을 자동화 여러 서버 마이그레이션 조율 AWS SMS는 복제 일정을 예약하고 애플리케이션을 구성하는 서버 그룹에 대한 진행 상황을 추적할 수 있도록 하여 서버 마이그레이션을 조율가능 서버 마이그레이션 증분 테스트 증분 복제 지원 기능을 통해 AWS SMS은(는) 마이그레이션된 서버에 대한 테스트를 신속하게 수행하고 확장가능 AWS SMS은(는) 증분 변경 사항을 온프레미스 서버에 복제한 후 그 차이만 클라우드로 전송하기 때문에 일부 변경 사항만 반복적으로 테스트를 통해 절약 가능 가장 많이 사용되는 운영 체제 지원 Windows 및 대표적인 몇 가지 Linux 배포판을 포함하는 운영 체제 이미지 복제를 지원 가동 중지 최소화 증분 AWS SMS 복제는 최종 전환 중 애플리케이션 가동 중지로 인한 비즈니스 영향을 최소화 고객이 한도 증가를 요청하지 않는 한, 계정당 50개의 VM을 동시에 마이그레이션 VM의 최초 복제부터 시작하여 VM당(계정당 아님) 90일의 서비스 사용 기간. 고객이 한도 증가를 요청하지 않는 한, 90일 후에는 진행 중인 복제를 종료 정당 50개의 동시 애플리케이션 마이그레이션 ( 각 애플리케이션에 대해 그룹 10개 및 서버 50개 제한 ) AWS Snowball Edge AWS Snowball Edge는 데이터 마이그레이션 및 엣지 컴퓨팅 디바이스이며, 두 가지 옵션으로 제공 페타바이트급 대용량 데이터를 전송하기 위한 서비스 Snowball Edge는 특정 Amazon EC2 인스턴스 유형과 AWS Lambda 함수를 지원하므로 고객은 AWS에서 개발하고 테스트한 후 원격 위치의 디바이스에 애플리케이션을 배포하여 데이터를 수집, 사전 처리 및 반환가능 서비스와 더불어 물리적인 실체가 있는 장비가 존재하여 AWS에 요청하면 Snow ball를 배송받고 On-premise의 데이터를 빠르게 Snowball로 이동시킨 뒤, 작업이 완료되면 이 물리 장비를 다시 AWS로 배송하고 S3 Bucket에 저장함 스토리지 용량은 최대 80TB까지 저장 가능 Snowball 이외에 기능이 추가된 Snowball Edge가 사용하는 경우 페타바이트 규모의 데이터를 AWS로 이송하는 경우 적합 VPN, Direct Connect, S3를 통한 직접적인 전송을 이용하기엔 데이터의 양이 많을 경우 Snowball을 사용하는 것이 좋음 또한 물리적으로 격리된 환경이거나 인터넷 환경이 좋지 않을 경우 사용 평균적으로 AWS로 데이터를 업로드하는데 1주일 이상이 소요되는 경우 Snowball 사용을 검토함 AWS Snoball의 이점 용이한 데이터 이동 Snowball Edge는 약 1주 만에 테라바이트 규모의 데이터를 이동 네트워크 조건이 AWS에서 대규모 데이터를 송수신하는 데 현실적으로 적합하지 않은 경우, 이를 사용하여 데이터베이스, 백업, 아카이브, 의료서비스 레코드, 분석 데이터 세트, IoT 센서 데이터 및 미디어 콘텐츠를 이동 간편한 사용 AWS에서 사전에 프로비저닝된 Snowball Edge 디바이스를 고객 위치로 자동으로 배송 디바이스를 반환할 준비가 되면, 전자 잉크 선적 레이블이 자동으로 업데이트되고 화물 운송업체가 업로드가 시작되는 올바른 AWS 시설로 운송 로컬에서 데이터 처리 및 분석 EC2 AMI를 실행하고 AWS Lambda 코드를 Snowball Edge에 배포하여 기계 학습 또는 다른 애플리케이션을 통한 로컬 처리나 분석을 실행 개발자와 관리자는 네트워크 연결 없이 일관된 AWS 환경으로 디바이스에서 직접 애플리케이션을 실행가능 독립형 스토리지 Snowball Edge 디바이스는 NFS(파일 공유 프로토콜) 또는 객체 스토리지 인터페이스(S3 API)를 통해 기존 온프레미스 애플리케이션에 로컬 스토리지를 제공 보안 Snowball Edge 디바이스는 변조 방지 엔클로저, 256-비트 암호화, 그리고 데이터의 보안 및 관리의 연속성을 보장하도록 설계된 업계 표준 Trusted Platform Module(TPM)을 사용 확장성 Snowball Edge 디바이스는 테라바이트 규모의 데이터를 전송할 수 있으며, 여러 대의 디바이스를 병렬로 사용하거나 함께 클러스터링하여 AWS에서 페타바이트 규모의 데이터를 송수신 ","aws-application-discovery-service#\u003cstrong\u003eAWS Application Discovery Service\u003c/strong\u003e":"","aws-dms--database-migration-service-#\u003cstrong\u003eAWS DMS ( Database Migration Service )\u003c/strong\u003e":"","aws-dms의-이점#\u003cstrong\u003eAWS DMS의 이점\u003c/strong\u003e":"","aws-migrate#\u003cstrong\u003eAWS Migrate\u003c/strong\u003e":"","aws-sms--server-migration-service-#\u003cstrong\u003eAWS SMS ( Server Migration Service )\u003c/strong\u003e":"","aws-snowball-edge#\u003cstrong\u003eAWS Snowball Edge\u003c/strong\u003e":""},"title":"AWS Migrate"},"/system/aws/amazonwebservice/aws_network/":{"data":{"":"AWS Network Amazon VPC ( Virtual Private Cloud ) AWS 상에 프라이빗 네트워크 공간을 구축할 수 있는 서비스 VPC를 이용하면 논리적인 네트워크 분리가 가능하고 라우팅 테이블과 각종 게이트웨이의 설정이 가능 AWS의 계정 전용 가상 네트워크 서비스 VPC 내에서 각종 리소스 ( EC2, RDS, ELB 등 )을 시작할 수 있으며 다른 가상 네트워크와 논리적으로 분리되어 있음 S3, Cloudfront 등은 다른 VPC 서비스로 VPC 내에서 생성되지 않음 각 Region 별로 VPC 가 다수 존재할 수 있음 VPC 하나의 사설 IP 대역을 보유하고, 서브넷을 생성하며 사설IP 대역 일부를 나누어 줄 수 있음 허용된 IP 블록 크기는 /16( IP 65536개 )- / 28 (IP 16개 ) 권고하는 VPC CIDR 블록 ( 사설 IP 대역과 동일 ) 10.0.0.0- 10.255.255.255( 10.0.0.0/8 ) 172.16.0.0- 172.31.255.255( 172.16.0.0/12 ) 192.168.0.0- 192.168.255.255( 192.168.0.0/16 ) Region 리전이란 AWS가 서비스를 제공하는 거점 ( 국가와 지역 )을 나타냅니다. 이는 모두 같은 방법 ( AWS 매니지먼트 콘솔, SDK, CLI )로 사용이 가능하며, 이를 통해 해외의 특정 서비스에 인프라 구축이 필요할 경우 큰 장점이 될 수 있음. AWS에서 사용하는 일종의 IDC의 집합으로 거의 모든 클라우드 서비스가 탑재되는 것으로 다수의 Availability Zone( 가용영역 )으로 구성됨 한 곳의 AZ의 기능이 마비되어도 다른 AZ가 기능을 수행 전 세계 주요 대도시에는 분포되어있음 AWS 사용자는 각 Region 마다 별도의 클라우드 망을 구축할 수 있음 Availability zone 가용 영역은 데이터 센터와 같은 의미라고 할 수 있습니다. 중국을 제외한 각각의 리전에는 2개 이상의 AZ가 존재하며, AWS 사용자가 원하는 AZ를 선택해서 시스템을 구축하는 것이 가능합니다. 즉, On Premise 구성으로 구현하기 힘든 여러 개의 데이터 센터를 사용한 시스템 구성 ( 한 국가 내부의 DR ) 구성 등을 쉽게 구현할 수 있습니다. VPC Peering VPC 간의 트래픽을 전송하기 위한 기능 Source VPC와 같은 / 다른 리전의 VPC를 Destination으로 선택하여 Peering 요청을 보낸 후, 수락시 Peering 가능 요청과 수락이 필요한 이유는 다른 계정의 VPC도 연결 가능하기 때문 Peering 생성 후 라우팅 테이블에 해당 peering을 집어넣으면 통신 시작 VPC peering은 Transit Routing 불가 ( 재가의 VPC가 하나의 VPC를 통해 통신하는 것 ) VPC Endpoint VPC 내 요소들과 비 VPC 서비스( S3, CloudWatch, Athena 등 )을 외부인터넷을 거치지 않고 아마존 내부 백본 네트워크를 통해 연결하는 방법 그러므로 후술한 Direct Connect와 같은 전용선 서비스나 VPN, 인터넷 게이트웨이와 같은 외부 연결이 되어 있지 않는 서브넷에서 아마존의 여러 서비스를 연결가능 간단히 말하면 아마존 서비스 전용선 VPC 엔드포인트에는 Interface Endpoint, Gateway Endpoint 두 종류가 존재 Gateway Endpoint는 S3와 Dynamo DB만 가능 Subnet VPC 내 생성된 분리된 네트워크로 하나의 서브넷은 하나의 AZ ( Avaiability Zone ) 에 연결 VPC가 가지고 있는 사설 IP 범위 내에서 ‘서브넷’을 쪼개어 사용가능 실직적으로 리소스들을 이 서브넷에서 생성이 되며 사설 IP를 기본적으로 할당받고 필요에 따라 공인 IP를 할당받음 하나의 서브넷은 하나의 라우팅 테이블과 하나의 NACL( Network ACL ) 을 가짐 서브넷에서 생성되는 리소스에 공인 IP 자동할당 여부를 설정할 수 있음 이 기능을 통해 Public Subnet과 Private Subnet을 만들어 커스터마이징 가능 서브넷 트래픽이 후술할 인터넷 게이트웨이로 라우팅이 되는 경우 해당 서브넷을 Public Subnet, 그렇지 않은 서브넷의 경우 Private Subnet이라 함 각 서브넷의 CIDR 블록에서 4개의 IP 주소와 마지막 IP 주소는 예약 주소로 사용자가 사용할 수 없음, 예를 들어 서브넷 주소가 172.16.1.0/24일 경우 172.16.1.0: 네트워크 주소 ( Network ID ) 172.16.1.1: VPC Router용 예약 주소 ( Gateway ) 172.16.1.2: DNS 서버의 IP주소 172.16.1.3: 향 후 사용할 예약 주소 172.16.1.255: 네트워크 브로드캐스트 주소 VPN ( Virtual Private Network ) AWS의 IPSEC VPN 서비스 이 VPN을 통해 AWS와 On-premise의 VPN을 연결하는 것이 가능 고객 측 공인 IP를 뜻하는 ‘Customer Gateway’와 AWS 측 게이트웨이인 ‘Virtual Private Gateway’ 생성 후 터널을 생성하면 사용 가능 반드시 VPC에서 VPN 터널 쪽으로 라우팅을 생성해야 함 Direct Connect AWS의 데이터센터 및 오피스 네트워크와의 전용선 서비스 표준 이더네세 광섬유 케이블을 이용하여 케이블 한쪽을 사용자 내부 네트워크의 라우터에 연결하고 한 쪽을 Direct Connect 라우터에 연결하여 내부 네트워크 AWS VPC를 연결 보통 On-premise의 네트워크와 VPC를 연결할 때 사용 VPN보다 더 안전하고 빠른 속도를 보장 받고 싶을 때 사용 ( 백업 등 ) 전용선 열결\nVPC 사용한 Public Subnet\u0026 Private Subnet 생성 실습 Amazon CloudFront .html, .css, .js 및 이미지 파일과 같은 정적 및 동적 웹 콘텐츠를 사용자에게 더 빨리 배포하도록 지원하는 웹 서비스 전 세계에 배치된 Edge location을 이용하여 효율적인 컨텐츠 배포 구조를 제공하는 것 Cloud Front는 HTTP/ HTTPS를 이용하여 S3 및 ELB, EC2, 외부 서버 등을 캐시하고 보다 빠른 속도로 콘텐츠를 전달하는 캐시 서버 Distribution은 Edge Location의 집합을 의미 Edge Location은 주변 Origin Server의 콘텐츠를 Edge Location에 캐싱하고 각 Edge Location 간 공유를 통해 콘텐츠를 전달 S3, ELB, EC2 등의 AWS 서비스뿐만 아니라 외부의 서버도 캐싱 가능 ( Custom Orgin ) TTL을 조절하여 캐시 주기를 통제할 수 있음 AWS Direct Connect 온 프레미스에서 AWS로 전용 네트워크 연결을 쉡게 설정할 수 있는 클라우드 서비스 솔루션 AWS와 사용자 데이터 센터, 사무실 등의 환경 사이에 프라이빗 연결이 가능 Direct Connect의 이점 대역폭 비용 감소\n대역폭 사용량이 많은 워크로드를 AWS에서 실행하려는 경우, AWS에서는 데이터를 직접 송수신하므로, 인터넷 서비스의 대한 의존도를 줄일 수 있음 전용 연결을 통해 전송되는 데이터 요금은 인터넷 데이터 전송 요금이 ㅇ닌 보다 저렴한 AWS Direct Connect 데이터 전송 요금으로 부과되어짐 일관된 네트워크 성능\n데이터와 데이터의 라우팅 방식이 선택되면 인터넷 기반 연결에서 효율적인 네트워크 환경의 제공이 가능 모든 AWS 서비스와 호환 가능\nAWS Direct Connect는 네트워크 서비스의 일종으로, 인터넷을 통해 액세스 할 수 있는 모든 AWS 서비스와 연동 AMAZON VPC로 프라이빗 연결\nAWS Direct Connect를 사용하여 온프레미스 네트워크에서 직접 Amazon VPC로 프라이빗 가상 인터페이스를 설정함으로써 네트워크와 VPC 간에 네트워크 연결을 제공할 수 있음 여러 가상 인터페이스를 사용하면 네트워크 격리를 유지하면서 여러 VPC 프라이빗 연결을 설정할 수 있음 탄력성\nAWS Direct Connect를 사용하면 요구 사항에 맞게 연결을 용량을 손쉽게 조정이 가능 간편성\nAWS Direct Connect는 직접 설치하는 것이 아닌, 설정하는 것으로 간편함 AWS Route 53 AWS의 DNS 서비스 ( 도메인 등록, DNS 라우팅, Health check ) 도메인 등록시 약 12.000원 정도 지불해야 하며, 최대 3일 정도 걸림 해당 도메인을 AWS 내 서비스 ( EC2, ELB, S3 등 ) 와 연결 할 수 있으며 AWS 외 요소들과도 연결 가능 도메인 생성 후 레코드 세트를 생성하여 하위 도메인을 등록할 수 있음 레코드 세트 등록시에는 IP 주소, 도메인, ‘Alias’ 등을 지정하여 쿼리를 라우팅할 수 있음 도메인 레지스트라 서비스를 통해 도메인 구매부터 정보 설정까지 Route 53으로 한번에 관리가 가능합니다. 장애 허용 아케텍처를 통해 시스템에 이상이 발생한 경우, 일시적으로 다른 서버로 전환하는 것이 가능합니다. DNS ( Domain Name System ) DNS란 도메인네임서버를 일컫으며, 인터넷은 서버들을 유일하게 구분할 수 있는 IP주소 체계를 보다 인간이 읽게 쉽게 하기 위해 계발되었다. 흔히 우리가 알고 있는 naver.com, google.com, daum.net 모두 DNS이다. AWS에서는 Route 53을 활용해 도메인 서비스를 지원한다. Route 53의 라우팅 정책 Simple : 동일 레코드 내에 다수의 IP를 지정하여 라우팅 가능, 값을 다수 지정한 경우 무작위로 반환함 Weighted : Region 별 부하 분산 가능, 각 가중치를 가진 동일한 이름의 A 레코드를 만들어 IP를 다르게 줌 Latency-based : 지연 시간이 가장 적은, 즉 응답시간이 가장 빠른 리전으로 쿼리를 요청 Failover : A/S 설정에서 사용됨, Main과 DR로 나누어 Main 장애시 DR로 쿼리 Geolocation : 각 지역을 기반으로 가장 가까운 리전으로 쿼리 수행, 레코드 생성시 지역을 지정할 수 있음 Geo-proximity : Traffic flow를 이용한 사용자 정의 DNS 쿼리 생성 가능 Multi-value answer : 다수의 IP를 지정한다는 것은 simpl와 비슷하지만 health check가 가능 ( 실패시 자동 Failover ) AWS Route 사용방법 AWS 서비스에 route를 검색 후 Route 53을 선택한다. Route 53의 원하는 서비스를 선택한다. 각 서비스의 간략한 설명 도메인 등록 단순 도메인을 구입하여 등록한다. 트래픽 흐름 처리 트래픽 처리 등의 룰을 추가한다. DNS 관리 호스트 도메인을 등록한다 모니터링 서비스는 DNS 흐름 프로세스를 모니터링 하는 서비스 AWS 콘솔 SSL/ TLS 설치 AWS 서비스에서 certificate 검색 인증서를 만들 것인지 혹은 사설 인증기관을 사용할 것인지를 선택 프로비저닝이 선택할 경우 기존의 다른 업체에서 이미 발급받은 경우 Certificate Manager을 통해 등록이 가능하며, 무료로 발급도 가능 적용시킬 도메인 이름 선택 DNS 검증 : Certificate Manager에서 제시하는 특정 레코드를 추가해서 본임임을 인증 이메일 검증 : 해당 도메인의 관리자 계정으로 이메일을 보내서 본임임을 인증, 이 방법을 이용하기 위해서는 해당 도메인의 메일 서버에 연동되어 있어야 가능 검증방법 선택 요청이 완료되면 해당 도메인에 다음과 같은 이름과 값으로 CNAME 기록을 추가 해야 하며, AWS가 아닌 업체를 통해서 했다면 해당 업체의 사이트에서 레코드를 추가하면 되며, Route 53을 통해 자동으로 레코드 생성이 가능 발급한 인증서를 로드 밸런서의의 기존에 생성했던 리스너를 추가시킨 후, 인증서를 추가하면 추가가 완료된 것을 확인할 수 있으며, http://가 아닌 https:// 접속이 가능한 것을 확인할 수 있다. ","#":"","amazon-cloudfront#\u003cstrong\u003eAmazon CloudFront\u003c/strong\u003e":"","amazon-vpc--virtual-private-cloud-#\u003cstrong\u003eAmazon VPC ( Virtual Private Cloud )\u003c/strong\u003e":"","aws-direct-connect#\u003cstrong\u003eAWS Direct Connect\u003c/strong\u003e":"","aws-network#\u003cstrong\u003eAWS Network\u003c/strong\u003e":"","aws-route-53#\u003cstrong\u003eAWS Route 53\u003c/strong\u003e":"","aws-route-사용방법#\u003cstrong\u003eAWS Route 사용방법\u003c/strong\u003e":"","aws-콘솔-ssl-tls-설치#\u003cstrong\u003eAWS 콘솔 SSL/ TLS 설치\u003c/strong\u003e":"","vpn--virtual-private-network-#\u003cstrong\u003eVPN ( Virtual Private Network )\u003c/strong\u003e":"","각-서비스의-간략한-설명#\u003cstrong\u003e각 서비스의 간략한 설명\u003c/strong\u003e":""},"title":"AWS Network"},"/system/aws/amazonwebservice/aws_security/":{"data":{"":"","#":"AWS Security 기본적으로 AWS의 보안적 요소는 설비, 인프라 등과 관련된 물리적인 부분과 네트워크 인프라 등은 AWS가 책임을 지고 보안 대책을 수행합니다. 이러한 인프라 위의 OS, 애플리케이션, 네트워크 설정, 계정 관리 등은 사용자가 책임을 져야 하는 공유 책임 모델의 구조를 띄고 있습니다. AWS가 제공하는 기본적인 보안 서비스 AWS 제공 서비스 서비스 개요 통신 경로 암호화 AWS 매니지먼트 콘솔로의 접속 또는 API를 사용할 때 HTTPS를 사용해 데이터를 암호화합니다. Security Group\u0026 NetworkACL 인스턴스들의 보안 그룹과 서브넷들의 통신 허가/ 거부 설정을 하는 네트워크 ACL을 사용해 예상하지 못하는 통신을 사전차단합니다. Identity and Access Management( IAM ) 사용자의 역할을 분리하고 최소한의 권한만을 부여하여 보안을 유지합니다. Multi Factor Authentication ( MFA ) AWS 계정 또는 IAM 계정에 일회성 비밀번호 인증을 추가합니다. Virtual Private Cloud ( VPC ) 퍼블릭 클라우드에 프라이빗 환경을 구축합니다. 다른 거점에서 VPN으로 접속할 수도 있습니다. Direct Connect ( 전용선 연결 ) On Premise 환경에서 AWS 전용선을 통해 직접 연결하여 데이터 도청, 변조 등의 위험을 줄일 수 있습니다. 데이터 암호화 EBS, S3, Glacier, Redshift, RDS에 저장하고 있는 데이터 또는 객체를 암호화할 수 있습니다. Hardware Security Module ( CloudHSM ) 암호화 키를 안전하게 저장하고 관리합니다. Trusted Advisor AWS 지원이 제공하는 서비스 중에 하나로, 각종 서비스의 설정과 운용 상황을 확인해서 개선할 부분을 제공해줍니다. Amazon Inspector Amazon Inspector는 AWS에 배포된 애플리케이션의 보안 및 규정 준수를 개선하는데 도움이 되는 자동 보안 평가 서비스 모범 사례로부터 애플리케이션의 노출, 취약성 및 편차를 자동으로 평가 심각도 수준에 따라 우선순위가 지정된 상세한 보안 평가 결과 목록을 생성 Amazon EC2 인스턴스의 의도하지 않은 네트워크 접근성과 이 EC2 인스턴스의 취약성을 확인 Amazon Inspector 평가는 일반 보안 모범 사례 및 취약성 정의에 매핑된 사전 정의 규칙 패키지로 제공 AWS Artfact AWS Artifact는 자신에게 해당되는 규정 준수와 관련된 정보를 제공하는 신뢰할 수 있는 중앙 리소스 AWS 보안 및 규정 준수 보고서와 엄선된 온라인 계약에 대한 온디맨드 액세스를 제공 보고서에는 SOC(Service Organization Control) 보고서와 PCI(Payment Card Industry) 보고서, 그리고 여러 지역의 인정 기구와 규정 준수 기관에서 AWS 보안 제어의 구현 및 운영 효율성을 입증하는 인증서가 포함 AWS CertificateManager AWS Certificate Manager는 AWS 서비스 및 연결된 내부 리소스에 사용할 공인 및 사설 SSL/TLS(Secure Sockets Layer/전송 계층 보안) 인증서를 손쉽게 프로비저닝, 관리 및 배포할 수 있도록 지원하는 서비스 AWS Certificate Manager는 SSL/TLS 인증서를 구매, 업로드 및 갱신하는 데 드는 시간 소모적인 수동 프로세스를 대신 처리 SSL/ TLS, HTTPS SSL/ TLS ( Secure Soket Layer, Transport Layer Security ) 상위 계층 메시지를 분할, 압축하고 메시지 인증 코드 ( MAC )추가 및 암호화하는 작업을 수행하는 프로토콜 Handshake 프로토콜에서 클라이언트와 서버는 상대방을 확인하고 사용할 키 교환 방식, 암호화 방식, 생성에 필요한 값 등을 전달하여 암호화 채널 수립을 위한 항목들을 협상 단말 ( PC, server 등 )과 단만간의 암호화 통신을 위한 프로토콜 SSLv1은 최초의 버전으로서 문제가 많아 발표되지 않고 사장됨 SSLv2부터 공개가 되었으며 보다더 나은 버전인 SSLv3가 나왔으면 이를 기반으로 TLSv1 생성 TLSv1.0 v1.1, v1.2, v1.3이 나왔지만 아직 많은 브라우저에서 TLSv3을 지원하지 않음 SSL handshake Client Hello: Client가 Server에게 자신이 사용가능한 Random byte( 대칭키 생성에 사용됨 ), Session ID, S니/ TLS 버전이 포함된 Cipher suite list를 전달 Server Hello: Server 가 Client가 보낸 Cipher suite List 중 하나를 선택해 전달 Client Key exchange: 키 교환 실시 ( 실제 데이터 암호화에 사용할 키를 전달하여, S니 인증서에서 추출된 공개키로 암호화 ) Server certificate: 서버의 인증서를 클라이언트에게 전송 Sever hello done: 서버 전달 종료 Change cipher Specs, Finished: 이후 보내는 메시지들은 협상된 암호화 알고리즘에 따라 보낼 것임을 통보 Finished, Change cipher Specs: 클라이언트가 보낸 메시지를 확인 후, handshake를 종료하고 하나의 대칭키로 통신한다고 통보 AWS CloudHSM AWS CloudHSM은 AWS 클라우드에서 자체 암호화 키를 손쉽게 생성 및 사용할 수 있도록 지원하는 클라우드 기반 하드웨어 보안 모듈(HSM) 사용자를 위해 하드웨어 프로비저닝, 소프트웨어 패치, 고가용성, 백업 등 시간 소모적인 관리 작업을 자동화하는 완전관리형 서비스 CloudHSM에서는 FIPS 140-2 레벨 3 인증 HSM을 사용하여 자체 암호화 키를 관리가능 모든 키를 대부분의 상용 HSM으로 내보내기 가능 CloudHSM을 사용하면 선결제 비용 없이 온디맨드로 HSM 용량을 추가 및 제거하여 신속하게 확장/축소가능 사용방법 AWS CloudHSM은 자체 Amazon Virtual Private Cloud(VPC)에서 실행되므로, Amazon EC2 인스턴스에서 실행되는 애플리케이션에 손쉽게 HSM을 사용 CloudHSM에서는 표준 VPC 보안 제어 기능을 사용하여 HSM에 대한 액세스를 관리가 가능 애플리케이션은 HSM 클라이언트 소프트웨어가 설정한 상호 인증된 SSL 채널을 사용하여 HSM에 연결 HSM은 고객의 EC2 인스턴스와 가까운 Amazon 데이터 센터에 위치하므로, 온프레미스 HSM과 비교하여 애플리케이션과 HSM 간 네트워크 지연 시간을 줄일 수 있음 업무 분리 및 역할 기반 액세스 제어는 AWS CloudHSM의 설계에 내제 AWS에서 하드웨어 보안 모듈(HSM) 어플라이언스를 관리하지만, 고객의 키에 대한 액세스 권한이 없음 고객이 자체 키를 제어하고 관리 애플리케이션 성능이 개선 다중 AZ(가용 영역)에 제공되는 변조 방지 하드웨어에 키를 안전하게 저장 고객의 HSM은 고객의 Virtual Private Cloud(VPC)에 상주하며 다른 AWS 워크로드와 격리 AWS DirectoryService AWS Directory Service는 다른 AWS 서비스에서 Amazon Cloud Directory 및 Microsoft Active Directory(AD)를 사용할 수 있는 몇 가지 방법을 제공하는 서비스 사용자, 그룹 및 디바이스에 대한 정보를 저장하고, 관리자는 이를 사용하여 정보 및 리소스에 대한 액세스를 관리 AWS Directory Service는 클라우드에서 기존 Microsoft AD 또는 LDAP(Lightweight Directory Access Protocol)–인식 애플리케이션을 사용하려는 고객에게 다양한 디렉터리 선택 옵션을 제공 AWS IAM AWS 리소스에 대한 엑세스를 안전하게 제어할 수 있는 서비스로 IAM을 사용하여 리소스를 사용하도록 권한을 부여하거나 인증된 대상을 제어 IAM 정책은 “Action ( 어떤 서비스에 )”, “Resource ( 어떤 기능 또는 범위를 )”, “Effect ( 허가할 것인가 )“라는 3가지 규칙을 기반으로 AWS 서비스를 사용하는 데 필요한 권한을 설정 주요기능 AWS 계정에 대한 공유 엑세스 서비스별 세분화된 권한 제공 가능 EC2에서 실행되는 앱을 위한 AWS 리소스 엑세스 권한 제공 멀티 팩터 인증 ( MFA ) 자격 증명 연동 AWS 서비스들은 IAM Role을 할당받아 권한을 부여받을 수 있음 Access Key와 Secret Access Key를 직접 입력하지 않고 권한 부여 가능 IAM 사용자 계정을 만들어 사용자에게 적절한 권한을 부여하고 사용 가능한 서비스를 제한할 수 있음 사용자와 그룹은 N : N 의 관계가 성립이 가능 정책 ( Policy ) User, Group, Role이 사용할 수 있는 권한의 범위를 지정하는 것 S3FullAccess, Administrator Access 등 다양한 엑세스 권한이 이미 정의되어 있으며 이를 ‘AWS 관리형 이라 함 사용자 정의 정책 생성 JSON 형식 또는 직접 선택을 통해 사용자 정의 정책 선택 가능 역할 ( Role ) 특정 권한을 가진 계정에 생성할 수 있는 IAM 자격증명 역할에는 다음과 같은 주체가 있음 AWS 계정의 IAM 사용자 AWS의 서비스 ( EC2, RDS, ELB 등 ) 외부 자격 증명 공급자 서비스에 의해 인증된 외부 사용자 역할 생성시 IAM 사용자, 서비스, 외부 사용자 등 주체를 정해야 함 하나의 역할에는 다수의 정책을 연결할 수 있음 생성된 역할을 서비스 혹은 IAM 사용자 등에 연결 Region에 국한되지 않고 사용 가능 신규 유저는 생성시 아무런 권한이 없으며 Access Key와 Secret Access Key가 할당 각 키는 최초 생성시에만 볼 수 있으며 즉시 보관해야 함 그룹\u0026 사용자 사용자는 IAM 사용자를 의미하여 관리자 계정에 의해 부여받은 권한에 한해 제한된 서비스에 접근할 수 있는 계정을 의미 콘솔 로그인과 프로그래밍 엑세스 가능 여부를 선택하여 생성 가능 콘솔 로그인이 승인된 경우, 별도의 링크를 통해 콘솔에 로그인 할 수 있음 각 사용자마다 정책을 부여할 수 있음 사용자 모두에게 일일이 부여하기 힘들거나 그룹단위로 통제하고 싶은 경우, Group을 사용할 수 있음 그룹은 이미 생성된 사용자와 권한을 설정할 수 있으며, 그룹 내 모든 사용자는 그룹의 권한을 적용받음 특징 권한 AWS의 서비스나 자원에 어떤 작업을 할 수 있는지 명시해두는 규칙 \" 서울 리전에 있는 모든 EC2를 조회할 수 있다” 와 같은 항목이 하나의 권한을 칭한다. 정책 권한들의 모음으로, 사용자나 권한들에 직접 적용은 불가능하며, 권한들로 만든 정책을 적용 정책은 사용자, 그룹, 역할에 적용할 수 있다. 사용자 사용자는 AWS의 기능과 자원을 이용하는 객체, 사용자별로 어떤 권한을 가졌는지 세분화해서 지정할 수 있으며, 사용자는 AWS Console에 접근할 수 있는 사람일 수도 있고, 자동화되어 실행되는 프로그램일 수도 있다. 접속하는 사용자인 경우에는 비밀번화가 제공되지만, 프로그램인 경우에는 액세스 키 ID와 비밀 엑세스 키가 제공된다. 그룹 여러 사용자에게 공통으로 권한을 부여할 수 있게 만들어진 개념이다. 하나의 그룹에 여러 명의 사용자를 지정이 가능 역할 어떤 행위를 하는 객체에 여러 정책을 적용한다는 점에서 사용자와 비슷ㅎ자ㅣ만 객체가 사용자가 아닌 서비스나 다른 AWS 계정의 사용자라는 점에서 차이가 있다. 사용자가 아닌 특정 서비스에서 생성한 객체에 권한을 부여하는 데 사용 인스턴스 프로파일 사용자가 사람을 구분하고 그 사람에 권한을 주기 위한 개념이었따면, 인스턴스 프로파일은 EC2 인스턴스를 구분하고 그 인스턴스에 권한을 주기 위한 개념 Amazon Cognito Amazon Cognito는 웹 및 모바일 앱에 대한 인증, 권한 부여 및 사용자 관리를 제공 사용자는 사용자 이름과 암호를 사용하여 직접 로그인하거나 Facebook, Amazon, Google 또는 Apple 같은 타사를 통해 로그인가능 Cognito를 사용한 사용자 인증과 접속허가 종류 Identity Provider를 사용한 인증 Cognito를 사용한 Credential 발행 IAM 규칙 Cognito는 SQLite 데이터베이스를 사용, 시간적으로 마지막에 수정된 데이터를 우선적으로 하는 방침을 가짐 Cognito는 Identity Pool 단위를 사용 AWS KMS ( Key Management Service ) AWS Key Management Service(AWS KMS)는 데이터 암호화에 사용하는 암호화 키인 고객 마스터 키(CMK)를 쉽게 생성하고 제어할 수 있게 해주는 관리형 서비스 AWS Organizations AWS Organizations는 AWS의 워크로드가 증가하고 확장됨에 따라 환경을 중앙에서 관리하는 서비스 계정 생성을 자동화하고, 비즈니스 요구를 반영하도록 계정 그룹을 생성하고, 거버넌스를 위해 이러한 그룹에 정책을 적용이 가능 AWS 계정에 대해 단일 결제 방법을 설정하여 결제 과정을 간소화가 가능 WS Organizations는 모든 AWS 고객이 추가 비용 없이 사용가능 AWS Shield 분산 서비스 거부 공격( DDoS )으로부터 웹 어플리케이션을 보호하는 서비스 Cloudfront와 통합되어 있기 때문에 AWS의 서비스가 아니더라도 Cloudfront의 origin이라면 보호가 가능 Shield의 종류 Shield Stanard 기본적으로 적용되는 서비스로 설정을 하지 않아도 AWS 서비스에 활성화 되어있음 Shield Advanced 추가 비용을 내고 추가적인 서비스를 제공받는 것으로 L7 트래픽 모니터링, 사후 분석 등의 기능을 제공 AWS WAF ( Web Application Firewall ) 웹 방화벽으로 Cloudfront와 ALB를 통해 서비스를 제공 ( ALB와 Cloudfront를 직접 지정하여 웹방화벽을 제공 ) WAF을 활용하면 다양한 종류의 웹 공격에 대한 정보를 지닌 Rule을 선택하여 활성화하거나, 특정 Ip의 요청을 막을 수 있음 웹방화벽 방화벽이 L4/ L4 Layer의 방어 ( IP와 Port 차단 )을 이용 웹 방화벽은 L7( HTTP 헤더, HTTP 본문, URI 문자열, SQL 명령어, 스크립팅 )을 이용한 공격을 방어 ","amazon-cognito#\u003cstrong\u003eAmazon Cognito\u003c/strong\u003e":"","amazon-inspector#\u003cstrong\u003eAmazon Inspector\u003c/strong\u003e":"","aws-artfact#\u003cstrong\u003eAWS Artfact\u003c/strong\u003e":"","aws-certificatemanager#\u003cstrong\u003eAWS CertificateManager\u003c/strong\u003e":"","aws-cloudhsm#\u003cstrong\u003eAWS CloudHSM\u003c/strong\u003e":"","aws-directoryservice#\u003cstrong\u003eAWS DirectoryService\u003c/strong\u003e":"","aws-iam#\u003cstrong\u003eAWS IAM\u003c/strong\u003e":"","aws-kms--key-management-service-#\u003cstrong\u003eAWS KMS ( Key Management Service )\u003c/strong\u003e":"","aws-organizations#\u003cstrong\u003eAWS Organizations\u003c/strong\u003e":"","aws-security#\u003cstrong\u003eAWS Security\u003c/strong\u003e":"","aws-shield#\u003cstrong\u003eAWS Shield\u003c/strong\u003e":"","aws-waf--web-application-firewall-#\u003cstrong\u003eAWS WAF ( Web Application Firewall )\u003c/strong\u003e":""},"title":"AWS Security"},"/system/aws/amazonwebservice/aws_storage/":{"data":{"":"AWS Storage S3 ( Simple Storage Service ) 웹 서비스 인터페이스( HTTP ) 를 이용하여 웹에서 언제 어디서나 원하는 양의 데이터를 저장하고 검색할 수 있는 스토리지 버킷( Bucket )과 객체 ( Object )로 나뉘며, 저장하고자 하는 모든 요소는 하나의 객체로 저장되고, 객체를 담는 곳이 버킷 S3 자체는 글로벌 서비스이지만 버킷을 생성 할 때에는 리전을 선택해야 함 객체는 객체 데이터와 메타 데이터로 나뉘며, 각자의 고유한 URL을 가지며 해당 URL로 접속 가능 버킷( Bucket )의 정의와 특징 객체를 담고 있는 구성 요소 크기는 무제한, 리전을 지정하여 버킷을 생성해야 함 버킷의 이름은 반드시 고유해야하며, 증복이 불가능 한번 설정된 버킷의 이름은 다른 계정에서 사용불가 객체( Object ) 의 정의와 특징 S3에 업로드되는 1개의 데이터를 객체라 함 키, 버전 ID, 값, 메타데이터 등으로 구성 객체 하나의 최소 크기는 1(0) byte ~ 5TB 스토리지 클래스, 암호화, 태그, 메타데이터, 객체 잠금 설정 가능 객체의 크기가 매우 클 경우 멀티파트 업로드를 통해 신속하게 업로드 가능 객체의 스토리지 클래스 객체의 접근빈도 및 저장기안에 따라 결정되는 객체의 특성 Standard Type : 클래스를 선택하지 않을 경우 선택되는 일반적인 클래스 Strandard_IA(Ifrequent Access ) : 자주 엑세스하지는 않지만 즉시 액세스할 수 있는 데이터여야하는 경우 선택되는 클래스 One Zone_iA : Standard_IA와 기능은 동일하나 Standard_IA의 경우 세 곳의 AZ에 저장되는 것과 달리 한 군데의 AZ에만 저장되어 해당 AZ가 파괴될 경우 정보 손실 가능성 존재 ( 저장 요금이 적음 ) Intelligent tiering : 엑세스 빈도가 불규칙하여 빈도를 가늠하기 어려운 경우 선택되는 클래스 Glancier : 검색이 아닌 저장이 주용도인 스토리지로 저장요금이 위 클래스들보다 훨씬 저렴한, 다만 저장이 주용도이기 때문에 검색이 3~ 5시간이 소요 Glacier Deep Archive : 10년 이상 저장할 데이터를 저장하는 스토리지 클래스 S3 사용 S3 생성 1. S3를 선택합니다. 2. S3 사용을 위해 버킷을 생성합니다. 버킷 생성이 주의사항 버킷 이름에 대문자사용이 불가능 버킷 이름에 특수문자 사용 불가능 버킷 이름이 중첩될 수 없음 퍼블릭 엑세스 차단을 위한 버킷설정 S3 사용자의 설정에 따라 엑세스를 차단\u0026 허용 설정이 가능 3. Bucket의 생성되었습니다. 4. 버킷을 선택하면 버킷을 사용할 수 있습니다. 5. 버킷의 파일을 사용자의 옵션에 맞춰 업로드합니다. 6. 업로드가 완료되었습니다. 7. 업로드 파일을 선택하면, 퍼블릭 전환, 다운로드 링크 등의 기능을 사용가능합니다. 8. 권한이 없는 사용자가 링크로 접근하면 다음과 같은 오류가 발생됩니다. 윈도우 예약 작업과 S3 활용 윈도우 S3 연동 1. 연동을 위해 Bucket의 backup 폴더를 생성합니다. 2. 계정 생성을 위해 IAM 서비스로 이동합니다. IAM이란 3. 사용자 추가를 선택하여 사용자의 옵션에 맞춰 새로운 사용자를 생성합니다. 4. 사용자 생성이 완료되었습니다. 사용자 생성 후, 연동을 위해 CLI를 설치합니다. 5. 설치가 완료되면, cmd 창에서 configure를 입력 후, 다운받은 csv파일의 정보들을 입력합니다. AWS Access Key ID : ex.csv 파일의 엑세스 ID 값 AWS Secret Access Key :ex.csv 파일의 보안 엑세스 키 값 Default region name : region 이름으로 ap-northeast-2 입력 Default output format : 포맷 형식으로 json을 입력 6. aws s3 sync [ 저장한 윈도우의 경로 ] s3 [ 저장될 버킷의 경로 ]를 입력합니다. aws s3 sync c:\\backup s3://mybucketbucket/backup 7. 자동화를 위해 .bat 파일을 생성합니다. 8. bat 파일을 작업 스케줄러에 등록합니다. 9. 등록이 완료되었습니다. 확인을 위해 실행을 클릭합니다. Amazon EFS ( Elastic File System ) AWS 클라우드 서비스와 온프레미스 리소스에서 사용할 수 있는 탄력적인 완전 관리형 탄력적 NFS 파일 시스템 애플리케이션을 중단하지 않고 온디맨드 방식으로 구성 파일의 추가/ 제거 함에 따라 자동적으로 용량의 확장 및 축소 데이터 일관성 및 보안체계 제공 네트워크 파일 시스템( NFS v4 )를 사용하는 파일 스토리지 서비스 VPC 내에서 생성되며, 파일 시스템 인터페이스를 통해 EC2에 엑세스 수천 개의 EC2에서 동시에 엑세스 가능하며, 탄력적으로 파일을 추가하고 삭제함에 따라 자동으로 Auto Scaling 가능, 즉 미리 크기를 프로비저닝 할 필요가 없음 페타바이트단위 데이터까지 확장 가능 최대 1천개의 파일 시스템 생성 스토리지 클래스 Standard Class : 자주 액세스하는 파일을 저장하는 데 사용하는 클래스 Infrequent Access( IA ) Class : 저장기간이 길지만 자주 액세스하지 않는 파일을 저장하기 위한 클라스 가용성 여러 가용영역에서 엑세스 가능 여러 가용영역에 중복 저장되기 때문에 하나의 가용영역이 파괴되더라도 다른 AZ에서 서비스 제공 가능 IPSEC VPN 또는 Direct Connect를 통해 On-premise에서 접속 가능 성능 모드/ 처리량 모드 성능 모드에 있어서 대부분의 파일시스템에 Bursting Mode를 권장하지만 처리량이 많을 경우, Provisioned Mode를 권장 수명 주기 관리 Standard Class: 자주 액세스하는 클래스 파일시스템 정책 여러 가용영역에서 엑세스 가능 여러 가용영역에 중복 저장되기 때문에 하나의 Amazon Glacier Glacier는 자주 사용하지 않는 데이터로 “Cold Data\"에 최적화된 스토리지 서비스 데이터 보관 및 백업을 목적으로 보안 기능과 함께 내구성 있는 저장 공간을 제공하는 매우 저렴한 스토리지 서비스 Glacier의 종류 아카이브 Glacler에 데이터가 저장되는 최소 단위, 하나의 파일 볼트 Glacler에 생성할 수 있는 최상위 디렉토리, 볼트는 리전별로 생성해야 하며, 각 리전별로 최대 1000개까지 가능 볼트 인벤토리 볼트에 저장된 아카이브의 목록과 크기, 생성 날짜 등 아카이브 정보, 24시간에 한 번씩 업데이트 Storage Gateway On-premise 환경에서 Cloud 상의 Storage를 지원할 수 있게 하는 하이브리드 스토리지 이름이 Storage ‘Gateway’인 이유는 Storage Gateway 자체가 스토리지의 역할을 하는 것이 아닌 스토리지( S3 )의 Gateway 역할을 하기 때문 Volume Gateway의 Stored Volume을 제외하고 나머지 유형은 EC2를 Gateway로 활용하여 Mount Point로 활용 가능 모든 Storage Gateway는 말 그대로 ‘Gateway‘를 생성해야 함 그 대상은 EC2 혹은 하드웨어 어플라이언스가 해당될 수 있음 EC2의 공인 IP를 Mount point로 지정하여 외부 네트워크에서 연결 가능 일반 PC에 마운트하여 사용하는 둥, 다양한 용도로 사용 가능 File Gateway FNFS와 SMB를 지원하는 Storage Gateway 유형 S3를 스토리지로 사용하며, Gateway( EC2 등 )을 통해 S3에 데이터를 저장하고 이를 직접 S3에서 엑세스 할 수 있음 하나의 파일을 하나의 오브젝트로 관리됨 S3에 오브젝트로 관리되는 만큼, S3의 다른 기능을 사용할 수 있음 Volume Gateway iSCSI를 지원하는 Storage Gateway 유형 두 가지 유형으로 나뉨 Cached Volume: S3를 기본 데이터 스토리로 사용하되, 자주 엑세스하는 데이터를 온프레미스 스토리지 게이트웨이의 캐시 및 업로드 버퍼 스토리지에 보관 Stored Volume: On-premise 스토리지를 기본 데이터 스토리지로 사용하고, 해당 데이터를 EBS Snapshot 형식으로 S3에 비동기 백업을 실시 Tape Gateway VTL ( Vitual tape Library )를 지원하는 Storage Gateway 가상 테이프데이터는 S3나 S3 Glacier에 저장될 수 있음 EBS ( Elastic Block Stroe ) EBS 지원 EC2가 갖는 블록 형태의 스토리지 애플리케이션의 기본 스토리지로 쓰거나 시스템 드라이브용으로 쓰기 적합 인스턴스 생성 시 루트 디바이스 볼륨이 생성되며 사용 중에는 언마운트할 수 없음, 추가로 여러 볼륨의 마운트가 가능하며, 추가볼륨에 대해서는 사용중이라도 마운트/ 언마운트가 가능 EBS를 특정 AZ에서 생성하더라도 다른 AZ의 인스턴스에 즉시 붙일 수 있음 인스턴스 스토어 볼륨과는 달리 EBS 기반 인스턴스는 중지 / 재시작이 가능 사용중인 EBS더라도 볼륨 유형과 사이즈를 변경할 수 있음( 사이즈의 축소는 불가 ) EBS의 볼륨 유형 범용 SSD( gp2 ): 시스템 부트 사용 가능, 대부분의 워크로드에서 사용 프로비져닝된 IOPS SSD( io1 ): 지속적인 IOPS 성능이나 16.000 IOPS 이상의 볼륨당 처리량을 필요로 하는 경우 적합 ( DB 워크로드 ) 처리량 최적회돤 HDD( st1 ): 시스템 부트 사용 불가능, IOPS가 아닌 처리량을 기준으로 하며 자주 엑세스하는 워크로드에 적합한 저비용 HDD 볼륨, 빅데이터나 데이터 웨어 하우스에 사용 Cold HDD( sc1 ): 시스템 부트 사용 불가능, 자주 엑세스하지 않는 대용량 데이터 처리에 적합, 스토리지 비용이 최대한 낮아야 할 경우 사용 ","#":"","amazon-efs--elastic-file-system-#\u003cstrong\u003eAmazon EFS ( Elastic File System )\u003c/strong\u003e":"","amazon-glacier#\u003cstrong\u003eAmazon Glacier\u003c/strong\u003e":"","aws-storage#\u003cstrong\u003eAWS Storage\u003c/strong\u003e":"","ebs--elastic-block-stroe-#\u003cstrong\u003eEBS ( Elastic Block Stroe )\u003c/strong\u003e":"","s3--simple-storage-service-#\u003cstrong\u003eS3 ( Simple Storage Service )\u003c/strong\u003e":"","s3-생성#\u003cstrong\u003eS3 생성\u003c/strong\u003e":"","storage-gateway#\u003cstrong\u003eStorage Gateway\u003c/strong\u003e":"","윈도우-s3-연동#\u003cstrong\u003e윈도우 S3 연동\u003c/strong\u003e":""},"title":"AWS Storage"},"/system/aws/awssaa/":{"data":{"":"Amazon Web Service Certified Solutions Architect\nAWS SAA의 개요 1장 AWS의 핵심 서비스 2장 EC2와 EBS 3장 S3와 Glacier 4장 VPC 5장 데이터베이스 6장 인증과 권한 7장 AWS 관리도구 8장 DNS와 네트워크 라우팅 9장 안전성 핵심요소 10장 성능 효율성 핵심요소 11장 보안 핵심요소 12장 비용 최적화 핵심요소 13장 운영 우수성 핵심요소 14장 평가문제 정리 15장 기출문제 정리 ","#":"","amazon-web-service-certified-solutions-architect#\u003cstrong\u003eAmazon Web Service Certified Solutions Architect\u003c/strong\u003e":""},"title":"AWS SAA 시험정리"},"/system/aws/awssaa/saa-1/":{"data":{"":"SAA 요약정리 정리를 들어가기 전 핵심요소 자격 시험의 합격과 실패는 현장에서의 실무 경험과 실습 중심의 학습, 시험에 필요한 세부적인 정보와 숫자를 얼마나 잘 기억하는지에 달려 있다. AWS SAA는 핵심 AWS 서비스 구성 요소와 운영은 물론 서비스 간의 상호작용 방식도 이해가 필요 기본적인 공부는 Amazon의 공식문서 및 여러 실습 경험을 필요로 한다. AWS SAA 참고자료 시험영역 출제 비율 1. 복원력을 갖춘 아키텍처 설계 34% 1-1. 안정적이고, 복원력을 갖춘 스토리지를 선택한다. 1-2. 어떻게 AWS 서비스를 사용해 결합 해제 매커니즘을 설계할지 결정한다. 1-3. 어떻게 멀티 티어 아키텍처 솔루션을 설계할지 결정한다. 1-4. 어떻게 고가용성 및/ 또는 내결함성을 갖춘 아키텍처를 설계할지를 결정한다. 2. 성능이 뛰어난 아키텍처 정의 24% 2-1. 성능이 뛰어난 스토리지 및 데이터베이스르 선택한다. 2-2. 캐싱을 적용해 성능을 개선한다. 2-3. 탄력성과 확장성을 갖춘 솔루션을 설계한다. 3. 안전한 애플리케이션 및 아키텍처 설명 26% 3-1. 어떻게 애플리케이션 티어를 보호할지 결장한다. 3-2. 어떻게 데이터를 보호할지 결정한다. 3-3. 단일 VPC 애플리케이션을 위한 네트워킹 인프라를 정의한다. 4. 비용에 최적화된 아키텍처 설계 10% 4-1. 어떻게 비용에 최적화된 스토리지를 설계할지 결정한다. 4-2. 어떻게 비용에 최적화된 컴퓨팅을 설계할지 결정한다. 5. 운영 면에서 탁월한 아키텍처 정의 6% 5.1 솔루션에서 운영 우수성을 지원할 수 있는 기능을 선택한다. 1장 AWS의 핵심 서비스 1장의 핵심내용 AWS 플랫폼 아키텍처와 그 기반 기술을 이해한다. AWS 관리 도구의 종류화 사용법을 이해한다. 지원 플랜의 종류와, 선택하는 방법을 이해한다. 클라우드 컴퓨팅과 가상화 모든 클라우드 운영 기술의 토대는 가상화라고 할 수 있다. 가상화란 단일 물리 서버를 하드웨어 리소스를 더 작은 단위로 나눌 수 있고, 물리 서버는 가상 머신 여러 개를 호스트할 수 있개 해주는 기술이다. 이러한 가상화의 장점은 가상 서버를 짧은 시간만에 프로비저닝해서 프로젝트에 필요한 시간만 정확하게 실행하고, 언제든지 종료해서 사용하던 리소스를 다른 워크로드에 즉시 활용할 수 있다. 클라우드 환경의 키워드는 확장성과 탄력성 확장성과 탄력성 키워드 설명 확장성 예기치 않은 수요가 발생하더라도 자동적으로 리소스츨 추가해서 효과적으로 대응할 수 있음 AWS 에서는 수요에 능동적으로 대처할 수 있게 설게 된 Auto Scaling 서비스를 사용해서 머신 이미지를 신속히 복제 및사용 탄력성 수요를 관리한다는 목적에서는 학장성과 동일한 못적을 자기고 있지만, 탄력성은 수요가 떨어질 때 용량을 자동으로 줄이는 개념 AWS 서비스의 범주 AWS 서비스의 서비스는 매우 다향하며, 현재도 그 범위가 확장되어가고 있다. 범주 기능 컴퓨팅 데이터 센터에서 물리 서버가 하는 역할을 복제한 클라우드 서비스 Auto Scaling, 로드 밸런싱, 서버리스 아키텍처에 이르는 고급 기능을 제공 네트워크 애플케이션 연결, 엑세스 제어, 향상된 원격 연결 스토리지 빠른 액세스와 장기적인 백업 요구에 모두 적합하게 설계된 여러 종류의 스토리지 플랫폼 데이터 베이스 관계형, NoSQL, 캐싱 등 데이터 형식이 필요한 사용 사레에 사용할 수 있는 관리형 데이터 솔루션 애플리케이션관 관리 AWS 계정 서비스와 운영 리소스 모니터링, 감사, 구성 보안과 자격 증명 인증 및 권한 부여, 데이터 및 연결 암호화, 타사 인증 관리 시스템과 통합 등을 관리하는 서비스 애플리케이션 통합 결합 해제, API를 사용한 애플리케이션 개발 프로세스를 설계하기 위한 도구 AWS 핵심 서비스 범주 서비스 기능 Computing EC2 AWS 상의 가상화된 인스턴스 Lambda 서버리스 애플리케이션 Auto Scaling 자동으로 인스턴스를 확장, 축소시키는 서비스 Elastic Load Balancing 네트워크의 트래픽을 분산시켜주는 라우팅 서비스 Elastic Beanstalk 컴퓨팅과 네트워킹 인프라를 프로비저닝하는 작업을 추상화한 관리형 서비스 Networking VPC 사용자 개인의 프라이빗 네트워크를 생성하는 서비스 Direct Connect AWS 서비스의 전용선을 통해 직접 연결하는 서비스 Router 53 AWS의 DNS서비스로 도메인 등록, 레코드 관리, 라우팅 프로토콜, 상태검사 등의 서비스를 제공 CloudFront AWS에서 제공하는 분산 글로벌 콘텐츠 전송 네트워크 ( CDN ) Storage S3 저렴하고 안정적인 다목적 객체 스토리지 서비스 Glacier 저렴하고 장기 저장할 수 있는 대형 데이터 아카이브를 제공하는 서비스 EBS EC2와 OS의 작업 데이터를 호스팅하는 가상의 데이터 드라이브 서비스 Storage Gateway AWS 클라우드 스토리지를 로컬 온프레미스 어플라이언스처럼 사용하는 하이브리드 스토리지 시스템 Database RDS 관리형 데이터베이스 인스턴스로 Mysql, Oracle, Aurora 등의 다양한 엔진을 제공 DynamoDB 빠르고 유연하며, 확장성이 뛰어난 관리현 서비스로 비관계형 (NoSQL) 데이터 베이스 워크로드에 적합 Application management CloudWatch 이벤트를 통해 프로세스 성능 및 활용률을 모니터링 하는 서비스 CloudFormation 탬플릿을 사용하여 AWS 리소스에 대한 사용을 스크립트화 시켜 사용하는 서비스 CloudTrail 계정내 모든 API 이벤트 기록을 수집하는 서비스 Config AWS 계정에서 변경 관리와 규정 준수를 지원하도록 설계된 서비스 Security, identification IAM AWS 계정의 사용자를 역할을 통해 관리하는 서비스 KMS AWS 리소스의 데이터를 보호하는 암호화 키를 생성하고 키사용을 관리하는 관리형 서비스 Directory Service AWS에서 자격 증면이나 관계를 정리할 때, Cognito, Microsoft AD도메인과 같은 자격 증명 공급자와 통합시키는 역할을 수행 Application Intergrated SNS 자동으로 주제에 관한 알림을 다른 서비스로 보내는 알림 서비스 SWF 수행해야하는 일련의작업을 조정하는 서비스로, 윤활유나 접착제의 역할을 수행 SQS 분산 시스템 내에서 이벤트 중심 메시징으로 결합을 해제해서 대형 프로세스의 개별 단계를 조정하는 서비스 APT Gateway Application 구현에 필요한 API를 생성 및 관리하는 서비스 AWS 플랫폼 아키텍처 AWS는 짧은 지연 시간 엑세스를 보장하는 것이 매우 중요하기 때문에, 이와 관련해서 CloudFront, Route53, Firewall Manager 등의 여러 서비스가 사용된다. AWS 계정 내의물리적 AWS 데이터 센터는 AZ ( Available Zone ), 가용영역이라 하며 아래와 같이 Region code로 표시된다. 같은 Region 내의 가용 영역은 6개까지가능하며, 리전 내에는 일종의 네트워크 주소 공간인 VPC가 있는 데, 리소스는 이 VPC에 배포된다. VPC 내에 서브넷을 만들 어 특정 가용 영역과 연결시켜, 리소스를 효과적으로 격리하고 내구성을 높이기 위한 복제를 수행할 수 있다. AWS 리전의 종류 Region name Region code Endpoint 미국 동부(오하이오) us-east-2 us-east-2.amazonaws.com 미국 동부(버지니아 북부) us-east-1 us-east-1.amazonaws.com 미국 서부(캘리포니아 북부) us-west-1 us-west-1.amazonaws.com 미국 서부(오레곤) us-west-2 us-west-2.amazonaws.com 아프리카(케이프타운) af-south-1 af-south-1.amazonaws.com 아시아 태평양(홍콩) ap-east-1 ap-east-1.amazonaws.com 아시아 태평양(뭄바이) ap-south-1 ap-south-1.amazonaws.com 아시아 태평양(오사카-로컬) ap-northeast-3 ap-northeast-3.amazonaws.com 아시아 태평양(서울) ap-northeast-2 ap-northeast-2.amazonaws.com 아시아 태평양(싱가포르) ap-southeast-1 ap-southeast-1.amazonaws.com 아시아 태평양(시드니) ap-southeast-2 ap-southeast-2.amazonaws.com 아시아 태평양(도쿄) ap-northeast-1 ap-northeast-1.amazonaws.com 캐나다(중부) ca-central-1 ca-central-1 .amazonaws.com 중국(베이징) cn-north-1 cn-north-1.amazonaws.com 중국(닝샤) cn-northwest-1 cn-northwest-1.amazonaws.com 유럽(프랑크푸르트) eu-central-1 eu-central-1.amazonaws.com 유럽(아일랜드) eu-west-1 eu-west-1.amazonaws.com 유럽(런던) eu-west-2 eu-west-2.amazonaws.com 유럽(밀라노) eu-south-1 eu-south-1.amazonaws.com 유럽(파리) eu-west-3 eu-west-3.amazonaws.com 유럽(스톡홀름) eu-north-1 eu-north-1.amazonaws.com 중동(바레인) me-south-1 me-south-1.amazonaws.com 남아메리카(상파울루) sa-east-1 sa-east-1.amazonaws.com AWS 안정성과 규정 AWS의 대부분의 서비스는 기본 규정, 법률, 보안의 대한 기초 사항이 존재한다. AWS 보안과 안정성을 위해 수 많은 노력과 시도들을 해왔으며, 이에 관련된 내용은 AWS 규정를 참조하길바란다. AWS의 공동 책임 모델 AWS의 서비스는 기본적으로 보안에 대학 책임은 AWS와 사용자가 책임을 분담하는 구조를 이루고 있다. 클라우드 상의 인프라를 안정적으로 관리하는 일은 AWS의 책임이지만, AWS의 리소스를 사용하는 것은 사용자의 일이며, 그에 따른 책임도 사용자에게 있다. AWS 책임의 따른 분류 사용자의 책임 클라우드 내부 사용자의 데이터 사용자 애플리케이션, 엑세스 관리 운영 체제, 네트워크, 엑세스 구성 데이터 암호화 \u003c—\u003e AWS의 책임 클라우드 자체 하드웨어와 네트워크 유지보수 AWS 글로벌 인프라 관리형 서비스 AWS 작업 AWS 서비스를 실행하려면 해당 서비스를 관리할 도구가 있어야 한다. AWS 서비스는 기본적으로 GUI 환경을 제공하지만, 보다 복잡한 환경을 구현할 경우에는 전문적인 관리 도구를 사용해야 할 수도 있다. AWS CLI AWS CLI를 사용하면 컴퓨터 명령줄에서 복잡한 AWS 작업을 실행할 수 있다. 작동방식에 익숙해지면 GUI에 비해 간단하고 효율적인 작업이 가능해진다. AWS SDK AWS 리소스에 엑세스하는 작업을 애플리케이션 코드에 통합하려면 쓰고 있는 언어에 맞는 SDk를 사용해야 한다. 기술지원 및 온라인 리소스 AWS에는 다양한 유형의 지원이 있으며, 지원마다 어떤 내용이 있는 지 이해할 필요가 있다. 지원플랜\n기본 플랜은 모든 계정에 무료로 제공되며, 문서, 백서, 지원 포럼 등의 고객 서비스에 요구할 수 있고, 청구 및 계정 지원 문제가 포함된다.\n개발자 플랜은 $29부터 시작하며, 계정 소유자 한 명만 일반적 지침과, 시스템 손상에 관해 문의할 수 있으며, 클라우드 지원 담당자가 응답한다.\n비즈니스 플랜은 $100 이상이며 문의할 수 있는 사용자 수에 제한이 없고, 신속한 응답을 보장한다. 시스템 손상, 개별적 지침, 문제 해결, 지원 API 등의 서비스를 제공한다.\n엔터프라이즈 지원 플랜은 다른 지원 모든 지원 플랜을 포함하며, 운영과 설계 검토를 위한 AWS 솔루션스 아키텍트 지원, 전담 기술 지원, 관리자 지원, 컨시어지 지원이 추가된다.\n엔터프라이즈 지원은 복잡한 미션 크리티컬 배포에 큰 도움이 될 수 있지만, 매월 최소 $15.000을 지급해야 한다.\n지원플랜 참고 사이트\n기타 지원 리소스 AWS 커뮤니티 포럼 AWS 설명서 AWS Well-Architected 1장 요약 클라우드 컴퓨팅은 물리적 리소스를 작고 유연한 가상 단위로 나누는 기술에 기반을 둔다.\nAWS는 거대한 물리적 리소스를 가상 단위로 나누어, 가장의 리소스를 종량제로 임대하여 저렴하게 제공해주는 서비스이다.\nAWS 장점은 탄력성과 확장성으로, 이는 자원의 소모를 자동적으로 유동적으로 비용을 최소화시킬 수 있다.\n많은 AWS의 서비스들이 있으며, 이를 통해 거의 모든 디지털 요구사항을 처리할 수 있다. 또한 이러한 서비스들은 지금도 확장되어가고 있다.\nAWS 리소스는 Management Console과 AWS CLI로 관리할 수 있으며, AWS SDK로 생성한 코드로도 관리할 수 있다.\n기술 및 계정 지원에는 지원 플랜, 설명서, 포럼, 백서 등이 있다.","#":"","1장-aws의-핵심-서비스#\u003cstrong\u003e1장 AWS의 핵심 서비스\u003c/strong\u003e":"","aws의-책임#\u003cstrong\u003eAWS의 책임\u003c/strong\u003e":"","saa-요약정리#\u003cstrong\u003eSAA 요약정리\u003c/strong\u003e":"","사용자의-책임#\u003cstrong\u003e사용자의 책임\u003c/strong\u003e":"","정리를-들어가기-전-핵심요소#\u003cstrong\u003e정리를 들어가기 전 핵심요소\u003c/strong\u003e":""},"title":"1장 AWS의 핵심 서비스"},"/system/aws/awssaa/saa-2/":{"data":{"":"2장 Amazon Elastic Compute Cloud와 Amazon Elastic Block Store 2장의 목표 복원력은 갖춘 아키텍처 설계 안정적이고, 복원력을 갖춘 스토리지를 선택 어떻게 고가용성 및/ 또는 내결함성을 갖춘 아키텍처를 설계할지를 결정 성능이 뛰어난 아키텍처 정의 성능이 뛰어난 스토리지 및 데이터베이스를 선택 탄력성과 확장성을 갖춘 솔루션을 설계 안전한 애플리케이션 및 아키텍처 설명 어떻게 애플리케이션 티어를 보호할지를 결정 어떻게 데이터를 보호할지 결정 비용에 최적화된 아키텍처 설계 어떻게 비용에 최적화된 스토리지를 설계할지를 결정 어떻게 비용에 최적화된 컴퓨팅을 설계할지 결정 EC2 인스턴스 EC2는 물리 서버의 기능을 함축적으로 가상화한 실제 서버와 유사하게 작동\n스토라지, 메모리, 네트워크 인터페이스가 새로 설치 된 기본 드라이브가 제공\nEC2 Amazon Machin Image ( AMI ) AMI란 EC2를 시작할 때, 루트 볼륨에 설치될 운영 체제와 소프트웨어를 기술한 템플릿 문서 AMI의 종류 종류 설명 Amazon 바른 시작 AMI 자주 이용되는 Linux, Windows 등이 등록되어지고, 최신 버전으로 업데이트, 공식적으로 지원하는 이미지 AWS Marketplace AMI AWS Marketplace AWS에서 공식적으로 지원하는 이미지이며, SAP, 시스코와 같은 공급업체가 제공 및 지원 Community AMI 100.000개 이상의 이미지가 제공디고 있으며, 특정한 요구에 맞게 커스터마이징 된 이미지 Private AMI 사용자가 자체 배포한 인스턴스에서 이미지를 생성해서 저장한 이미지, S3에 저장할 수 있으며, 이를 통해 As기능을 사용할 수 있다. EC2 Instacne Type AWS는 사용자가 선택한 하드웨어 프로파일, 즉 인스턴스 유형에 따라 하드웨어 리소스를 인스턴스에 할당한다. AWS를 이용하는 사용자는 자신의 니즈에 맞게 인스턴스의 유형을 사용함으로써 자원을 효율적으로 이용할 수 있다. 인스턴스의 유형은 자주 변경되며 AWS Instance Type에서 확인할 수 있다. EC2 인스턴스 유형 패밀리와 최상위 명칭 인스턴스 유형 패밀리 유형 범용 T3, T2, M5, M4 컴퓨팅 최적화 C5, C4 메모리 치적화 X1e, X1, R5, R4, z1d 가속화된 컴퓨팅 P3, P2, G3, F1 스토리지 최적화 H1, l3,D2 범용 서비스 : 컴퓨팅, 메모리, 네트워크, 리소스를 균형 있게 제공하며, 리소스의 확장이 쉽다. M5, M4 : 주로 중소 규모 데이터 운영에 권장되며, M 인스턴스는 실제 호스트 서버에 물리적 연결된 내장 인스턴스 스토리지 드라이브와 함께 제공 컴퓨팅 최적화 : 대규모 요청을 받는 웹 서버와 고성능 머신 러닝 워크로드에 적합 메모리 최적화 : 처리량이 많은 데이터베이스, 데이터 분석, 캐싱 작업에 유용 가속화된 컴퓨팅 : 고성능 범용 그래픽 처리 장치(GPGPU)가 제공되어, 3D 시각화/ 렌더링, 재무 분석, 전산 유체역학 같은 고부하 워크로드 인스턴스에 적합 스토리지 최적화 : 지연시간이 짧은 대용량 인스턴스 스토리지 볼륨을 사용 AWS Region 사용자는 AWS의 데이터 센터의 서버를 이용할 것이며, 이는 지리적 리전으로 구성되어 있다. EC2 리소스느 사용자가 선택한 리전에서만 관리할 수 있으며, 각 리전에 따라 서비스와 기능은 물론 비용도 다르므로 최신 공식 문서를 확인해야한다. Virtual Private Cloud ( VPC ) VPC는 사용자가 사용할 네트워크 내부대역을 생성하는 것으로, 프로젝트 단위로 작업을 허용하기 유용하다. 다중 VPC를 생성해도 금액이 발생하지 않으며, NATgateway, VPN 서비스를 사용하는 경우에는 비용이 발생된다. 태넌시 EC2 인스턴스를 시작할 때, 테넌시 모델을 선택할 수 있다. 기본 설정은 공유 테넌시이며, 여러 인스턴스가 한 물리 서버에서 동시에 가상 머신으로 실행된다. 인스턴스 동작 구성 인스턴스를 생성할 때, 이를 부트스트랩이라 하며, 스크립트 파일을 작성하거나, CLI에서 user-data 값을 사용하면 필요한 상태로 인스턴스를 구성할 수 있다. 인스턴스 요금 세 가지 모델 중에서 하나를 선택해 EC2 인스턴스를 구매해서 사용할 수 있다. 요금 모델 설명 온 디맨드 사용자가 사용한 만큼만 비용이 발생하게 구성 예약 미리 사용량을 할당받아 정해진 만큼 지불하며, 1, 3년으로 구성 스팟 특정 리전에서 실행되는 인스턴스 유형에 대해 사용자가 최대 입찰 요금을 입력해서 인스턴스를 사용 리소스 태그 AWS 계정에 리소스를 다수 배포할수록 추적이 어려워진다. 또한 다수의 VPC, 보안 그룸, 볼륨 등과 연계되면 복잡성은 한 층 더 강해진다. 이를 위해 AWS 계정에서는 리소스를 빠르게 식별할 수 있도록 리소스마다 목적 및 다른 리소스와의 관계 등을 정리할 수 있다. 리소스 태그는 키/ 값으로 구성된다. 키 값 production-server server1 production-server security-grouop1 staging-server server1 test-server security-grouop1 서비스 할당량 한 리전당 생성할 수 있는 VPC의 수는 5개 허용된 키 페어의 수는 5.000개 그 외의 추가적인 제한은 AWS 최신 할당량 정보 EC2 Storage Voulme 볼륨은 스토리지 드라이브로, 물리 드라이브를 가상으로 나눈 공간을 의미한다. AWS에서는 여러 유형의 볼륨 드라이브가 있으며, 각 유형이 동작하는 방식이 달라 이해가 필요하다. Elastic Block Store Volume ( EBS ) EBS는 필요한 수 만큼 인스턴스에 연결할 수 있으며, 물리 서버의 하드 드라이브, 플래시 드라이브, USB 드라이브와 유사하게 사용된다. 물리 드라이브에서와 같이 어떤 EBS 볼륨 유형을 선택하느냐에 따라 성능과 비용은 달라진다. AWS SLA에서 99.999%의 가용성으로 충분한 안정성을 가지고 있다. EBS의 볼륨 유형 볼륨 타입 설명 EBS 프로비저닝 된 IOPS SSD 고성능 I/O 작업이 필요할 때 최대 32.000 IOPS와 최대 500MB/s 처리량을 제공한다. EBS 범용 SSD 대다수 일반 서버 워크로드에서 사용되며, 이론적으로 짧은 징녀 시간 성능을 제공한다. 처리량 최적화 HDD 로그 처리와 빅 데이터 작업 등의 높은 처리량을 요구하는 워크로드에 적합한 성능을 저렴한 비용에 제공한다. 콜드 HDD 번번하게 엑세스하지 않는 대용량 작업에 콜드 HDD는 가장 낮은 가격에 제공한다. EBS 볼륨 기능 모든 EBS 볼륨은 스냅샷을 통해 복사할 수 있고, 기존 스냅샷을 다른 인스턴스에 공유해서 연결할 수 있으며, AMI로 등록할 수 있는 이미지로 변경할 수 있다. EBS 볼륨은 암호화해서 EC2 인스턴스가 저장하거나 송수힌 하는 데이터를 보호할 수 있으며, EBS에서는 내부에서 암호화 키를 자동으로 관리하거나 AWS KMS에서 제공되는 키를 사용할 수 있다. 인스턴스 스토어 볼륨 인스턴스 슽어 볼륨은 EBS 볼륨과는 다른 임시 디스크로, 디스크가 연결된 인스턴스가 종료되었을 때, 영구히 삭제된다. EBS 대신 인스턴스 스토어 볼륨을 사용하는 경우는 다음과 같다. 인스턴스 스토어 볼륨은 인스턴스를 호스팅하고 있는 서버에 물리적 고속 NVMe 인터페이스로 연결된 SSD이다. 인스턴스 스토어 볼륨 요금은 인스턴스 요금에 포함되어 있다. 인스턴스 스토어 볼륨은 단기 역할 수행이나 외부에서 데이터를 가져와서 처리 후, 폐기하는 배포 모델에 적합하다. EC2 인스턴스 엑세스 EC2 인스턴스는 네트워크에 연결된 모든 장치와 마찬가지로 고유한 IP로 식별 네트워크 인스턴스의 범위 처음 주소 끝 주소 10.0.0.0 10.255.255.255 172.16.0.0 172.31.255.255 192.168.0.0 192.168.255.255 프라이빗 서브넷으로 생성된 인스턴스는 서브넷 내부에서만 통신할 수 있고, 인터넷에는 직접 연결할 수 없다. 다른 리소스와 연결등의 필요로 인스턴스에 여러 네트워크 인터페이스가 있어야 하는 경우, 하나 이상의 가상 탄력적 네트워크 인터페이스를 만들어 연결할 수 있다. EC2 인스턴스 보안 사용자에게는 무단으로 EC2 인스턴스가 사용되지 않도록 적절하고 효과적으로 엑세스 제어를 구성해야할 책임이 있다. AWS는 이를 위해 보안 그룹, IAM 역할, NAT 인스턴스, 키 페어 등 네 가지 도구를 지원한다. 보안그룹 EC2 보안 그룹은 방화벽 역할을 한다. 보안 그룹의 기본 설정은 수신하는 모든 트래픽을 거부하며, 보안 그룹에 지정한 트래픽 유형만을 허용하는 정책 규칙을 설정한다. 보안그룹은 트래픽 유형만을 허용하는 정책 규칙을 설정하며, 네트워크에서 송수신하느 모든 데이터 패킷은 그 규칙에 따라 평가해서 허용 및 거부된다. IAM IAM이란 루트 계정이 아닌, 다른 사용자를 생성하여 역할(권한)을 부여함으로써, 사용가능한 범위를 분리시키는 것 IAM 역할을 사용해서 EC2 인스턴스를 비롯한 AWS 리소스에 엑세스 하는 것에 대한 제어가 가능 NAT 디바이스 인터넷이 항상 필요한 것이 아닌, 업데이트 등의 주기적으로 필요할 때만 인스턴스에 인터넷을 연결하도록 하는 서비스 NAT 게이트 웨이 or NAT 인스턴스로 프라이빗 게이트웨이를 지정함으로써 사용할 수 있다. 키 페어 키 페어는 암호화방식으로 키 값을 생성 후, 페어에 맞는 키만이 인스턴스의 접속이 가능하도록 한다. 기타 EC2 서비스 AWS System Manager System Manager은 AWS 클라우드와 온프레미스 인프라를 운영하는 리소스를 모니터링 및 관리하기 위한 도구의 모음 배치 그룹 배치 그룹은 지연 시간이 짧은 네트워크 상호 연결이 필요한 여러 EC2 인스턴스에 효율적으로 사용된다. 클러스터 그룹은 단일 가용 영역 안에서 물리적으로 접근 및 서로 연결된 인스턴스로 시작 분산형 그룹은 장애 관련 데이터나 서비스 손실 윟머을 줄이기 위해 물리적으로 분리된 하드웨어 인스턴스를 분산 AWS Elastic Container Service와 AWS Fargate 대규모 Docker 컨테이너 기반에서 실행되는 애플리케이션은 본질적으로 AWS와 같은 플랫폼에 적합 미리 사용된 미리 구축된 Docker를 사용하여 다른 AWS 서비스와 연계하여 사용할 수 있음 AWS Lambda 서버리스 애플리케이션은 프로그래밍 코드 기반으로 실행 서버에서 동작하지만 사용자가 서버를 제어하는 대신, 사전 설정된 이벤트로 Lambda 서버를 트리거해서 코드를 실행 및 구성 VM Import/ Export Local의 VMware 이미지를 S3를 통해 AWS상에서 사용할 수 있게 하는 서비스 Elastic Load Balancing과 Auto Scaling 로드 밸런서는 효율적으로 트래픽을 관리하고 여러 EC2 인스턴스에 전송해 서버 리소스를 효율적으로 분산하여 사용하는 서비스 2장 요약 Amazon머신 이미지 ( AMI )를 선택하고 시작할 때 스크립트나 user data를 입력해서 EC2의 기본 소프트웨어 스택을 정의\n인스턴스 유형으로 하드웨어 프로파일을 정하며, 태넌시 설정으로 다른 인스턴스와 물리적 호스트를 공유할 것인지를 결정\nEC2 인스턴스를 비롯한 모든 AWS 리소스에 시스템 전반의 명명 규칙에 따라서 쉽게 식별할 수 있게 태그를 부여할 수 있다.\n리소스의 제한이 있으머, 할당량을 넘는 리소스를 생성할 때에는 추가적인 신청을 해야한다.\n1년 이상 인스턴스를 실행할 때, 온디매드 대신 예약 인스터느를 구매하며 크게 비용을 절약할 수 있다.\n서비스가 끊끼는 게 중요하지 않은 경우, 스팟 인스턴스를 사용하는 것이 합리적이다.\nEBS에는 4가지 볼륨 유형이 있다.\n높은 IOPS와 짧은 지연 시간을 지원하는 두 가지 SSD 유형과 두 가지 기존 하드 디스크 드라이브 유형이 있다.\n볼륨 선택은 워크로드와 예산에 따라 결졍되며, 인스턴스 유형에는 임시 인스턴스 스토어 볼륨이 사용된다.\n일부 EC2 인스턴스 유형에는 임시 인스턴스 스토어 볼륨이 사용되는 데, 스토어 볼륨은 데이터 엑세스는 빠르지만 인스턴스가 종료되면 데이터가 종료된다.\n모든 EC2 인스턴스는 최소 하나의 프라이빗 주소를 가지고 있으며, 인터넷 엑스세가 필요하면 임시 퍼블릭 IP 주소를 할당한다.\nEIP를 통해 영구적인 IP를 할당할 수도 있다.\nEC2 인스턴스를 보호하기 위해서 보안 그룹이라고 하는 소프트웨어 방화벽으로 액세스를 허용하거나 차단하고, IAM 역할, NAT 인스턴스/ 게이트웨이, 키 페어 등이 사용된다.\n시험핵심 EC2 인스턴스를 프로비저닝하고 시작하는 방법을 이해한다.\n워크로드에 적합한 하드웨어/ 소프트웨어 프로파일을 선택 방법을 이해한다.\nEC2 요금 모델과 필요에 맞는 요금 선택 방법을 이해한다.\n배포 프로파일에 맞게 보안과 액세스의 균형을 조절해서 보안 그룹을 구성하는 방법을 이해한다.\n실행 중인 인스턴스에 액세스하는 방법을 이해한다.\n스토리지 볼륨 유형의 기능과 작동을 이해한다.\n스토리지 볼륨에서 스냅샷 생성 방법과 다른 인스턴스에 스냅샷을 연결하는 방법을 파악한다.","#":"","2장-amazon-elastic-compute-cloud와-amazon-elastic-block-store#\u003cstrong\u003e2장 Amazon Elastic Compute Cloud와 Amazon Elastic Block Store\u003c/strong\u003e":"","2장-요약#\u003cstrong\u003e2장 요약\u003c/strong\u003e":"","기타-ec2-서비스#\u003cstrong\u003e기타 EC2 서비스\u003c/strong\u003e":""},"title":"2장 EC2와 EBS"},"/system/aws/awssaa/saa-3/":{"data":{"":"3장 Amazon Simple Storage Service와 Amazon Glacier Storage Service 3장의 목표 복원력을 갖춘 아키텍처 설계\n안정적이고/ 복원력을 갖춘 스로티리지를 선택한다. 어떻게 멀티 티어 아키텍처 솔루션을 설계할지 결정한다. 어떻게 고가용성 및 내결함성을 갖춘 아키텍처를 설계할지 결정한다. 성능이 뛰어난 아키텍처 정의\n성능이 뛰어난 스토리지 및 데이터베이스를 선택한다. 탄력성과 확장성을 갖춘 솔루션을 설계한다. 안전한 애플리케이션 및 아키텍처 설명\n어떻게 애플리케이션 티어를 보호할지 결정한다. 어떻게 데이터를 보호할지 결정한다. 비용에 최적화된 아키텍처 설게\n어떻게 비용에 최적화된 스토리지를 설계할지 결정한다. Amazon simple Storage Service ( 이하 S3 ) S3는 개인 애플리케이션, 다수 AWS 서비스의 데이터를 보관하며,다음 워크로드를 위한 훌륭한 플랫폼이다.\nS3의 주요기능\n백업 아카이브 로그 파일, 재해 복구 이미지 유지 관리 분석을 위한 데이터 저장 정적 웹 사이트 호스팅 S3는 객체 스토리지로, 전 장에서 배운 EC2는 인스턴스를 구동하는 반면 S3는 무제한 객체 스토리지 공간을 효과적으로 제공한다.\n객체 스토리지와 블록 스토리지의 차이점\nType 차이점 예시 블록 스토리지 물리적 디스크를 개별 블록으로 나눠 데이터를 저장하고 파일 시스템으로 관리 Window NTFS, Linux Btrfs, ext 등 객체 스토리지 구조화되지 않은 평면의 저장소에 데이터를 저장하여 무제한의 스토리지를 구현가능 S3, swift 등 S3에 파일을 쓸 때는 2KB 메타 데이터가 함께 저장되며, 이 메타 데이터는 세부 정보를 구성하는 키로 만들어지며, 데이터 사용 권한과 파일 시스템처럼 보여지는 중첩 버킷 내 위치 정보 등이 저장된다. S3 서비스 아키텍처 S3 파일은 버킷으로 구성되며, AWS 계정당 기본으로 만들 수 있는 버킷은 100개이다.\n버킷 또한 할당량에 초과사용을 요청할 수 있다.\nS3 버킷은과 내용은 한 AWS 리전에만 존재하며, 버킷의 주소는 S3 글로벌 시스템 내에서 유일해야한다. ( 증복이 허용되지 않음 )\n이는 버킷에 보다 쉽게 접속하기 위해 규칙을 정해놓은 것이다.\n$ https://[ bucketname ].s3.[ region code ].amazon.com/[ filepath ] # 엑세스를 위한 URL $ s3://[ bucketname ]/[ filepath ] # CLI 환경에서의 엑세스 이론적으로 버킷에는 무한정의 대이터를 저장할 수 있지만, 단일 객체의 크기는 5TB를 넘을 수 없고, 한 번에 용량에 업로드할 수 있는 용량 크기는 최대 5GB이다.\n100MB보다 큰 객체를 업로드시에는 멀티 파트 업로드를 사용해서 데이터의 손실 및 업로드가 중지되는 위험을 줄일 수 있다.\n멀티 파워 업로드 : 데이터를 나눠서 업로드 하는 방식 ( ex : 분할 압축 ) 단, 상위 수준의 API에서는 멀티 파트 업로드가 자동이지만, 하위 수준에서는 수동으로 나눠야 한다.\n암호화 웹 사이트와 같이 퍼블릭에서 엑세스하는 용도가 아니라면, S3에 저장할 데이터는 항상 암호화해야 한다.\nS3에 저장 중인 데이터를 보호하기 위해서 암호화 키를 사용할 수도 있고, S3에서 다른 위치로 전송하는 데이터를 보호하기 위해서 Amazon 암호화 API 엔드포인트 만을 사용할 수 있다.\n저장 중 데이터는 서버 측 암호화 혹은 클라이언트 측 암호화를 사용해서 보호할 수 있다.\n서버 측 암호화 서버 측의 암오화는 S3 플랫폼 자체의 암호화를 의미하며, 데이터 객체를 암호화해서 적합한 인증으로 복호화하는 작업이 AWS에서 이루어진다. Amazon S3가 관리하는 암호화 키(SSE-S3)를 사용하면 AWS가 자체 엔터프라이즈 표준 키를 사용해 암호화 복호화 프로세스의 모든 단계를 관리한다. AWS KMS-관리형 키를 사용하는 서버 측 암호화(SSE-KMS)를 사용하면 SSE-S3가 제공하는 기능에 더해 완벽한 키 사용 추적과 봉투 키를 사용할 수 있다. 고객 제공 암호화 키에 의한 서버측 암호화(SSE-C)는 고객이 S3에 제공한 자체 키로 객체를 암호화 한다. 클라이언트 측 암호화 S3로 데이터를 전송하기 전에 암호화하는 것으로, AWS KMS-관리현 고객 마스터 키(KMS-CMK)를 사용하며, 업로드 전에 고유 키로 객체를 암호화한다. 복잡한 암호화 절차를 단순화하기 때문에 서버 측 암호화를 많이 사용하지만, 회사 내에서 암호화 키의 모든 권한을 가지고 있어야 하는 경우도 있을 때 주로 사용된다. 로깅 S3 이벤트 추적을 로그 파일에 저장하는 기능은 처음에는 비활성화 되어있다.\nS3 버킷에서 일어나는 많은 활동을 로그 데이터로 만들어 기록할 필요 없기 때문이며, 로그파일을 기록하는 것을 이를 로깅이라한다.\n로깅을 활성화 할 때는 원본 버컷과 대상 버킷을 지정해야 하며, 하나의 대상 버킷에 여러 원본 버킷 로그를 저장했을 때 쉽게 시벽할 수 있도록 구분 기호와 접두사를 사용한다.\nS3는 CloudWatch나 CloudTrail와 같은 AWS 서비스 및 다른 서버들의 로그저장하는 데에도 사용된다.\nS3 생성 로그는 구성화면, 잠시 후 다음과 같은 작업 상세 항목이 기본으로 나타난다.\n요청자 게정과 IP 주소 원본 버킷 이름 요청 동작(GET, PUT) 요청 개시 시간 응답 상태 ( 오류 코드 포함 ) S3 내구성과 가용성 객체를 저장할 때 여러 S3 스토리지 클래스 중에서 선택할 수 있으며, 내구성, 가용성, 지출가능 비용에 따라 선택한다. 관련용어 설명 키워드 설명 내구성 데이터가 손실되지 않을 확률 가용성 객체가 사용 가능한 기간 지출가능 비용 사용시 지불해야하는 가격 내구성 내구성은 백분율로 측정되며, Amazon Glacier의 경우 99.999999999% 내구성을 보장하는 데 이는, 10.000.000개의 객체를 저장하면 1만 년 동안 객체가1개 손실될 확률을 의미한다.\n즉 S3 Standard/Glacier 플랫폼에 저장한 데이터를 인프라 장애로 손실할 가능성은 거의 없다고 볼 수 있다.\nS3는 최소 3개의 가용 영역에 데이터를 자동으로 복제하기 때문에, 높은 내구성을 보장할 수 있다.\n하지만 복원력이 없는 두 클래스도 존재하는 데, Amazon S3 One Zone-IA(Infrequent Access)은 단일 가용 영역에만 저장하며, RRS(Reduced Redundancy Storage)는 다른 클래스보다 적은 영역에 복제하기 때문에 99.99% 내구성만을 보장한다.\nS3 스토리지 안정성 보장 표준 S3 Standard S3 Standard-IA S3 One Zone-IA RRS 내구성보장 99.999999999% 99.999999999% 99.9999999999% 99.99% 내결함성을 위한 동시 복제 시설 수 2 2 1 1 가용성 객체 가용성도 백분율로 측정하며, 1년 동안 해당 객체를 요청했을 때 즉시 응답할 수 있는 기간을 백분율로 나타낸다.\n만약 Amazon S3 Standard 클래스는 연간 99.99%의 가용성을 가지고 있는 데, 이는 1년 동안 중단 시간이 1시간 이내를 의미한다.\nS3 스토리지 표준 가용성 보장 S3 Standard S3 Standard-IA S3 One Zone-IA RRS 가용성 보장 99.99% 99.9% 99.5% 99.99% 데이터의 최종 일관성 S3는 데이터를 여러 장소에 복제하므로, 기존 데이터가 업데이트되면 시스템에 전파하느 동안에 지연시간이 발생할 수 있다. 단 이는 객첼르 생성(PUT) 할 때에는 객체 버전이 충돌할 가능성이 없으므로, 쓰기 후 읽기 일관성이 제공된다. 객체 수명 주기 S3에서 시작하는 워크로드는 대개 백업 아카이브와 관련이 있다.\n백업 아카이브는 주기적으로 저장되므로 이에 대한 관리가 필요한 데, S3버전 관리를 통해 해결이 가능하다\n버전 관리 기본적으로 동일한 파일을 업로드 시키는 경우에는 덮어씌어지는 데, 이는 심각한 문제를 초래할 수 있다. S3도 이와 동일하게 작동하지만, 버킷 수준에서 버전 관리를 활성화하게 되면 이전 객첼르 보존할 수 있어, 기존 버전에 계속 엑세스 하는 것이 가능하다. 수명 주기 관리 버킷에서 수명 주기 규칙을 구성하면 지정한 기간이 경과했을 때 자동으로객체가 다른 스토리지 클래스로 옮겨진다. 예를 들면 첫 30일동안은 S3 Standard 클래스에서 보관되지만, 그 이후에는 보다 저렴한 One Zone IA으로 옮겨진다. 과거 버전을 지속저으로 유지해야하는 경우 장기 저장용 Storage 서비스 Glacier으로 365일 보관이 가능하다. S3 객체 엑세스 S3에 데이터를 저장해서 사용하겠다고 결정했다면 중요성에 맞게 S3에 저장된 객체에 엑세스하는 방법과 업무의 보안상 필요에 맞는 요청만 엑세스하도록 제한하는 방법이 필요하다. 엑세스 제어 외부 사용자는 버킷의 객체에 엑세스가 불가능하지만, ACL (엑세스 제어 목록), S3 버킷 정책, IAM 정책을 통해서 버킷이나 객체 수준에서 접근이 가능토록 할 수 있다. 위와 같은 ACL, S3 버킷 정책, IAM은 일부 중복되어 있으며, 이는 점차 서비스가 발전해오면서 새로운 기능이 추가된 서비스가 생성되었기 때문이다. 현재는 ACL대신 S3 버킷 정책이나 IAM을 사용하길 권장하고 있다. S3 버킷 정책 (JSON 형식으로 S3 버킷에 연결)은 외부 계정과 사용자가 S3 버킷에 엑세스하는 것을 제어할 수 있는 반면, IAM 정책은 IAM이 관리하느 계정, 즉 사용자와 역할이 S3를 비롯한 여러 리소스에 엑세스하는 방식을 제어하고자 할 때 사용된다. 미리 서명된 URL 외부 엑세스가 제한된 프라이빗 객체에 임시로 엑세스할 수 있게 할 때, 미리 서명된 URL을 사용할 수 있다. 미리 서명된 URL은 시간 제한이 존재하며, 기간이 지나면 사용이 불가능해지며 프로그래밍 방식으로 객체에 엑세스가 가능하다. $ aws s3 presign s3://[ MybucketName ] /[ FilePath ] --expires-in [ second ] # second 만큼의 초 시간의 특정 File의 엑세스를 허용하는 CLI 명령어 정적 웹 사이트 호스팅 S3 버킷은 정적 웹 사이트 HTML 파일 호스팅에도 사용 정적 웹 사이트는 웹 페이지와 스크립트를 랜더링할 때 서버가 아닌 클라이언트 시스템 서비스를 사용 $ aws s3api put-bucket-acl --bucket [ MybucketName ] --acl Public-read # 버킷의 호스팅 설정을 추가하는 CLI 명령어 $ aws s3 website s3://[ MybucketName ] --index-document index.html --error-document error.html $ 버킷의 정적 웹 사이트를 호스팅하며 메인 페이지와 에러 페이지를 설정한다. ( 수정 가능 ) S3 Select와 Glacier Select AWS는 S3나 Glacier에 저장한 데이터에 엑세스할 수 있는 또 다른 방법을 제공하는 데, 이를 Select이라 한다. 이를 사용하면 SQL와 유사한 쿼리로 저장된 객체에서 관련 데이터만 검색하는 것이 가능ㅎ다ㅏ. Amazon Glacier Glcanier는 S3 스토리지 클래스의 일부로 Glacier는 대부분 S3 클래스와 마찬가지로 99.999999999% 내구성을 보장하고, S3 구셩 주기에 통합할 수 있다. 단 S3와 다른 점은 S3는 단일 객체 최대 크기가 5TB인 반면, Glacier은 40TB까지의 대형 아카이브를 지원하고, S3에서는 암호화를 선택해야하지만, Glacier는 인간이 읽을 수 없는 ID가 주어진다. Glacier의 단점은 데이터를 가져오는 데 걸리는 시간으로 S3는 즉시 엑세스가 가능하지만, Glacier 아카이브에서 객체를 가져오려면 몇 시간이 걸릴 수도 있다. 이와 같이 Glacier의 목적은 데이터의 필요성과 사용빈도가 낮은 한경에서 장기적으로 데이터를 보관할 수 있는 저렴한 스토리지로 사용할 수 있다는 것이다. 스토리지 요금 스토리지 요금은 버전 관리와 객체 수명주기로 계속해서 파일이 이동하므로 과정이 복잡하다 할 수 있다. 서울 리전 스토리지 요금 예시 클래스 스토리지 용량 요금/GB/월 비용/ 월 Standard 20G $0.018 $0.36 Standard 65G $0.0144 $0.938 Standard 520G $0.005 $2.6 합계 $3.398 이 외에도 트래픽 관련 요금이 부과되며 기타 모든 요금에 대한 정보는 여기에서 확인할 수 있다. AWS 월 사용량 계산기 기타 스토리지 관련 서비스 AWS에는 S3, Glacier 이외에도 다양한 Storage Service 있다. Amazon Elastic File System ( EFS ) EFS자동 확장 가능한 공유 파일 스토리지 서비스. 동일 VPC 내의 Network File System ( NFS )으로 여러 EC2 인스턴스에 장착한다. AWS Direct Connect 연결로 온프레미스 서버에서 엑세스할 수 있도록 설계되어 있다. AWS Storage Gateway 온 프레미스의 로컬 백업과 아카이브 운영 요구 사항을 클라우드 스토리지 서비스를 사용해 해결하려면 복잡하진다. AWS Storage Gateway는 소프트웨어 형식의 게이트웨이로 VMware, EC2 ,Hyper-V 와 등에 사용하면 보다 쉽게 S3, EBS로 데이터의 이전이 가능하다. AWS Snowball 대용량 데이터 세트를 일반 인터넷 연결로 클라우드에 마이그레이션하려면 많은 시간과 대역폭이 필요로 한다. 테라 혹은 페타바이트 크기의 데이터를 옮길 때문 AWS에서 256비트로 암호화한 물리적 장치인 Snowball을 사용자에게 배송하며, 이를 AWS다시 수거해 S3에 올려준다. 요약 Amazon S3는 적은 유지 관리 노력으로 대용량 아키이브와 데이터 스토리지를 운영할 수 있도록 안정성과 고가용성을 갖춘 객체 스토리지를 제공한다.\n객체는 게층화 돼 있지 않은 버킷에 저장되어 있지만, 접두사를 사용해서 일반 파일 시스템에 있는 것처럼 보일 수 있다.\nAWS가 제공하는 암호화 키 또는 자체 암호화 키를 사용해 S3 자체 데이터를 암호화 할 수 있으며, 대개 필수로 데이터를 암호화한다.\n암호화의 종류로는 서버 측 암호화, 클라이언트 측 암호화로 저장 중 암호화가 이루어진다.\nS3는 데이터 복제 정도가 다른 여러 스토리지 클래스를 제공해서 사용자가 내구성, 가용성, 비용을 고려해서 선택할 수 있게 한다.\n기존 ACL, S3 버킷 정책, IAM을 통해 보안 주체, 대상, 시간을 제어할 수 있다. 일시적으로 제한된 데이터 엑세스를 제공하는 안전한 방법으로는 미리 서명된 URL을 사용한다.\nSQL과 유사한 S3 Select와 Glacier Select를 사용하면 데이터 요청 크기와 비용을 줄일 수 있으며, S3 버킷에 저렴하고 간단한 정적 웹 사이트를 만들 수도 있다.\nAmazon Claier은 데이터 아카이브를 볼트에 저장하고 가져올 때는 몇 시간이 걸리지만, S3 스토리지 클래스보다 비용이 저렴하다.\n시험 핵심 S3 리소스 구성되는 방식을 이해한다. S3 객체는 버킷에 저장되는 데, 버킷 이름은 글로벌하게 고유해야 하며, 버킷은 Region과 연결된다. 객체는 구조화되지 않은 버킷에 저장되지만 접두사와 기호를 사용해서 데이터에 폴더 게층 구조를 나타낼 수 있다. 데이터 전송을 최적화하는 방법을 이해한다. S3 버킷에 저정하는 개별 객체의 크기는 5TB이며, 100MB보다 큰 객체는 멀티 파트 업로드를 사용해야 한다. 5TB보다 큰 객체는 멀티 파트 업로드 이외의 다른 업로드 방법이 없다. S3 데이터 보안 방법을 이해한다. AWS에서 생성한 키 또는 비공개 키로 서버 측 암호화를 사용하면 S3 버킷 내에서 데이터를 보호할 수 있다. 클라이언트 측 암호화를 사용해 S3 전송되기 전에도 데이터를 암호화 할 수 있다. S3 객체의 내구성과 가용성을 측정하는 방법을 이해한다. 다양한 S3 클래스와 Glacier는 여러 수준에서 인프라 안정성과 데이터 가용성을 약속한다. S3 객체 버전 관리와 수명 주기 관리를 이해한다. 객체를 덮어 쓴 뒤에도 덮어쓰기 전 객체를 보존해서 액세스 할 수 있다. 지연 시간이 짧은 스토리지 클래스에 지연 시간이 긴 클래스로 자동 전환하는 방법은 오래된 객체를 관리할 수 있고, 최종적으로 삭제 예약 또한 가능하다. S3 객체를 보호하는 방법을 이해한다. 기존 버킷과 객체 기반 ACL 규칙으로 엑세스를 제어할 수 있고, 더욱 유연한 S3 버킷 정책이나 곚어 수준 IAM 정책으로 엑세스를 제어할 수 있다. 미리 서명된 URL를 통해 임시로 엑세스를 허용할 수 있다. 정적 웹 사이트를 만드는 방법을 이해한다. S3에 HTML, 미디어 파일을 저장하고 Route 53과 CloudFront를 사용해서 DNS 도메인 이름으로 액세스 할 수 있는 암호화된 HTTPS 페이지 웹사이트로 제공할 수 있다. S3와 Glacier의 차이를 이해한다. Glacier는 자주 요청되지 않으리라고 예상하는 데이터 아카이브를 위한 저렴한 장기 보존 스토리지이다. ","#":"","3장-amazon-simple-storage-service와-amazon-glacier-storage-service#\u003cstrong\u003e3장 Amazon Simple Storage Service와 Amazon Glacier Storage Service\u003c/strong\u003e":"","시험-핵심#\u003cstrong\u003e시험 핵심\u003c/strong\u003e":""},"title":"3장 S3와 Glacier"},"/system/aws/awssaa/saa-4/":{"data":{"":"4장 Amazon Virtual Private Cloud 4장의 목표 복원력을 갖춘 아키텍처 설계 어떻게 멀티 티어 아키텍처 솔루션을 설계할지 결정한다. 안전한 어플리케이션 및 아키텍처 설계 단일 VPC 어플리케이션을 위한 네트워킹 인프라르 정의한다. 단일 VPC 애플리케이션을 위한 네트워킹 인프라르 정의한다. Virtual Private Cloud ( 이하 VPC ) VPC란 EC2의 네트워크 계층으로, EC2 인스턴스를 비롯한 여러 AWS 서비스에 네트워크 리소스를 담을 수 있는 가상의 네트워크를 의미한다.\n모든 VPC는 기본적으로 다른 모든 네트워크와 격리되어 있지만, 필요할 때는 인터넷 및 다른 VPC와 연결이 가능하다.\nVPC는 한 리전안에서만 존재할 수 있으며, 한 리전에 만든 VPC는 다른 리전에서는 볼 수 없다.\nVPC에는 라우터, 스위치, VLAN과 같은 기존 네트워크 구성 요소들이 존재하지 않으며, 확장성을 실현하기 위해 소프트웨어 기능으로 추상화하였다.\nVPC CIDR 블록 기존 네트워크와 동일하게 VPC는 하나 이상의 연속적인 IP 주소 범위로 구성되며, CIDR ( Classless Inter Domain Routing ) 블록으로 표시된다.\nVPC 내 인스턴스를 비롯한 리소스에 해당되는 IP 주소는 CIDR 블록으로 결정되며, VPC를 만들 때는 기본 CIDR 블록 주소의 할당이 필수이다.\n간략한 CIDR 설명\nIP 접두사라는 CIDR 블록의 /16 부분은 접두사의 길이를 의미하며, VPC CIDR 접두사의 길이는 /16부터 /28까지를 의미한다.. CIDR과 IP 주소는 반비레 관계이며 접두사의 길이가 작을 수록 CIDR의 IP 주소는 많아진다. 예를 드면 /28 접두사의 길이는 16개의 IP 주소만이 사용 가능하다. 주소 대역 할당 IP 대역 10.0.0.0 10.255.255.255 ( 10.0.0.0/8 ) 172.16.0.0 172.31.255.255 ( 172.16.0.0/12 ) 192.168.0.0 192.168.255.255 ( 192.168.0.0/16 ) VPC는 온 프레미스 네트워크나 다른 VPC 등 다른 네트워크에 연결하려면 사용할 VPC CIDR과 다른 네트워크에 연결하려면 주소와 중복되지 않도록 해야 한다.\n기본 CIDR 블록은 변경할 수 없으므로 VPC를 만들기 전에 주소 요구 사항을 신중히 검토해야 한다.\n보조 CIDR 블록 VPC를 만든 후에도 보조 CIDR 블록을 지정할 수 있다.\n보조 CIDR 블록은 기본 CIDR 주소 범위나 퍼블릭 라우팅이 가능한 범위 내에서 생성돼야 하며, 기본 블록 또는 다른 보조 블록과 겹치지 않아야 한다.\nVPC가 172.16.0.0/16일 경우 172.17.0.0/16으로 지정할 수 있지만, 192.168.0.0/16으로는 지정할 수 없다.\nIPv6 CIDR 블록 VPC에 IPv6 CIDR을 할당 할 수 있으나, IP 접두사를 지정할 수 있는 기본 CIDR과는 달리 IPv6에서는 CIDR을 지정할 수 없다.\nAWS에 요청을 하면, AWS가 VPC에 IPv6을 할당한다.\nIPv6의 VPC CIDR의 접두사의 길이는 항상 /56이다.\n서브넷 서브넷은 VPC 내 논리 컨테이너로 EC2 인스턴스를 배치하는 장소이다.\n서브넷으르 통해 인스턴스를 서로 격리하고, 인스턴스 간 트래픽 흐름을 제어하고, 인스턴스를 기능별로 모을 수 있다.\n인스턴스는 서브넷 안에 있어야 하며, 한 서브넷에 생성된 인스턴스는 다른 서브넷으로 이동이 불가능하다.\n서브넷 CIDR 블록 서브넷의 CIDR은 VPC CIDR의 일부이면서, VPC 내에서 고유해야 한다.\n서브넷의 모든 IP의 첫 4개, 끝 1개는 사용할 수 없다.\n172.16.100.0 ~ 172.16.100.3 127.16.100.255 서브넷 CIDR 접두사의 길이 제한은 VPC CIDR과 동일할 수 있지만, 이리하면 공간이 남지 않기에 보통 서브넷의 CIDR은 VPC보다 길다.\nVPC는 보조 CIDR을 가질 수 있지만, 서브넷에는 하나의 CIDR만이 사용 가능하다.\n만약 VPC가 보조 CIDR을 가지고 있을 경우, 서브넷은 적합한 CIDR을 선택해 생성할 수 있다.\n가용 영역 서브넷은 하나의 가용영역 ( Availability Zone 이하 AZ ) 내에서만 존재할 수 있으며, 가용 영역은 상대적으로 작은 지리적 위치, 데이터 센터의 개념이다.\nAWS 리전의 가용영역들은 서로 연결되어 있으며, 한 가용 영역에 잘애가 발생하더라도 다른 영역에 장애의 영향이 미치지 않도록 설게되어있다.\n즉, 서로 다른 가용영역에 서브넷은 만든 후, 인스턴스를 각각 생성하면 애플리케이션은 복원성을 사용가능하다.\nSubnet AZ Instance web-subnet1 us-east-1a web1 web-subnet2 us-east-1b web2 위와 같이 만약 us-east-1a의 생성된 web1의 인스턴스의 문제가 발생하더라도, web2 인스턴스의 서브넷은 다른 AZ에 속해있기 때문에 무중단 서비스가 가능하다. IPv6 CIDR 블록 VPC에 IPv6 CIDR을 할당하면 해당 VPC 내 서브넷에 IPv6 CIDR을 할당할 수 있다. IPv6 서브넷의 접두사 길이는 /64로 고정되어있다. 탄력적 네트워크 인터페이스 탄력적 네트워크 인터페이스 ( Elastic Network Interface 이하 ENI )를 사용하면 인스턴스가 AWS 서비스, 다른 인스턴스, 온 프레미스 서버, 인터넷 등 다른 네트워크의 리소스와 통신할 수 있다.\n기본적으로 물리 서버의 네트워크 인터페이스와 동일한 기능을 수행한다.\n기본 프라이빗 주소와 보조 프라이빗 IP 주소 각 인스턴스는 기본 프라이빗 주소를 가지고 있어야 하는 데, 그 주소는 서브넷 CIDR에서 지정한 범위 내 주소여야 한다.\n기본 프라이빗 IP 주소는 인스턴스의 기본 ENI에 연결되며 이 주소는 변경하거나 삭제가 불가능하다.\n보조 프라이빗 IP 주소에는 ENI를 할당할 수 있으며, 보조 주소는 ENI가 연결된 서브넷의 주소 내에서 할당된다.\nENI를 인스턴스에 추가해서 연결할 수 있고, 이 ENI를 다른 서브넷에 둘 수도 있지만, 해당 인스턴스와 같은 가용 영역에 있어야하며, ENI와 연결된 주소는 ENI가 있는 서브넷에서 가져와야 한다.\n탄력적 네트워크 인터페이스 연결 ENI는 인스턴스와 독립적으로 존재가 가능하며, 먼저 ENI를 생성한 후 인스턴스를 생성하고 할당할 수 있다. 인터넷 게이트웨이 인터넷 게이트웨이는 퍼블릭 IP 주소를 할당받은 인스턴스가 인터넷과 연결돼서 인터넷으로부터 요청을 수신하는 기능을 수행한다.\n처음 VPC를 생성하고, VPC에는 인터넷 게이트웨이가 연결되어 있지 않으므로, 직접 인터넷 게이트웨이를 만들어 VPC와 연결해야한다.\n인터넷 게이트웨이는 인터넷 서비스를 제공하는 업체의 온 프레미스에 설치하는 인터넷 라우터와 유사하지만, AWS에서 인터넷 게이트웨이는 라우터와 동일하게 동작하지는 않는다.\n기존 네트워크에서는 핵심 라우터의 주소를 인터넷으로 향하는 기본 게이트웨이 IP 주소로 구성해서 각 서버가 인터넷에 엑세스 할 수 있도록 한다.\n인터넷 게이트 웨이는 관리형 IP나 네트워크 인터페이스가 없는 대신, 식별을 위해 AWS 리소스 ID가 할당된다.\n인터넷 게이트 웨이는 igw-로 시작하며 그 뒤에는 영 숫자나 문자열이 온다.\n인터넷 게이트웨이를 사용하려면 인터넷 게이트웨이를 대상으로 하는 기본 라우팅을 라우팅 테이블에 만들어야 한다.\n라우팅 테이블 VPC는 리소스 형태로 가상 라우터를 구성하지 않음, 소프트웨어 함수로 IP 라우팅을 구현한 내재된 라우터를 제공한다.\n사용자는 가상 라우터에서 인터페이스 IP 주소나 동적 라우팅 프로토콜을 구성하지 않고 내재된 라우터에서 라우팅 테이블만 관리하면 된다.\n각 라우팅 테이블은 하나 이상의 라우팅과 하나 이상의 서브넷의 연결로 구성되며, 기존의 라우터와 거의 동일한 방식으로 여러 서브넷에 연결되어 있다.\n라우팅 테이블과 서브넷은명시적으로 직접 연결하지 않더라도 서브넷에 암시적으로 기본 라우팅 테이블이 연결되므로, 모든 서브넷은 라우팅 테이블으로 연결된다.\n라우팅 라우팅은 라우팅 테이블과 연결된 서브넷 내 인스턴스에서 트래픽을 저달하는 방법을 결정한다.\nIP라우팅은 원본 IP 주소가 아닌 대상 IP주소를 기반으로 작동하므로, 하위의 요소는 제공해야 한다.\n대상 주소 대상 IP 주소는 CIDR 표기법의 IP 접두사여야 하며, 대상에는 CIDR은 사용할 수 없고, 인터넷 게이트웨이, ENI 등의 AWS 리소스가 지정되어야한다.\n모든 라우팅 테이블에는 각각 다른 서브넷에 있는 인스턴스들이 서로 통신할 수 있게 하는로컬 라우팅이 포함되어 있다.\n기본 라우팅 인스턴스가 인터넷에 액세스하게 하려면 인터넷 게이트웨이를 가리키는 기본 라우팅을 생성해야 한다. 대상 주소 대상 172.31.0.0/16 LOCAL 0.0.0.0/0 igw-xxxxxxxxx 인터넷 상의 모든 호스트 IP 주소를 표기할 때는 0.0.0.0/0 접두사를 사용하므로, 기본 라우팅 대상 주소로 0.0.0.0/0의 접두사를 등록해야 한다.\n인터넷 게이트웨이를 가리키는 기본 라우팅이 포함된 라우팅 테이블과 연결된 서브넷을 퍼블릭 서브넷이라 한다.\n이와 반해서 프라이빗 서브넷은 기본 라우팅을 포함하지 않는다. 0.0.0.0/0과 172.31.0.0/16이 증복되어 있을 때, 트래픽을 라우팅할 위치를 결정할 때 내재된 라우터는 가장 근접하게 일치된 항목을 기반으로 라우팅 한다.\nAWS docs에서는 VPC 당 내재된 라우터가 하나 존재한다고 기술되어 있으며, 이 라우터란 실제 개별 리소스가 아닌 IP 라우팅 기능을 추상화한 것으로 이해하면 된다.\n보안 그룹 보안 그룹은 방화벽과 같은 기능을 제공하며, 인스턴스의 ENI의 송수신되는 트래픽을 허가해서 인스턴스를 오가는 트래픽을 제어해야 한다.\n모든 ENI에는 최소한 하나의 보안 그룹이 연결되어야 하고, 한 ENI에 여러 개의 보안 그룹을 연결할 수 있으며, 한 보안 그룹을 여러 개의 ENI에 연결할 수도 있다.\n실제 인스턴스는 ENI를 하나만 연결해서 사용하기 땜누에 하나의 인스턴스에 하나의 보안 그룹만이 연결되어 있다고 생각하기 쉬운데, 인스턴스에 ENI가 여러 개 있는 경우에는 반드시 확인이 필요하다.\n보안 그룹 인바운드 규칙 인바운드 규칙은 연결된 ENI에 허용할 트래픽을 정의하는 것으로 아래의 3 가지의 필수 요소를 포함한다.\n출발 주소 ( source address ) 프로토콜 포트 범위 새로 생성한 보안 그룹에는 인바운드 규칙이 없으며, 연결한 인스턴스의 모든 트래픽을 차단한다.\n필요에 따라 특정 트래픽을 허용하려면 그 때마다 인바운드 규칙을 만들어야 하고, 이러한 이유로 보안 그룹에서 규칙의 순서는 중요하지 않다.\n아래의 항목은 HTTP와 SSH의 예시이다.\n원본 프로토콜 포트 범위 151.123.231.2 ( 자기자신이 사용하고 있는 특정 IP ) TCP ( SSH ) 22 0.0.0.0/0 ( 모두 접속할 수 있어야 하므로 ) HTTP 80 보안 그룹 아웃바운드 규칙 아웃바운드 규칙은 인스턴스에 연결된 ENI르 통해 송수힐 수 있는 트래픽을 정의한 것으로, 아래의 세가 요소를 포함한다.\n목적지 주소 프로토콜 포트 범위 보안 그룹의 아웃바운드 규칙은 인바운드 규칙보다 제한이 적으며, 보안 그룹을 생성할 때, AWS는 기본적으로 하단의 표로 모든 프로토콜과 포트가 열려 있는 아웃 바운드 규칙을 생성한다.\n목적지 주소 프로토콜 포트 범위 0.0.0.0/0 모두 모두 원본과 대상 주소 규칙의 원본이나 대상 주소에 CIDR 또는 보안 그룹의 리소스 ID를 지정할 수 있고, 보안 그룹에 연결된 모든 인스턴스 규칙이 적용된다. 상태 저장 방화벽 보안 그룹은 상태 저장 방화벽 역할을 수행한다.\n상태 저장이란 보안 그룹이 트래픽을 한 방향으로 전달되도록 허용할 때, 반대 방향의 응답 트래픽을 지능적으로 허용하는 것을 의미한다.\n기본 보안 그룹 각 VPC엔느 삭제할 수 없는 기본 보안 그룹이 있으며, 필요하면 규칙을 수정할 수 있다.\n사용자 지정 봉나 그룹을 만들어 대신 사용하는 것도 가능하다.\n네트워크 엑세스 제어 목록 ( Network Access Contol List : 이하 NACL ) NACL은 원본 또는 대상 주소 CIDR, 프로토콜, 포트를 기반으로 트래픅을 허용하는 인바운드와 아웃바운드 규칙을 포함한다.\nNACL은 보안 그롭, 방화벽의 기능을 수행하며, 각 VPC에는 삭제할 수 없는 NACL이 있다는 점도 보안 그룹과 유사하다.\n하지만 NACL은 보안그룹과 달리, ECI가 아닌 서브넷에 연결되며, 서브넷과 연결된 NACL은 해당 서브넷과 송수신되는 트래픽을 제어한다.\n즉, 서브넷 내의 인스턴스간 트래픽을 제어할 때는 NACL을 사용할 수 없으며, 보안 그룹을 사용해야한다.\n서브넷은 하나의 NACL만 연결할 수 있으며, VPC에 서브넷을 만들면 기본적으로 VPC의 기본 NACL이 연결된다.\n서브넷과 NACL이 같은 VPC에 있다면 하나의 NACL을 여러 서브넷에 연결하는 것이 가능하다.\nNACL 인바운드 규칙 인바운드 규칙은 서브넷에 진입할 수 있는 트래픽을 정의하며, 다음 요소들을 포함한다.\n규칙 번호 프로토콜 포트 범위 출발 주소 동작 ( 허용/ 거부 ) IPv6 CIDR이 없는 VPC는 기본 NACL에는 표 4.5에 나열된 두 가지 인바운드 규칙이 포함되어있다.\n규칙번호 프로토콜 포트 범위 출발 주소 허용/ 거부 100 모두 모두 0.0.0.0/0 ALLOW * 모두 모두 0.0.0.0/0 DENY NACL 규칙은 규칙 번호의 오름차순으로 처리된다. NACL 아웃바운드 규칙 아웃바운드 또한 인바운드 규칙과 거의 같은 형식을 따르며, 아래의 요소들을 포함한다.\n규칙 번호 프로토콜 포트 범위 대상 주소 동작 ( 허용/ 거부 ) 각 기본 NACL은 하단에 나열된 아웃바운드 규칙으로 제공되며, 아웃바운드 규칙은 대상 주소를 제외하고는 기본 인바운드 규칙과 동일하다.\n규칙번호 프로토콜 포트 범위 도착 주소 허용/ 거부 100 모두 모두 0.0.0.0/0 ALLOW * 모두 모두 0.0.0.0/0 DENY NACL은 상태 비저장이므로 응답 트래픽을 자동으로 허용하지 않기 때문에, 만약 인바운드에서 HTTPS의 트래픽을 허용한다면, 아웃바운드 규칙에서도 응답 트래픽을 명시적으로 허용해야한다. 기본적으로 최신 운영 체제에서는 49125-65535 범위의 임시 포트를 허용하지만, 이 범위는 충분하지않을 수 있다. 네트워크 엑세스 제어 목록과 보안 그룹을 같이 사용 사용자가 인스턴스를 시작할 때, 보안 그룹을 올바르게 지정해야 하는 부담을 줄이기 위해 보안 그룹과 NACL을 함께 사용할 수 있다.\nNACL은 서브넷에 적용되므로 NACL 규칙은 보안 그룹 구성 방법과 관계없이 서브넷에서 송수신하는 모든 트래픽에 적용된다.\nNACL이나 보안 그룹은 규칙을 변경하면 해당 변경 사항이 즉시 적용 ( 실제로는 몇 초 의 시간이 소요 )\nNACL에서는 원본 또는 대상 주소를 지정해야 CIDR을 사용할 수 있으며, 이는 보안 그룹의 원본이나 대상 주소를 다른 보안 그룹을 지정할 수 있는 보안 그룹 규칙과는 다른 점이다.\n퍼블릭 IP 주소 퍼블릭 IP주소는 퍼블릭 인터넷으로부터 인스턴스에 엑세스하는 데 필수 요소로 인터넷이 아닌 프라이빗 네트워크 내에서만 통신할 수 있는 RFC 1918 주소와는 다르다 할 수 있다.\nAWS 외부에서 직접 인터넷을 통해 연결하려면 인스턴스에 퍼블릭 IP 주소가 필요하다.\n인스턴스에서 인터넷으로 전송만 하는 용도의 퍼블릭 IP 주소를 연결하는 방법도 있지만, 인스턴스 간에는 프라이빗 IP 주소로 통신하기 때문에, VPC 인프라 내에서 인스턴스 간 통신에는 퍼블릭 IP 주소가 필요하지 않다.\n퍼블릭 주소는 처음에만 할당이 가능하고, 생성 후에는 할당이 불가능하다.\n사용자가 재시작 하지 않았더라도, AWS 자체 유지보수 기능의 이해 변동 될 수 있다.\nIP 주소가 바뀌어도 무방한 인스턴스에는 퍼블릭 IP를 사용해도 되지만, 장기간 같은 IP 주소를 유지해야하는 인스턴스에는 탄력적 IP 주소를 사용하는 것이 좋다.\n탄력적 IP 주소 ( Elastic IP Address : 이하 EIP ) EIP는 AWS에서 사용자의 요청하면 계정에 할당되는 퍼블릭 IP 주소로 계정에 EIP가 할당되면 사용자가 직접 해제하지 않는 한 해당 주소를 독점적으로 사용할 수 있다.\nAWS 외부에서 보면 EIP와 자동 할당된 퍼블릭 IP 간에는 차이점이 없다.\nEIP는 인스턴스에 연결되지 않은 상태로 할당된다.\n네트워크 주소 변환 ENI를 퍼블릭 IP 주소와 연결할 때는 ENI는 프라이빗 IP 주소를 그대로 유지한다.\n퍼블릭 IP를 ENI와 연결하더라도 ENI가 새로운 주소로 재구성되는 것이 아니며, 인터넷 게이트웨이는 네트워크 주소 변환이라는 프로세스를 활용해 퍼블릭 IP 주소와 ENI 프라이빗 주소를 연결한다.\n퍼블릭 IP가 있는 인스턴스에서 인터넷의 호스트로 연결하면 그 호스트는 인스턴스의 퍼블릭 IP에 있는 트래픽이 발생한 것으로 간주한다.\n예를 들면, 퍼블릭 IP가 있는 호스트에 프라이빗 IP 주소에 보내었다 해도 게이트 웨이를 지나 퍼블릭 IP를 통해 접속하게 된다.\n이 때 게이트웨이는 자동적으로 주소를 변화시키며, 사용자는 개입이 불가능하다.\n네트워크 주소 변환 장치 네트워크 주소 변환은 인터넷 게이트웨이에서 이루어지지만, 다음 두 가지 서비스도 네트워크 주소변환을 수행한다.\nNAT 게이트웨이 NAT 인스턴스 AWS 서비스 속에서 이는 NAT 디바이스라 하며, 인스턴스가 인터넷에 액세스할 수 있게 되면서 인터넷상의 호스트에서는 인스턴스에 직접 도달하지 못하게 할 때 사용한다.\n이 기능은 이느턴스가 업데이트 패치를 받거나 데이터를 업로드할 때 인터넷에 연결할 필요는 있지만, 클라이언트 요청에 응답할 필요는 없을 때 유용하다.\nNAT 디바이스를 사용하면 인스턴스가 액세스 할 필요가 있더라도 퍼블릭 IP 주소를 할당하지 않으므로, 인터넷 상의 호스트가 인스턴스에 직접 액세스하는 것은 불가능하다.\nNAT 디바이스의 인터페이스는 퍼블릿 서브넷에 위치하면서 퍼블릭 IP가 연결된다.\n하단은 NAT 디바이스를 사용할 때의 IP 주소 구성이다.\n이름 서브넷 프라이빗 IP 퍼블릭 IP EC2-1 Private 10.0.0.10 EC2-2 Private 10.0.0.11 NAT 디바이스 Public 10.0.1.10 18.212.132.21 EC2-1, 2의 외부주소의 인터넷 호스트로 패킷을 전송하면 그 패킷은 먼저 NAT 디바이스로전달되고, NAT 디바이스에서는 아래와 같이 패킷을 변환한다. 원본 패킷의 원본 IP 주소 원본 패킷의 대상 IP 주소 –\u003e 변환 –\u003e 변환 패킷의 원본 IP 주소 변환 패킷의 대상 IP 주소 EC2-1 ( 10.0.0.10 ) 외부 주소 –\u003e 변환 –\u003e NAT 디바이스 ( 10.0.1.10 ) 외부 주소 내부 대역에서 NAT 디바이스와의 변환을 마치게 되면 하단과 같이 NAT 디바이스와 인터넷 게이트웨이로의 변환이 다시 한번 이루어진다. 원본 패킷의 원본 IP 주소 원본 패킷의 대상 IP 주소 –\u003e 변환 –\u003e 변환 패킷의 원본 IP 주소 변환 패킷의 대상 IP 주소 NAT 디바이스 ( 10.0.1.10 ) 외부 주소 –\u003e 변환 –\u003e NAT 디바이스 ( 18.212.132.21 ) 외부 주소 이와 같이 여러 인스턴스가 같은 NAT 디바이스를 사용할 수 있으므로 같은 퍼블릭 IP 주소를 공유해서 아웃 바운드에 연결할 수 있다. NAT 디바이스가 수행하는 기능을 포트 주소 변환 ( Port Address Translation : 이하 PAT )이라고 한다. NAT 디바이스를 사용한 라우팅 테이블 구성 프라이빗 서브넷에서 트래픽이 인터넷으로 전송돼야 하는 경우, 트래픽이 NAT 디바이스를 향하도록 경로가 설정되어 있어야 한다.\nNAT 디바이스에서 트래픽이 인터넷으로 전송되야 할 경우, 트래픽은 인터넷 게이트웨이로 향하도록 경로가 설정되어야 한다.\n따라서 NAT 디바이스와 라우팅가 인스턴스의 기본 라우팅은 다르게 구성되어야 하며, 복수의 라우팅 테이블을 사용해야 하므로 이에 따라 서브넷도 분리되어야 한다.\n서브넷 대상 주소 대상 프라이빗 0.0.0.0/0 NAT 디바이스 퍼블릭 0.0.0.0/0 igw-0e538022a0fddc318 위의 표는 프라이빗과 퍼블릿의 기본 라우팅을 나타낸다. NAT 게이트웨이 NAT 게이트웨이는 AWS에서 관리하는 NAT 디바이스이며, 인터넷 게이트웨이처럼 하나의 NAT 게이트웨이로 어떠한 용량의 요청도 수행할 수 있다.\n한 종류의 NAT 게이트웨아만 제공되고, 자동 확장해서 모든 대역폭 요구에 대응하므로 용량 관리를 위한 디바이스에 엑세스할 필요가 없다.\nNAT 게이트웨이를 생성할 때, EIP도 함께 할당해서 연결해야하고, 퍼블릭 서브넷 한 곳에 배포해서 인터넷에 엑세스할 수 있게 해야 한다.\nNAT 게이트웨이는 포함되어 있는 서브넷에서 프라이빗 IP주소를 할당받는다.\nNAT 게이트웨이를 생성 후에는 기본 라우팅을 만들어야 인스턴스의 인터넷 연결 트래픽이 NAT 게이트웨이로 전달된다.\nNAT 게이트웨이의 명칭은 nat-xxxxxxxxx… 이며 기본 라우팅을 여러 개 만들 수 있다.\nNAT 게이트웨이는 ENI를 사용하지 않으므로 보안 그룹을 적용할 수는 없지만, 서브넷에 NACL을 적용해서 트래픽을 제어할 수 있다.\nNAT 인스턴스 NAT 인스턴스는 사전 구성된 Linux 기반 AMI를 사용하는 일반적인 EC2 인스턴스로, 만들 때도 동일한 단게를 가진다.\nNAT 게이트웨이와 다방면에서 동일하다 할 수 있지만, 몇 가지 다른 점이 존재한다.\nNAT 인스턴스는 대역폭 요구가 증가하더라도 자동으로 확장되지 않는다. 즉, 적절한 성능을 갖춘 인스턴스를 초기에 맞게 생성해야한다.\nNAT 인스턴스는 ENI가 있으므로 보안 그룹을 적용해야 하며, 퍼블릭 IP주소를 할당해야 한다.\nNAT 인스턴스의 한 가지 이점은 배스천 호스트 ( Bastion Host : 점프 호스트 )로 사용해서 퍼블릭 IP가 없는 인스턴스에 연결할 수 있다는 것이며, NAT 게이트웨이로는 이 작업을 수행할 수 없다.\n인스턴스나 가용 영역에 장애가 발생하면 다른 NAT 인스턴스로 확장하는 정도로는 간단한 대처가 불가능하며, 이는 기본 라우팅에 여러 NAT 인스턴스를 대상으로 경로를 지정할 수 없기 때문이다.\n즉, 높은 탄력성이 요구되면 NAT 게이트웨이를 사용하는 것이 현명하다.\nVPC 피어링 VPC 피어링을 구성하면 VPC의 인스턴스가 프라이빗 AWS 네트워크를 통해 다른 VPC와 통신하게 할 수 있다.\n다른 리전에 있는 인스턴스와 통신이 필요할 때에도 이와 같은 작업을 수행할 수 있으며, 한 계정의 인스턴스를 다른 계정의 인스턴스와 연결할 수 있다.\nVPC 피어링을 활성화하려면 두 VPC 사이에 VPC 피어링 연결을 설정해야 하며, VPC 피어링 연결은 아래와 같은 특징이 있다.\n두 VPC 사이의 지점간의 연결이다. 두 VPC 간에는 단 하나의 피어링만 설정할 수 있다. 두 VPC 간에는 서로 다른 CIDR을 사용해야 한다. VPC 피어링 연결은 인스턴스 간 통신만 허용된다. 즉, 한 VPC 인스턴스에서 피어링 된 다른 VPC사이의 지점간 연결이며, 두 VPC 간에는 단 하나의 피어링만 설정할 수 있고, 두 VPC의 CIDR 블록은 겹치지 않아야 한다.\n인터넷 게이트웨이나 NAT 디바이스는 VPC 피어링으로 공유해서 사용할 수 없지만, Networkk Load Balancer만은 예외적으로 공유할 수 있다.\n2개 이상의 VPC를 연결하려면 한 VPC와 다른 모든 VPC 각각 1:1 피어링 연결을 생성해야 하며, 데이지 체인 방식 ( 전이전 구성 )으로 라우팅은 불가능하다.\n피어링 연결을 사용할면 트래픽이 양방향으로 오가도록 두 VPC에 새로운 라우팅을 만들어야 하고, 각 라우팅의 대상이 될 접두사는 대상 VPC 범위 내에 있어야 한다.\n각 경로의 대상은 pcx-xxxxxxx로 시작되는 피어링 연결 식별자로 존재한다.\n원본 VPC CIDR 대상 VPC CIDR 대상 10.0.0.0/16 172.31.0.0/16 pcx-xxxxxxxxxxx 172.31.0.0/16 10.0.0.0/16 pcx-xxxxxxxxxxx 위에 표는 라우팅이 서로 반대로 설정된 것은 양방향으로 트래픽을 허용한다는 의미이다. 대상 CIDR은 대상 VPC CIDR과 정확히 일치할 필요는 없으며, 특정 서브넷 사이에서 피어링을 사용하려면 서브넷 CIDR을 대신 지정할 수 있다. 요약 VPC 서비스는 EC2 및 다른 AWS 서비스의 네트워크 인프라를 제공하는 서비스이다.\nAWS는 기존 네트워크보다 구성하기 쉽도록 일부 네트워크 구성 요소를 추상화했지만, 여전히 VPC를 설계하기 위해서는 네트워크 기초 지식이 필요하다.\nAWS 각 리전의 기본 VPC에 자동으로 기본 서브넷, 기본 라우팅 테이블, 기본 보안 그룹, 기본 NACL을 제공한다. 많은 이들이 기반부터 VPC를 구성하지 않고 기본 VPC를 장기간으로 사용하고 있다.\nAWS 아키텍트라면 가상 네트워크 인프라를 기반부터 구성하는 방법을 이해하는 것이 중요하다.\n기본 VPC에서 구축한 인프라를 수정할 수 없을 때가 많지만, 대신 VPC를 기초부터 구성해서 인프라를 복제해야 하는 임무를 받을 수도 있다.\n이 과정에서 여러 트러블을 해결하고 기초를 공부할 수 있고, 기능에 대한 이해의 학습을 도와준다.\n기존의 네트워크에서는 서버 IP주소를 자유롭게 구성하고, 다른 서브넷으로 이동하며, 다른 물리적 위치로 이동시킬 수도 있다. 즉, 중간에 네트워크 계획을 변경할 수 있는 유연성이 존재하지만, VPC 생성 시에는 불가능하며, 이를 위해 사전에 인프라를 신중하게 계획해야 하므로, 전체 VPC와 EC2 구성 요소를 맞추는 방법을 이해하는 것이 중요하다.\nCIDR로 표현되는 연속적 IP 주소 범위를 정하는 것으로 VPC를 생성하기 한다. 즉, CIDR의 범위를 사전에 모든 인스턴스들을 수용할 수 있을만큼 충분한 여유 주소를 넣어 두고 생성한다.\nVPC CIDR을 서브넷으로 나눌 때는, 한 가용 영역에만 있는 컨테이너이기 때문에 인스턴스를 어디에 배치할지 사전에 결정해둬야 한다.\n서브넷에 인스턴스를 만들고 난 인스턴스를 다른 서브넷으로 옮길 수 없다.\n인스턴스를 시작하기 전에 보안 그룹을 구성할 필요가 있으며, 모든 인스턴스의 ENI에 보안 그룹을 하나 이상 연결해야 하며, 네트워크 엑세스 제어 목록은 상대적으로 변경이 가능해 유연성을 가지고 있으며, NACL은 언제든지 서브넷에 연결할 수 있고 제거가 가능하다.\n인터넷에서 인스턴스에 엑세스할 수 있게 하려면 기본적으로 인터넷 게이트웨이를 프로비저닝하고 기본 라우팅과 퍼블릭 IP 주소를 할당시켜야 한다.\nNAT 게이트웨이나 인스턴스 혹은 VPC 피어링 연결을 사용하기로 했다면 여러 라우팅 테이블을 수정해야 한다.\n시험 핵심 VPC나 서브넷에 필요한 IP 주소의 수를 기반으로 올바른 CIDR 블록 접두사 길이를 결정할 수 있어야 한다. 접수다의 길이는 /16- /28까지 허용되며, 접두사의 길이가 길수록 사용할 수 있는 IP 주소 수는 줄어든다. 서브넷의 중요성을 이해한다. 서브넷은 EC2 인스턴스가 있는 논리적 컨테이너이다.\n각 서브넷의 CIDR 블록은 그 서브넷에 속해 있는 VPC CIDR의 일부다.\n한 서브넷에 속한 인스턴스는 그 서브넷의 CIDR 범위 내에서 프라이빗 IP주소를 가져온다.\n모든 서브넷에 처음 4개의 IP 주소와 마지막 IP 주소는 AWS에서 예약하기 때문에 인스턴스에 할당이 불가능하다.\n가용 영역 장애가 미치는 영향을 파악한다. 한 영역에 장애가 발생하면 해당 영역의 모든 서브넷과 해당 서브넷의 모든 인스턴스가 함께 중단된다.\n한 영역에 장애가 일어나더라도 서비스의 중단을 피하려면 인스턴스를 여러 영역에 배포해 중복해서 구축한다.\n탄력적 네트워크 인터페이스(ENI)를 생성하고 사용하기 위한 규칙을 이해한다. 모든 인스턴스에는 기본 프라이빗 IP 주소를 사용하는 기본 네트워크 인터페이스가 존재하야 하며, 인스턴스에 연결하는 추가 ENI는 기본 ENI와 같은 서브넷에 존재해야한다. 라우팅 테이블을 생성, 수정, 사용할 수 있어야 한다. VPC의 기본 라우팅 테이블의 목적과 VPC의 서브넷의 관계를 알아야 한다. 인터넷 게이트웨이와 기본 라우팅 테이블을 사용해 퍼블릭 서브넷을 만드는 방법 또한 이해해야 한다. 보안 그룹과 네트워크 엑세스 제어 목록 간의 차이점을 파악한다. 상태 저장 보안 그룹과 상태비저장 NACL이 같은 결과를 얻기 위해서 각각 다르느 규칙을 사용해야 하는 이유를 이해한다. 네트워크 주소 변환이 어떻게 작동되는 지 이해한다. 인터넷 게이트웨이에서의 네트워크 주소 변환과 NAT 디바이스에서의 네트어크 주소 변환의 차이점을 이해한다. NAT 디바이스에서의 네트워크 주소 변환은 포트 주소변환(PAT)라 하며, 여러 인스턴스가 한 NAT 디바이스의 단일 퍼블릭 IP 주소를 공유할 수 있다. 여러 VPC 간에 VPC 피어링을 만들고 구성할 수 있어야 한다. VPC 피어링 연결의 제한을 파악한다. VPC 피어링 연결은 전이 라우팅과 IPv6를 지원하지 않는다. 일부 리전에서는 리전 간에도 피어링할 수 있다. ","#":"","4장-amazon-virtual-private-cloud#\u003cstrong\u003e4장 Amazon Virtual Private Cloud\u003c/strong\u003e":"","시험-핵심#\u003cstrong\u003e시험 핵심\u003c/strong\u003e":""},"title":"4장 VPC"},"/system/aws/awssaa/saa-5/":{"data":{"":"5장 데이터베이스 5장의 목표 복원력을 갖춘 아키텍처 설계 안정적이고/ 복원력을 갖춘 스토리지를 선택한다.\n어떻게 AWS 서비스를 사용해 결합 해제 매커니즘을 설계할지 결정한다.\n어떻게 멀티 티어 아키텍처 솔루션을 설계할지 결정한다.\n어떻게 고 가용성 및/ 내결함성을 갖춘 아키텍처를 설계할지 결정한다.\n성능이 뛰어난 아키텍처 정의 성능이 뛰어난 스토리지 및 데이터베이스를 선택한다. 탄력성과 확장성을 갖춘 솔루션을 설계한다. 데이터베이스 데이터베이스를 사용하면 어플리케이션을 데이터를 저장하고 구성하며, 신속하게 검색할 수 있다.\n단층 파일(Flat File)에 데이터를 저장할 수도 있지만, 데이터양이 증가하게 디면 검색 속도가 느려지는 단점이 있으며, 개발자는 데이터를 저장하고 검색하기 위해 직접 파일 시스템에서 작업하는 대신, 데이터베이스로 작업을 수행함으로써 애플리케이션 개발에 집중하는 것이 가능하다.\n데이터베이스에 기반한 애플리케이션을 구현할 때, 애플리케이션의 가용성과 성능은 데이터베이스 선택과 구성 방법에 다려 있으며, 데이터베이스는 관계형과 비관계형 두 가지로 나뉘어 지며, 사용자는 데이터 저장, 구성, 검색 방법에 따라 애플리케이션에 가장 적합한 데이터베이스를 선택할 수 있다.\n데이터베이스에 장애가 발생할 때 데이터를 보호 및 복구하는 방법 뿐이 아닌, 애플리케이션의 필요한 수준의 성능과 안정성을 얻기 위해 AWS가 제공하는 데이터베이스 서비스를 학습한다.\n관계형 데이터베이스 관계형 데이터베이스는 하나 이상의 테이블을 포함한 열과 행이 있어 스프레드시트로 시각화가 가능한 데이터베이스를 의미한다.\n관계형 데이터베이스 테이블에서 열은 속성, 행은 레코드 또는 튜플이라고 한다.\n열과 속성 관계형 데이터베이스 테이블에 데이털르 추가하기 전에, 각 열의 이름과 입력될 데이터의 형식을 사전에 정의해야 한다.\n열에는 순서가 있으며, 테이블을 생성한 후에는 이 순서를 변경할 수는 없다.\n열의 순서를 정하려면 테이블에 있는 속성 간에 관계를 만들어야 하며, 여기에서 관계형 데이터베이스라는 용어가 등자하게 되었다.\n하단은 테이블의 예시를 나타낸다.\n사원 ID(숫자) 부서(문자열) 성(문자열) 이름(문자열) 셍일(날짜) 101 전산 Smith Charlotte 7-16-87 102 마케팅 Colson Thomas 7-4-00 데이터는 각 열에서 정의된 형식에 반드시 일치해야 하며, 이와 다르게 숫자에 문자열을 입력하는 등의 작업을 진행할 경우 오류가 발생하게 된다. 관계형 데이터베이스의 이점은 데이터를 어떻게 쿼리할지 이해할 필요가 없다는 것이며, 데이터가 일관된 형식으로 존재하는 한 필요한 데이털르 원하는 방식으로 얻기 위해 여러 쿼리를 가공할 수 있다. 관계형 데이터베이스는 임의의 열에 데이터를 쿼리하고 사용자가 데이터를 제공 방식을 지정해야 하는 애플리케이션에 적합하다. 다중 테이블 사용 모든 데이터를 단일 테이블에 저장하면 불피요한 중복이 생기기 때문에 데이터베이스가 불필요하게 커지고 쿼리 속도가 느려지므로, 일반적으로 애플리케이션은 다중 테이블을 연결해서 사용한다.\n하단은 상단의 테이블을 원하는 자료를 모아 생성한 테이블이다.\n부서 ID(숫자) 부서명(문자열) 10 전산 20 마케팅 사원 테이블의 각 사원 레코드에 부서명을 입력하는 대신 부서 테이블에 레코드를 하나를 생성한 후, 사원 테이블의 사원 ID를 통해 각 부서를 참조하는 것이 가능하다. 이러한 관계에서 부서 테이블은 상위 테이블(Prarent Table)이며, 사원 테이블은 하위 테이블(Child Table)이다. 사원 테이블의 부서 열에 있는 값은 부서 테이블의 부서 ID를 참조한다. 여기서 부서 ID는 기본 키(Primary Key)라 하며, 기본 키는 행을 고유하게 식별하기 위해서 테이블 내에서 유일해야 한다. 사원 테이블은 부서 ID를 외래 키(Foregin Key)로 참조한다. 데이터베이스가 여러 테이블의 열이 어떻게 연관됐는지 알 수 있도록 기본 키와 외래 키를 반드시 정해야 하며, 데이터베이스는 외래 키 제약 조건을 활성화해 하위 테이블이 외래 키를 참조할 때 해당 키가 상위 테이블에도 존재하는 지 확인해야 한다. SQL 관계형 데이터베이스에서는 구조화 질의 언어 SQL(Structured Query Language)를 사용해 데이터를 저장하고 쿼리하고 유지 관리 작업을 수행하므로 SQL 데이터베이스라고 불린다.\nSQL문은 관계형 데이터베이스 관리 시스템(RDBMS, Relational Database Management System) 마다 조금씩 차이가 있으며, 이는 주요 프로그래밍 언어들에 SQL 문을 만들고 데이터베이스에 입출력하는 라이브러리가 이기 때문으로, AWS 아키텍처로써 SQL까지는 알 필요는 없지만 AWS 관리형 데이터베이스에 작업하기 위한 일반적인 SQL 용어의 개념은 이해할 필요가 있다.\n데이터 쿼리 SELECT 문은 SQL 데이터베이스에서 데이터를 쿼리하는 데 사용되며, 데이터베이스에서 조회하고 싶은 특정 열을 지정할 뿐 아니라 모든 열에서 값을 기반으로 쿼리가 가능하다.\n테이블의 에측 가능한 구조와 외래 키 제약 조건을 사용해서 SELECT 문과 함께 JOIN 절을 사용해 여러 테이블의 데이터를 결합할 수 있다.\n데이터 저장 INSERT 문을 사용하면 테이블에 직접 데이터를 삽입할 수 있으며, 대량의 레코드를 저장해야 할 때 COPY 명령을 사용하면 적절하게 형식을 ㅁ맞춘 파일에서 지정한 테이블로 데이터를 복사할 수 있다. 온라인 트랙잭션 처리와 온라인 분석 처리 관계형 데이터베이스는 구성에 따라 온라인 트랜잭션 처리(OLTP, OnLine Transaction Processing)과 온라인 분석 처리(OLAP, Online Analytical Processing) OLTP OLTP 데이터베이스는 초당 몇 회씩 순차적으로 데이터를 버번하게 읽고 쓰는 애플리케이션에 적합하며, OLTP 데이터베이스는 빠른 쿼리에 최적화 되어 있다.\nOLTP 데이터베이스는 정기적이고 예측 가능한 경향이 있으며, 요구조건에 따라 메모리가 상당량 필요할 수 있으며, 이는 빠른 액세스를 위해 자주 사용하는 테이블의 일부를 메모리에 저장하기 때문이다.\nOLTP 데이터베이스는 1분당 수백 건의 주문을 처리해야 하는 온라인 주문 시스템을 지원하는 데 적합하다.\nOLAP OLAP 데이터베이스는 복잡한 대형 데이터 세트 쿼리에 최적화 되어 있으며, 상단하 스토리지와 컴퓨팅이 필요하여 데이터웨어하우징 애플리케이션으르 구축하여 여러 OLTP 데이터베이스를 단일 OLAP 데이터베이스로 모으는 것이 일반적이다.\n보통 대형 OLAP 데이터베이스에서는 복잡한 쿼리로 인한 컴퓨팅 부하를 여러 데이터베이스 서버가 나눠 처리하며, 파티셔닝이라는 프로세스에서 각 서버는 데이터베이스 일부를 맡아 처리한다.\nAmazone Relational Database Server ( 이하 RDS ) RDS는 클라우드에서 관계형 데이터베이스 시스템을 실행할 수 있게 하는 관리형 데이터베이스 서비스로, 데이터베이스 시스템 설정, 백업 수행, 고 가용성 보장, 데이터베이스와 기반 운영체제 패치 적용 등과 같은 작업을 수행한다.\nRDS를 사용하면 데이터베이스 장애로부터 복구, 데이터 복원, 데이터베이스 확장을 쉽게 사용하여 애플리케이션이 요구하는 수준의 가용성과 성능을 달성할 수 있다.\nRDS를 사용해 데이터베이스를 배포할 때, 격리된 데이터베이스 환경인 데이터베이스 인스턴스 구성에서부 시작한다. 데이터베이스 인스턴스는 지정한 가상 프라이빗 클라우드(VPC)에 존재하나, EC2 인스턴스와는 다르게 AWS가 전적으로 데이터베이스 인스턴스를 관리한다. SSH를 사용해 엑세스할 수 없으며, EC2 인스턴스 사이에서도 보이지 않는다.\n데이터베이스 엔진 데이터베이스 엔진은 데이터베이스에 데이터를 저장, 구성, 반환하는 소프트웨어이며, 데이터베이스 인스턴스는 하나의 데이터베이스 엔진만 실행한다.\nRDS는 다음 여섯 가지 데이터베이스 엔진 중에서 선택할 수 있다.\nMySQL MySQL은 블로그 및 전자상거래와 같은 OLTP 애플리케이션으로 설계되었으며, RDS는 5.5, 5.6, 5.6 등 최신 MySQL Community Edition 버전을 제공한다. MySQL은 myISAM과 InnoDB 두 가지 스토리지 엔진에서 하나를 선택할 수 있지만, 유일하게 RDS 관리형 자동 백업과 호환할 수 있는 엔진은 InnoDB이다. Maria DB Maria DB는 MySQL과 바이너리 수준의 호환성을 가지면서 기능을 향상한 데이터베이스이다. Maria DB는 오라클이 MySQL을 개발한 회사를 인수한 이후, MySQL의 미래를 우려해서 개발되었으며, MariaDB는 XtraDB와 InnoDB 스토리지 엔진을 지원하지만, AWS에서는 RDS와의 호환성을 최대화하기 위해 InnoDB를 사용할 것이 권장된다. Oracle Oracle은 가장 널리된 DBMS로 일부 애플리케이션은 데이터베이스사양으로 Oracle을 데이터베이스로 명시하기도 한다. RDS는 다음 Oracle 데이터베이스 에디션을 제공한다. Standare Edition One(SE1) Standare Edition Two(SE2) Standare Edition(SE) Enterprise Edition One(SE) PostgreSQL PostgreSQL은 Oracle과 호환되는 오픈 소스 데이터베이스이며, Oracle 기반으로 애플리케이션을 제작하였어도, 비용을 위해 PostgreSQL을 선택하기도 한다. Amazone Aurora Amazon Aurora는 Amazon이 MySQL과 PostgreSQL과 바이이너리 수준의 호환성을 가지면서 기능을 향상시킨 데이터베이스이며, 가상 스토리지 계층을 사용해서 하부 스토리지 쓰기 횟수를 줄이기 때문에 MySQL, PostgreSQL보다 쓰기 성능이 우수하며 하단의 세 가디 에디션을 제공한다. MySQL 5.6-compatible MySQL 5.7-compatible PostgreSQL compatible Aurora는 에디션에 따라서 PostgreSQL이나 MySQL과 부러오기/ 내보내기 도구, 스냅샷에서 호환되며, 두 오픈 소스 데이터베이스에서 언할하게 마이그레이션 할 수 있도록 설게되어 있다. Aurora는 MySQL 호한 에디션에서 InnoDB 스토리지 엔진만 지원하며, MySQL에서 사용할 수 있는 Aurora Backtrack 기능으로 데이터베이스를 지난 72시간 이내 특정 시점으로 단 몇 초 만에 복구가 가능하다. Microsoft SQL Server 여러 Microsoft SQL Server과 Express, Web, Standard, Enterprise 에디션을 사용할 수 있으며, 다양한 기능을 사용해서 데이터베이스 업그레이드를 수행하지 않고도 온프레미스에 배포된 기존 SQL 데이터베이스를 RDS로 마이그레이션 할 수 있다. 라이센스 고려사항 RDS는 데이터베이스 엔진을 실행하는 데 필요한 두 가지 소프트웨어 라이선스 모델을 제공하며, 라이센스가 포함된 모델은 RDS 인스턴스 요금에 라이센스 비용이 포함되여 제공된다.\n기존 보유 라이센스 사용(BTOL : Bring Your Own License)모델을 선택하려면 실행 중인 데이터베이스 엔진의 라이센스를 확보해야 한다.\n라이센스가 포함된 모델 MariaDB나 MySQL은 GNU GPL(General Public License)v2.0을 사용하며, PostgreSQL은 PostgreSQL 라이선스를 사용하고, 별도의 라이선스 비용은 없다.\nRDS에서 실행하는 Microsoft SQL 서버의 모든 버전과 에디션은 라이선스를 포함하며, Oracle Database Standard Edition One과 Oracle Database Standard Edition Tow도 라이선스를 포함하고 있다.\n데이터베이스 옵션 그룹 데이터베이스 엔진은 데이터베이스 관리와 보안 향상을 지원하는 다양한 기능을 제공한다.\n옵션 그룹은 옵션이라는 관리 및 보안 기능을 지정해서 하나 이상의 인스턴스에 적용할 수 있게 한다.\n옵션을 사용하려면 메모리가 더 필요하므로 인스턴스에 충반한 메모리가 있는지 확인하고 필요한 것만 활성화 해야한다.\n옵션 그룹에서 사용 가능한 옵션들은 데이터베이스 엔진마다 다르며, Microsoft SQL Server와 Oracle은 TDE를 제공해 스토리지에 쓰기를 수행하기 전에 엔진이 데이터를 암호화하게 한다.\nMySQL과 MariaDB는 데이터베이스 사용자 로그인 쿼리 활동을 기록하게 하는 감사 플러그인을 제공한다.\n데이터베이스 인스턴스 클래스 데이터베이스 인스턴스를 시작할 때 처리 성능, 메모리, 네트워크 대역폭, 디스크 처리량이 어느 정도 필요한지를 결정해야 하며, RDS는 여러 데이터베이스를 다양한 성능 요구 사항을 충족하기 위해 다양한 데이터베이스 인스턴스 클래스를 제공한다.\n선택을 잘못 했거나 요구 사항이 변경될 때 인스턴스를 다른 클래스로 전환할 수도 있으며, RDS 데이터베이스 인스턴스 클래스를 다음의 세 가지 유형으로 분류한다.\n표준 256G 메모리 64v CPU 25Gbps 네트워크 대역폭 10.000Mbps(1.280Mbps) 디스크 처리량 메모리 최적화 (대용량의 처리량) 3.940GB 메모라 128 vCPU 25Gbps 네트워크 대역폭 14.000Mbps(1.750P 디스크 처리향 순간확장 가능 (개발 및 테스트 용도) 32GB 메모리 8 vCPU 스토리지 데이터베이스 인스턴스에 적합한 스토리지 선택은 충분한 디스크 공간 확보 이상으로 중요하다.\n데이터베이스 기반 애플리케이션의 성능 요구사항을 충족하기 위해서는 얼마나 빠른 스토리지를 선택할지도 판단해야 한다.\nIOPS의 이해 AWS는 초당 입력/ 출력 작업(IOPS, Input/ Output Operations Per Second)를 사용해 스토리지 성능을 측정한다.\n입출력 작업은 스토리지 읽기 또는 쓰기 작업으로 다른 모든 조건이 같을 때, IOPS가 큰 데이터베이스는 데이터를 저장하고 검색하는 속도가 더 빠르다.\nRDS는 스토리지 타입에 따라 IOPS를 할당할 수 있으나, 임계 값을 초과할 수는 없다.\n데이터베이스 스토리지의 속도는 할당된 IOPS 수에 제한되며, 단일 I/O 작업에서 전송할 수 있는 데이터의 양은 데이터베이스 엔진이 사용하는 페이지 크기에 달려 있어, 요구되는 IOPS 수준을 파악하려면 먼저 필요한 디스크 처리량을 확인해야 한다.\nMySQL과 MariaDB의 페이지 크기는 16KB이므로, 디스크에 16KB의 데이터 쓰기가 하나의 I/O 작업을 구성한다.\n반면 Oracle, PostgreSQL, Microsoft SQL Server는 8KB 크기의 페이지를 사용하며, 이 경우 16KB의 데이터를 쓰면 I/O 작업이 두 번 이루어진다.\n페이지 크기가 클수록 단일 I/O 작업에서 더 많은 데이터를 전송할 수 있다.\n페이지의 크기가 16KB라고 하고, 데이터베이스가 초당 102,400KB(100MB)의 데이터를 읽어야 한다고 할 때, 이러한 성능 요구를 달성하려먼 데이터베이스는 매초 16KB 페이지 크기로 6,400 페이지를 읽어여 하며, 페이지 당 I/O 작업 하나로 계산하기 때문에 스토리지와 인스턴스 클래스는 6,400 IOPS를 유지해야 한다. 이 때, IOPS 수와 페이지 크기는 반비례 관계이며, 페이지가 클 수록 같은 처리량을 달성하는 데 필요한 IOPS는 작아진다.\n스토리지 유형에 따라 IOPS 수가 달라지며, RDS는 다음 세가 유형의 스토리지를 제공한다. 범용 SSD 데이터베이스의 대부분은 범용 SSD(gp2) 스토리지로 충분하다.\n범용SSD 스토리지는 속도가 빠르고 한 자릿수 밀리초 지연 시간을 제공하며, 최대 16TB의 보륨을 할당할 수 있다.\nRDS는 기본적으로 기가바이트당 3 IOPS 성능을 볼륨에 할당하며, 최대 10,000 IOPS까지 볼륨을 할당할 수 있다.\n볼륨이 커지면 성능이 향상되며, 데이터베이스 엔진의 따라 만들 수 있는 스토리지 볼륨의 최소 크기는 다르다.\ngp2 스토리지 유형의 최대 처리량은 1,280(160MB)이며, 최대 처리량을 만족하기 위해서는 인스턴스가 적어도 1,280(160MB) 이상의 디스크를 지원할 수 있어여 하며, 처리량을 유지하기 위해 IOPS를 할당해야 한다.\n예시로 Maria DB를 16KB 페이지 크기로 실행한다고 가저하였을 때, 1,280Mbps 디스크 처리량을 유지하는 데 필요한 IOPS 수는 1,280MBPS/0.128MB = 10,000 IOPS이다.\n즉, 볼륨에서 1,280Mbps의 디스크 처리량을 달성하려면 10,000 IOPS가 할당되어야 하며, 이것은 gp2에서 할당 가능한 최대 IOPS 수라는 것에 주목한다. 이것을 다시 계산해보면 이 정도의 IOPS를 확보라혀면 볼륨 크기가 3,333,3GB(3.34TB)가 되어야한다.\n최대 3,000 IOPS가 필요하지만 그렇게 큰 스토리지가 필요하지 않을 때, 필요한 IOPS를 얻기 위해서는 스토리지를 과도하게 할당할 필요는 없다. 1TB보다 작은 볼륨은 일시적으로 3,000 IOPS까지 순간 확장이 가능하며, 순간 확장 지속 시간은 다음과 같은 공식으로 결정된다.\n순간 확장 지속시간(초) = (Credit 잔약)/[3,000 - 3 X (저장용량(GB))]\n데이터베이스 인스턴스를 처음 부팅할 때, 5,400,000 IOPS의 Credit 잔액을 갖게 되며 인스턴스가 기준치 이상으로 IOPS를 사용하면 그 만큼 Credit 잔액이 차감된다.\nCredit 잔액이 고갈되면 순간 확장 기능을 사용할 수 없으며, 예를 들어 200GB 볼륨의 순간 확장 지속시간은 2,250초(37.5분)이다.\nCreidt 잔액은 1초마다 IOPS 기준치가 보충된다.\n프로비저닝된 IOPS SSD(io1) 앞에 나온 식이 복잡하다면 프로비저닝된 IOPS SSD를 사용하면 인스턴스를 만들 때 필요한 IOPS 수를 간단하게 할당할 수 있다.\nio1 스토리지에서는 순간 확장의 개념이 없으며, 프로비저닝된 IOPS 수는 사용 여부와 관계없이 일정한 성능이 제공되고 그에 따른 비용이 청구되므로, 일관된 짧은 지연 시간에 성능이 필요한 OLTP 데이터베이스에 유용하다.\n표준 또는 메모리 최적화 인스턴스 클래스를 사용할 때, RDS는 프로비저닝된 IOPS의 성능 변동 범위가 10% 이내로 유지되는 기간을 1년의 99.9%로 보장한다.\n즉 지정한 IOPS 수보다 낮은 성능이 제공되는 기간이 1년 동안 약 2시간 45분밖에 안 된다는 의미이기도 하다.\n4,000Mbps 처리량의 표준 인스턴스와 16KB 페이지 크기의 데이터베이스 엔진을 사용한다고 가정하면 최대 31,250 IOPS를 달성할 수 있으며, 이러한 성능을 달성하려면 인스턴스를 생성할 때 32,000 IOPS를 프로비저닝해야 하며, 프로비전이된 IOPS는 1,000단윈로 지정할 수 있다.\n데이터베이스 엔진에 따라 달성할 수 있는 최대 IOPS 수와 할당할 수 있는 스토리지 크기가 다르며, Oracle, PostgreSQL, MariaDB, MySQL, Aurora를 사용하면 100GB ~ 16TB의 스토리지를 선택할 수 있고, 1,000~ 4,000 프로비저닝된 IOPS를 할당할 수 있다.\nMicrosoft SQL Server는 최대 16TB 스토리지를 제공한고 1,000~ 32,000 범위의 프로비저닝된 IOPS를 제공한다.\nIOPS 기가바이트 비율은 최소 50:1(IOPS:GB)이어야 하며, 32,000 IOPS가 필요하다면 최소 640GB의 스토리지를 제공해야 한다.\n마그네틱 스토리지 마그네틱 스토리지는 RDS 구형 인스턴스의 호환성을 위해 제공되며 최대 크기는 4TB, 최대 성능은 1,000 IOPS이다. 읽기 전용 복제본 데이터베이스 인스턴스가 성능 요구 사항을 충족하지 못할 때, 병목 현상 발생 위치에 따라 해결 방법을 적용할 수 있다.\n만약 메모리, 컴퓨팅, 네트워크 속도, 디스크 처리량에 문제가 발생 시에 인스턴스 클래스를 업그레이드 하여 데이터베이스를 확장할 수 있는 데 이를 수직확장(Scale Up)이라 한다.\n리소스를 증가시키는 수직확장 외에 읽기 전용 복제본이라는 추가 데이터베이스 인스턴스를 생성하는 작업을 수행하는 것을 수평확장(Scale Out)이라 한다.\n수평확장은 Oracle과 Microsoft SQL Server를 제외한 모든 데이터베이스 엔진에 읽기 전용 복제본을 지원하며, Aurora에는 Aurora 복제본이라는 특정 유형의 읽기 전용 복제본이 존재한다.\n읽기 전용 복제본은 데이터베이스 쿼리만 제공하는 데이터베이스 인스턴스로, 마스터 데이터베이스 인스턴스의 쿼리 부하 부분을 맡는다.\n즉 마스터 데이터베이스 인스턴스는 데이터 쓰기만을 책임지게 되므로, 읽기 작업량이 매우 많은 애플리케이션에 적합하다.\nRDS는 최대 5개 읽기 복제본을 둘 수 있으며, Aurora에서는 최대 15개까지 가능하다.\n마스터로부터 모든 읽기 복제본에 비동기로 복제되므로, 데이터가 마스터 데이터베이스에 저장되는 시점과 그 데이터가 복제본에 저장되는 시점에는 지연이 발생한다.\n지연이 발생하는 이유로 읽기 전용 복제본은 재해 복구에는 적합하지 않으며, MySQL의 경우 복제 지연 시간을 설정이 가능하다.\nRDS는 읽기 전용 복제본을 만들면 도메인 이름을 제공하며, 이를 읽기 전용 엔드포인트라고 한다.\nRDS의 읽기 전용 복제본이 다수 존재할 경우, 해당 복제본 중 하나에 연결해 로드 밸런싱하므로, 사용자는 데이터를 읽기만을 하는 분석 도구만 있다면 그 도구에 읽기 전용 엔드포인트를 지정해 주면 된다.\n읽기 전용 복제본과 마스터는 서로 다른 가용 영역에 둘 수 있으며, 다른 리전에도 두는 것이 가능하다.\n마스터 인스턴스는 장애가 발생했을 시에, 읽기 전용 복제본을 마스터로 승격시킬 수는 있지만, 비동기 복제의 특성이 존재하므로 어느 정도의 손실은 감수해야 한다.\n고 가용성(다중-AZ) 데이터베이스 인스턴스가 중단되어도 데이터베이스를 계속 운영하려면, RDS의 다중 AZ배포를 통해 여러 가용 영역에 데이터베이스 인스턴스를 다수 배포한다.\n다중 AZ 배포를 사용하면 한 가용 영역에 읽기 및 쓰기를 처리하는 기본 데이터베이스 인스턴스를 두고, 다른 가용 영역에는 예비 데이터베이스 인스턴스를 두게 되며, 기본 인스턴스가 중단되면 보통 2분 이내에 예비 인스턴스로 장애 조치가 수행된다.\n하단은 인스턴스 중단의 대표적인 이유를 나타낸다.\n가용 영역 중단 데이터베이스 인스턴스 유형 변경 인스턴스의 운영 체제 패치 데이터베이스 인스턴스를 만들 때나 만든 후라도 다중 AZ를 구성할 수 있다.\n모든 데이터베이스 엔진은 다중 AZ를 지원하지만 구현 방식은 약간씩 다르며, 인스턴스를 만든 후에 다중 AZ를 활성화하면 성능이 상당히 떨어지므로 유지 관리 주기를 짧게 설정해야 한다.\nOracle, PostgreSQL, MariaDB, MySQL, Microsoft SQL Server의 다중-AZ 다중 AZ 배포시, 모든 인스턴스가 같은 리전에 존재해야 하며, RDS는 주 인스턴스에서 예비 인스턴스로 데이터를 동기식(Synchronously)으로 복제하며, 이 때 지연시간이 발생할 수 있으므로, EBS 최적화 인스턴스와 프로비저닝된 IOPS SSD 스토리지를 사용해야 한다.\n예비 인스턴는 읽기 전용 복제본이 아니므로, 읽기 트래픽 처리가 불가능하다.\nOracle과 같이 기존 보유 라이선스(BYOL) 모델을 사용할 경우, 기본 인스턴스와 예비 인스턴스 모두 라이선스를 보유하고 있어야 한다.\nMySQL과 MariaDB는 다른 리전에서 다중 AZ 읽기 전용 복제본을 만들 수 있으며, 다른 리전으로 장애 조치를 수행할 수 있다.\nAmazon Aurora에서 다중-AZ Amazon Aurora의 다중 AZ 구현 방식은 위에서 설명한 방식과는 차이가 있으며, Amazon Aurora 클러스트는 기본 인스턴스로 구성되며, 항상 기본 인스턴스를 가리키는 클러스터 엔드폰이트를 함께 제공한다.\nAurora 클러스터에는 Aurora 복제본도 포함될 수 있으며, 기본 복제본과 모든 복제본은 단일 클러스터 볼륨을 공유한다.\n이 클러스터 볼륨은 3개 가용 영역에 동시에 복제되며, 필요에 따라 최대 64TB까지 자동으로 확장된다.\n기본 인스턴스에 자애가 발생했을 때, Aurora 복제본이 없으면 Aurora는 새로운 기본 인스턴스를 생성하고, Aurora 복제본이 있으면 Aurora는 복제본을 기본 복제본으로 승격시킨다.\n백업과 복구 RDS는 데이터베이스 인스턴스의 EBS 볼륨 스냇샷 기능을 제공한다. 일단 EBS 스냅샷처럼 인스턴스에 기반한 모든 데이터베이스는 스냅샷을 생성하여 S3에 저장할 수 있으며, 스냅샷은 중복성을 위해 같은 리전 여러 영역에 보관된다.\nMicrosoft SQL Server 이외의 데이터베이스 엔진에서는 다중 AZ를 사용하지 않는한 스냅샷을 하면 몇 초 동안 모든 I/O 작업이 일시 중단되므로 사용량이 적은 시간에 스냅샬을 생성해야 한다.\n백업 및 복구가 필요할 때 고려해야할 두가지 지표가 존재한다.\n복구 목표시간(Recovery Time Objective 이하 RTO)으로 장애 후 데이터르르 복구하고 처리를 재개하는 데까지 최대의 최대 허용시간을 의미한다. 복구 목표 지점(Recovery Point Object 이하 RPO)으로서 데이터 손실을 허용할 수 있는 최대 기간을 의미하며, RDS 백업 옵션을 선택할 때는 RTO, RPO 요구를 모두 고려해야 한다. RDS 스냅샷을 복수할 때 스냅샷을 새 인스턴스로 복구하는데, 복구 시간은 몇 분정도도 걸리며 크기에 따라 차이가 존재한다.\n새 인스턴스에 더 빠른 성능의 프로비저닝된 IOPS를 할당하면 복구 시간이 빨라진다.\n자동화된 스냅샷 RDS는 매일 30분 백업 기간에 인스턴스 스냅샷을 자동 생성할 수 있으며, 이 기간은 사용자가 지정할 수도 있고 RDS가 자동으로 수행하게 할 수도 있다.\n스냅샷을 진행하면 성능에 영향을 주기 때문에 데이터베이스가 가장 적게 사용되는 시간에 진행하는 것이 좋으며, RDS 백업을 진행하도록 설정하면, 리전마다 다르게 8시간 간격으로 80분 백업을 진행한다.\n자동 백업을 사용하면 특정 시점 복구가 가능해지며, 데이터베이스 변경 로그를 5분마다 S3로 저장한다.\n장애 이벤트가 발생하면 최대 5분 불량의 데이터만 손실이 발생하며, 특정 시점 복구는 몇 시간이 걸릴 수도 있으며, 트랜잭션 로그에 있는 데이터의 양에 따라 차이가 있다.\nRDS는 자동화된 스냅샷을 일정 기간동안 유지하고, 기간이 지나면 삭제한다. 사용자는 1일에서 35일 사이의 보존 기간을 선택할 수 있으며, 기본 값은 7일이다.\n자동 스냅샷을 사용하지 않으려면 보존 기간을 0으로 설정하고, 자동 스냅샷을 비활성화하면 기존의 자동화된 스냅샷 모두가 즉시 삭제되고, 특정 시점 복구가 비활성화된다.\n보존 기간을 0에서 다른 값으로 변경하면 즉시 스냅샷이 트리거된다.\n데이터베이스 인스턴스에 대해 수동으로 스냅샷을 수행할 수 있으며, 자동화된 스냅샷과 달리 수동 스냅샷은 삭제할 때까지 유지된다. 인스턴스를 삭제하면 사용자는 RDS의 최종 스냅샷 작업 수행 여부와 자동 스냅샷 여부를 선택하야 하고, 최종 스냅샷과 모든 수동 스냅샷은 유지되지만, 자동 백업을 유지하지 않기로 선택한다면 자동 스냅샷은 즉시 삭제된다.\n유지 관리 항목 RDS는 관리형 서비스이므로 패치 및 업그레이드 처리는 AWS가 책임지며, 데이터베이스 인스턴스에서 운영 체제 보안과 안정성 패치 등의 유지 관리를 몇 달에 한 번씩 정기적으로 수행한다.\n유지 관리 기간 동안 데이터베이스 엔진을 업그레이드 할 수도 있으며, AWS에서 새 버전의 데이터베이스 엔진을 지원하게 되면, 사용자는 새 버전 업그레이드를 결정할 수 있다.\n메이저 버전 업그레이드는 이전 버전과 호환하지 않는 데이터베이스 변경 사항이 포함되어 있을 수 있으므로, 메이저 버전 업그레이드는 사용자가 직접 적용해야 한다.\nAWS는 데이터베이스를 다시 빌드할 필요가 없는 nonbreaking 마이너 버전 번경을 적용한다.\n유지 관리 기간을 매주 30분으로 지정해 유지 관리 작업이 수행되는 시기를 결정할 수 있으며, 유지 관리와 백업을 같은 기간에 지정할 수 있다. 유지 관리 기간을 30분으로 설정해도 작업은 유지 관리 기간을 넘어서 진행될 수도 있다.\nAmazon Redshift Redshift는 OLAP 데이터베이스를 위해 설계된 PostgreSQL 기반의 관리형 데이터베이스 웨어하우스 솔루션으로 RDS와는 별개의 서비스로 존재한다.\nRedshfit는 열 기반 스토리지를 사용하므로, 저장 속도와 효율성이 향상되고 개별 열의 데이터를 더 빨리 쿼리할 수 있다.\nRedshift는 ODBC와 JDBC 데이터베이스 커넥터를 지원한다.\nRedshift는 압축 인코딩을 사용해 각 열의 스토리지에서 차지하는 크기를 줄이며, 수동으로 열 단위 압축을 수행할 수 있다.\nCOPY 명령을 사용해 파일에서 Redshift 데이터베이스로 데이터를 가져올 때 Redshift는 어떤 열을 압축할지 선택할 수 있다.\n컴퓨팅 노드 Redshift 클러스터에는 두 가지 범주로 나눠진 하나 이상의 컴퓨터 노드가 있다. 고밀도는 컴퓨팅 노드의 마그네틱 스토리지에 최대 326TB 데이터를 저장할 수 있고, 고밀도 스토로지 노드의 고속 SSD에 최대 2PB 데이터를 저장할 수 있다.\n둘 이상의 컴퓨팅 노드가 있을 때, Redshift에는 클라이언트와 통신하고 컴퓨팅 노드 간의 통신을 조정하는 리더 노드가 포함되어 있다. 이 리더 노드의 추가 비용은 없다.\n데이터 분산 스타일 Redshift 데이터베이스의 행은 컴퓨팅 노드에 걸쳐 분산되며, 데이터가 분산되는 방식은 분산 스타일에 따라 다르다.\nEVEN 분산은 기본 스타일이며 리더 노드가 데이터를 모든 컴퓨팅 노드에 걸쳐 고르게 분산시킨다.\nKEY 분산은 열 1개 값에 따라 데이터를 분산시키며, 값은 값을 가진 열은 같은 노드에 저장된다.\nALL 분산에서는 테이블이 컴퓨팅 노드에 분산된다.\n비관계형 데이터베이스 No-SQL 비관계형 데이터베이스는 초당 수만 개의 트랜잭션을 일관성 있게 처리하도록 설계되어 있다.\n관계형 데이터베이스에서 다룰 수 있는 데이터를 저장할 수 있다 하더라도 비관계형 데이터베이스는 비정형 데이터라고 하는 것에 최적화 되어있다.\n비정형의 데이터는 정형의 데이터가 아니라는 것을 설명하기 위해 사용되지만, 더 정확한 표현은 다중-정형 데이터라고 할 수 있다.\n이와 같이 비관계형 데이터베이스에 저장하는 데이터의 형태는 다양하고 계속변경할 수 있다.\n비관계형과 관계형 데이터베이스에는 공통된 요소가 존재핸다.\n비관계형 데이터베이스는 No-SQL 데이터베이스라 불리며, 컬렉션으로 구성된다. 컬렉션은 때로는 테이블이라 불리기도 하며 관계형 데이터베이스에서 행 또는 튜플 개념과 유사한 항목이 테이블에 저장된다.\n각 항목은 하나 이상의 속성으로 구성되며, 이 속성은 SQL 데이터베이스의 칼럼에 해당한다.\n속성은 키, 데이터 형식, 값이라고 하는 고유한 이름으로 구성되며, 속성은 키-값 페어라고도 불린다.\n데이터 저장 비관계형 데이터베이스가 관계형 데이터베이스와 다른 점은 스키마가 없으며, 테이블의 모든 항목이 같은 속성을 갖도록 요구하지 않는다는 것이다.\n각 항목에는 테이블 내에서 고유한 값이 있는 기본 속성이 있어야 하는 데, 기본 키는 항목을 고유하게 식별하고 값에 따라 정렬하기 위해서 사용된다.\n비관계형 데이터베이스는 저장 데이터 형식이 유연할 때 사용하며, 테이블을 만들 때 기본 키 속성 이외에는 속성을 미리 정의하지 않아도 되며, 항목을 작성하거나 수정할 때 바로 속성을 작성하는 데, 이 때 속성은 순서가 없고 서로 관계도 없으므로 비관계형이라고 부른다.\n비관계형 데이터베이스에서는 여러 테이블에 걸쳐 데이터를 나눈 뒤 이 데이터를 병합해서 쿼리할 수 있는 방법이 없으므로, 애플리케이션은 모든 데이터를 하나의 테이블에 보관하는 경우가 많으며, 이는 중복으로 이어지고 데이터베이스가 커지면서 심각한 스토리지 비용을 발생시킬 수 있다.\n데이터 쿼리 비관계형 데이터베이스는 비정형 데이터를 저장할 수 있는 유연성이 있지만, 쿼리가 제한돼 있다는 단점이 따르며, 기본 키 기반의 쿼리에 최적화 되어 있다.\n다른 속성을 쿼리할 때 속도가 더 느려지므로 비관계형 데이터베이스는 복잡하거나 임의의 쿼리에는 적합하지 않으며, 테이블을 만들기 전에 데이터에 어떠한 쿼리를 수행할지 정확히 이해해야 한다.\n하단의 표는 데이터 쿼리의 예시이다.\n키 형식 값 사원 ID(기본 키) 숫자 101 부서 문자 전산실 성 문자 Smith 이름 문자 Charlotte 비관계형 데이터베이스에서 Charlotte라는 사원이 있는 모든 부서의 목록을 조회하는 것은 어려울 수 있으며, 사원 ID로 항목이 정렬되어 있으므로, 이름의 값이 Charlotte인 항목을 찾으려면 시스템은 모든 항목을 검색해야하는 문제점이 존재한다. 각 항목의 데이터들은 정형화되어 있지 않기 때문에, 모든 속성마다 검색해서 부서 속성이 포함된 항목을 판별해야 하며, 이러한 쿼리는 느릴 뿐 아니라 컴퓨팅 자원도 상당히 소모한다. 비관계형 데이터베이스 유형 비관계형 데이터베이스가 키-값 저장소, 문서 지향적 저장소, 그래프 데이터베이스 등으로 분류되며, 기본적으로는 모든 비관계형 데이터베이스는 키-값 저장소 데이터베이스이다.\n문서 지향적 저장소는 값으로 지정된 문서의 내용을 분석하고 메타 데이터를 추출하는 특정한 비관계형 데이터베이스 애플리케이션이다.\n그래프 데이터베이스는 여러 항목에 있는 속성 간의 관계를 분석하며, 이는 레코드간의 관계를 묶는 관계형 데이터베이스와는 다르다. 그래프 데이터베이스는 비정형 데이터에서 이와 같은 관계를 찾아낸다.\nDynamoDB DynamoDB는 초당 수천 개 읽기 및 쓰기를 처리할 수 있는 관리형 비관계형 데이터베이스 서비스로, 데이터를 여러 파티션에 걸쳐 분산시켜서 이러한 성능을 얻는다.\n파티션은 테이블용 스토리지 할당으로, 여러 가용 영역의 SSD에 백업된다.\n파티션/ 해시 키 테이블을 만들 때 기본 키와 데이터 형식을 지정해야 한다.\n기본 키는 테이블의 항목을 고유하게 식별하므로, 값이 테이블 내에서 유일해야 하며, 하단과 같이 두 가지의 유형의 기본 키를 생성할 수 있다.\n파티션 키는 해시키라고도 하며 단일 값을 가지는 기본 키며, 파티션 키만 기본 키로 사용할 때 이를 단순 기본 키라고 한다.\n이메일 주소, 고유 사용자 이름, 임의로 생성한 ID 식별자 등이 파티션 키로 사용하기에 적합하며, 파티션 키로 저장할 수 있는 최대 크기는 2.048 바이트이다.\n기본 키로 파티션 키와 정렬 키를 조합해서 사용할 수도 있으며, 이를 복합 키라 한다.\n파티션 키는 고유할 필요는 없지만, 파티션 키와 정렬 키의 조합은 고유해야 하며, 사람이 성을 파티션 키로 이름을 정렬 키로 쓰는 예를 살펴보자. 이 방법으로 하면 테이블용 복합 키로 다음 값을 사용할 수 있디.\n성(파티션 키) 이름(정렬 키) Lewis Clive Lewis Warren Williams Warren 성 Lewis나 이름 Warren은 이 테이블에서 유일하지 않지만, 파티션 키와 정령 키를 함께 사용하면 고유한 기본 키를 생성 할 수 있다.\nDynamoDB는 기본 키를 기반으로 파티션에 걸쳐 항목을 배포한다.\n앞의 예에서 보면 성이 Lewis인 항목은 모두 같은 파티션에 저장되며, DynamoDB는 정렬 키를 사용해서 오름차순으로 항목을 정렬하고, 정렬 키로 저장할 수 있는 최대 크기는 1,024바이트이다.\n대량의 읽기 쓰기 작업이 발생하는 파티션을 핫 파티션이라 하며, 이는 성능에 악영향을 끼친다.\n핫 파티션이 되는 것을 피하려면 파티션 키를 최대한 고유하게 생성해야 한다.\n속성과 항목 각 키-값 페어는 속성을 구성하고, 하나 이상의 속성은 항목을 구성한다. DynamoDB가 저정할 수 있는 항목의 최대 크기는 400KB이며, 이는 대략 50,000개의 영어 단어 수와 동일하다.\n모든 항목은 최소한 기본 키와 키에 해당하는 값을 가지고 있으며, 속성을 생성할 때는 데이터 형식을 정하고, 하단과 같이 세 가지 범주로 정할 수 있다.\n스칼라\n문자열 데이터 형식은 UTF-8 인코딩을 사용해 최대 400KB의 유니코드 데이터를 저장할 수 있고, 문자열 길이는 0보다 커야 한다.\n숫자 데이터 형식은 최대 38자리의 양수나 음수를 저장하며, DynamoDB는 앞과 끝의 0을 자른다.\n바이너리 데이터 형식은 바이너리 데이터를 Base64 비트 인코딩 형식으로 저장하며, 문자열 형식과 마찬가지로 최대 항목 크기는 400KB로 제한한다.\n부울 데이터 형식은 ture 또는 false 값을 저장할 수 있다.\nnull 데이터 형식은 정의되지 않았거나 알려지지 않은 속성을 나타내며, null 데이터 형식에는 null 값이 포함되어야 한다.\n집합\n집합 데이터 형식은 수서가 없는 스칼라 값 목록을 담고 있으며, 값은 집합 내에서 고유해야 하고, 집합에는 하나 이상의 값이 포함되어 있어야 하며, 숫자 집합, 문자열 집합, 바이너리 집합의 작성이 가능하다. 문서\n문서 데이터 형식은 스칼라 집합 데이터 형식의 제약을 벗어나는 여러 형식의 데이터를 담을 수 있도록 설계되어 있으며, 최대 32레벨의 문서 형식을 중첩할 수 있다. 목록 문서 형식은 순서가 지정된 모든 형식의 값 모음을 저장할 수 있다. 하단은 목록 문서의 예시를 나타낸다 Chroes : [\"Make coffee\", Groceries : [\"milk\", \"eggs\", \"cheese\"], \"Pay bills\", Bills:[water: [60], electric:[100]]] # Chroes 목록에는 문자열 데이터, 숫자 데이터, 중첩 목록이 포함되어 있다. 맵 데이터 형식 맵 데이터 형식은 정렬되지 않은 키-값 페어의 집합을 JSON과 유사한 형식으로 저장할 수 있으며, 목록형식과 마찬가지로 포함할 수 있는 데이터 형시에는 제한이 없다. 하단은 맵 데이터 형식의 예시을 나타낸다. { Day: \"Friday\", Chores: [ \"Make coffee\", \"Groceries\", { Milk: { Quantity: 1}, eggs: { Quantity: 12}, } \"Mow the lawn\"], } 처리용량 테이블을 만들 때 애플리테이션에 필요한 초당 읽기 및 쓰기 횟수를 지정해야 하며, 이를 프로비저닝된 처리량이라 한다.\nDynamoDB는 테이블을을 만들 때 지정한 읽기 용량 단위 (Read Capacity Unitss : RCU) 및 쓰기 용량 단위 (Write Capacity Units : WCU) 갯수로 파티션을 예약한다.\n최대 4KB 크기의 항목을 기준으로 할 때, 1개의 RCU는 1개의 강력한 일관된 초당읽리를 제공하며, 일관된 읽기를 매초 8KB를 읽으려면 2개의 RCU가 필요하다.\n1개의 RCU는 초당 2개의 최종적 일관된 읽기를 제공하며, 최종적 일관된 읽기를 매초 8KB 항목을 읽으려면 1개의 RCU만 있으면 된다.\n데이터 쓰기의 경우, 1개의 WCU는 최대 1KB 크기의 항목 1개 쓰기를 제공하며, 1KB 미만인 항목을 초당 100개 쓰기 해야 한다면, 100개 WCU가 필요하다. 2KB 항목을 초당 10개 쓰기 위해서는 20개의 WCU가 필요하다.\nDynamoDB가 제공하는 최대 처리 용량은 사용자가 지정하며, 이를 초과하면 DynamoDB요청을 차단하고, ‘HTTP 400(bad request)’ 오류를 발생시킬 수 있다. AWS SDK는 조정 후 요청 재시도 기능을 지원하므로, 요청을 조정해서 애플리케이션이 데이터를 읽거나 쓰는 것을 막을 수는 있지만, 애플리케이션의 반응 속도는 느려지게 된다.\nAuto Scaling 테이블에 얼마만큼 처리량을 프로비저닝해야 할지 정확하지 않거나 시간에 따라 처리량의 요구가 달라질 것으로 예상할 때, Auto Scaling을 구성해서 정해 놓은 임계치에 가깝게 도달하면 자동으로 프로비저닝된 처리량을 증가하게 할 수 있다.\nAuto Scaling을 구성할 때 최소/ 최대 RCU와 WCU를 지정하고, 목표 사용률을 지정한다.\nDynamoDB는 RCU와 WCU를 자동으로 조정해서 이 목표 사용률에 따라 사용률을 유지한다.\n예를 들어 70%, 최소 10 RCU, 최대 50 RCU로 설정하는 경우, 21 RCU를 소비할 때 Auto Scaling은 프로비저닝된 용량을 약 30 RCU로 조정한다.\n소비자가 14 RCU로 떨어지면 Auto Scaling은 프로비저닝된 처리량을 20 RCU로 축소한다.\n적절한 사용률을 설정하면 작업에 균형을 이룰 수 있으나, 사용률을 높게 설정할수록 프로비정된 용량을 초과할 가능성은 커지고, 요청이 제한될 수 있다.\n반면 사용률을 너무 낮게 설정하면 필요하지 않은 용량에 비용을 지급하게 된다.\n예약 용령 100 이상의 WCU나 RCU가 필요할 때 예약 처리 용량을 구매해서 비용을 절약할 수 있다. RCU와 WCU를 별도로 예약해야 하며, 각각 100,000유닛으로 제한되 있으며, 1년이나 3년 사용 기간을 약정하고 선불로 지급한다. 데이터 읽기 DynamoDB는 테이블에서 두 가지 방식으로 데이터를 읽는다.\n스캔은 모든 테이블 항목을 나열하며, 읽기 집약적 작업이므로 프로비저닝된 용량 단위를 모두 사용할 가능성이 있다.\n쿼리는 파티션 키값을 기반으로 항목을 반환하며, 쿼리를 수행할 때 검색하는 파티션의 키의 값은 항목의 갑과 정확히 일치해야 한다.\n테이블에 정렬 키가 포함되어 있으면, 정렬 키로도 쿼리할 수 있다.\n정렬 키를 사용하면 정확한 값, 키보다 크거나 작은 값, 값의 범위, 값의 시작 등으로 더 유연하게 검색을 수행할 수 있다.\n보조 인덱스 보조 인덱스는 DynamoDB에서 데이터를 쿼리할 때 발생하는 두 가지 문제를 해결한다.\n사용자는 특정 항목을 쿼리할 때 파티션 키를 정확하게 지정해야한 한다.\n보조 인덱스를 만들 때 기본 테이블에서 인덱스로 복사할 속성을 선택할 수 있는 데, 이를 프로젝션된 속성 (Projected Attributes)라고 한다.\n보조 인덱스는 항상 기본 테이블의 파티션 키와 정렬 키 속성을 포함하며, 파티션 키와 정렬 키, 키 값만은 선택해서 복사하거나 키 값에 다른 속성을 추가해서 필요한 방식으로 데이터를 추출할 수 있다.\n글로벌 보조 인덱스 테이블을 만든 후에 언제든지 글로벌 보조 인덱스 (Global Secondary Index)를 만들 수 있다.\n글로벌 보조 인덱스에서 파티션 키와 해시 키는 기본 테이블과 다를 수 있지만, 기본 키 선택과 같은 규칙이 여전히 적용된다.\n인덱스의 기본 키는 고유하게 유지해야 하고, 복합 기본 키를 사용하면 파티션 키에서 같은 값을 가진 항목이 같은 파티션에 저장된다.\n글로벌 보조 인덱스에서 읽을 때는 항상 읽기 일관성이 유지되며, 항모을 테이블에 추가하더라도 즉시 보조 인덱스로 복사되지 않을 수 있다.\n로컬 보조 인덱스 로컬 보조 인덱스 (Local Secondary Index : LSI)는 기본 테이블과 동시에 만들어져야 하며 일단 만들면 삭제할 수 없다.\n파티션 키는 항상 기본 테이블과 같아야 하지만, 정령 키는 다룰 수 있다.\n예를 들어 기본 테이블에 LastName이 파티션 키이고 FirstName이 정렬 키이면, 파티션 키를 LastName이고 정렬 키를 BirthYear로 하는 로컬 보조 인덱스를 만들 수 있다.\n로컬 보조 인덱스의 읽기 시간을 얼마에 지정하냐에 ㄸ라 강력한 일관성 또는 최종적 일관성이 될 수 있다.\n요약 관계형 데이터베이스 또는 비관계형 데이터베이스의 사용 여부는 애플리케이션의 속성에 달려 있다.\n관계형 데이터베이스는 오랫동안 사용되어 왔으며, 많은 애플리케이션 개발자들은 기본적으로 관게형 데이터베이스에 맞게 데이터를 설계한다.\n애플리케이션은 특정 데이터베이스의 SDK를 사용해 데이터베이스와 상호 작용하므로 애플리케이션의 요구에 따라 특정 데이터베이스 엔진이 필요하게 된다.\n이러한 이유로 AWS RDS는 가장 널리 사용되는 6개 데이터베이스 엔진과 광범위한 버전 호환성을 지원함, 이는 애플리케이션을 변경하지 않고 기존 데이터베이스를 가져와서 RDS로 옮길 수 있도록 하려는 것이다.\n비관계형 데이터베이스는 최근에 창안되었으며, DynamoDBsms Amazon이 소유권을 가지고 있는 비관계형 데이터베이스 서비스이다.\n보통 관계형 데이터베이스용으로 설계된 애플리케이션과는 달리 온프레미스에서 배포해 사용하던 비관게형 데이터베이스용 애플리케이션은 대부분 코드를 변겨해야 DynamoDB로 이식할 수 있다.\n따라서 DynamoDB를 사용하는 애플리케이션을 개발하거나 재개발할 때 개발자에게 데이터베이스를 설계하는 법을 자문할 수도 있다.\n이 경우 파티션 키, 정렬 키, 데이터 형식을 선택하는 방법과 애플리에키션 성능 요구를 충족하기 위해 처리 용량을 할당하는 법을 이해하는 것이 중요하다.\nAWS 아키텍트는 적절한 데이터베이스와 AWS 서비스를 사용해서 성능 및 가용성 요구사항을 결정하고 올바르게 구현해야 한다.\n시험핵심 관계형 데이터베이스와 비관계형 데이터베이스의 차이점을 이해한다. 관계형 데이터베이스에서는 테이블을 생성하기 전에 속성을 정해야 한다.\n테이블에 입력하는 모든 데이터는 사전에 정한 속성과 부합해야 한다.\n데이터를 읽고 쓰는 데 SQL을 사용하므로 이를 SQL 데이터베이스라고도 한다.\n비 관계형 데이터베이스에서 테이블을 만들 때 요구하는 것은 기본 키 속성뿐이다.\n테이블의 모든 항목은 기본 키를 포함해야 한다는 것만 제외하면 속성을 다양하게 가질 수 있다는 유연성도 있다.\n비관계형 데이터베이스 또는 NoSQL 데이터베이스는 비정형 데이터를 저장한다.\nRDS가 지원하는 여러 데이터베이스 엔진을 파악하자 RDS는 MySQL, MariaDB, Oracle, PostgreSQL, Amazon Aurora, Microsoft SQL Server와 같이 많이 사용되는 대부분 데이터베이스 엔진을 지원한다.\n기본 보유 라이센스 사용과 라이센스 포함된 모델의 차이점을 이해해야 하며, 어떤 데이터베이스 엔진이 어떤 라이센스 모델을 지원하는 지 파악해야 한다.\n특정 스토리지 요구 사항에 맞는 인스턴스 클래스와 스토리지 유형으르 선택할 수 있어야 한다. 메모리와 스토리지가 관계형 데이터베이스의 제약 요인이 되는 경향이 있으므로 데이터베이스의 성능 요구 사항을 기반으로 올바른 인스턴스 클래스와 스토리지 유형을 선택하는 방법을 알고 있어야 한다.\n표준, 메모리 최적화, 순간 확장 가능의 세 가지 인스턴스 클래스를 파악해야 하며, 또한 세 클래스의 범용 SSD(gp2), 프로비저닝된 IOPS SSD(io1), 마그네틱 세 가지 스토리지 유형과 어떤 관련이 있는지 알아야 한다.\n다른 AZ와 읽기 전용 복제본의 차이점을 이해한다. 다중 AZ와 읽기 전용 복제본 모두 추가 데이터베이스 인스턴스를 만든다는 점에서는 연관되지만, 몇 가지 주요 차이점이 존재한다.\n읽기 전용 복제보은 쿼리를 처리할 수 있지만, 다중 AZ 배포에서 예비 인스턴스는 불가능하다.\n마스터 인스턴스는 읽기 전용 복제본에 비동기로 복제하지만, 다중 AZ 구성에서는 기본 인스턴스에서 예비 인스턴스로 동기로 데이터 복제가 이루어 진다.\nAuroa 복제본은 작동 방식과 Aurora 다중 AZ가 다른 데이터베이스 엔진의 AZ와 어떻게 다른지 이해해야 한다.\nDynamoDB 테이블에 적합한 기본 키 형식을 결정할 수 있어야 한다. DynamoDB 테이블은 두 가지 종류의 기본 키를 제공한다.\n단순 기본 키 파티션 키로만 구성되며 단일 값을 가지고 있다. DynamoDB는 파티션 키의 값에 따라 항목을 파티션에 분산시킨다.\n단순 기본 키를 사용할 때 파티션 키는 테이블 내에서 고유해야 하며, 복합 기본 키는 파티션 키와 정렬 키로 구성된다.\n파티션 키는 고유할 필요는 없지만, 파티션 키와 정렬 키의 조합은 고유해야한다.\nDynamoDB 처리 용량이 어떻게 작동하는지 파악한다. 테이블을 생성할 때 쓰기 용량 단위와 읽기 용량 단위로 처리용량을 지저해야 한다.\n다음 두 가지의 따라 읽기 작업이 읽기 용량 단위를 얼마나 소모할지 결정된다.\n읽기 작업이 강력하게 읽관적인지 최종적으로 일관적인지와 1초에 읽을 데이터의 용량이다. 최대 4KB 크기 항모글 강력한 일관된 읽기 작업을 할 때 하나의 읽기 용량을 단위로 사용한다. 최종적으로 일관된 읽기작업은 그 절반을 소비한다. 쓰기 용량 단위 하나를 사용해 쓰기 작업을 하면 초당 하나의 1KB 항목을 쓸 수 있다. ","#":"","5장-데이터베이스#\u003cstrong\u003e5장 데이터베이스\u003c/strong\u003e":"","5장의-목표#\u003cstrong\u003e5장의 목표\u003c/strong\u003e":"","시험핵심#\u003cstrong\u003e시험핵심\u003c/strong\u003e":""},"title":"5장 데이터베이스"},"/system/aws/awssaa/saa-6/":{"data":{"":"6장 인증과 권한 부여 - AWS Identity and Access Management 6장의 목표 안전한 애플리케이션 및 아키텍처 설명 어떻게 애플리케이션 티어를 보호할지 결정한다.\n어떻게 데이터르 보호하맂 결정한다.\nAWS 리소스는 기업의 소중한 자산이므로 엄격하게 보호돼야 한다.\n그렇다고 관리자와 고객조차 액세스 할 수 없을 정도로 보안 수준을 높일 수도 없다.\n높은 보안 수준을 유지하면서 관리자와 고객의 액세스는 허용할 수 있는 완벽한 절충점이 IAM이다.\nIAM은 사용자의 요청을 인증 (Authentification),해서 정당하게 사용하는 것을 확인하고 권한 부여 (Authorization)로 필요한 만큼 정확하게 액세스 할 수 있게 할 수 있다.\nAWS는 주로 IAM으 통해 자격을 인증학 권한을 부여한다.\n6장에서는 IAM 자격증명, 보안 주체를 학습하며 자격 증면은 AWS의 사용자의 역할을 나타내고, 여기서의 역할이란 애플리케이션 서비스, 사용자, 그룹에 일시적으로 할당할 수 있는 자격 증명을 의미한다.\n자격증명은 다른 서비스와 연동해서 사용할 수 있으며, AWS 계정 외부의 사용자나 애플리케이션을 인증하고, AWS 리소스에 임시 액세스하는 데 Kerberos, Microsoft Active Directory, LDAP (Lightweight Directory Access Protocol)과 같은 외부 서비스를 사용할 수 있다.\n정책을 통해서 AWS 계정의 모든 리소스아 상호 작용할 수 있는 방법을 정교하게 정의해서 자격 증명을 제어할 수 있다.\n정책은 보안 주체(Principal, 자격 증명 기반) 또는 리소스에 연결한다.\n계정에서 보안 주체의 작업을 자세히 통제하기 위한 정책 수립\n보안 주체의 자격을 증명하기 위해 사용되는 다양한 종류의 키 또는 토큰 관리\n자격 증명 연동을 사용해서 IAM을 외부 공급자와 통합하기 위한 Single Sign-On 솔루션 제공\n리소스를 적절하게 보호하기 위해 계정과 역할을 구성하는 모범 사례 구현\nIAM 자격 증명 새 AWS 계정을 만들면 하나의 자격 증명이 같이 만들어지며, 이 자격 증명은 루트 사용자이다.\n루트는 기본적으로 계정에 연결된 서비스와 리소스 전체에 완전한 권한을 가지고 있으며, 오직 루트 사용자만이 모든 서비스에 완벽하게 액세스 할 수 있다.\n루트의 모든 권한을 가지고 있는 사용자는 해커에 공격에 매우 위험하며, 이 경우 계정 전체가 위험에 빠질 수 있다.\nAWS는 보안 취약점 노출을 줄이기 위해 루트 계정을 철저하게 보호하고 일상적 작업에 필요한 구체적 권한을 다른 사용자에게 위임하는 것을 제안한다.\nIAM 정책 ","#":"","6장-인증과-권한-부여---aws-identity-and-access-management#\u003cstrong\u003e6장 인증과 권한 부여 - AWS Identity and Access Management\u003c/strong\u003e":"","6장의-목표#\u003cstrong\u003e6장의 목표\u003c/strong\u003e":""},"title":"6장 인증과 권한"},"/system/aws/awssaa/saa_all/":{"data":{"":"aws 총정리 Global service = can`t choose a region IAN IAM (Identity and Access Management, Global service) root account create by default, shouldn`t be used or shared Users are people within your organization, and can be grouped Group only contain users, not other groups -\u003e Group can`t contain other group Users don`t have to belong to a group, and usre can belong to multiple groups User or Groups can be assigned JSON documents called polcies These polices define the permissions of the users In AWS you apply the least privilege principle: don`t give more permissions than a user needs IAM Policies inheritance consists of\nVersion : policy language version, always include “2012-10-17” Id : an identifier for the policy (optional) Statement : one or more individual statements (required) Satements consists of\nSid : an identifier for the statement (optional) Effect : whether the statement aloows or denies access (Allow, Deny) Principal: account/user/role to which this policy applied to Action : list of actions this policy allows or denies Resource : list of resources to which the actions applied to Condition : conditions for when this policy is in effect (optional) IAN Password Policy\nStrong passwords = higher security for your account In AWS, you can setup a password policy: Set a minimum password length Requrire specific character types: including uppercase letters lowercase letters numbers non-alphanumeric charcters Allow all IAM users to change their own passwords Require users to change their password after some time (password expiration) Prevent password rescue MFA\nMFA ( Multi Factor Authentication ) MFA = password you know + security device you own Virtual MFA device google authenticator(phone only), Authy (Multi-device) Universal 2nd Factor (U2F) Security Key To access AWS AWS Management console / password + MFA AWS Command Line Interface(CLI) / access keys AWS Software Deveploper Kit (SDK) / acccess Keys Cloud shell ’’' · 미국 동부 (오하이오 주) · 미국 동부 (버지니아 주) · 미국 서부 (오리건 주) · 아시아 태평양 (뭄바이) · 아시아 태평양 (시드니) · 아시아 태평양(도쿄) · 유럽(프랑크푸르트) · 유럽(아일랜드) ’'' EC2 On-Demand Instanceds ( Pay for what you use) : short workload, predictble pricing Reserved ( Up to 75% discount compared to On-demand) : (MINIMUM 1 Year) Reserved Instances : Long workloads Convertible Reserved Instances : long workloads with flexible instances can change the EC2 instance type Up to 54% dicount Scheduled Reserved Instances : example - every Thursday between 3 and 6 pm Spot Insances : short workloads, cheap, can lose instances (less reliable) Can get discount of up to 90% compared to On-demand Instances that you can “lose” at any point of time if your max price is less than the current spot price Dedicated Hosts : book an entire physical server, control instance placemenet Allocated for your account for a 3-year period reservation More expensive Useful for software that have complicated licensing model (BYOL-Bring Your Own License) Or for companies thay have strong regulatory or compliance needs Elastic IP can only have 5 Elastic IP in account (can ask AWS to increase that) ","#":"","aws-총정리#\u003cstrong\u003eaws 총정리\u003c/strong\u003e":""},"title":"aws SAA 총정리"},"/system/aws/awstraining/":{"data":{"#":"Amazon Web Service Training\nAWS 시작하기 AWS 사용자 계정 생성 AWS CLI 활용 AWS 사용자 정의 VPC 생성 AWS EC2 생성 AWS AMI 생성 AWS Elastic IP 할당 AWS ELB ( 2 Tier ) 생성 AWS AutoScaling AWS RDS 생성 AWS 3Tier 구현 AWS S3 생성 ","amazon-web-service-training#\u003cstrong\u003eAmazon Web Service Training\u003c/strong\u003e":""},"title":"AWS Training"},"/system/aws/awstraining/3tier/":{"data":{"":"**** **** ","heading#****":"","heading-1#****":""},"title":"AWS 3Tier 구현"},"/system/aws/awstraining/ami/":{"data":{"":"AWS AMI 생성 저번 장에서는 EC2를 생성해보았습니다. 이번 Marketplace에서 AMI를 사용해서 인스턴스를 만들고, 생성한 인스턴스를 사용해서 AMI를 만들어 보도록하겠습니다. AMI에 대한 학습을 원하는 분들은 AWS AMI를 참고해주세요. AWS AMI 생성 먼저 EC2 생성을 위해 인스턴스 시작을 클릭 합니다. AMI 선택화면이 나오면 AWS Marketplace에서 CentOS를 입력 후, 선택합니다. 이와 같이 Marketplace에서는 사람들이 만들어둔 이미지를 사용할 수 있습니다. ( 단, 유료도 있으니 주의가 필요합니다. ) AMI를 선택 후, EC2를 생성합니다. 혹시 생성방법을 모르시는 분들은 EC2 생성 를 참조해주세요 인스턴스의 생성이 완료되면 퍼블릭 IP로 접속합니다. $ sudo yum install -y httpd $ sudo systemctl enable httpd Apache 설치 및 자동시작을 등록합니다. 퍼블릭 IP로 접속하여 확인해보세요. 이제 AMI를 만들어보겠습니다. AWS Console 환경에서 AMI를 만들 인스턴스를 우 클릭 후, 이미지 -\u003e 이미지 생성을 클릭합니다. 이미지 이름과 설명을 입력 후, 원하는 볼륨을 설정하여 이미지를 생성합니다. 메뉴바의 이미지에서 AMI를 클릭하면, 현재 만든 AMI를 확인할 수 있습니다. 만든 AMI의 상태가 available이 되면, 다시 인스턴스 생성으로 돌아와 AMI 선택에서 이번에는 AWS Marketplace가 아닌, 나의 AMI를 선택하여 생성합니다. 생성 후, 퍼블릭 IP로 접속하면 Apache가 설치되어 있는 것을 확인 할 수 있습니다. 이와 같이 AMI를 사용하면, 보다 편리하고 빠르개 인스턴스를 생성할 수 있습니다. AWS CLI로 AMI 생성 AMI 또한 AWS CLI를 통해 생성이 가능합니다. $ aws ec2 create-image \\ --instance-id [ 인스턴스 ID ] --name \"[ AMI 이름 ]\" --description \"[ AMI 설명 ]\" # 인스턴스 ID를 가진 인스턴스를 AMI 이름과 AMI 설명을 가진 AMI 이미지로 생성합니다. $ aws ec2 describe-images \\ --image-id [ AMI ID ] # AMI ID를 가진 AMI에 대한 정보를 알려줍니다. ","aws-ami-생성#\u003cstrong\u003eAWS AMI 생성\u003c/strong\u003e":"","aws-ami-생성-1#\u003cstrong\u003eAWS AMI 생성\u003c/strong\u003e":"","aws-cli로-ami-생성#\u003cstrong\u003eAWS CLI로 AMI 생성\u003c/strong\u003e":""},"title":"AWS AMI 생성"},"/system/aws/awstraining/as/":{"data":{"":"AWS AutoScaling 이번 장에서는 CloudComputing의 꽃이라고도 할 수 있는 AutoScaling 서비스를 구축해보겠습니다. AutoSacling의 대한 개념은 AutoScaling을 참조해주세요. AWS AutoScaling ( 이하 As ) As 그룹 생성을 위해 AWS에 접속 합니다. 인스턴스를 생성하여, Apache가 자동시작되어있게 설정 후, AMI를 생성합니다. AMI 생성 참고 AMI 생성 후, As 그룹 생성을 위해 좌측의 메뉴에서 Auto Scaling \u003e Auto Scaling 그룹 생성을 클릭합니다. As그룹에서 시작하기를 클릭합니다. 내 AMI에서 생성한 AMI를 선택합니다. AMI 선택이 완료되면 기본적인 시작 구성들을 입력합니다. 보안 그룹을 기존의 80번 포트가 열려있는 보안그룹을 사용했습니다. 기본 구성이 완료되면, 바로 As 그룹생성으로 이동됩니다. 그룹 생성에서 위의 그림과 VPC와 서브넷을 설정합니다. 여기서 시작구성에서 선택한 보안그룹과 선택한 보안그룹이 일치하지 않으면 에러가 발생합니다. As 그룹생성에서 정책을 설정합니다. 평균 cpu의 사용이 5분간 30%이상이면 증가하는 정책을 생성합니다. 이와 동일하기 평균 cpu의 사용이 5분간 30%미만이면 삭제되는 정책을 생성합니다. 생성이 완료되면 As 보안그룹을 통해 인스턴스의 수, 최소, 최대 용량을 확인할 수 있습니다. 인스턴스가 생성된 것을 확인 할 수 있습니다. $ apt -y update $ apt -y install stress $ stress -c 1 이제 As 확인을 위해 stress를 설치 후 작동시킵니다. stress를 실행 후, 설정시간이 경과하면 인스턴스가 증가됨을 확인할 수 있습니다. 이와 같이 As를 통해 인스턴스를 정책에 따라 자동적으로 증가\u0026 감소 시키는 것이 가능합니다. 또한 저번 장에서 구축했던 ELB에 As 그룹을 등록하면, 자동적으로 로드 밸런싱을 되어 유동적인 자원관리가 가능해집니다. AWS CLI를 통한 생성 $ aws autoscaling create-launch-configuration \\ --launch-configuration-name launch-config-sample \\ --image-id [ AMI ID ] \\ --key-name [ key name ] \\ --no-ebs-optimized \\ --instance-type [ instance type ] \\ --instance-monitoring Enabled=true \\ --security-groups [ 보안그룹 ID ] \\ --associate-public-ip-address As 주요 설정 파라미터\n항목 이름 설명 Auto Sacling Group Name As 그룹명 Launch Configuration As 그룹에서 사용할 Launch Configuration Load Balancers As 그룹에서 사용할 ELB Desired As 그룹조건에 해당하지 않는 일반적인 인스턴스의 수 Min As 그룹에서 사용할 인스턴스의 최솟값 Max As 그룹에서 사용할 인스턴스의 최댓값 Health Check Type As 그룹에서 사용할 헬스 체크 판단 유형 ( EC2 or ELB ) Health Check Period As 그룹의 헬스 체크가 시작될 때 까지의 초 Termination Policies As 그룹에 속한 인스턴스의 삭제방침 Availability Zone As 그룹이 사용할 가용영역 Subnet As 그룹이 사용할 서브넷 Default Cooldown 스케일링 처리 후에 새로운 스케일링 처리를 받을 때 까지의 시간 Placemenet Group 낮은 레이턴시 ( Latency ) 환경과 논 블로킹 통신이 가능한 Placement Group을 선택 Suspended Processes 처리를 일시적으로 정지시킬 프로세스 목록 ( AWS CLI의 suspend-processes 명령어로 설정 ) Enabled Metrics CloudWatch 에서 활성화 되어 있는 매트릭스 목록 Scaling Policy 유형\n유형 설명 ChangelnCapacity 그룹의 현재 용량을 지정한 수의 인스턴스만큼 늘리거나 줄입니다. ExactCapacity 그룹의 현재 용량을 지정된 수의 인스턴스로 변경합니다. PercentChangelnCapacity 그룹의 현재 용량을 지정된 비율만큼 늘리거나 줄입니다. Scaling Policy의 주요 파라미터\n항목 이름 설명 Scaling policy Name 이름 지정 Execute policy when 실행할 조건 ( CloudWatch의 Alram으로 설정 ) Take the action Auto Scaling Group의 목표 인스턴스 수를 설정 And than wait 다른 스케일링 처리가 실행되고 있을 때 대기할 시간 ","aws-autoscaling#\u003cstrong\u003eAWS AutoScaling\u003c/strong\u003e":"","aws-autoscaling--이하-as-#\u003cstrong\u003eAWS AutoScaling ( 이하 As )\u003c/strong\u003e":"","aws-cli를-통한-생성#\u003cstrong\u003eAWS CLI를 통한 생성\u003c/strong\u003e":""},"title":"AWS AutoScaling"},"/system/aws/awstraining/aws-lambda-crawling/":{"data":{"":"AWS Lambda Crawling AWS Lambda Crawling $ pip3 install [ Package ] -t . $ pip3 install bs4 -t . $ '[ 7z 경로, 다른 zip도 가능 ]' a '[ 압축할 패키지 이름 ]' '[ 압축할 패키지 경로 ]' $ 'C:\\Program Files\\7-Zip\\7z.exe' a 'C:\\AWSLambda\\bs4.zip' '.' ","aws-lambda-crawling#\u003cstrong\u003eAWS Lambda Crawling\u003c/strong\u003e":"","aws-lambda-crawling-1#\u003cstrong\u003eAWS Lambda Crawling\u003c/strong\u003e":""},"title":"AWS Lambda Crawling"},"/system/aws/awstraining/cli/":{"data":{"":"AWS CLI 활용 AWS CLI 활용 이번 시간에는 AWS CLI을 활용하는 방법에 대해 알아보도록 하겠습니다. AWS CLI의 대한 개념과 설치는 AWS CLI를 참고해주세요. AWS CLI 기본설정 먼저 여기서는 Window 10, Powershell에서 진행하도록 하겠습니다. Linux나 Mac 등 타 OS도 AWS CLI가 설치되어 있으면 모두 동일하니 똑같이 진행하셔도 문제없습니다. 먼저 프롬프트 혹은 터미널을 실행 후, aws configure을 입력합니다. 그러면 엑세스 키와 시크릿 키, 리전 그리고 파일형식을 입력하는 값이 나오는 데, 만약 전 시간에서 사용자 계정을 만들면서 학습했던 프로그래밍 엑세스 방식이 생각나신다면, 한결 수월하게 해결하실 수 있습니다. 혹시 모르시거나 깜박하신 분들은 AWS IAM, AWS 사용자 계정 생성을 참고해주세요. $ aws configure # aws 인증 값 등록 AWS Access Key ID [ AcceseeKeyId ]: ********* # 계정의 AccessKeyId를 입력 AWS Secret Access Key [ SecretAccessKey ]: ******** # 계정의 SecretAccesskey를 입력 Default region name [ Region ]: ap-northeast-2 # 리전의 이름을 입력 Default output format [ File format ]: json # 파일의 포맷 형식을 입력 $ aws ec2 describe-security-groups # 확인 ( 차후에 명령어에 대해 설명드리겠습니다. ) AWS CLI 사용방법 그럼 이제 본격적인 CLI 사용방법에 대해 알아보도록 하겠습니다. profile을 설정 위에서 configure을 통해 aws CLI을 사용하기 위한 인증을 마쳤습니다. 하지만 만약 인증을 마친 유저에 대한 권한이 다르다면, 또 다른 계정을 사용해야 한다면 어떻게 해야할까요? 이를 위해 AWS CLI에서는 –profile 명령어를 통해 별도이 설정파일로 저장할 수 있습니다. $ aws configure --profile [ User ] AWS Access Key ID ... : *** ... Default output format ... : *** # 개별 설정파일 등록 $ aws [ 명령어 ] --profile [ User ] $ [ User ]의 권한으로 명령어를 실행 위의 명령어를 통해 [ User ]의 profile을 지정 후 저장 후, –profile [ User ] 옵션을 통해 사용합니다. 등록한 모든 설정파일은 보통 사용자계정 폴더 내부의 .aws에 생성됩니다. AWS CLI 기본적인 명령어 형태 AWS CLI의 기본적인 명령어 형태는 다음과 같습니다. $ aws [ 서비스 이름 ] [ 리소스 조작 명령어 ] ------------------------------------------------------------------------------------------------- 옵션 | 처리 ------------------------------------------------------------------------------------------------- --profile | 설정한 profile로 명령어를 실행합니다. --region | 리전을 지정합니다. --output | 출력 형식을 지정합니다. --filters | 참조 계열 명령어를 사용할 때, 검색 조건을 지정해서 필터링 합니다. --query | 실행 결과 내용을 압축해서 출력합니다. Region과 output 옵션을 사용한 검색 조건 지정 $ aws ec2 describe-security-groups --region ap-northeast-2 --output [ json, text, table ] # ap-northeast-2 리전에서 각 형식으로 ec2 보안그룹에 대한 정보를 참조 각 형식의 차이점을 확인해보세요. filters 옵션을 사용한 검색 조건 지정 –filters 옵션을 사용하면 참조 계열 명령어를 실행할 때, 검색 조건을 지정할 수 있습니다. 지정할 수 있는 –filters 옵션의 필터 이름은 서비스, 리소스에 따라 차이가 있어 AWS CLI 명령어 레퍼런스를 참고해주세요.. $ aws [ 서비스 이름 ] [ 리소스 조작 명령어 ] --filters \"Name=[ 필터 이름 A ], Values=[ 조건A1 ]\" \"Name=[ 필터 이름 B ], Values=[ 조건B1 ], [ 조건B2 ]\" 위와 같이 –filters의 사용방법은 1개의 필터에 큰 따옴표(\")를 감싸고, “Name=“에 필터이름, “Values=“에 필터 이름에 대응하는 조건을 작성하는 것으로, 쉼표(,)를 통해 복수의 조건을 작성하는 것 또한 가능합니다. $ aws ec2 describe-instances --filters \"Name=private-ip-address,Values=10.0.0.10\" # 프라이빗 ip가 10.0.0.10인 ec2를 참조 $ aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.medium, m3.medium\" # 인스턴스의 타입이 t2.medium, m3.medium인 인스턴스를 참조 $ ec2 describe-instances --filters \"Name=tag:Project,Values=AWS Training\" # 태그의 값이 AWS Training인 인스턴스를 참조 $ aws ec2 describe-instances --filters \"Name=instance-type,Values=t2.medium, m3.medium\" \"Name=tag:Project,Values=AWS Training\" # 2번째 조건과 3번째 조건을 함께 사용하는 인스턴스 참조 $ aws ec2 describe-images --filters \"Name,Values=SampleAMI2020*\" # AMI images 중에서 이름이 SampleAMI로 시작하는 모든 인스턴스를 참조 query 옵션을 사용한 출력 결과 압축 query 옵션을 사용해서 명령어를 실행 할 때 실행 결과를 압축할 수 있습니다. filters와 마찬가지로 각 서비스에 따라 사용할 수 있는 query가 다르며 가능한 명령어들은 AWS CLI 명령어 레퍼런스를 참고해주세요. $ aws [ 서비스 이름 ] [ 리소스 조작 명령어 ] --query '[ 쿼리 이름 ( 1계층 )[].쿼리 이름(2 계층)....'] query는 계층 구조로 되어 있습니다. 따라서 AWS CLI 명령어 레퍼런스를 참고해서 계층 구조와 출력할 항목을 query 옵션으로 지정해야 합니다. query은 추가로 json, table 형식으로 출력할 때는 쿼리 이름이 키 ( 별칭 )을 붙여야 하며, –filter와 마찬가지로 조건을 지정할 수 있습니다. ( \u003c, \u003c=, ==, \u003e=, \u003e, !=) $ aws ec2 describe-instances --query 'Reservations[].Instances[].InstanceId' # 모든 인스턴스의 인스턴스 ID를 참조 $ aws ec2 describe-instances --query 'Reservations[].Instances[].[InstanceId,PrivateIpAddress]' # 모든 인스턴스의 인스턴스 ID, 프라이빗 IP를 참조하는 방법 $ aws ec2 describe-instances --query 'Reservations[].Instances[?InstanceType='t2.small'].[InstanceId, PrivateIpAddress] --output json # 인스턴스 유형이 t2.small인 것의 인스턴스 ID, 프라이빗 IP를 json형식으로 출력 $aws ec2 describe-instances --query 'Reservations[]. Instances[?InstanceType=='t2.small'].{IDLInstanceID,IP:PrivateIpAddress,Name:Tags[?Key=='Name'].Value}' --output json # 인스턴스 유형이 t2.small인 것의 인스턴스 ID, 프라이빗 ID에 ID, IP라는 키를 붙여 JSON 형식으로 출력하는 방법 지금까지 기본적인 AWS CLI에 명령어에 대해 알아보았습니다. 하지만 아직, 인스턴스나 VPC, 각종 서비스에 대해 공부를 하지 않아 아직까지는 간단하게 어떻게 작동되는 지, 어떠한 형식을 가지고 있는 지만 익혀두시면 충분합니다. ","#":"","aws-cli-사용방법#\u003cstrong\u003eAWS CLI 사용방법\u003c/strong\u003e":"","aws-cli-활용#\u003cstrong\u003eAWS CLI 활용\u003c/strong\u003e":"","aws-cli-활용-1#\u003cstrong\u003eAWS CLI 활용\u003c/strong\u003e":""},"title":"AWS CLI 활용"},"/system/aws/awstraining/cloudformation/":{"data":{"":"AWS CloudFormation 이번 장에서는 CloudFormation의 탬플릿을 사용하여 서버를 자동 구축되도록 생성해보도록 하겠습니다. CloudFormation의 대한 개념은 CloudFormation을 참고하세요. CloudFormation을 활용한 자동구축 CloudFormation 아키텍처 예시 먼저, AWS에서 CloudFormation 검색 후 클릭합니다. 스택 생성을 클릭합니다. 스택 생성을 위해 아래의 값을 cloudformation_instance.template 을 생성하여 업로드 합니다. 보통 Templates 파일은 S3에 저장된 것을 사용하지만, 여기서는 로컬환경에서 가져와 사용해보도록 하겠습니다. CloudFormation Templates 참조 { \"AWSTemplateFormatVersion\" : \"2010-09-09\", \"Resources\": { \"Instance\": { \"Type\": \"AWS::EC2::Instance\", \"Properties\":{ \"Monitoring\": \"false\", \"ImageId\": \"[ AMI ID ]\", \"KeyName\": \"[ Key ]\", \"InstanceType\": \"t2.micro\", \"NetworkInterfaces\": [ { \"DeviceIndex\": \"0\", \"AssociatePublicIpAddress\": \"true\", \"DeleteOnTermination\": \"true\", \"SubnetId\": \"[ 서브넷 ID ]\", \"GroupSet\": [\"[ 보안 그룹 ]\"] } ] } } }, \"Description\": \"SampleInstance\" } CloudFormation 기본형식 { \"AWSTemplateFormatVersion\" : \"version date\", \"Description\" : \"JSON string\", \"Parameters\": { set of parameters }, \"Mappings\": { set of mappings }, \"Conditions\": { set of conditions }, \"Resources\": { set of resources }, \"Outputs\": { set of outpus } } 옵션 설명 AWSTemplateFormatVersion 템플릿의 버전 Description 템플릿의 대한 설명 ( 시스템이 읽지 않음 ) Parameters 스택 생성 때에 전달할 값, 탬플릿 내부에서 Ref 함수로 참조 Mappings 해시 테이블처럼 키에 따라 값을 지정할 수 있으며, 리전마다 사용할 AMI를 다르게 하는 경우 등의 사용 Conditions 조건을 판단, 조건에 일치하는 경우 실행할 리소스를 지정할 수 있음 Resources 생성할 자원을 정의, EC2 인스턴스, 보안 그룹 등의 생성할 자원 유형을 지정하고 설정 ( 아직 모든 서비스를 이용할 수는 없음 ) Outputs 탬플릿으로 생성한 결과를 출력 업로드가 완료되면, 스택의 이름을 지정합니다. 스택 옵션 구성에서는 IAM 역할, 그 외에도 스택의 정책과 옵션들을 구성할 수 있습니다. 여기에서는 기본 값으로 진행하겠습니다. 스택의 생성이 완료되면, 그림과 같이 상태에서 로그를 확인하실 수 있습니다. 생성이 완료되면 인스턴스가 생성된 것을 확인 할 수 있습니다. CloudFormation 업데이트 이번에는 생성된 Stack을 업데이트 하는 방법에 대해 알아보도록 하겠습니다. 먼저, 위에서 생성한 Stack \u003e 업데이트를 클릭합니다. 스택 업데이트를 클릭하면 현재 템플릿을 사용하면서 스택의 옵션만 바꿀건지, 혹은 탬플릿 자체를 변경할 것인지에 대한 옵션이 나옵니다. 저희는 탬플릿 자체에 대한 옵션을 바꾸기 위해 기존 templates 파일을 아래와 같이 수정하여 업데이트 하도록 하겠습니다. Designer 편집기에서 추가하셔도 상관은 없습니다. { \"AWSTemplateFormatVersion\" : \"2010-09-09\", \"Resources\": { \"Instance\": { \"Type\": \"AWS::EC2::Instance\", \"Properties\":{ \"Monitoring\": \"false\", \"ImageId\": \"ami-01af223aa7f274198\", \"KeyName\": \"Study\", \"InstanceType\": \"t2.micro\", \"NetworkInterfaces\": [ { \"DeviceIndex\": \"0\", \"AssociatePublicIpAddress\": \"true\", \"DeleteOnTermination\": \"true\", \"SubnetId\": \"subnet-463b3d0a\", \"GroupSet\": [\"sg-f553af91\"] } ], \"UserData\": { \"Fn::Base64\" : { \"Fn::Join\" : [\"\", [ \"#!/bin/bash\\n\", \"yum update -y\\n\", \"yum install -y httpd\\n\", \"service httpd start\\n\", \"chkconfig httpd on\\n\" ]] } } } } }, \"Description\": \"SampleInstance\" } 탬플릿 업로드 후, 위와 동일하게 생성하면 보기와 같이 업데이트를 확인하실 수 있습니다. 생성된 인스턴스로 접속하면, Apache가 설치되어 있는 것을 확인하실 수 있습니다. CloudForamtion 파라미터 설정 위에서는 고정 값으로 스택을 생성했지만, 만약 고정 값으로 생성을 진행할 경우, 에러가 발생할 수 있으며, 여러 탬플릿을 생성해야하는 번거로움이 존재합니다. 이번에는 파라미터 값을 설정하여 CloudFormation을 사용하는 방법에 대해 알아보도록 하겠습니다. 먼저 위에서 생성한 템플릿을 Parameter 값을 사용하도록 수정하여 보겠습니다. { \"AWSTemplateFormatVersion\" : \"2010-09-09\", \"Parameters\": { \"ImageId\" : { \"Type\": \"String\", \"Default\": \"ami-01af223aa7f274198\", \"Description\": \"IamgeId\" }, \"KeyName\" : { \"Type\": \"String\", \"Default\": \"Study\", \"Description\": \"Keypair name\" }, \"InstanceType\" : { \"Type\": \"String\", \"Default\": \"t2.micro\", \"Description\": \"InstanceType\" }, \"AssociatePublicIpAddress\" : { \"Type\": \"String\", \"Default\": \"true\", \"Description\": \"PublicIP\", \"AllowedValues\": [\"true\", \"false\"] }, \"DeleteOnTermination\" : { \"Type\": \"String\", \"Default\": \"true\", \"Description\": \"DeleteOnTermination\", \"AllowedValues\": [\"true\", \"false\"] }, \"SubnetId\" : { \"Type\": \"String\", \"Default\": \"subnet-463b3d0a\", \"Description\": \"SubnetId\" }, \"GroupSet\" : { \"Type\": \"String\", \"Default\": \"sg-f553af91\", \"Description\": \"GroupSet\" } }, \"Resources\": { \"Instance\": { \"Type\": \"AWS::EC2::Instance\", \"Properties\":{ \"Monitoring\": \"false\", \"ImageId\": { \"Ref\" : \"ImageId\" }, \"KeyName\": { \"Ref\" : \"KeyName\" }, \"InstanceType\": { \"Ref\" : \"InstanceType\" }, \"NetworkInterfaces\": [ { \"DeviceIndex\": \"0\", \"AssociatePublicIpAddress\": { \"Ref\" : \"AssociatePublicIpAddress\" }, \"DeleteOnTermination\": { \"Ref\" : \"DeleteOnTermination\" }, \"SubnetId\": { \"Ref\" : \"SubnetId\" }, \"GroupSet\": [{ \"Ref\" : \"GroupSet\" }], } ], \"UserData\": { \"Fn::Base64\" : { \"Fn::Join\" : [\"\", [ \"#!/bin/bash\\n\", \"yum update -y\\n\", \"yum install -y httpd\\n\", \"service httpd start\\n\", \"chkconfig httpd on\\n\" ]] } } } } }, \"Description\": \"SampleInstance\" } 특정 리소스 값들은 파라미터 값에서 가져오는 방식으로 수정하였습니다. 위와 같이 설정을 마친 후, 동일하게 스택의 생성을 진행해봅니다. 템플릿을 업로드 후 동일하게 진행합니다. 하지만 전과는 다르게 수정한 파라미터 값들을 선택하거나 기입하는 선택란이 추가되었습니다. 기본적으로 Default 값들을 출력하며, AllowedValues 값이 존재할 시 그 값들만을 선택가능합니다. 생성이 완료되었습니다. 이와 같이 파라미터 값을 사용하면 보다 편리하게 서비스들의 구현이 가능합니다. ","#":"","aws-cloudformation#\u003cstrong\u003eAWS CloudFormation\u003c/strong\u003e":"","cloudforamtion-파라미터-설정#\u003cstrong\u003eCloudForamtion 파라미터 설정\u003c/strong\u003e":"","cloudformation-업데이트#\u003cstrong\u003eCloudFormation 업데이트\u003c/strong\u003e":"","cloudformation을-활용한-자동구축#\u003cstrong\u003eCloudFormation을 활용한 자동구축\u003c/strong\u003e":""},"title":"AWS CloudFormation"},"/system/aws/awstraining/cognito/":{"data":{"":"AWS Cognito AWS Cognito Cognito는 기본적으로 모바일에서 인증을 진행 후, 인증 혹은 비인증에 해당하는 리소스에 대한 사용 권한을 부여 받는 형식으로 진행됩니다. Cognito 서비스 사용을 위해서 먼저, AWS에 접속하여 Cognito를 검색합니다. Cognito에 대한 개념은 Cognito를 참고하세요. Cognito의 메인 페이지에서 \u003e 자격 증명 풀 관리를 클릭합니다. 새 자격 증명 풀을 생성합니다. 인증되지 않은 자격 증명은 비인증 사용자에 대한 엑세스 권한을 설정하는 옵션입니다. 인증 공급자는 사용자의 인증을 확인해 OpenID Connect 기반의 프로바이저입니다. 생성이 완료되면 새로운 IAM 권한을 생성하고 허용합니다. 이제 다시 풀 관리로 진입하면, 생성된 풀의 확인이 가능합니다. 생성한 풀을 선택하여 해당 풀에 진입합니다. 풀에 대한 자격증명을 편집합니다. AWS Mobile SDK 이제 다음으로 AWS Mobile SDK에 대한 사용방법을 알아보도록 하겠습니다. AWS Moblie SDK 참조 핸드폰의 기종에 맞춰 안드로이드 스튜디오 혹은 Xcode를을 준비해주세요. 여기서는 Vmware의 Mac환경을 설치하여 진행하도록 하겠습니다. VMware Mac 환경설치 $ sudo gem install bundler $ sudo gem install cocoapods $ pod setup # CocoaPods 라이브러리를 설치합니다. $ git clone https://github.com/awslabs/aws-sdk-ios-samples.git $ Samples 코드를 다운 받습니다. $ cd [ 다운로드 경로 ]/S3TransferManager-Sample/Objective-C $ cat Podfile # 해당 디렉토리로 이동하여 Podfile을 확인합니다. Podfile은 프로젝트에 필요한 라이브러리를 작성하는 파일입니다. $ pod install # Podfile의 작성되어 있는 라이브러리들을 설치합니다. AWS Mobile SDK을 사용하기 위한 환경을 구현합니다. 설치가 완료되면 Xcode를 실행합니다. 단, .xcodeproj가 아닌 .xcworkspace파일을 실행해야 합니다. #import \u003cFoundation/Foundation.h\u003e NSString *const AWSAccountID = @\"Your-AccountID\"; NSString *const CognitoPoolID = @\"Your-PoolID\"; NSString *const CognitoRoleUnauthID = @\"Your-RoleUnauthID\"; NSString *const CognitoRoleAuth = @\"Your-RoleID\"; NSString *const S3BucketName = @\"Your-S3-Bucket-Name\"; Constant.m 파일을 편집합니다. ","aws-cognito#\u003cstrong\u003eAWS Cognito\u003c/strong\u003e":"","aws-cognito-1#\u003cstrong\u003eAWS Cognito\u003c/strong\u003e":"","aws-mobile-sdk#\u003cstrong\u003eAWS Mobile SDK\u003c/strong\u003e":""},"title":"AWS Cognito"},"/system/aws/awstraining/ebs/":{"data":{"":"AWS Elastic Fire System EFS AWS 서비스에서 EFS를 클릭합니다. 스토리지 생성을 위해 파일 시스템 생성을 클릭합니다. 네트워크 엑세스를 구성합니다. 여기서는 기본 VPC에서 가용영역 a, c를 사용하겠습니다. 파일 시스템 설정 구성을 설정합니다. 여기서는 후에 설정을 전부 기본 값을 사용하여 생성합니다. 생성된 내용을 확인합니다. EFS 사용하기 위해 가용영역 a, c에 인스턴스를 생성합니다. $ yum install make git binutils $ git clone https://github.com/aws/efs-utils $ cd efs-utils $ ./build-deb.sh $ cd build $ yum install -y ./amazon-efs-utils-1.5-1.deb $ yum install -y install nfs-common 패키지들을 설치합니다. ","aws-elastic-fire-system#\u003cstrong\u003eAWS Elastic Fire System\u003c/strong\u003e":"","efs#\u003cstrong\u003eEFS\u003c/strong\u003e":""},"title":"정리 중"},"/system/aws/awstraining/ec2/":{"data":{"":" 이번 장에서는 저번 장에서 생성했던 사용자 정의 VPC의 대역에 EC2를 생성해 보도록 하겠습니다. EC2 또한 중요한 개념이므로, EC2에 대한 학습을 원하는 분들은 AWS EC2를 참고해주세요. EC2 ( Elastic Compute Cloud ) 생성 기본적인 EC2 생성의 순서\n1. AMI ( Amazon Machin Image ) 선택 2. Instance type 선택\n2. Instance Network 설정\n3. Storage 설정\n5. Tag 설정\n6. Security Group 설정 여기에서는 처음에는 기본 VPC, 후에는 전장에서 생성했던 VPC에 생성해보도록 하겠습니다.. 검색에서 EC2를 입력후 인스턴스로 들어갑니다. 대시보드에서 현재 사용량을 확인할 수 있습니다. 인스턴스 생성을 위해 좌측 메뉴에 인스턴스를 클릭합니다. 인스턴스 시작을 클릭합니다. AMI 선택에서는 이미지 파일을 선택할 수 있습니다. 여기에서는 Amzon Linux를 생성하겠습니다. 또한 AMI는 직접 만들 수 있으며, AWS Marketplace를 통해서 타 유저의 이미지를 구매할 수도 있습니다. AWS 인스턴스 유형에서는 cpu, ram 스토리지의 유형과 사양 등을 선택할 수 있습니다. 여기에서는 과금이 발생하지 않게 t2.micro를 선택하겠습니다. 인스턴스 세부 정보 구성에서는 인스턴스의 수, VPC 서비넷 대역, 용량 예약, IAM 역할 등 세부 정보를 설정할 수 있습니다. 여기서는 기본 값으로 생성하겠습니다. 설정 항목 설명 인스턴스 갯수 기동할 인스턴스의 수를 나타냅니다. 구매 옵션 구매 옵션을 선택합니다. 체크 시 스팟 인스턴스로 구입할 수 있습니다. 네트워크 인스턴스를 기동할 VPC를 나타냅니다. 서브넷 인스턴스가 소속될 서브넷을 나타냅니다. 퍼블릭 IP 자동 할당 자동적으로 퍼블릭 IP를 부여할지 설정합니다. 배치 그룹에 인스턴스 추가 배치 그룹을 선택합니다. 체크 시 배치 그룹에서 인스턴스를 생성합니다. 용량 예약 용량 예약은 특정 가용 영역에서 인스턴스가 시작되도록 예약합니다. IAM 역할 EC2 인스턴스에 부여할 IAM권한을 설정합니다. 종료 방지 EC2 인스턴스의 삭제를 막습니다. 모니터링 CloudWatch를 통한 모니터링 서비스를 활성화합니다. 화성화하면 1분 간격으로 CloudWatch에 데이터가 전송됩니다. ( 일반적으로 5분 간격을 설정 ) 테넌시 하드웨어 점유 옵션으로, Shred를 선택시 공유, Dedicated를 선택하면 완전 점유합니다. 사용자 데이터 인스턴스 실행시 셀 스크립트 또는 cloud-init 디렉티브를 작성할 수 있습니다. 스토리지 추가에서는 볼륨을 추가할 수 있습니다. 태그 추가에서는, 인스턴스에 대한 세부사항을 정의할 수 있습니다. 여기서는 기본 값으로 생성하겠습니다. 보안 그룹에서는 생성되는 EC2에 대한 보안 그룹을 지정합니다. 현재는 접속을 위해 TCP 22번 포트만 열어둔 상태로 생성하겠습니다. 검사에서는 현재까지의 설정을 확인할 수 있습니다. 인스턴스 시작을 누르면 키 페어를 선택창이 등장합니다. 여기서는 키 페어를 하나 생성하도록 하겠습니다. 리눅스 운영체제를 통할 때에는 pem 파일을, Window를 사용할 때에는 ppk 파일을 사용하기에, 여기서는 pem 파일을 Putty key generator를 이용해 변경시켰습니다. 생성이 완료되면, 인스턴스로 돌아와 생성되어진 인스턴스를 확인합니다. 기본적으로 Pendig은 생성, Running은 실행가능, stop은 중지상태, shuttinf-down은 삭제 중, Terminated는 삭제된 상태를 의미하며, Running 상태에서만 요금이 부과됩니다. 상단에는 간략한 EC2들의 정보를 나타내며, 하단에는 상세 정보를 나타냅니다. 여기서 접속을 위해 IPv4 퍼블릭 IP를 복사합니다. Putty를 사용해 퍼블릭 IP와 프라이빗 키를 등록하면 접속이 가능합니다. 기본적으로 기본 계정은 AWS Linux : EC2-user, Ubuntu : ubuntu Centos : centos 입니다. 생성된 인스턴스에서 정상적으로 핑이 나가는 것을 확인하실 수 있으나, Window에서는 핑이 가지 않는 것을 확인하실 수 있습니다. 이는 전에 선택한 보안그룹으로 22번/TCP 포트밖에 사용하지 않았기 때문으로, 만약 보안그룹에 ICMP를 열어둔다면, Ping이 가능하게 할 수 있습니다. 이와 같이 보안그룹은 AWS에서 다방면으로 매우 중요한 역할을 수행합니다. 이어서 인스턴스의 상태변경은 인스턴스를 선택후 작업, 혹은 오른쪽마우스로 가능합니다. 저는 삭제를 위해 종료를 클릭하겠습니다. 삭제가 완료되었습니다. 사용자 정의 VPC 대역에 EC2 생성 저번 장에서 생성했던 VPC 대역에 EC2를 생성해보도록 하겠습니다. 인스턴스의 Network 설정까지는 동일하며, 그 후는 아래와 같습니다. 인스턴스의 Network 설정에서 네트워크, 서브넷, 퍼블릭 IP 자동할당을 설정합니다. 단, 두 개의 인스턴스를 생성하며, 하나의 인스턴스는 IP 자동할당 활성화, 다른 인스턴스는 IP자동할당을 비활성화인 채로 생성합니다. 여기에서는 저번장에서 생성한 2개의 서브넷을 사용했습니다. Pulbic : 할당 활성화, Private : 할당 비활성화 또한 위와 같이 ssh의 접속이 가능하게 보안그룹을 설정합니다. 생성이 완료되었습니다. 이제 Putty를 통해 퍼블릭과 프라이빗에 접속합니다. 하지만, 프라이빗 대역은 퍼블릭 IP를 할당받지 않아 접속이 불가능합니다. 하지만 퍼블릭 서브넷의 인스턴스는 공통의 라우팅 테이블로 igw를 사용하기 때문에 프라이빗 IP를 알수 있어 접속이 가능합니다. 또한 이와 같은 방법으로 보안그룹을 ssh접속이 가능한 한 컴퓨터만 혹은 서브넷이나 그룹등을 지정하거나, gateway를 특정 인스턴스로 지정하여 보안성을 높이는 것이 가능합니다. CLI를 통한 인스턴스 관리 $ aws ec2 help aws ec2에 명령어를 알려줍니다. $ aws ec2 create-key-pair --key-name [ 키 페어 이름 ] --query 'KeyMaterial' --output text | out-file \u003e [ 키 페어 경로 ].pem # KeyMaterial은 키의 값입니다. $ impkey='cat~/[ import 시킬 키 파일의 경로 ]' $ aws ec2 import-key-pair --key-name [ 키 페어 이름 ] --public-key-material ${impkey} # 외부 키 페어 임포트 방법 # AWS 콘솔 EC2-Key Pairs -\u003e Import Key Pair로도 가능합니다. 키 페어를 생성합니다. $ aws ec2 delete-key-pair --key-name [ 키 페어 이름 ] 키 페어를 삭제합니다. $ aws ec2 create-security-group --group-name [ 보안 그룹 이름 ] --description [ 보안 그룹 설명 ] --vpcid [ VPC ID ] 보안 그룹을 생성합니다. $ aws ec2 authorize-security-group-ingress \\ --group-id [ Security-group-id ] \\ --protocol tcp \\ --port 22 \\ --cidr 0.0.0.0/0 # Security-group-id의 보안 그룹의 22/tcp의 모든 접속이 가능하게 설정을 추가합니다. $ aws ec2 describe-security-groups \\ --group-ids [ Security-group-id ] \\ --output json 보안 그룹을 생성합니다. $ aws ec2 describe-security-groups --group-ids [ 보안 그룹 ID ] 보안 그룹의 상세 설명을 출력합니다. $ aws ec2 create-security-group --group-name [ 보안 그룹 이름 ] --description [ 보안 그룹 설명 ] EC2를 생성할 때, 보안 그룹을 생성합니다. $ aws ec2 run-instances \\ --image-id [ 이미지 이름 ] \\ --count [ 인스턴스 수 ] \\ --instance-type [ falvor ] \\ --key [ 키 페어 이름 ] \\ --security-group-ids [ 보안 그룹 ID ] --subnet-id [ 서브넷 ID ] \\ --associate-public-ip-addres # 퍼블릭 IP 할당 유무 --user-data file://[ 파일경로 ] # 인스턴스를 생성합니다. $ aws ec2 create-tags \\ --resources [ 인스턴스 id ] --tags Key=name,Value=[ 태그 내용 ] # 인스턴스에 태그 등록 인스턴스를 생성합니다. #!/bin/bash apt install -y apache2 #cloud-config packages: - apache2 user data 사용시 사용가능한 형식 aws ec2 describe-instances --filters \"[ 필터 값 ], Value=[ 값1, 값2 ]\" 특정 ec2를 나열합니다. aws ec2 start-instances --instance-ids [ 인스턴스 ID ] aws ec2 stop-instances --instance-ids [ 인스턴스 ID ] aws ec2 terminate-instances --instance-ids [ 인스턴스 ID ] 인스턴스의 상태를 변경합니다. 예제 ","aws-ec2-생성#\u003cstrong\u003eAWS EC2 생성\u003c/strong\u003e":"","aws-ec2-생성-1#\u003cstrong\u003eAWS EC2 생성\u003c/strong\u003e":"","cli를-통한-인스턴스-관리#\u003cstrong\u003eCLI를 통한 인스턴스 관리\u003c/strong\u003e":"","ec2--elastic-compute-cloud--생성#\u003cstrong\u003eEC2 ( Elastic Compute Cloud ) 생성\u003c/strong\u003e":"","사용자-정의-vpc-대역에-ec2-생성#\u003cstrong\u003e사용자 정의 VPC 대역에 EC2 생성\u003c/strong\u003e":"","예제#\u003cstrong\u003e예제\u003c/strong\u003e":"AWS EC2 생성 AWS EC2 생성 "},"title":"AWS EC2 생성"},"/system/aws/awstraining/ec2site/":{"data":{"":"EC2 동적 사이트 구축 이번 장에서는 EC2와 WordPress, RDS를 활용해 동적 사이트를 구축해보겠습니다. 이 장에서는 RDS 복제본 사용시 과금이 청구될 수 있습니다. 이를 원치 않는 분들은, RDS 설정 시, Multi-AZ 설정을 하지 않고, 1개의 Master RDS만 생성 후 진행하세요. EC2 동적 사이트 구축 VPC VPC 이름 IPv4 CIDR VPC-WordPress 10.0.0.0/16 Subnet Subnet 이름 VPC AZ IPv4 CIDR WordPress-Public-Subnet VPC-WordPress ap-northeast-a 10.0.1.0/24 WordPress-Public-Subnet2 VPC-WordPress ap-northeast-c 10.0.2.0/24 RDS-Private-Subnet VPC-WordPress ap-northeast-a 10.0.11.0/24 RDS-Private-Subnet2 VPC-WordPress ap-northeast-c 10.0.12.0/24 Routing Table Routing Table 이름 VPC Subnet Public-rt VPC-WordPress WordPress-Public-Subnet, WordPress-Public-Subnet2 Private-rt VPC-WordPress RDS-Private-Subnet, RDS-Private-Subnet2 보안 그룹 보안 그룹 이름 VPC 인 바운드 규칙 아웃 바운드 규칙 WordPress-sg VPC-WordPress SSH : 22/TCP : 0.0.0.0/24, HTTP : 80/TCP : 0.0.0.0/24 모든 트래픽 : 0.0.0.0/0 RDS-sg VPC-WordPress WordPress-sg, 3306/TCP : WordPress-sg 모든 트래픽 : 0.0.0.0/0 먼저 위의 아키텍처와 표와 같이 VPC와 서브넷을 생성해주세요. 인터넷 게이트 생성 후, VPC에 연결하세요. 모든 라우팅 테이블의 게이트웨이는 인터넷 게이트웨이로 지정해주세요. VPC 사용자 정의 VPC 생성 EC2를 활용한 동적 사이트 구축 기본적인 설정을 끝마치셨다면, 이제 동적 사이트 아래의 순서에 맞춰 구현해보겠습니다. 1. RDS 생성\n2. 인스턴스 생성\n3. ELB 생성\n1. RDS 생성 먼저 RDS 서브넷의 생성을 위해, RDS \u003e 서브넷 그룹 \u003e DB 서브넷 그룹을 위와 동일하게 생성합니다. 아래의 형식에 맞춰 RDS를 생성합니다. RDS 설치 참고 기본설정 설정 항목 값 License Model genral-publicl-license DB Engine Version 5.7.28 DB Instance Class db.t2.micro Multi-AZ Deployment General Purpose ( SSD ) Allocated Storage 20 GB DB 인스턴스 식별자 WordPressDB 마스터 사용자 ID, PW root/qwer1234 네트워크 설정 설정항목 값 VPC VPC-WordPress Subnet Group rds-private Publicly Accessible no AZ ap-northeast-2a VPC Security Groups RDS-sg 백업 설정 설정항목 값 백업 보존 기간 1일 백업 기간 기본 설정 없음 유지 관리설정 설정항목 값 마이너 버전 자동 업그레이드 사용 유지 관리 기간 기본 설정 없음 삭제 방지 삭제 방지 활성화 X 개인 설정 및 이 이외 값을 기본 값을 유지합니다. 2. 인스턴스 생성 다음의 값으로 인스턴스를 생성합니다. 설정 항목 값 AMI Amazon Linux AMI Instance Type t2.micro Network VPC-WordPress Subnet WordPress-Public Auto-asstign Public Enable Name WordPress-a Security Group Wordpress-sg 생성이 완료되면 아래의 미들웨어들을 설치합니다. 유저 이름은 ec2-user입니다. $ sudo yum install -y php php-mysql php-gd php-mbstring # 관련 미들웨어를 설치합니다. $ sudo yum install -y mysql # mysql을 설치합니다. $ wget -O /tmp/wordpress-4.1-ja.tar.gz https://ko.wordpress.org/wordpress-4.6.1-ko_KR.tar.gz # wordpress-4.6.1...의 파일을 wordpress-4.1-ja.tar.gz의 이름으로 다운받습니다. $ sudo tar zxf /tmp/wordpress-4.1-ja.tar.gz -C /opt # wordpress 압축파일을 /opt에 압축을 해제합니다. $ sudo ln -s /opt/wordpress /var/www/html # 심볼릭 링크를 생성합니다. $ sudo chown -R apache:apache /opt/wordpress # wordpress의 소유 권한을 apache로 수정합니다. $ sudo chkconfig httpd on # httpd가 정상적으로 작동하는 지 체크합니다. $ sudo service httpd start $ httpd 서비스를 시작합니다. 설치가 완료되면 DB 접속을 위한 계정을 생성합니다. $ mysql -u root -p -h [ RDS Endpoint ] $ password : qwer1234 mysql\u003e create user 'wordpress-user'@'%' identified by 'wordpress'; mysql\u003e create database wordpress; mysql\u003e grant all privileges on wordpress.* to \"wordpress-user\"@\"%\"; mysql\u003e flush privileges; 계정 생성이 완료되었으면, http://인스턴스의 Public IP/wordpress/wp-admin/install.php로 접속합니다. 데이터베이스, 사용자명, 비밀번호에 위에서 생성한 값들을 입력 후, 데이터베이스의 호스트에는 RDS의 엔드포인트 값을 입력합니다. 설치를 실행합니다. 사이트의 제목과 관리자명 등을 설정합니다. 설정이 완료되면, 설정한 계정을 통해 로그인합니다. Wored Press의 설치가 완료되었습니다. 생성이 완료되면 AMI 이미지를 생성합니다. AMI 이미지를 통해 동일한 인스턴스를 다른 가용영역에 생성합니다. 3. ELB 생성 80/tcp 외부로 ALB를 생성해주세요. ELB 생성 참고 ELB의 DNS로의 접속이 확인되면, 대상그룹 또한 확인합니다. 다음으로는 WordPress의 접속 IP를 변경해보겠습니다. WordPress의 관리자로 접속하여 설정 -\u003e 워드프레스 주소, 사이트 주소를 변경합니다. 워드프레스 주소 : http://[ ALB DNS ]/wordpress 사이트 주소 : http://[ ALB DNS ] ALB에서 Desciption -\u003e Edit stickiness에서 로드밸런서의 쿠키 값을 사용하도록 설정 후, 시간은 1800초로 설정합니다. 이제 마지막으로 WordPress-sg의 80/tcp 포트의 대상을 0.0.0.0/0이 아닌, ALB를 대상으로 설정 및, SSH 접속을 해제하면 모든 설정이 완료됩니다. Marketplace를 사용 위 처럼 직접 구현하는 방법 외에도 이미 구현되어 있는 AMI를 구입하여 사용하는 방법도있습니다. 위 그림과 같이 Markplace에서 구입이 가능합니다. Marketplace로 구현을 할 경우, 이미 구축되어진 인프라를 사용하는 만큼, 간편하고 빠르게 사용할 수 있지만, 세부적인 사항에 대해서는 설정이 어렵다는 단점이 있습니다. ","#":"","ec2-동적-사이트-구축#\u003cstrong\u003eEC2 동적 사이트 구축\u003c/strong\u003e":"","ec2-동적-사이트-구축-1#\u003cstrong\u003eEC2 동적 사이트 구축\u003c/strong\u003e":"","ec2를-활용한-동적-사이트-구축#\u003cstrong\u003eEC2를 활용한 동적 사이트 구축\u003c/strong\u003e":"","marketplace를-사용#\u003cstrong\u003eMarketplace를 사용\u003c/strong\u003e":""},"title":"EC2 동적 사이트 구축"},"/system/aws/awstraining/eip/":{"data":{"":"AWS Elastic IP 할당 AWS Elastic IP ( 이하 EIP )란 인스턴스의 IP를 고정적으로 할당시킨 IP를 뜻합니다. 만약 인스턴스를 생성할 시, 퍼블릭 IP를 활성화 하면, 인스턴스를 자동 실행시마다 유동적으로 IP가 변화하여 문제가 되는 데, 이러한 문제들을 해결할 수 있습니다. AWS Elastic IP 할당 EIP를 생성하기 위해 메뉴에서 EC2 서비스에서 네트워크 및 보안 -\u003e 탄력적 IP를 선택합니다. 탄력적 IP 주소 할당을 클릭합니다. Amazon의 IPv4 주소 풀로 할당 받습니다. EIP의 생성이 완료되면, 할당을 위해 Actions -\u003e EIP 주소 연결을 클릭합니다. EIP의 연결 대상을 인스턴스 혹은 네트워크 인터페이스로 설정하여 연결을 진행합니다. 사실상, 인스턴스를 체크하여도 선택된 인스턴스의 네트워크 인터페이스에 EIP를 할당 하는 것입니다. EIP를 할당한 인스턴스를 선택하면 퍼블릭 IP주소가 탄력적 IP로 바뀐 것을 확인할 수 있습니다. EIP를 삭제하기 위해서는 EIP에 연결된 인터페이스가 없어야 하며, 삭제를 위해서는 EIP 주소 릴리스를 선택해줍니다. 프리티어에서도 EIP 한개의 사용이 무료이지만, 할당하고있는 EIP만 무료이며, 만약 할당받지 않은 채로 유지되면 과금이 부과되어 주의가 필요합니다. AWS CLI로 EIP 할당 $ aws ec2 allocate-address EIP를 할당 받습니다. $ aws ec2 associate-address \\ --instance-id [ 인스턴스 ID ] \\ --allocation-id [ EIP ID ] 인스턴스 ID를 가진 인스턴스에 EIP ID를 가진 EIP를 할당합니다. ","aws-cli로-eip-할당#\u003cstrong\u003eAWS CLI로 EIP 할당\u003c/strong\u003e":"","aws-elastic-ip-할당#\u003cstrong\u003eAWS Elastic IP 할당\u003c/strong\u003e":"","aws-elastic-ip-할당-1#\u003cstrong\u003eAWS Elastic IP 할당\u003c/strong\u003e":""},"title":"AWS Elastic IP 할당"},"/system/aws/awstraining/elasticbeanstalk/":{"data":{"":"Elastic Beanstalk 사이트 구축 이번 장에서는 Elastic Beanstalk를 활용해서 WordPress 사이트를 구축해보겠습니다. Elastic Beanstalk가 무엇인지는 Elastic Beanstalk를 참조해주세요. Elastic Beanstalk 사이트 구축 Elastic Beanstalk는 zip 형식으로 애플리케이션을 압축해서 AWS 상에 업로드 할 수 있습니다. WordPress를 사용하기 위해 WordPress에서 zip 형식으로 다운로드 합니다. 다운로드가 완료되면 AWS에서 Elastic Beanstalk를 검색합니다. Elastic Beanstalk의 생성을 위해 Create Application을 클릭합니다. 애플리케이션의 이름과 태그를 설정합니다. 플랫폼에서는 사용할 플랫폼을 설정할 수 있습니다. 여기서는 PHP를 선택합니다. 애플리케이션 코드에서는 코드 업로드를 클릭합니다. 소스 코드는 위에어 다운로드 한 WordPress.zip 파일을 업로드 합니다. 업로드가 완료되면 추가 옵션 구성을 클릭하여 세부설정으로 진입합니다. ElasticBeanstalk의 구성을 위해 사용자 지정을 클릭 후 아래항목으로 이동합니다. 먼저 최하단으로 진입하여 데이터베이스 설정을 진행으르 진행 후, 네트워크 설정을 진행합니다. 네트워크 및 데이터베이스에 대한 설정은 [EC2 동적 사이트 구축) (https://mung0001.github.io/docs/cloudcomputing/awstraining/ec2site/)의 VPC 및 보안그룹을 사용하였습니다. 설정이 완료되면 다시 위로 올라와 인스턴스의 보안그룹과 키 페어를 등록합니다. 설정이 완료되면 앱 생성을 클릭하여 ElasticBeanstalk를 생성합니다. 위의 그림과 설치가 완료되면 EC2, RDS등이 설치된 것을 확인 할 수 있습니다. 다음으로는 Elastic Beanstalk의 URL를 통해 http://[ 생성한 애플리케이션 URL ]/wordpress로 진입합다. WordPress가 설치된 것을 확인할 수 있습니다. 위의 그림과 같이 설정을 진행합니다. ElasticBeanstalk에 의해 생성된 db의 이름은 기본적으로 ebdb로 생성되어 있습니다. 데이터베이스의 호스트는 생성된 RDS의 EndPorint를 설정합니다. 다음의 웹 사이트 이름, 관리자의 대한 추가 설정을 마치면 WordPress의 생성이 완료되었습니다. 이와 같이 ElasticBeanstalk를 사용하면 AWS의 다양한 서비스와 PIP 뿐만이 아닌, 다양한 패키지들을 간단하게 생성이 가능합니다. Elastic Beanstalk의 eb ( awsebcli ) 활용 eb 명령어는 Elastic Beanstalk 전용 CLI로, AWS CLI와 별도로 설치가 필요합니다. $ pip install awsebcli # awsebcli 설치 $ eb --version $ awsebcli 설치확인 awsebcli를 설치합니다. $ cd /[ WordPress 압축 푼 파일 경로 ] $ eb init -p php # php 플랫폼 지정 다운 받은 WordPress의 압축을 해제하고, 해당 디렉토리를 플랫폼으로 지정합니다. $ eb create [ RDS 이름 ] --database --timoute 30 # eb 애플리케이션에 사용할 RDS 생성 RDS를 생성합니다. define('DB_NAME', $_SERVER['RDS_DB_NAME']); define('DB_USER', $_SERVER['RDS_USERNAME']); define('DB_PASSWORD', $_SERVER['RDS_PASSWORD']); define('DB_HOST', $_SERVER['RDS_HOSTNAME']); define('FORCE_SSL_LOGIN', true); define('FORCE_SSL_ADMIN', true); 압축을 해제한 WordPress 디렉토리 내의 wp-config-ample.php를 복사해서 wp-config.php를 생성 후, wp-config파일을 수정합니다. 위와 동일하게 wp-includes/functions.php 또한 수정합니다. ","elastic-beanstalk-사이트-구축#\u003cstrong\u003eElastic Beanstalk 사이트 구축\u003c/strong\u003e":"","elastic-beanstalk-사이트-구축-1#\u003cstrong\u003eElastic Beanstalk 사이트 구축\u003c/strong\u003e":"","elastic-beanstalk의-eb--awsebcli--활용#\u003cstrong\u003eElastic Beanstalk의 eb ( awsebcli ) 활용\u003c/strong\u003e":""},"title":"Elastic Beanstalk 사이트 구축"},"/system/aws/awstraining/elb/":{"data":{"":"AWS ELB 생성 이번 장에서는 생성된 인스턴스들을 로드밸런싱하는 방법에 대해 알아보도록 하겠습니다. ELB 또한 중요한 개념이니, ELB에 대한 학습을 원하는 분들은 AWS ELB를 참고해주세요. AWS ELB 생성 ELB에 대한 생성 순서은 아래의 순서대로 진행합니다. 1. 인스턴스 생성\n2. 대상그룹 생성\n3. 로드 밸런서 생성\n인스턴스 생성 먼저 기본 VPC에 가용영역 a와 c에 한 대씩, 총 두 대의 인스턴스를 생성해주세요. 보안 그룹은 80은 모두에게, 8009는 서로간만 통신이 가능하게 설정해주세요. 그 후, a,c 인스턴스에 Apach와 Tomcat을 설치 및 연동시켜주세요. $ apt-get -y update $ apt-get -y upgrade $ apt-get install -y apache2 # apache2 설치 $ systemctl enable apache2 $ ufw allow 80/tcp # apache2 자동시작 및 방화벽 허용 등록 $ apt-get install -y libapache2-mod-jk # 연동 모듈 $ vi /etc/apache2/workers.properties workers.tomcat_home=/usr/share/tomcat8 workers.java_home=/usr/lib/jvm/java-8-openjdk-amd64 worker.list=tomcat8 worker.tomcat8.port = 8009 worker.tomcat8.host = [ 서로 다른 인스턴스 IP ] worker.tomcat8.type = ajp13 worker.tomcat8.lbfactor = 1 # 워커 파일 생성 $ vi /etc/apache2/mods-available/jk.conf JkWorkersFile /etc/libapache2-mod-jk/workers.properties --\u003e JkWorkersFile /etc/apache2/workers.properties $ vi /etc/apache2/sites-available/000-default.conf DocumentRoot /var/www/html --\u003e DocumentRoot /var/lib/tomcat8/webapps/ROOT SetEnvIF Request_URI \"/*.html\" no-jk JkMount /*.jsp tomcat8 # jsp 파일만 tomcat에서 실행 $ vi /var/www/html/index.html 각 인스턴스에 따라 apache1 and apache2를 입력합니다. $ systemctl restart apache2 Apache 설치 $ apt -y update $ apt -y upgrade $ apt-get install lrzsz # JAVA 간편 다운로드를 위한 Irzsz 설치 $ apt-get install -y openjdk-8-jre $ apt-get install -y openjdk-8-jdk # JAVA 설치 $ which javac $ readlink -f /usr/bin/javac # 자바 위치 확인 $ vi /etc/profile export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export PATH=$JAVA_HOME/bin/:$PATH export CLASS_PATH=$JAVA_HOME/lib/:$CLASS_PATH $ source /etc/profile # 환경변수 설정 $ echo $JAVA_HOME $ $JAVA_HOME/bin/javac -version # 확인 $ apt-get install tomcat8 -y # tomcat8 설치 $ /usr/share/tomcat8/bin/version.sh # tomcat 설치 확인 $ ufw allow 8080/tcp $ ufw allow 8009/tcp # 방화벽 포트 열기 $ systemctl enable tomcat8 # tomcat 자동시작 $ apt-get install -y libapache2-mod-jk # 연동 모듈 설치 $ vi /etc/tomcat8/server.xml \u003cConnector port=\"8009\" protocol=\"AJP/1.3\" redirectPort=\"8443\" /\u003e 주석 헤제 $ systemctl restart tomcat8 $ vi /var/lib/tomcat8/webapps/ROOT/index.jsp \u003c%@ page language=\"java\" contentType=\"text/html; charset=utf-8\"%\u003e \u003c!-- 로컬 정보 --\u003e Local IP : \u003c%= request.getRemoteAddr() %\u003e\u003cbr\u003e Local Host : \u003c%= request.getRemoteHost() %\u003e\u003cbr\u003e \u003c!-- 서버의 기본 경로 --\u003e Context : \u003c%= request.getContextPath() %\u003e \u003cbr\u003e URL : \u003c%= request.getRequestURL() %\u003e \u003cbr\u003e URI : \u003c%= request.getRequestURI() %\u003e \u003cbr\u003e Path : \u003c%= request.getServletPath() %\u003e\u003cbr\u003e Server Port : \u003c%= request.getServerPort() %\u003e\u003cbr\u003e\u003cbr\u003e 서버 Root 경로 : \u003c%= application.getRealPath(\"/\") %\u003e\u003cbr\u003e 서버 Root 경로 : \u003c%= request.getRealPath(\"/\") %\u003e\u003cbr\u003e \u003c% String strServerIP = request.getServerName(); // 서버 ip String strServerPort = Integer.toString(request.getServerPort()); // 서버 port String serverRootUrl = \"http://\"+ strServerIP +\":\"+ strServerPort +\"/\"; // Root 경로 out.println(serverRootUrl ); %\u003e Tomcat 설치 Application Load Balancer 생성 ( 이하 ALB ) ALB에 대한 설명은 ALB Link를 참조해주세요. ALB를 생성하기 위해 메뉴에서 로드 밸런서 -\u003e 로드밸런서 생성을 클릭합니다. 로드 밸런서의 유형 중 ALB를 선택합니다. 위의 그림과 같이 ALB의 구성에 대한 설정을 진행합니다. 체계의 인터넷 연결은 외부대역과의 통신을 위한 설정이고, 내부는 서브넷끼리의 통신을 위한 설정입니다. 리스너는 로드 밸런서에서 읽은 포트를 설정합니다. 가용 영역은 로드 밸런서가 활성화될 가용 영역을 지정합니다. 보안그룹은 외부와의 통신을 위해 80/tcp를 모두에게 개방하게 설정합니다. 위의 그림과 같이 라우팅 구성에 대한 설정을 진행합니다. 대상 유형은 라우팅의 대상이 될 서비스를 지정하는 설정입니다. 프로토콜과 포트는 대상 유형의 라우팅을 지정하는 설정입니다. 상태검사는 경로로 접속하였을 때, 접속이 가능하면 Health, 불가능하면 Unhealth로 나타냅니다. 설정이 끝나면, 대상 등록에 인스턴스를 등록합니다. ELB의 생성이 완료되면, DNS 접속을 통해 확인할 수 있습니다. 또한 대상 그룹으로 이동하여 healthy 상태를 체크할 수 있습니다. Network Load Balancer 생성 ( 이하 NLB ) NLB에 대한 설명은 NLB Link를 참조해주세요. NLB 생성을 위해 다시 로드밸런서로 돌아와, 로드 밸런서 생성을 클릭합니다. 로드 밸런서 유형에서 NLB를 선택합니다. NLB의 구성을 위와 같이 설정합니다. 8009 포트는 톰캣과 아파치이 연동을 위한 포트 입니다. 라우팅 테이블을 구성합니다. 프라이빗 주소의 8009 포트로 인스턴스들을 등록합니다. $ vi /etc/apache/workers.properties worker.tomcat8.host = [ NLB DNS ] Apache의 워커 파일은 NLB의 DNS로 설정합니다. 설정해 두었던 ALB로 접속하여 index.jsp로 접속하면 톰캣을 통해 jsp로 접속하는 것을 확인 할 수 있습니다. 대상그룹 또한 healthy를 확인할 수 있습니다. Classic Load Balancer 생성 ( 이하 CLB ) CLB에 대한 설명은 CLB Link를 참조해주세요. CLB를 생성하기 위해 다시 로드 밸런서 생성을 클릭하세요. 로드 밸런서 유형 중 CLB를 클릭하세요. ELB와 동일하게 외부대역으로 설정합니다.. 보안그룹 또한 기존 ELB-sg를 사용합니다. 상태검사를 설정합니다. #\n인스턴스를 추가하고, 로드 밸런싱을 활성화합니다. #\nCLB의 DNS로 접속해보면, ELB와 같은 결과를 얻을 수 있습니다. #\nCLB또한 상태검사가 가능합니다. ","#":"","aws-elb-생성#\u003cstrong\u003eAWS ELB 생성\u003c/strong\u003e":"","aws-elb-생성-1#\u003cstrong\u003eAWS ELB 생성\u003c/strong\u003e":""},"title":"AWS ELB ( 2 Tier ) 생성"},"/system/aws/awstraining/game/":{"data":{"":"AWS 끄투온라인 서버 구축 AWS 끄투온라인 서버 구축 끄투 온라인은 오픈소스의 끝말잇기 게임입니다. EC2를 생성합니다. EC2 생성은 EC2 생성을 참조해주세요. OS 유형 disk security group Ubuntu18.04 t2.mini 8 all-open 인스턴스를 생성 후, 아래와 같이 진행합니다. $ sudo apt -y update $ sudo apt -y upgrade $ sudo apt -y install node.js $ sudo apt -y install npm $ npm install -g grunt grunt-cli $ sudo apt -y install postgresql $ sudo apt -y install git $ sudo git clone https://github.com/JJoriping/KKuTu.git # 서버 구축에 필요한 패키지들을 설치합니다. $ sudo su - postgres $ psql postgres=# ALTER USER postgres with encrypted password 'qwer1234'; postgres=# CREATE DATABASE main; postgres-# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+---------+---------+----------------------- main | postgres | UTF8 | C.UTF-8 | C.UTF-8 | postgres | postgres | UTF8 | C.UTF-8 | C.UTF-8 | template0 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C.UTF-8 | C.UTF-8 | =c/postgres + | | | | | postgres=CTc/postgres # 게임 데이터의 삽입을 위한 DB를 생성합니다. # 새로운 커널 하나를 다시 킨 후 $ cd KKuTu/Server/lib/sub/ $ mv global.inc.json global.json $ mv auth.inc.json auth.json $ vi global.json \"PASS\":\"...\", \u003e\"PASS\":\"qwer1234\", \"PG_PASSWORD\": \"...\",\u003e \"PG_PASSWORD\": \"qwer1234\", # DB에 접속하기 위한 패스워드를 수정합니다. $ cd ~/KKuTu $ sudo -u postgres psql --quiet main \u003c ./db.sql # DB를 삽입합니다. $ chmod +x server-setup.bat $ ./server-setup.bat $ node ./Server/lib/Game/cluster.js 0 1 $ ctrl + z $ bg $ disown -h $ node Server/lib/Web/cluster.js 1 $ ctrl + z $ bg $ disown -h $ netstat -anlp | grep :8496 $ netstat -anlp | grep :80 # 확인 IP ] 접속 ","aws-끄투온라인-서버-구축#\u003cstrong\u003eAWS 끄투온라인 서버 구축\u003c/strong\u003e":"","aws-끄투온라인-서버-구축-1#\u003cstrong\u003eAWS 끄투온라인 서버 구축\u003c/strong\u003e":""},"title":"EC2 끄투온라인 서버 구축"},"/system/aws/awstraining/iam/":{"data":{"":"AWS 사용자 계정 생성 AWS 사용자 계정 생성 이번 시간에는 AWS 계정생성에 이어 AWS 사용자 계정을 생성해보도록 하겠습니다. IAM이 무엇인지는 AWS IAM을 참고해주세요. 먼저 AWS에 로그인 후, IAM 서비스를 검색합니다. IAM 서비스에 진입하여, 메뉴에서 사용자를 클릭합니다. 사용자 추가를 선택합니다. 사용자의 이름을 기입하고, 엑세스 유형을 선택합니다. AccesskeyId와 SecreKey는 AWS CLI, API, SDK 등 기타 개발 도구의 사용되며, Login url, Password 콘솔창의 로그인시 사용됩니다. 유형 AccessKeyId SecreKey Login url Password 프로그래밍 방식 O O X X AWS Management Console Access 방식 X X O O 여기서는 프로그래밍 방식 및 AWS Console 방식을 모두 체크하겠습니다. 체크가 완료되면, 편하게 사용할 수 있도록, 직접 암호를 입력합니다. 다음으로 그룹에 사용자를 추가하기 위해 그룹을 생성합니다. 그룹에는 AdminstratorAccess ( 관리자 권한 ) 역할을 추가합니다. 이 역할에 대해서는 끝에서 다시 한번 다루겠습니다. 설정이 완료되면, 다음으로 진행하고, 마지막으로 사용자가 추가됨을 확인할 수 있습니다. 전에 선택한 엑세스 유형에 맞춰 csv 파일의 항목이 다르며, 두 가지를 전부 선택한 저는 엑세스 및 시크릿 키와 url이 전부 포함되어 있습니다. 여기서 다운받는 csv파일은 후에, 같은 값으로는 다운 받을 수 없으니, 삭제되지 않도록 잘 저장해야합니다. url로 접속 후, 설정한 ID와 비밀번호를 입력하면 해당 User로 접속이 완료됩니다. 이를 통해 특정 유저에게 특정권한만을 주어, 해킹 및 실수 등을 예방 및 관리가 가능합니다. IAM 정책 커스터마이징 IAM 계정을 생성하며, IAM 정책을 통해 생성그룹에 권한을 부여 했습니다. 그렇다면 IAM 정책을 커스터마이징할 수는 없을까요? 먼저, 정책은 json 파일 형식으로 되어 있으며, 이를 직접 만들기에는 까다로울 수 있지만, 있는 것을 복사한 뒤 수정하는 것은 그렇게 어렵지는 않습니다. ( 또한 최근에는 개념만 충분히 숙지하고 계시면 콘솔 창에서 클릭만으로도 가능합니다… ) 이번에 이를 확인하여 보겠습니다. 먼저 IAM 서비스의 메뉴에서 정책을 선택합니다. 그 후, User 계정을 생성할 때 사용했던 AdminstratorAccess를 선택합니다. 그 후, 권한에서 { }json을 클릭 후 해당 내용들을 확인합니다. 옵션 Version Statment Effect Action Resource 의미 파일의 버전 파일 정책의 내용 허가 또는 거부 ( Allow or Deny ) 설정 대상 서비스와 대상 조작을 작성 설정 대상 리소스를 작성 즉, 여기에서는 “*” = 모든 대상 서비스와 대상에 대한 조작은 모두 허가한다는 것입니다. 이 의외에도 특정 IP에 대한 권한만을 주거나 할 수 있으며, 보다 자세한 사항은 IAM 정책 LINK을 참조하시길 바랍니다. 그럼 이번에는 직접 정책을 생성해보도록 하겠습니다. 다시 정책으로 돌아와, 상단의 정책생성을 클릭합니다. 상단을 보면, 시각적 편집기와 JSON 파일 형식을 확인할 수 있습니다. 여기서는 동일하게 S3에 대한 모든 권한을 가질 수 있는 정책을 생성해보도록 하겠습니다. 다음으로는 이름과 설명을 기입 후, 정책을 생성합니다. 정책이 생성되면, 정책 필터를 통해 확인이 가능합니다. 생성된 정책에 진입하면, 작성했던 Json파일의 내용을 확인할 수 있습니다. 다음으로는 다시 정책생성으로 돌아와 콘솔창을 통해 생성해 보도록 하겠습니다. 서비스 : S3\n작업 : 모든 S3 작업\n리소스 : 모든 리소스\n요청 조건 : 선택 안함\n다시 이름과 설명을 기입 후, 정책을 생성합니다. 정책 필터를 통해 S3-User로 진입합니다. Json형식으로 확인해보면, Sid를 제외한 값이 모두 동일함을 확인할 수 있습니다. 이를 통해, 콘솔이나 json파일을 통해 동일한 값으로 생성할 수 있음을 알 수 있었습니다. 이에 대한 확인을 IAM을 통해 새로운 계정을 생성 후, 생성한 권한을 주어, S3에 진입 후, bucket을 생성하거나, 혹은 생성해 둔 bucket을 통해 알 수 있습니다. ( S3 권한만을 주었기 때문에, 다른 서비스에 대한 이용은 불가능합니다. ) ","aws-사용자-계정-생성#\u003cstrong\u003eAWS 사용자 계정 생성\u003c/strong\u003e":"","aws-사용자-계정-생성-1#\u003cstrong\u003eAWS 사용자 계정 생성\u003c/strong\u003e":"","iam-정책-커스터마이징#\u003cstrong\u003eIAM 정책 커스터마이징\u003c/strong\u003e":""},"title":"AWS 사용자 계정 생성"},"/system/aws/awstraining/noserver/":{"data":{"":"AWS 서버리스 사이트 구축 이번 장에서는 S3를 통해 서버가 없는 정적인 사이트를 구현해보도록 하겠습니다. 이와 같이 서버리스의 가장 큰 특징은 EC2처럼 상시 실행 상태 중이 아니여도, 사용자가 요청시에만 실행이 가능하기 때문에 비용면과 운영면에서 효율적이라 할 수 있습니다. AWS에서는 S3에서 웹 호스팅 기능을 제공하고 있어, 이를 통해 구현해보도록 하겠습니다. AWS 서버리스 사이트 구축 먼저, AWS에 접속하여 S3 서비스를 검색 후, 클릭합니다. S3를 시작하기 위해 버킷을 생성합니다. 버킷의 이름을 지정하고, 리전을 선택합니다. 기본 값으로 설정을 진행합니다. 단, 그림과 같이 퍼블릭 엑세스의 대한 차단을 해제합니다. 버킷의 생성이 완료되었습니다. 다음으로는 생성된 버킷을 호스팅 등록하기 전에, 버킷의 정책을 먼저 생성하겠습니다. 버킷의 생성이 완료되면, 생성된 버킷을 클릭합니다. 생성된 버킷에서 권한 -\u003e 버킷 정책을 클릭 후, 하단의 정책 생성기를 클릭합니다. 그림은 정책생성기로, 원하는 정책옵션을 선택하면 그 옵션을 Json파일로 변환시켜주는 역할을 수행합니다. 여기서는 아래의 값으로 설정을 진행합니다. Select Type : S3 Bucket Policy\nPrincipal : \" * “ ( Principal는 리소스로의 접근을 허가 또는 거부할 사용자, 계정, 서비스, 엔티티를 나타냅니다.) Actions : GetObject ( Actions는 허가할 조작을 나타냅니다.)\nARN : arn:aws:s3::: [ 버킷 이름 ]/[ Key_name ] ( 허용할 파일 혹은 디렉토리를 나타냅니다. 여기서 /Key_name은 /*을 사용합니다. )\n생성 후, Add Statement를 클릭하면 현재 선택한 옵션들은 Json 형식으로 바꾸어 줍니다. { \"Id\": \"Policy1593408879908\", \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1593408870453\", \"Action\": [ \"s3:GetBucketObjectLockConfiguration\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::s3-serverless-web/*\", \"Principal\": \"*\" } ] } 다음은 미리 index.html과 error.html 파일을 업로드 하겠습니다. 위의 그림과 같이 파일을 업로드 합니다. 모든 설정은 기본 값으로 설정합니다. 이제, 호스팅을 위해 S3에서 정적 웹 사이트 호스팅을 설정하겠습니다. S3의 속성 -\u003e 정적 웹 사이트 호스팅을 선택합니다. 정적 웹 사이트 호스팅 창이 나오면 인덱스 문서 및 오류 문서의 업로드한 파일을 기입 후 저장합니다. 이제 S3 EndPoint로 접속하면 index.html을 확인할 수 있습니다. 또한 에러 발생시에는 error.html이 보여지는 것을 확인할 수 있습니다. 이제 이것으로 기본적인 S3를 사용한 정적사이트 구축이 완료되었습니다. 이어서 서비스와 기능을 추가시켜보도록 하겠습니다. AWS 서버리스 사이트 세부설정 Redirection Rules Redirection Rules란 특정 경로 또는 HTTP 오류 코드 등의 조건에 따라 라우팅을 지정해주는 기능입니다. \u003cRoutingRules\u003e \u003cRoutingRule\u003e \u003cCondition\u003e \u003cKeyPrefixEquals\u003ehello/\u003c/KeyPrefixEquals\u003e \u003c/Condition\u003e \u003cRedirect\u003e \u003cReplaceKeyPrefixWith\u003ebye/\u003c/ReplaceKeyPrefixWith\u003e \u003c/Redirect\u003e \u003c/RoutingRule\u003e \u003c/RoutingRules\u003e # KeyPrefixEquals로 진입한 트래픽을 ReplaceKeyPrefixWith로 진입시킵니다. Redirection Rules의 설정을 위해 다시 정적 웹 사이트 호스팅 설정을 클릭합니다. 후 위의 값을 리디렉션 규칙에 작성합니다. 위 설정을 마치면 Endpoint에 hello로 진입시 bye로 진입되는 것을 확인 할 수 있습니다. \u003cRoutingRules\u003e \u003cRoutingRule\u003e \u003cCondition\u003e \u003cHttpErrorCodeReturnedEquals\u003e404\u003c/HttpErrorCodeReturnedEquals\u003e \u003c/Condition\u003e \u003cRedirect\u003e \u003cReplaceKeyWith\u003eindex.html\u003c/ReplaceKeyWith\u003e \u003c/Redirect\u003e \u003c/RoutingRule\u003e \u003c/RoutingRules\u003e # HttpErrorCodeReturnedEquals는 특정 에러가 발생하면 에러를 보여주는 대신 ReplaceKeyWith 값을 보여줍니다. 이와 동일하게 위의 값을 다시 리디렉션 규칙에 작성합니다. 설정을 마치면, Endpoint/의 모든 Null 값이 index.html로 옮겨지는 것을 확인할 수 있습니다. 차후 Lmabda, DNS, CDN 서비스를 추가하여 업데이트 하겠습니다. DNS 설정 ","#":"","aws-서버리스-사이트-구축#\u003cstrong\u003eAWS 서버리스 사이트 구축\u003c/strong\u003e":"","aws-서버리스-사이트-구축-1#\u003cstrong\u003eAWS 서버리스 사이트 구축\u003c/strong\u003e":"","aws-서버리스-사이트-세부설정#\u003cstrong\u003eAWS 서버리스 사이트 세부설정\u003c/strong\u003e":""},"title":"AWS 서버리스 사이트 구축"},"/system/aws/awstraining/owncloud/":{"data":{"":"Nas-Owncloud 실습 Owncloud를 활용하여 Ec2 Nas 만들기 EC2 생성\n\u003e OS : Ubuntu 18.04 \u003e Flavor : t2.micro \u003e Storage : 100G ( 원하는 만큼, 차후에 EFS 등으로도 가능합니다. ) \u003e VPC : Custop \u003e 보안그룹 : Custop 인스턴스를 생성합니다. 먼저, Owncloud를 사용하기 위해서는 LAMP를 설치해야합니다. $ sudo apt install -y tasksel $ sudo tasksel install -y lamp-server # LAMP 간편 설치 $ sudo apt install -y apache2 # apache2 설치 $ sudo apt install -y mysql-server # mysql 설치 $ sudo apt install -y php7.2 $ sudo apt install -y libapache2-mod-php7.2 $ sudo apt install -y php-mysql # php 및 연동모듈 설치 $ apache2 -v $ mysql --version $ php -v # 확인 LAMP란? $ wget -nv https://download.owncloud.org/download/repositories/10.0/Ubuntu_18.04/Release.key -O Release.key $ apt-key add - \u003c Release.key $ echo 'deb http://download.owncloud.org/download/repositories/10.0/Ubuntu_18.04/ /' | sudo tee /etc/apt/sources.list.d/owncloud.list # Ubuntu의 기본패키지에는 Owncloud가 지정되어 있지 않음 Owncloud 저장소 지정 $ sudo apt -y update $ sudo apt -y upgrade $ sudo apt -y install php-bz2 php-curl php-gd php-imagick php-intl php-mbstring php-xml php-zip owncloud-files $ ls -l /var/www/owncloud/ # 확인 Owncloud 설치 $ mysql -u root -p $ mysql\u003e CREATE DATABASE [ DB 이름 ]; $ mysql\u003e GRANT ALL ON [ DB이름 ].* to '[ 계정 ]'@'localhost' IDENTIFIED BY '[ PW ]'; $ mysql\u003e FLUSH PRIVILEGES; $ mysql\u003e exit owncloud DB 및 원격접속 계정생성 $ sudo vi /etc/apache2/apache2.conf \u003cDirectory /var/www/owncloud\u003e Options FollowSymlinks AllowOverride All Require all granted \u003c/Directory\u003e apache2.conf에서 owncloud에 대한 접근 권한을 설정합니다. $ sudo vi /etc/apache2/sites-available/000-default.conf DocumentRoot /var/www/html \u003e DocumentRoot /var/www/owncloud apache2의 기본 경로를 수정합니다. $ sudo mkdir /data $ sudo chmod 0770 /data $ sudo chown www-data:www-data /data # owncloud 사용을 위한 권한 및 소유자 변경 저장의 사용할 폴더를 미리 만들어 둡니다. $ sudo systemctl restart apache2 $ sudo service apache2 restart apache2를 재시작합니다. http://IP를 통해 접속합니다. 알맞은 값을 기입 후 설치를 완료합니다. 설치가 완료되면, 루트계정을 통해 접속합니다. 설치가 완료되었습니다. Elastci IP 를 주어 고정시킬 수 있고, 방화벽, 보안그룹의 설정을 통해 특정 IP만을 접속하게 할 수 있습니다. 다음은 owncloud를 커스터마이징 해보도록 하겠습니다. Owncloud 설정 도메인 등록\n$ vi /var/www/owncloud/config/config.php array ( 0 =\u003e 'IP' 1 =\u003e 'Domain' ) 도메인 접근 허용 설정 ","nas-owncloud-실습#\u003cstrong\u003eNas-Owncloud 실습\u003c/strong\u003e":"","owncloud-설정#Owncloud 설정":"","owncloud를-활용하여-ec2-nas-만들기#\u003cstrong\u003eOwncloud를 활용하여 Ec2 Nas 만들기\u003c/strong\u003e":""},"title":"AWS OwnCloud"},"/system/aws/awstraining/rds/":{"data":{"":"AWS RDS 생성 AWS RDS는 우리가 흔히 아는 Database ( Oracle db, Mysql, MariaDB )와 동일한 역할을 수행하지만, 보다 편리하고 안전하게 관리가 가능합니다. AWS RDS는 중요한 개념이므로, RDS에 대한 개념이 학습이 필요한 들은 AWS RDS를 참고해주세요. AWS RDS 생성 먼저, RDS의 생성을 위해 AWS의 접속하여 RDS를 검색 후 클릭합니다. 데이터베이스 생성 -\u003e 데이터베이스 생성을 클릭합니다. 여러 DB와 옵션을 사용할 수 있지만, 여기에서는 프리 티어 내에서 사용할 수 있도록 성정하도록 하겠습니다. 프리 티어의 체크 및 MySQL을 선택합니다. RDS도 원리는 인스턴스에 DB가 설치된 것으로, CPU와 RAM이 존재합니다. DB 엔진 버전 : DB의 버전을 설정하는 옵션입니다. DB 인스턴스 클래스 : DB 인스턴스의 타입을 설정하는 옵션입니다. 다중 AZ 배포 : 서로 다른 가용영역에 배포하는 옵션 입니다. 스토리지 자동 조정 : DB의 용량이 할당된 용량을 초과하면, 자동적으로 스토리지의 량이 증가하게 할 수 있는 옵션입니다. DB 인스턴스 식별자 : RDS의 이름입니다. 마스터 사용자 이름 : RDS 접속 시 사용할 사용자입니다. 네트워크 및 보안 설정에서는 RDS가 생성될 VPC와 Subnet 및 퍼블릭 엑세스가 가능하게 할지 결정할 수 있습니다. 보안그룹은 기존 보안그룹을 사용해도 되지만, 여기서는 새로운 VPC 보안 그룹을 만들어 사용하겠습니다. RDS 내의 DB의 이름 및 포트, 파라미터 그룹 등을 설정합니다. RDS를 자동 백업 및 스냅샷에 대한 설정입니다. 읽기 복제본을 위해서는 설정이 되어있어야 합니다. 모니터링 서비스 및 발신 로그 유형을 선택합니다. 여기서는 선택하지 않습니다. RDS의 유지관리 및 삭제방지의 대한 설정입니다. RDS는 자동적으로 업데이트가 가능하고, 삭제 방지의 대한 설정이 가능합니다. RDS의 생성이 완료되면 RDS \u003e 데이터베이스에서 확인이 가능합니다. RDS에 접속을 위해 생성한 RDS를 클릭하여 연결\u0026보인 \u003e 보안그룹을 클릭하여 수정하겠습니다. 인 바운드 규칙을 그림과 같이 수정합니다. 혹은 접속한 동일 VPC의 서브넷의 IP대역으로 수정도 가능합니다. 이제 다시 RDS에 돌아와 파라미터 그룹을 생성 하겠습니다. 파라미터 그룹을 생성하는 그본 파라미터 그룹을 사용하면 한글 사용시 에러가 발생하기 때문입니다. 그림과 같이 파라미터 그룹 \u003e 파라미터 그룹 생성을 클릭합니다. 파라미터 그룹을 생성합니다. 파라미터 그룹을 수정하기 위해 생성한 파라미터 그룹을 클릭 후, 편집을 진행합니다. charcter을 검색 후, character-set-client-handshake, skip-character-set-client-handshake, validate_password_special_char_count를 제외한 모든 값을 utf8로 설정합니다. charcter과 동일하게 collation을 검색 후, collation_connection, collation_server의 값을 utf8_unicode-ci로 설정합니다. 파라미터 그룹이 생성되면, 다시 데이터베이스로 돌아와 수정을 클릭합니다. 데이터베이스 옵션에서 DB 파라미터 그룹을 생성한 파라미터 그룹으로 수정합니다. 즉시 적용을 선택합니다. RDS가 수정중임을 확인할 수 있습니다. 수정이 완료되면, 생성한 RDS를 클릭하여 연결\u0026보안에서 엔드포인트를 확인합니다. $ apt -y install mysql-client 이후 동일한 VPC 내에서 인스턴스를 하나 생성해 mysql-client를 설치 후 접속을 진행합니다. mysql -u [ 생성시의 마스터 이름 ] -p -h [ RDS의 엔드포인트 ]를 통해 접속을 진행합니다. RDS 생성시에 설정한 DB로 접속이 가능함을 확인할 수 있습니다. $ mysql\u003e show variables like 'c%'; # Variable 확인 $ mysql\u003e set session [ Variable_name ]=[ 변경 값 ] # Variable 변경 위 처럼 직접변경 또한 가능합니다. 이것으로 기본적인 RDS에 대한 생성을 마치겠습니다. ","aws-rds-생성#\u003cstrong\u003eAWS RDS 생성\u003c/strong\u003e":"","aws-rds-생성-1#\u003cstrong\u003eAWS RDS 생성\u003c/strong\u003e":""},"title":"AWS RDS 생성"},"/system/aws/awstraining/s3/":{"data":{"":"AWS S3 생성 이번 장에서는 S3를 생성해보도록 하겠습니다. S3 또한 중요한 개념이니, S3에 대한 학습을 원하는 분들은 AWS S3를 참조해주세요. AWS S3 생성 AWS 서비스에서 S3를 검색합니다. 버킷 생성을 클릭합니다. 버킷의 이름과 리전을 선택합니다. 참고로 S3는 VPC에 영향을 받지 않습니다. 옵션을 선택합니다. 여기서는 기본 값으로 생성을 진행합니다. S3에 대한 권한을 설정합니다. 기본적으로 차단되어 있는 것이 좋으며, 경우에 따라 설정 값을 변경합니다. 생성이 완료되면 버킷을 클릭합니다. 버킷을 클릭한 후, IMG 폴더를 생성합니다. IMG 폴더로 진입하여 jpg 이미지 파일을 업로드 합니다. 이미지 파일을 선택하면 다운로드 링크, URL 링크를 확인할 수 있습니다. 여기에서는 URL 링크로 진입하여 보겠습니다. 링크로 진입하여도, 그림이 나타나지 않습니다. 이는 초기 버킷을 생성할 때, 퍼블릭 엑세스를 차단하였기 때문입니다. 이에 대한 수정을 위해 버킷에서 퍼블렉 엑세스 설정을 편집을 클릭합니다. 퍼블릭 엑세스 차단을 해제 후 저장합니다. 다시 파일을 선택하여 퍼블릭 설정을 클릭합니다. URL로 접속하면 이미지가 나타납니다. CLI S3 생성 $ aws s3 help $ aws s3api help s3에 대한 명령어를 출력합니다. $ aws s3 mb s3://[ 버킷 이름 ] 버킷을 생성합니다. $ aws s3 ls $ aws s3 ls s3://[ 버킷 이름 ]/path 버킷 및 폴더를 나열합니다. $ aws s3 rb s3://[ 버킷 이름 ] $ aws s3 rb s3://[ 버킷 이름 ] --force 버킷을 삭제합니다. $ aws s3 cp file.txt s3://my-bucket/ --grants [ 권한 ] $ aws s3 sync [ local path ] s3://[ bucket path ]/[ path ] [ local path ]에서 [ bucket ]의 [ path ]에 모든 것을 Pull ( 다운로드 ) 합니다. $ aws s3 sync s3://[ bucket path ]/[ path ] [ local path ] [ local path ]에서 [ bucket ]의 [ path ]에 모든 것을 Push ( 업로드 ) 합니다. ","aws-s3-생성#\u003cstrong\u003eAWS S3 생성\u003c/strong\u003e":"","aws-s3-생성-1#\u003cstrong\u003eAWS S3 생성\u003c/strong\u003e":"","cli-s3-생성#\u003cstrong\u003eCLI S3 생성\u003c/strong\u003e":""},"title":"AWS S3 생성"},"/system/aws/awstraining/ses/":{"data":{"":"AWS SES 메일 시스템 구축 이번 장에서는 SES로 메일을 전송하는 시스템을 구축하여 보겠습니다. 단, SES 사용을 위해서는 버지나이 북부, 오레곤, 아일랜드만이 사용이 가능합니다. 메일 시스템 구축 순서\n1. Simple Email Service ( SES ) 사용\n2. EC2 인스턴스로 메일 서버를 구축\n3. 서드 파티 도구를 사용\nAWS SES 메일 시스템 구축 먼저, SES 서비스를 이용하기 위해 AWS에서 SES를 검색합니다. Email Addresses \u003e Verify a New Email Address를 클릭하여 인증을 진행합니다. 사용하실 메일주소를 입력 후, 인증을 진행합니다. 사용하실 메일로 접속하여 인증을 진행하면, 다음과 같이 verified 항목이 체크됩니다. 확인을 위해 등록하신 메일주소를 체크하고 상단의 Send a Test Email을 클릭합니다. 값을 입력하고 이메일을 발송합니다. 메일주소로 접속하면, 메일이 도착한 것을 확인할 수 있습니다. 좌측 메뉴의 Sending Statistics를 클릭하면 현재 메일 사용량과 제한을 알 수 있습니다. 또한 현재 그림에는 보이지 않지만 상단의 Request a Sending Limit Increase를 클릭하면 허용량을 증가시키는 것이 가능합니다. 단, 신청 시, 완료까지 평균적으로 1일의 시간이 소요됩니다. 메일함 완성 후에 업데이트 예정 ","aws-ses-메일-시스템-구축#\u003cstrong\u003eAWS SES 메일 시스템 구축\u003c/strong\u003e":"","aws-ses-메일-시스템-구축-1#\u003cstrong\u003eAWS SES 메일 시스템 구축\u003c/strong\u003e":""},"title":"AWS SES 메일 시스템 구축"},"/system/aws/awstraining/start/":{"data":{"":"AWS 시작히기 AWS 계정 생성 AWS 서비스를 이용하기 위한 계정을 생성하고, MFA를 사용하여 보안을 강화하는 방법에 대해 알아보도록 하겠습니다. -먼저 AWS을 통해 AWS에 접속합니다.\nAWS 계정 새로 만들기를 선택합니다. 다음 항목들을 기입 후, 계정 만들기를 선택합니다. 프로페셔널과 개인 중 맞는 항목을 선택 후, 아래 항목들을 기입합니다. 영어 주소를 모를시 Link를 참조하세요. 프로페셔널 : 조직, 기업의 사용\n개인 : 개인적으로 사용\n사용가능한 카드에 대한 정보를 입력합니다. 여기서 amazon에서 $1를 뺏어감니다…. 후 실습예제 중에서는, 최대한 프리 티어를 기준으로 사용하지만, 특정 서비스 사용시 과금이 발생할 수 있습니다. 각 항목에 알맞은 정보를 기입 후, 인증을 진행합니다. 인증 진행 후, 기본 플랜을 선택합니다. 가입이 완료되면 다시 초기화면으로 돌아와 이메일 주소와 암호를 입력 후 진행합니다. 다음으로는 서비스를 다루기 앞서, 보안을 위해 MFA를 등록하겠습니다. 메인 창에서 IAM을 입력 후, IAM에 진입합니다. IAM 진입이 완료되면, 중앙에 메인페이지에 보이는 루트 계정에서 MFA 활성화를 선택 후, MFA 관리를 클릭합니다. 멀티 팩터 인증 ( MFA )를 클릭 후, MFA 활성화를 클릭합니다. 혹시 다른 인증방법이 궁금하신 분들은 Link를 참조하세요. 가상 MFA 디바이스를 클릭 후, Authenticator를 구글 스토어 혹은 앱 스토어에서 다운로드 받습니다. 앱을 실행 시킨 후, QR 코드를 입력 후, MFA 코드를 2차례 입력합니다. 등록이 완료되면 다음과 같이 일련번호를 확인 할 수 있습니다. 계정을 로그아웃 후, 다시 로그인하면 다음과 같이 MFA코드를 입력창이 나옵니다. 설치한 Authenticator을 실행 후, MFA 값을 입력하면 성공적으로 로그인이 가능합니다. 다음으로는 IAM을 통한 사용자 계정생성에 대해 알아보도록 하겠습니다. ","aws-계정-생성#\u003cstrong\u003eAWS 계정 생성\u003c/strong\u003e":"","aws-시작히기#\u003cstrong\u003eAWS 시작히기\u003c/strong\u003e":""},"title":"AWS 시작하기"},"/system/aws/awstraining/vpc/":{"data":{"":"AWS 사용자 정의 VPC 생성 AWS 사용자 정의 VPC 생성 이제 본격적으로 AWS 서비스들에 대해서 다루어 보겠습니다. 그 중, AWS 서비스의 근간이 VPC를 생성해 보도록 하겠습니다. VPC 중요한 개념이므로, VPC에 대한 개념이 부족한 분들은은 AWS VPC를 참고해주세요. GUI 환경에서의 사용자 정의 VPC 생성 기본적인 VPC 생성의 순서\n1. VPC 네트워크 생성 2. Internet Gateway 설정\n2. Subnet 설정\n3. Route Table 설정\n5. Network ACL 설정\n6. Security Group 설정 여기에서는 ACL은 기본 값, Security Group에 대한 설정은 인스턴스를 생성할 때 설정하였습니다.\nAWS 서비스에서 VPC를 검색합니다. VPC 대시보드에서는 VPC서비스의 전체적인 서비스 상태를 확인할 수 있습니다. 좌측 메뉴에서 VPC 생성을 위해 VPC를 선택합니다. AWS 가입시 기본적으로 기본 VPC가 생성되며, Custom VPC를 생성하기 위해 VPC 생성을 클릭합니다. VPC 생성을 위해 VPC의 이름과 주소대역을 CIDR 형식으로 작성합니다. 생성이 완료되었습니다. 이제 인터넷에 연결하기 위해 인터넷 게이트웨이로 이동합니다. 인터넷 게이트 웨이를 생성합니다. 인터넷 게이트 웨이 생성 후, VPC를 연결합니다. 이제 서브넷 대역을 생성하기 위해 서브넷을 선택합니다. 현재 기본 VPC의 서브넷 대역이 3개가 존재합니다. 새로 생성한 Custom-VPC의 서브넷을 생성하기 위해 서브넷 생성을 클릭합니다. 저는 퍼블릭 대역 10.0.0.0/24와 프라이빗 10.0.10.0/24의 서브넷 대역을 생성해보겠습니다. 생성이 완료된 후, 라우팅 설정을 위해 라우팅 테이블을 클릭합니다. 퍼블릭, 프라이빗 라우팅 테이블을 생성합니다. 퍼블릭에서 하단에 서브넷 연결을 클릭 후, 서브넷 연결 편집에서 퍼블릭 서브넷을 등록시킵니다. 이와 동일하기 프라이빗 라우팅 테이블에 프라이빗 서브넷을 등록시킵니다. 서브넷 생성을 완료 후, 서브넷 연결의 좌측에 라우팅 편집을 클릭합니다. 퍼블릭과 프라이빗을 인터넷 게이트웨이에 연결시킵니다. 이것으로 GUI를 통한 VPC의 생성이 완료되었습니다. 다음 장에서 생성된 VPC대역에 인스턴스를 생성해보겠습니다. AWS CLI로 VPC 생성 이번에는 VPC를 CLI 환경을 통해 생성해보도록 하겠습니다. CLI환경 또한 동일한 순서로 생성을 진행하겠습니다. $ aws ec2 create-vpc --cidr-block 10.0.0.0/16 # 10.0.0.0/16의 CIDR을 가진 VPC를 생성합니다. $ aws ec2 modify-vpc-attribute --vpc-id [ VPC-ID ] --enable-dns-hostnames # [ VPC-ID ]를 가진 VPC에 DNS를 사용하도록 설정합니다. $ aws ec2 create-vpc --cidr-block 10.0.0.0/16 --instance-tenancy dedicated # 만약 vpc 네트워크의 Tenanacy를 Dedicated로 생성한다면 다음의 명령어를 통해 실행시킵니다. VPC를 생성 및 설정합니다. $ aws ec2 create-internet-gateway # 인터넷 게이트웨이를 생성합니다. $ aws ec2 attach-internet-gateway --internet-gateway-id [ igw ID ] --vpc-id [ VPC ID ] # VPC ID에 해당하는 VPC에 igw ID에 해당하는 인터넷 게이트웨이를 연결시킵니다. 인터넷 게이트웨이를 생성합니다. $ aws ec2 create-subnet --vpc-id [ VPC-ID ] --availability-zone ap-northeast-2a --cidr-block 10.0.0.0/24 # VPC-ID에 해당하는 VPC에 ap-northeast-2a에 가용영역에서 10.0.0.0/24에 subnet을 생성합니다. $ aws ec2 create-subnet --vpc-id [ VPC-ID ] --availability-zone ap-northeast-2c --cidr-block 10.0.10.0/24 # VPC-ID에 해당하는 VPC에 ap-northeast-2c에 가용영역에서 10.0.10.0/24에 subnet을 생성합니다. subnet을 생성 및 설정합니다. $ aws ec2 create-route-table --vpc-id [ VPC-ID ] # VPC-ID에 해당하는 VPC에 route table을 생성합니다. $ aws ec2 associate-route-table --route-table-id [ rtb-id ] --subnet-id [ subnet-id ] # rtb-id에 해당하는 route table에 subnet-id에 해당하는 subnet을 등록시킵니다. $ aws ec2 create-route --route-table-id [ rtb-id ] --destination-cidr-block 0.0.0.0/0 --gateway-id [ igw-id ] # rtb-id에 해당하는 route table에 모든 게이트웨이를 [ igw-id ]에 연결합니다. Route table을 생성 및 설정합니다. $ aws ec2 describes-vpcs --vpc-id [ VPC-id ] $ aws ec2 describes-subnets --subnet-id [ subnet-id ] $ aws ec2 describes-internet-gateway --inernet-gateway-id [ igw-id ] $ aws ec2 describes-route-tables --route-table-id [ rtb-id ] # 생성 및 설정 확인 생성 및 설정을 확인합니다. 다음 장에서는 이번에 생성한 VPC를 사용하여, EC2를 생성해보도록 하겠습니다. ","#":"","aws-cli로-vpc-생성#\u003cstrong\u003eAWS CLI로 VPC 생성\u003c/strong\u003e":"","aws-사용자-정의-vpc-생성#\u003cstrong\u003eAWS 사용자 정의 VPC 생성\u003c/strong\u003e":"","aws-사용자-정의-vpc-생성-1#\u003cstrong\u003eAWS 사용자 정의 VPC 생성\u003c/strong\u003e":""},"title":"AWS 사용자 정의 VPC 생성"},"/system/azure/azuretraining/":{"data":{"":"Azure Azure Training\nAzure\nAzure\nAzure\nAzure Docs\nAzure\nAzure\nAzure","azure#\u003cstrong\u003eAzure\u003c/strong\u003e":""},"title":"Azure Training"},"/system/azure/azuretraining/az-900/":{"data":{"":"Az-900 시험대비 문제풀이 다음 각 명령문에 대해 해당 명령문이 참이면 Yes 거짓이면 No를 선택하십시오. 문제 Yes No PaaS는 Azure 서비스에서 모든 시스템의 제공하고 호스트는 웹, 애플리케이션을 모두 컨트롤 한다. PaaS의 웹, 애플리케이션에서 자동으로 Scale ability를 수행한다. PaaS는 솔루션은 사용자 지정 응용 프로그램에 기능을 지속적으로 추가하는 전문 개발 서비스를 제공한다. 문제 Yes No PaaS는 Azure 서비스에서 모든 시스템의 제공하고 호스트는 웹, 애플리케이션을 모두 컨트롤 한다. ㅇ PaaS의 웹, 애플리케이션에서 자동으로 Scale ability를 수행한다. ㅇ PaaS는 솔루션은 사용자 지정 응용 프로그램에 기능을 지속적으로 추가하는 전문 개발 서비스를 제공한다. ㅇ PaaS는 Platform as a service로 모든 부분을 호스트가 사용하는 것은 IaaS이다.\nScale UP, OUT은 Azure에서 수행한다.\nAzure의 기술지원 솔루션에는 전문 개발 서비스를 제공한다.\n다음 각 명령문에 대해 해당 명령문이 참이면 Yes 거짓이면 No를 선택하십시오. 문제 Yes No Azure는 CapEx, OpEx 사이의 비용을 유연하게 사용한다. 만약 두 개의 VM을 B2S 크기로 생성하면 다른 VM은 매월 항상 같은 가격이 결제된다. 만약 VM이 정지되어도 스토리지 비용은 지불해야한다. 문제 Yes No Azure는 CapEx, OpEx 사이의 비용을 유연하게 사용한다. ㅇ 만약 두 개의 VM을 B2S 크기로 생성하면 다른 VM은 매월 항상 같은 가격이 결제된다. ㅇ 만약 VM이 정지되어도 스토리지 비용은 지불해야한다. ㅇ SaaS 솔루션을 구현할 때 고 가용성으로 구성해야 하는 경우 고려해야하는 사항은?\n변경할 필요 없다.\n확장 성 규칙 정의\nSaaS 솔루션 설치\nSaaS 솔루션 구성\n4 고 가용성을 구성하기 위해서는 솔루션을 구성해야한다.\n여러 서버가 포함 된 온-프레미스 네트워크가 있습니다. 모든 서버를 Azure로 마이그레이션 할 계획입니다. 단일 Azure 데이터 센터가 장기간 오프라인 상태가 되는 경우 일부 서버를 사용할 수 있도록 솔루션을 권장해야합니다. 다음 중 추천하는 것은? 내결함성\n탄력성\n확장성\n낮은 지연\n1\n끊끼지 않는 서비스와 연관된 것은 내결함성입니다.\n사설 클라우드에서 인프라를 호스팅하는 조직은 데이터 센터를 폐기할 수 있습니다. 위의 설명이 정확하면 변경할 필요 없음을 선택하고, 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요 없습니다.\n하이브리드 클라우드\n공용 클라우드\nHyper-V 호스트\n공용 클라우드 프라이빗 클라우드를 폐기하기 위해서는 공용(퍼블릭)클라우드를 100% 사용해야합니다.\n퍼블릭 클라우드의 두 가지 특징은 무엇입니까? 전용 하드웨어\n보안되지 않은 연결\n제한된 저장\n측정 가격\n셀프 서비스 관리\n4, 5\n1, 3번은 프라이빗 환경의 특징이며 2번은 초기 퍼블릭 환경의 문제점입니다.\n퍼블릭 클라우드는 사용한 만큼의 가격만 지불하고, 클라이언트가 직접 서비스를 관리할 수 있습니다.\n공용 웹 사이트를 Azure로 마이그레이션하려는 경우 월별 사용 비용을 지불하도록 계획해야합니다. 밑줄이 그어진 텍스트를 검토하십시오. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오 변경이 필요하지 않습니다.\nVPN 배포\n모든 웹 사이트 데이터를 Azure로 전송하기 위한 지불\n웹 사이트 연결 수 줄이기\n1\n마이그레이션 시, 추가적인 설정 없이 진행할 수 있습니다.\n회사에서 모든 데이터와 리소스를 Azure로 마이그레이션 할 계획입니다. 회사의 마이그레이션 계획에 따르면 PaaS (Platform as a Service) 솔루션 만 Azure에서 사용해야합니다. 계획된 마이그레이션을 지원하는 Azure 환경을 배포해야합니다.솔루션 : Azure App Service 및 Azure SQL 데이터베이스를 만듭니다. 이것이 목표를 충족합니까? Yes\nNo\n1\nAzure App Service와 Azure SQL은 PaaS입니다.\n회사에서 모든 데이터와 리소스를 Azure로 마이그레이션 할 계획입니다. 회사의 마이그레이션 계획에 따르면 PaaS (Platform as a Service) 솔루션 만 Azure에서 사용해야합니다. 계획된 마이그레이션을 지원하는 Azure 환경을 배포해야합니다. 솔루션 : Microsoft SQL Server가 설치된 Azure App Service 및 Azure 가상 머신을 만듭니다. 이것이 목표를 충족합니까? Yes\nNo\n2\nVM은 IaaS입니다.\n회사에서 모든 데이터와 리소스를 Azure로 마이그레이션 할 계획입니다. 회사의 마이그레이션 계획에 따르면 PaaS (Platform as a Service) 솔루션 만 Azure에서 사용해야합니다. 계획된 마이그레이션을 지원하는 Azure 환경을 배포해야합니다. 솔루션 : Azure App Service 및 Azure Storage 계정을 만듭니다. 이것이 목표를 충족합니까? Yes\nNo\n2\nAzure Storage는 IaaS입니다.\n회사는 회사의 모든 고객이 사용하는 App1이라는 계정을 호스팅합니다. App1은 매월 처음 3 주 동안 사용량이 적고 매월 마지막 주 동안 사용량이 매우 높습니다. 이러한 유형의 사용 패턴에 대한 비용 관리를 지원하는 Azure Cloud Services의 이점은 무엇인가요? 고 가용성\n높은 지연\n탄력성\n부하 분산\n3\n자원 리소스를 추가, 제거하는 특징은 탄력성입니다.\n웹 애플리케이션을 Azure로 마이그레이션 할 계획입니다. 웹 애플리케이션은 외부 사용자가 액세스합니다. 웹 애플리케이션을 관리하는 데 사용되는 관리 노력의 양을 최소화하려면 클라우드 배포 솔루션을 권장해야합니다. 추천에 무엇을 포함해야합니까? SaaS\nPaaS\nIaaS\nDaaS\n2\n호스트의 입장에서 가장 관리의 양이 적은 것은 SaaS이지만, 웹 애플리케이션이 포함되어 PaaS입니다.\nAzure 가상 머신 및 Azure SQL 데이터베이스에 어떤 클라우드 배포 솔루션이 사용 되나요? 답변하려면 답변 영역에서 적절한 옵션을 선택하십시오. VM은 IaaS, SQL은 PaaS입니다.\n100 개의 서버가 포함 된 온-프레미스 네트워크가 있습니다. 사용자에게 추가 리소스를 제공하는 솔루션을 권장해야합니다. 솔루션은 자본 및 운영 비용을 최소화해야합니다. 추천에 무엇을 포함해야합니까? 퍼블릭 클라우드로의 완전한 마이그레이션\n추가 데이터 센터\n사설 클라우드\n하이브리드 클라우드\n3\n이미 서버가 있으며, 이에 대한 비용최소화를 위해서는 프라이빗 클라우드를 구성해야합니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 온-프레미스 네트워크에서 Azure로 여러 서버를 마이그레이션 할 계획입니다. 서버에 공용 클라우드 서비스를 사용할 때의 주요 이점을 식별해야합니다. 무엇을 식별해야합니까? 퍼블릭 클라우드는 민간 기업이 아닌 공공 소유입니다.\n퍼블릭 클라우드는 기업에 클라우드를 향상시킬 수 있는 능력을 제공하는 크라우드 소싱 솔루션 입니다.\n모는 고용 클라우드 리소스는 모든 공용 구성원이 자유롭게 액세스 할 수 있습니다.\n퍼블릭 클라우드는 여러 기업이 각각 클라우드에서 리소스의 일부를 사용하는 공유 엔티티입니다.\nD\n퍼블릭 클라우드는 여러 기업이 한 기업의 클라우드 리소스를 사용합니다.\n여러 Azure 가상 머신을 배포 할 계획입니다. 단일 데이터 센터가 실패하는 경우 가상 머신에서 실행중인 서비스를 사용할 수 있는지 확인해야합니다. 솔루션 : 가상 머신을 둘 이상의 확장 집합에 배포합니다. 이것이 목표를 충족합니까? Yes\nNo\n2\n가용성 집합은 데이터 센터 내에서의 논리적 그룹입니다.\n여러 Azure 가상 머신을 배포 할 계획입니다. 단일 데이터 센터가 실패하는 경우 가상 머신에서 실행중인 서비스를 사용할 수 있는지 확인해야합니다. 솔루션 : 두 개 이상의 가용성 영역에 가상 머신을 배포합니다. 이것이 목표를 충족합니까? Yes\nNo\n1 가용성 영역은 각 데이터 센터이므로, 하나의 데이터 센터가 실패해도 정상적으로 작동합니다.\n여러 Azure 가상 머신을 배포 할 계획입니다. 단일 데이터 센터가 실패하는 경우 가상 머신에서 실행중인 서비스를 사용할 수 있는지 확인해야합니다. 솔루션 : 가상 머신을 둘 이상의 지역에 배포합니다. 이것이 목표를 충족합니까? Yes\nNo\n2\n지역은 각 지역에 따라 규정과 법이 다를 수 있습니다.\n데이터 센터의 Hyper-V 호스트에서 호스팅되는 가상 머신 1,000 개가 있습니다. 모든 가상 머신을 Azure 종량제 구독으로 마이그레이션 할 계획입니다. 계획된 Azure 솔루션에 사용할 비용 모델을 식별해야합니다. 어떤 지출 모델을 식별해야합니까? 운영\n탄성\n자본\n확장 가능\n1\n후불로 지불하는 것을 OpEx 또는 운영 모델이라 합니다.\n답변하려면 왼쪽 열에서 오른쪽 설명으로 적절한 혜택을 드래그하십시오. 각 혜택은 한 번, 두 번 이상 사용하거나 전혀 사용하지 않을 수 있습니다. 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 회사에 여러 서버가 포함 된 온 프레미스 네트워크가 있습니다. 회사는 네트워크 관리자의 다음과 같은 관리 책임을 줄일 계획입니다. ✑ 애플리케이션 데이터 백업 ✑ 실패한 서버 하드웨어 교체 ✑ 물리적 서버 보안 관리 ✑ 서버 운영 체제 업데이트 ✑ 공유 문서에 대한 권한 관리 회사는 여러 서버를 Azure 가상 머신으로 마이그레이션 할 계획입니다. 계획된 마이그레이션 후 감소 할 관리 책임을 식별해야합니다. 어떤 두 가지 책임을 식별해야합니까? 각 정답은 완전한 해결책을 제시합니다.\n실패한 서버 하드웨어 교체\n애플리케이션 데이터 백업\n물리적 서버 보안 관리\n서버 운영 체제 업데이트\n공유 문서에 대한 권한 관리\n1, 3\n서버 하드웨어와 물리적 서버 보안은 호스트가 설정할 수 없습니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 한 리전의 리소스 구독은 항상 공유되는 것은 아닙니다. 리소스 그룹의 태그를 적용시켰을 때, 모든 리소스 그룹이 태그가 적용되는 것은 아닙니다. 모든 리소스 권한을 주었어도, 비용 등의 몇몇의 권한은 줄 수 없습니다. 회사에서 Azure에 AI (인공 지능) 솔루션을 배포 할 계획입니다. 회사는 예측 분석 솔루션을 구축, 테스트 및 배포하기 위해 무엇을 사용해야합니까? Azure Logic Apps\nAzure Machine Learning Studio\nAzure batch\nAzure Cosmos DB\n2\n(Azure AI 참조)[https://mung0001.github.io/docs/azure/microsoftazure/azure07/]\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. Azure Advisor는 보안과는 연관이 있지만, Azure AD(Datalog서비스)와는 연관이 없습니다. Azure Advisor는 가용성, 비용, 보안성 등의 컨설팅을 지원합니다. Azure Advisor은 직접적으로 VM의 네트워크 세팅에는 관여하지 않습니다. 여러 지원 엔지니어가 다음 표에 표시된 컴퓨터를 사용하여 Azure를 관리 할 계획입니다. 각 컴퓨터에서 사용할 수있는 Azure 관리 도구를 식별해야합니다. 각 컴퓨터에 대해 무엇을 식별해야합니까? 답변하려면 답변 영역에서 적절한 옵션을 선택하십시오. 모든 리소스에는 CLI, Portal, PowerShell을 사용할 수 있습니다. Azure 정책은 클라우드 인프라에 개체를 배포하고 Azure 환경에서 일관성을 구현하기위한 공통 플랫폼을 제공합니다. 밑줄이 그어진 텍스트를 검토하십시오. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\nResource groups provide\nAzure Resource Manager provides\nManagement groups provide\n3\nAzure Policies Provide는 Resource Manager와 관련있습니다.\nAzure 서비스를 올바른 설명과 일치시킵니다. 대답하려면 왼쪽 열에서 오른쪽 설명으로 적절한 Azure 서비스를 끕니다. 각 서비스는 한 번, 두 번 이상 또는 전혀 사용하지 않을 수 있습니다 회사에는 여러 사업부가 있습니다. 각 사업부에는 일상적인 작업을 위해 20 개의 서로 다른 Azure 리소스가 필요합니다. 모든 사업부에는 동일한 유형의 Azure 리소스가 필요합니다. Azure 리소스 생성을 자동화하는 솔루션을 권장해야합니다. 권장 사항에 무엇을 포함해야합니까? Azure Resource Manager 템플릿\nVirtual Machine scale sets\nthe Azure API Management Service\nmanagement groups\n여러 서비스 및 자동화를 위해서는 stack을 생성하기 위한 탬플릿을 사용해야합니다.\nAzure 서비스를 올바른 정의와 일치시킵니다. 대답하려면 왼쪽 열에서 오른쪽 설명으로 적절한 Azure 서비스를 끕니다. 각 서비스는 한 번, 두 번 이상 또는 전혀 사용하지 않을 수 있습니다. 중요한 LOB (기간 업무) 응용 프로그램을 Azure에 배포 할 계획입니다. 애플리케이션은 Azure 가상 머신에서 실행됩니다. 응용 프로그램에 대한 배포 솔루션을 권장해야합니다. 이 솔루션은 99.99 %의 보장 된 가용성을 제공해야합니다. 배포를 위해 권장해야하는 최소 가상 머신 수와 최소 가용 영역 수는 얼마입니까? 답변하려면 답변 영역에서 적절한 옵션을 선택하십시오. 99.99%의 가용성을 위해서는 2개의 리전에 VM이 존재해야합니다.\n여러 리소스의 이벤트를 중앙 리포지토리로 연결하려면 어떤 Azure 서비스를 사용해야합니까? Azure Event Hubs\nAzure Analysis Services\nAzure Monitor\nAzure Log Analytics\n3\nAzure Monitor을 사용해야합니다.\nAzure 환경이 있습니다. Android 노트북에서 새 Azure 가상 머신을 만들어야합니다. Azure Cloud Shell에서 PowerShell을 사용합니다. 이것이 목표를 충족합니까? Yes\nNo\n1\n핸드폰에서도 웹을 통해 Cloud Shell을 사용할 수 있습니다.\nAzure 환경이 있습니다. Android 노트북에서 새 Azure 가상 머신을 만들어야합니다. PowerApps 포털을 사용합니다. 이것이 목표를 충족합니까? Yes\nNo\n2\nPowerApps은 웹 페이지 레이아웃을 만드는 도구입니다.\nAzure 환경이 있습니다. Android 노트북에서 새 Azure 가상 머신을 만들어야합니다. Azure Portal을 사용합니다. Yes\nNo\nA\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 데이터 베이스는 최소 3개의 복제본을 가집니다. 데이터 베이스는 설정시에 백업이 생성됩니다. 스토리지 용량 제한은 2 TB 이상을 가지는 스토리지 서비스도 있습니다 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 모든 지역의 가용성영역이 있는 것은 아닙니다. window server는 VM을 통해만 생성가능한 것이 아닙니다. 복제하는 데에 사용하는 것은 맞지만, 주 목적은 보안, 내결함성 등입니다. Azure 지역에는 지연 시간이 짧은 네트워크를 사용하여 연결된 하나 이상의 데이터 센터가 포함됩니다. 변경이 필요하지 않습니다.\nIs found in each country where Microsoft has a subsidiary office\nCan be found in every country in Europe and the Americas only\nContains one or more data centers that are connect by using a high-latency network\n1\n20 개의 가상 머신을 Azure 환경에 배포 할 계획입니다. VM1이라는 가상 머신이 다른 가상 머신에 연결할 수 없도록하려면 VM1을 별도의 가상 네트워크에 배포해야합니다. 변경이 필요하지 않습니다.\n다른 가상 머신과 다른 운영 체제 실행\n별도의 리소스 그룹에 배포\n두 개의 네트워크 엔터페이스가 있습니다.\n1\n네트워크 설정이 아닌 NIC 등의 설정을 통해 가능합니다.\n여러 Azure 가상 머신에 동시에 권한을 위임해야하는 경우 동일한 Azure 지역에 Azure 가상 머신을 배포해야합니다. 변경이 필요하지 않습니다.\n동일한 Azure Resource Manager 템플릿 사용\n동일한 리소스 그룹\n동일한 리전\n3\n동일한 리소스 그룹에 생성하면 권한이 유지됩니다.\n회사의 개발자 팀은 매주 50 개의 사용자 지정 가상 컴퓨터를 배포 한 다음 제거 할 계획입니다. 가상 머신 중 30 개는 Windows Server 2016을 실행하고 가상 머신 중 20 개는 Ubuntu Linux를 실행합니다. 가상 머신을 배포하고 제거하는 데 필요한 관리 노력을 최소화 할 Azure 서비스를 권장해야합니다. 무엇을 추천해야합니까? 1. Azure 예약 Vm 인스턴스\nAzure 가상 머신 확장 집합\nAzure DevTest Labs\nMicrosoft management Desktop\n3\n개발자팀은 DevTest Labs를 사용하여 편하게 관리가 가능합니다.\nAzure SQL Data Warehouse의 이점 중 하나는 플랫폼에 고 가용성이 내장되어 있다는 것입니다. 변경이 필요하지 않습니다.\n자동 확장\n데이터 압축\n버전 관리\n1\n고 가용성이란 데이터가 손실되지 않는 것을 뜻하므로, 보기에는 존재하지 않습니다.\n지원 엔지니어는 Azure CLI를 사용하여 여러 Azure 관리 작업을 수행 할 계획입니다. 컴퓨터에 CLI를 설치합니다. 지원 엔지니어에게 CLI를 실행하는 데 사용할 도구를 알려야합니다. 지원 엔지니어에게 어떤 도구를 사용하도록 지시해야합니까? 명령 프롬프트\nAzure 리소스 탐색기\nWindows PowerShell\nwindows Defender 방화벽\n네트워크 및 공유 센터\nA, 3\nCLI, WindowPowerShell, CloudShell, Web console\nAzure Cloud Shell을 사용하여 Azure를 관리해야합니다. 어떤 Azure Portal 아이콘을 선택해야하나요? 답변하려면 답변 영역에서 적절한 아이콘을 선택하십시오. Azure에 20TB의 데이터를 저장할 계획입니다. 데이터는 Microsoft Power BI를 사용하여 가끔 액세스되고 시각화됩니다. 데이터에 대한 스토리지 솔루션을 추천해야합니다. 어떤 두 가지 솔루션을 권장해야합니까? 각 정답은 완전한 해결책을 제시합니다. Azure Data Lake\nAzure Cosmos DB\nAzure SQL Datawarehouse\nAzure SQL 데이터베이스\nPostgreSQL 용 Azure 데이터베이스\n1, 3 Cosmos DB (NoSQL), SQL DB(최대 4TB). PostgreSQL DB (최대 16TB)\n10 개의 웹앱 이 포함 된 Azure 환경이 있습니다. 모든 Azure 리소스를 관리하려면 어떤 URL에 연결해야합니까? 답변하려면 답변 영역에서 적절한 옵션을 선택하십시오. Azure 서비스에 대한 액세스를 보호하기 위해 Azure 가용성 영역을 사용할 수 있는 오류 유형을 식별해야합니다. 무엇을 식별해야합니까? 물리적 서버 오류\nAzure 지역 오류\n저장 실패\nAzure 데이터 센터 오류\n4\n가용성 영역과 연관있는 것은 데이터 센터입니다.\n귀사의 네트워크를 Azure로 확장 할 계획입니다. 네트워크에는 IP 주소 131.107.200.1을 사용하는 VPN 어플라이언스가 포함되어 있습니다. VPN 어플라이언스를 식별하는 Azure 리소스를 만들어야합니다. 어떤 Azure 리소스를 만들어야합니까? 답변하려면 답변 영역에서 적절한 리소스를 선택하십시오. Windows Server 2016을 실행하는 VM1이라는 가상 머신이 있습니다. VM1은 미국 동부 Azure 지역에 있습니다. VM1의 가용성에 영향을 줄 수있는 서비스 오류 알림을 보려면 Azure Portal에서 어떤 Azure 서비스를 사용해야합니까? Azure 패브릭 서비스\nAzure 모니터\nAzure VM\nAzure Advisor\n2\nAzure 모니터를 통해 오류 로그를 확인할 수 있습니다.\nAzure 관리자는 Azure 리소스를 만드는 PowerShell 스크립트를 실행할 계획입니다. 스크립트를 실행하는 데 사용할 컴퓨터 구성을 권장해야합니다. Linux를 실행하고 Azure CLI 도구가 설치된 컴퓨터에서 스크립트를 실행합니다. 이것이 목표를 충족합니까? Yes\nNo\n2\nLinux CLI와 PowerShell은 서로 다른 도구입니다.\nAzure 관리자는 Azure 리소스를 만드는 PowerShell 스크립트를 실행할 계획입니다. 스크립트를 실행하는 데 사용할 컴퓨터 구성을 권장해야합니다. Chrome OS를 실행하고 Azure Cloud Shell을 사용하는 컴퓨터에서 스크립트를 실행합니다. 이것이 목표를 충족합니까? Yes\nNo\n1 Cloud Shell은 PowerShell을 지원합니다.\nAzure 관리자는 Azure 리소스를 만드는 PowerShell 스크립트를 실행할 계획입니다. 스크립트를 실행하는 데 사용할 컴퓨터 구성을 권장해야합니다. macOS를 실행하고 PowerShell Core 6.0이 설치된 컴퓨터에서 스크립트를 실행합니다. 이것이 목표를 충족합니까? Yes\nNo\n1 MacOS에 PowerShell 설치가 가능합니다.\n10 개의 가상 네트워크와 100 개의 가상 머신이 포함 된 Azure 환경이 있습니다. 모든 Azure 가상 네트워크에 대한 인바운드 트래픽 양을 제한해야합니다. 무엇을 만들어야합니까? 하나의 NSG\n10개의 가상 네트워크 게이트웨이\n10 Azure ExpreeRoute 회로\n하나의 Azure 방화벽\n4 Azure firewalld은 트래픽을 제한할 수 있습니다.\n여러 Azure 가상 머신이 포함 된 Azure 환경이 있습니다. 온-프레미스 네트워크의 클라이언트 컴퓨터가 Azure 가상 머신과 통신 할 수 있도록하는 솔루션을 구현할 계획입니다. 계획된 솔루션에 대해 만들어야하는 Azure 리소스를 권장해야합니다. 권장 사항에 어떤 Azure 리소스를 포함해야합니까? 각 정답은 솔루션의 일부를 제공합니다. 가상 네트워크 게이트웨이\n로드 밸런서\n애플리케이션 게이트웨이\n가상 네트워크\n게이트웨어 서브넷\n1, 5 이미 VM이 있으므로 가상네트워크는 존재합니다. 즉 가상 네트워크와 게이트웨이 와 서브넷을 추가하면 됩니다.\nAzure 서비스를 올바른 설명과 일치시킵니다. 대답하려면 왼쪽 열에서 오른쪽 설명으로 적절한 Azure 서비스를 끕니다. 각 서비스는 한 번, 두 번 이상 또는 전혀 사용하지 않을 수 있습니다. 3번째는 Data Lake입니다. 다음 작업을 수행하는 데 사용해야하는 Azure Portal의 블레이드를 식별해야합니다. ✑ 보안 권장 사항을 봅니다. ✑ Azure 서비스의 상태를 모니터링합니다. ✑ 사용 가능한 가상 머신 이미지를 찾습니다. 각 작업에 대해 어떤 블레이드를 식별해야합니까? 답변하려면 답변 영역에서 적절한 옵션을 선택하십시오.\nAzure 환경이 있습니다. Android 노트북에서 새 Azure 가상 머신을 만들어야합니다. Azure Cloud Shell에서 Bash를 사용합니다. 이것이 목표를 충족합니까? Yes\nNo\n1\nCloud Shell을 통해 생성이 가능합니다.\nAzure 가상 머신을 만들 계획입니다. 가상 머신의 데이터 디스크를 저장하는 데 사용해야하는 스토리지 서비스를 식별해야합니다. 무엇을 식별해야합니까? 응답하려면 응답 영역에서 적절한 서비스를 선택하십시오. 데이터 디스크를 저장하는 데에는 Blobs가 주로 사용됩니다.\n회사에서 여러 서버를 Azure로 이동할 계획입니다. 회사의 규정 준수 정책에 따르면 FinServer라는 서버는 별도의 네트워크 세그먼트에 있어야합니다. 규정 준수 정책 요구 사항을 충족하는 데 사용할 수있는 Azure 서비스를 평가하고 있습니다. 어떤 Azure 솔루션을 권장해야합니까? FinServer 용 리소스 그룹 및 다른 모든 서버용 다른 리소스 그룹\nFinServer 용 가상 네트워크 및 다른 모든 서버용 다른 가상 네트워크\nFinServer 용 VPN 및 서로 서버용 가상 네트워크 게이트웨이\n모든 서버에 대한 하나의 리소스 그룹 및 FinServer에 대한 리소스 잠금\n2\n따로 구성한다는 말에 주의해야합니다.\nWindows 10을 실행하는 여러 컴퓨터의 네트워크 드라이브를 Azure Storage에 매핑 할 계획입니다. 계획된 매핑 된 드라이브에 대해 Azure에서 저장소 솔루션을 만들어야합니다. 무엇을 만들어야합니까? Azure SQL DB\nVM data disk\nFiles service in a storage account\nBloss service in a storage account\n3 SMB를 통해 파일 서비스를 적합한 통해 저장소 솔류션을 만들 수 있습니다.\nAzure 데이터베이스 솔루션을 구현할 계획입니다. 다음 요구 사항을 충족하는 데이터베이스 솔루션을 구현해야합니다. ✑ 여러 지역의 데이터를 동시에 추가 할 수 있습니다 . ✑ JSON 문서를 저장할 수 있습니다. 어떤 데이터베이스 서비스를 배포해야합니까?\nCosmosDB의 특징입니다.\n회사에서 모든 네트워크 리소스를 Azure로 마이그레이션 할 계획입니다. Azure를 탐색하여 계획 프로세스를 시작해야합니다. 먼저 무엇을 만들어야합니까? 구독\n리소스 그룹\n가상 네트워크\n관리 그룹\n1\n모든 권환의 기초는 구독입니다.\n규칙에 따라 자동으로 이메일 알림을 보내는 온-프레미스 애플리케이션이 있습니다. 애플리케이션을 Azure로 마이그레이션 할 계획입니다. 애플리케이션을위한 서버리스 컴퓨팅 솔루션을 권장해야합니다. 추천에 무엇을 포함해야합니까? web app\nAzure Marketplace의 서버 이미지\nlogic app\nAPI app\n3\nmail, 문서 등의 작업을 수행할 시에는 logic app을 사용합니다.\nAzure에 웹 사이트를 배포 할 계획입니다. 이 웹 사이트는 전 세계 사용자가 액세스 할 수 있으며 대용량 비디오 파일을 호스팅합니다. 최상의 비디오 재생 환경을 제공하기 위해 사용해야하는 Azure 기능을 권장해야합니다. 무엇을 추천해야합니까? 애플리케이션 게이트웨이\nAzure ExpressRoute circuit\na content delivery network (CDN)\nan Azure Traffic Manager profile\n3\nCDN은 전 세계의 호스팅을 위해 사용됩니다.\n귀사는 Azure에 데이터를 업로드 할 수백만 개의 센서를 배포 할 계획입니다. 계획된 솔루션을 지원하기 위해 만들어야하는 Azure 리소스를 식별해야합니다. 어떤 두 Azure 리소스를 식별해야합니까? 각 정답은 솔루션의 일부를 제공합니다. Azure data Lake\nAzure Queue storage\nAzure File Storage\nAzure IoT Hub\nAzure Notification Hubs\n2, 4\n센서는 IoT Hub에 관한 설명이고, 식별을 위한 통신은 Queue에 대한 설명입니다.\nAzure 웹앱이 있습니다. iPhone에서 웹 앱의 설정을 관리해야합니다. 사용할 수있는 두 가지 Azure 관리 도구는 무엇인가요? 각 정답은 완전한 해결책을 제시합니다. Azure CLI\nAzure potal\nAzure Cloud Shell\nWindows PowerShell\nAzure 저장소 탐색기\n2, 3\n모바일에서 관리할 수 있는 도구들은 모바일 앱, 포탈, 포탈의 클라우드 쉘이 있습니다.\n서비스 수준 계약 (SLA)이 99.95 % 인 Azure 웹앱과 SLA가 99.99 % 인 Azure SQL 데이터베이스로 구성된 애플리케이션이 있습니다. 애플리케이션의 복합 SLA는 99.94 %에 해당하는 두 SLA의 제품입니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n애플리케이션과 관련된 최저 SLA (99.95%)\n애플리케이션과 관련된 최고 SLA (99.99%)\n두 SLA의 차이 (0.05%)\n1 99.95 x 99.99 = 99.94\nRG1이라는 자원 그룹에 삭제 잠금이있는 경우 글로벌 관리자 그룹의 구성원 만 RG1을 삭제할 수 있습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오 변경이 필요하지 않습니다.\n관리자 전에 삭제 잠금을 제거해야합니다.\n관리자 전에 Azure 정책을 수정해야합니다.\n관리자 전에 Azure 태그를 추가해야합니다.\n2\n제거를 위해서는 관리자가 삭제 잠금을 제거해야합니다.\nAzure Germany는 독일의 합법적 인 거주자 만 사용할 수 있습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n독일에 등록된 기업만\n독일에 기반을 둔 파트너로부터 Azure 라이선스를 구매한 기업만 해당\n데이터가 독일에 있어야하는 사용자 또는 기업\nD\n가상 머신을 생성 한 후에는 TCP 포트 8080에서 가상 머신으로의 연결을 허용하도록 NSG (네트워크 보안 그룹)를 수정해야합니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n가상 네트워크 게이트웨이\n가상 네트워크\n라우팅 테이블\n1 port로 인바운드, 아웃바운드를 수정하는 서비스는 NSG입니다.\nAzure 환경에는 여러 Azure 가상 머신이 포함되어 있습니다.HTTP를 통해 인터넷에서 VM1이라는 가상 머신에 액세스 할 수 있는지 확인해야합니다. 솔루션 : DDoS 보호 계획을 수정합니다. 이것이 목표를 충족합니까? Yes\nNo\n2 방화벽을 수정해야합니다.\nAzure 환경에는 여러 Azure 가상 머신이 포함되어 있습니다. HTTP를 통해 인터넷에서 VM1이라는 가상 머신에 액세스 할 수 있는지 확인해야합니다. 솔루션 : Azure 방화벽을 수정합니다. 이것이 목표를 충족합니까? 1\n위와 동일\nAzure 환경에는 여러 Azure 가상 머신이 포함되어 있습니다. HTTP를 통해 인터넷에서 VM1이라는 가상 머신에 액세스 할 수 있는지 확인해야합니다. 해결 방법 : Azure Traffic Manager 프로필을 수정합니다. 이것이 목표를 충족합니까? Yes\nNo\n2\n방화벽을 수정해야합니다.\nAzure Government를 사용하여 클라우드 솔루션을 개발할 수있는 두 가지 유형의 고객은 무엇입니까? 각 정답은 완전한 해결책을 제시합니다. 캐나다 정부 계약자\n유럽 정부 계약자\n미국 정부 기관\n미국 정부 계약자\n유럽 정부 기관\n3, 4 Azure Government는 미국 정부 기관의 준수에 따라 생성되었습니다.\nAzure AD (Azure Active Directory) 사용자가 익명 IP 주소를 사용하여 인터넷에서 Azure AD에 연결할 때 사용자에게 암호를 변경하라는 메시지가 자동으로 표시되는지 확인해야합니다. 어떤 Azure 서비스를 사용해야합니까? Azure AD Connect 상태\nAzure AD ID 관리 권한\nAzrue ATP\nAzure AD ID Protect\n4 ID Protect를 통해 계정보안이 관리됩니다.\n회사에서 여러 웹 서버와 여러 데이터베이스 서버를 Azure에 배포 할 계획입니다. 웹 서버에서 데이터베이스 서버로의 연결 유형을 제한하려면 Azure 솔루션을 권장해야합니다. 추천에 무엇을 포함해야합니까? NSG\nAzure Service Bus\nLocal Network Gateway\nroute filter\n1\n기본적으로 NSG를 통해 IP, 포트 연결유형을 제한 할 수 있습니다.\n애플리케이션은 보안 토큰을 검색하기 위해 무엇에 연결해야합니까? Azure Storage 계정\nAzure Active Dirctory\n인증서 저장소\nAzure Key Vault\n4\nKey Vault는 보안 토큰을 관리합니다.\n리소스 그룹은 여러 구독에서 Azure 리소스의 규정 준수를 관리하는 기능을 조직에 제공합니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n관리 그룹\nAzure 정책\nAzure App Service 계획\n3\n리소스 그룹을 관리하는 것은 Azure 정책입니다.\n네트워크에 Active Directory 포리스트가 있습니다. 포리스트에는 5,000 개의 사용자 계정이 있습니다. 회사는 모든 네트워크 리소스를 Azure로 마이그레이션하고 온-프레미스 데이터 센터를 폐기 할 계획입니다. 계획된 마이그레이션 후 사용자에게 미치는 영향을 최소화하는 솔루션을 권장해야합니다. 무엇을 추천해야합니까? Azure MFA (Multi-Factor Authentication) 구현\n모든 Active Directory 사용자 계정을 Azure AD (Azure Active Directory)에 동기화\n모든 사용자에게 암호를 변경하도록 지시\n각 사용자에 대해 Azure Active Directory (Azure AD)에서 게스트 사용자 계정 만들기\n2 Azure AD가 동일한 역할을 수행합니다.\n인증서를 저장하려면 어떤 Azure 서비스를 사용해야합니까? Azure 보안 센터\nAzure Storage 게정\nAzure Key Vault\nAzure 정보 보호\n3\n인증서의 관리는 Azure Key Vault에서 관리됩니다.\nRG1이라는 리소스 그룹이 있습니다. RG1에서 가상 네트워크 및 앱 서비스를 만들 계획입니다. RG1에서만 가상 머신 생성을 방지해야합니다. 무엇을 사용해야합니까? lock\nAzure 역할\ntag\nAzure 정책\n4 정책을 통해 삭제를 제한할 수 있습니다.\nAzure Information Protection은 무엇을 암호화 할 수 있나요? 네트워크 트래픽\n문서 및 이메일 메시지\nAzure Storage 계정\nAzure SQL 데이터베이스\n2\nAzure Information Protection은 문서 및 이메일 메시지를 관리합니다.\n회사의 Azure 환경이 규정 요구 사항을 충족하는지 평가하려면 무엇을 사용해야합니까? 지식 센터 웹 사이트\nthe Advisor blade from the Azure portal\nCompliance Manager from the Security Trust Portal\nthe Security Center blade from the Azure portal\n4\nAzure Portal에 Azure 규정 준수사항이 있습니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 회사는 신용 카드 정보가 포함 된 Microsoft Word 문서에 워터 마크를 자동으로 추가하는 Azure 정책을 구현합니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요 없습니다.\nDDoS Protection\nAzure Iformation Protection\nAzure AD ID Protection\n3\nAzure Information Protection을 통해 워터마크를 자동으로 추가하는 Azure 정책을 구현할 수 있습니다.\nAzure Monitor에서 지난 14 일 동안 특정 가상 머신을 끈 사용자를 볼 수 있습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\nAzure Event Hubs\nAzure 활동 로그\nAzure 서비스 상태\n3\nAzure 활동 로그를 통해 확인이 가능합니다.\nG1이라는 리소스 그룹에 VNET1이라는 Azure 가상 네트워크가 있습니다. 가상 네트워크가 RG1에서 허용되는 리소스 유형이 아님을 지정하는 Azure 정책을 할당합니다. VNET1은 자동으로 삭제됩니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n다른 리소스 그룹으로 자동 이동\n계속 정상적으로 작동합니다.\n읽기 전용 개체\n1 해당 리소스는 관리가 불가능해집니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 3번째 문항은 Yes입니다.\n회사에 여러 지역의 리소스가 포함 된 Azure 환경이 있습니다. 회사 정책에 따르면 관리자는 사무실이 있는 국가의 지역에서만 추가 Azure 리소스를 만들 수 있어야합니다. 정책 요구 사항을 충족하는 데 사용해야하는 Azure 리소스를 만들어야합니다. 무엇을 만들어야합니까? 읽기 전용 잠금\nAzure 정책\n관리 그룹\n예약\n2\nAzure 정책을 통해 요구사항을 만족시킬 수 있습니다.\n권한 부여는 사용자의 자격 증명을 확인하는 프로세스입니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n인증\n연맹\n발권\n2 권한 부여가 아닌 인증을 통해 자격 증명을 받습니다.\n다음 요구 사항을 충족하는 Azure 솔루션을 구성해야합니다. ✑ 공격으로부터 웹 사이트 보호 ✑ 시도 된 공격에 대한 세부 정보가 포함 된 보고서를 생성합니다. 솔루션에 무엇을 포함해야합니까?\nAzure 방화벽\nNSG\nAzure 정보 보호\nDDoS 보호\nD 문제는 DDoS 공격에 대한 설명입니다.\nAzure 환경에 대한 여러 보안 서비스를 구현할 계획입니다. 다음 보안 요구 사항을 충족하기 위해 사용해야하는 Azure 서비스를 식별해야합니다. ✑ 센서를 사용하여 위협 모니터링 ✑ 조건에 따라 Azure MFA (Multi-Factor Authentication) 적용 각 요구 사항에 대해 어떤 Azure 서비스를 식별해야합니까? 답변하려면 답변 영역에서 적절한 옵션을 선택하십시오.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 회사는 모든 온-프레미스 데이터를 Azure로 마이그레이션 할 계획입니다. Azure가 회사의 지역 요구 사항을 준수하는지 여부를 식별해야합니다. 무엇을 사용해야합니까? 지식 센터\nAzure Market Place\nAzure Potal\n보안 센터\n4 보안센터를 통해 확인할 수 있습니다.\nAzure Key Vault는 Azure AD (Azure Active Directory) 사용자 계정에 대한 비밀을 저장하는 데 사용됩니다. 명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\nAzure AD 관리 계정\n개인 식별 정보\n서버 애플리케이션\n1\nAzure Key Vault에는 Azure AD에 대한 정보를 저장합니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 여러 Azure 가상 머신을 배포 할 계획입니다. 인터넷의 장치가 가상 머신에 액세스하는 데 사용할 수있는 포트를 제어해야합니다. 무엇을 사용해야합니까? NSG\nAzure AD 역할\nAzure AD 그룹\nAzure Key Vault\n1\nNSG 에서 Port를 통한 접근을 제어합니다.\nAzure 구독에 여러 가상 머신이 있습니다. 새 구독을 만듭니다. 가상 머신은 새 구독으로 이동할 수 없습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n가상 머신을 새 구독으로 이동할 수 있습니다.\n가상 머신은 모두 동일한 리소스 그룹에 있는 경우에만 새 구독으로 이동할 수 있스니다.\n가상 머신은 Windows Server 2016을 실행하는 경우에만 새 구독으로 이동할 수 있습니다.\n2\n구독은 수정 및 이전이 가능합니다.\nAzure 환경에서 여러 관리되는 Microsoft SQL Server 인스턴스를 만들려고 시도하고 Azure 구독 제한을 늘려야한다는 메시지를받습니다. 한계를 높이려면 어떻게해야합니까? 서비스 상태 경고 만들기\n지원 계획 업그레이드\nAzure 정책 수정\n새 지원 요청 생성\n4\n요청함으로써 사용제한을 풀 수 있습니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 회사에는 10 개의 사무실이 있습니다. Azure Portal에서 여러 청구 보고서를 생성 할 계획입니다. 각 보고서에는 각 사무실의 Azure 리소스 용률이 포함됩니다. 보고서를 생성하기 전에 어떤 Azure Resource Manager 기능을 사용해야합니까? 태그\n탬플릿\nlock\n정책\n1\n태그를 통해 분류가 가능합니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. Azure 리소스를 배포합니다. 서비스 중단으로 인해 리소스를 장기간 사용할 수 없게됩니다. Microsoft는 자동으로 귀하의 은행 계좌를 환불 해드립니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요가 없습니다.\n리소스를 다른 구독으로 자동 마이그레이션\n자동으로 계정에 입금\nAzure 크레딧으로 교환 할 수 있는 쿠폰 코드를 전송\n3 자동으로 크레딧으로 변환됩니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 회사에서 Azure로 마이그레이션 할 계획입니다. 회사에는 여러 부서가 있습니다. 각 부서에서 사용하는 모든 Azure 리소스는 부서 관리자가 관리합니다. 부서에 대해 Azure를 분할하는 기능을 제공하는 Azure 배포를 권장해야합니다. 솔루션은 관리 노력을 최소화해야합니다. 추천에 무엇을 포함해야합니까? 여러 구독\n여러 Azure AD 디렉터리\n여러 지역\n여러 리소스 그룹\n문제풀이\n회사에 사용되지 않는 다음 리소스가 포함 된 Azure 구독이 있습니다 ✑ Azure Active Directory (Azure AD)의 사용자 계정 20 개 ✑ Azure AD의 그룹 5 개 ✑ 공용 IP 주소 10 개 ✑ 네트워크 인터페이스 10 개 회사의 Azure 비용을 줄여야합니다. 해결 방법 : 사용하지 않는 네트워크 인터페이스를 제거합니다. 이것이 목표를 충족합니까?\nYes\nNo\n2\n네트워크 인터페이스는 비용이 지불되지 않습니다.\n회사에 사용되지 않는 다음 리소스가 포함 된 Azure 구독이 있습니다 ✑ Azure Active Directory (Azure AD)의 사용자 계정 20 개 ✑ Azure AD의 그룹 5 개 ✑ 공용 IP 주소 10 개 ✑ 네트워크 인터페이스 10 개 회사의 Azure 비용을 줄여야합니다. 해결 방법 : 사용하지 않는 사용자 계정을 제거합니다. 이것이 목표를 충족합니까?\nYes\nNo\n2\n구독은 비용과 연관이 없습니다.\n월간 가동률을 어떻게 계산해야합니까? 답변하려면 답변 영역에서 적절한 옵션을 선택하십시오. 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 가능한 가장 낮은 비용으로 모범 사례 정보, 건강 상태 및 알림, 청구 정보에 대한 연중 무휴 액세스를 제공하는 지원 계획 솔루션은 표준 지원 계획입니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\nDeveloper\nBasic\nPremier\n3\n24시간 및 사례 정보 등의 기본 솔루션을 제공하는 것은 Basic입니다.\n여러 Azure 가상 머신을 배포 할 계획입니다. 단일 데이터 센터가 실패하는 경우 가상 머신에서 실행중인 서비스를 사용할 수 있는지 확인해야합니다. 솔루션 : 가상 머신을 둘 이상의 리소스 그룹에 배포합니다. 이것이 목표를 충족합니까? Yes\nNo\n2\n리소스 그룹이 아닌 가용영역에 배포해야합니다.\n어떤 Azure 지원 플랜에서 새 지원 요청을 열 수 있나요? Premier 및 Professional Direct 전용\nPremier, Professional Direct 및 Standard 전용\nPremier, Professional Direct, Standard 및 Developer 전용\nPremier, Professional Direct, Standard, Developer 및 Basic\n4\n기술지원은 Basic은 지원하지 않지만 지원 요청은 모두 가능합니다.\nsupport.microsoft.com에서 Azure 지원 요청을 만들 수 있습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택하십시오. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요가 없습니다.\nAuzre Portal\n지식 센터\n보안 및 규정 준수 관리 센터\n2 Azure Portal에서 지원 요청이 가능합니다.\nAzure 서비스 수준 계약 (SLA)에서 보장되는 것은 무엇인가요? 가동 시간\n가용성\n대역폭\n성능\n2\nSLA에서 보장하는 것은 가용성입니다.\nAzure 서비스는 공개 미리보기 상태 일 때 모든 Azure 고객이 사용할 수 있습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요 없습니다.\n비공개 미리보기\n개발\nEA (기업 계약) 구독\n1 공개 미리보기는 모두 사용하고 있고, 비공개는 특정 고객만 사용이 가능합니다.\n회사의 지원 정책에 따르면 Azure 환경은 전화 또는 이메일로 지원 엔지니어에 액세스 할 수있는 옵션을 제공해야합니다. 지원 정책 요구 사항을 충족하는 지원 계획을 추천해야합니다. 솔루션 : 기본 지원 계획을 권장합니다. 이것이 목표를 충족합니까? Yes\nNo\n2\n기본 지원 계획에는 기술지원이 존재하지 않습니다.\n회사에서 Azure를 구매할 계획입니다. 회사의 지원 정책에 따르면 Azure 환경은 전화 또는 이메일로 지원 엔지니어에 액세스 할 수있는 옵션을 제공해야합니다. 지원 정책 요구 사항을 충족하는 지원 계획을 추천해야합니다. 솔루션 : 표준 지원 계획을 권장합니다. 이것이 목표를 충족합니까? Yes\nNo\nA\n회사에서 Azure를 구매할 계획입니다. 회사의 지원 정책에 따르면 Azure 환경은 전화 또는 이메일로 지원 엔지니어에 액세스 할 수있는 옵션을 제공해야합니다. 지원 정책 요구 사항을 충족하는 지원 계획을 추천해야합니다. 솔루션 : 표준 지원 계획을 권장합니다. 이것이 목표를 충족합니까? Yes\nNo\nA\n귀사는 Microsoft에 Azure 환경의 아키텍처 검토를 요청할 계획입니다. 회사는 현재 기본 지원 계획을 가지고 있습니다. 회사에 대한 새로운 지원 계획을 추천해야합니다. 솔루션은 비용을 최소화해야합니다. 어떤 지원 계획을 추천해야합니까? Premier\nDeveloper\nProfessional Direct\nStandard\n1\n아키텍처의 검토를 위해서는 프리미어가 지원 플랜이 필요합니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오 Azure Cost Management를 사용하려면 무엇이 필요합니까? 개발/ 테스트 구독\n소프트웨어 보증\nEA\n종량제 구독\n3\nEA를 통해 비용관리가 가능합니다.\nAzure 평가판 계정이 지난주에 만료되었습니다. 이제 추가 Azure AD (Azure Active Directory) 사용자 계정을 만들 수 없습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n기존 Azure 가상 머신 시작\nAzure에 저장된 데이터에 엑세스\nAzure Portal 엑세스\n3\n가장 맞는 말이 C입니다.\n회사에는 10 개의 부서가 있습니다. 이 회사는 Azure 환경을 구현할 계획입니다. 각 부서가 사용하는 Azure 서비스에 대해 서로 다른 결제 옵션을 사용할 수 있는지 확인해야합니다. 부서별로 무엇을 만들어야합니까? 예약\n구독\n리소스 그룹\n컨테이너 인스턴스\n2\n구독을 통해 서로 다른 결제 옵션을 설정할 수 있습니다.\n다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. 다음 각 명령문에 대해 해당 명령문이 참이면 예를 선택하십시오. 그렇지 않으면 아니오를 선택하십시오. Azure에서 IaaS (Infrastructure as a Service) 리소스를 프로비저닝 할 계획입니다. IaaS의 예는 어떤 리소스입니까? Azure Web, APP\nAzure VM\nAzure logic APP\nAzure SQL Database\n2\nIaaS에 포함되는 서비스는 VM 입니다.\n회사의 개발자 팀은 매주 50 개의 가상 머신을 배포 한 다음 제거 할 계획입니다. 모든 가상 머신은 Azure Resource Manager 템플릿 을 사용하여 구성됩니다. 가상 머신을 배포하고 제거하는 데 필요한 관리 노력을 최소화 할 Azure 서비스를 권장해야합니다. 무엇을 추천해야합니까? Azure VM\nAzure DevTest Labs\nAzure 가상머신 확장 집합\nMicrosoft 관리 데스크톱\n2\nDevTest Labs를 사용해야합니다.\nSubscription1이라는 Azure 구독이 있습니다. Azure Portal에 로그인하고 RG1이라는 리소스 그룹을 만듭니다. Azure 설명서에서 VM1이라는 가상 머신을 만드는 다음 명령이 있습니다. az vm create –resource-group RG1 –name VM1-이미지 UbuntuLTS –generate-ssh-keys- 명령을 사용하여 Subscription1에 VM1을 만들어야합니다. 솔루션 : Azure Portal에서 Azure Cloud Shell을 시작하고 PowerShell을 선택합니다. Cloud Shell에서 명령어를 실행합니다. 이것이 목표를 충족합니까? Yes\nNo\n1\nPowerShell 에서도 Azure CLI 명령어를 사용할 수 있습니다.\nMicrosoft가 후속 서비스가 없는 Azure 서비스에 대한 지원을 종료 할 계획 인 경우 Microsoft는 최소 12 개월 전에 알림을 제공합니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 1. 변경할 필요가 없습니다. 2. **6 개월** 3. **90 일** 4. **30 일** 1\n회사에서 여러 사용자 지정 응용 프로그램을 Azure에 배포 할 계획입니다. 이 응용 프로그램은 회사의 고객에게 송장 서비스를 제공합니다. 각 응용 프로그램에는 몇 가지 필수 응용 프로그램 및 서비스가 설치됩니다. 모든 애플리케이션에 클라우드 배포 솔루션을 권장해야합니다. 무엇을 추천해야합니까? SaaS\nPaaS\nIaaS\n3\nAzure에서 서버리스 컴퓨팅을 제공하는 서비스는 무엇입니까? Azure Vm\nAzure Function\nAzure Storage account\nAzure Container Instacne\n2\n코드를 관리하기위한 버전 제어 도구 집합을 제공하는 Azure 서비스는 무엇인가요? Azure Repos\nAzure DevTest Labs\nAzure Storage\nAzure Cosmos DB\n1\n여러 Azure 구독 및 가상 네트워크에서 네트워크 트래픽 필터링을 제공하는 서비스는 무엇입니까? Azure 방화벽\n애플리케이션 보안 그룹\nAzure DDos 보호\nNSG\n1\nAzure Cloud Shell에서 ISO 27001과 같은 회사의 규제 표준 및 규정을 추적 할 수 있습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택하십시오. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요가 없습니다.\nMicrosoft 클라우드 파트너 포털\n준수 관리자\n보안 센터\n문제풀이\nMicrosoft 온라인 서비스 개인 정보 보호 정책은 Microsoft가 처리하는 데이터, Microsoft가 데이터를 처리하는 방법 및 데이터 처리 목적을 설명합니다. 설명이 정확하면 “변경할 필요 없음\"을 선택하십시오. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요가 없습니다.\nMicrosoft 온라인 서비스 약관\nMicrosoft 온라인 서비스 수준 계약\nMicrosoft Azure에 대한 온라인 구독 계약\n1\n회사에서 Azure로 마이그레이션 할 계획입니다. 회사에는 여러 부서가 있습니다. 각 부서에서 사용하는 모든 Azure 리소스는 부서 관리자가 관리합니다. 부서를 위해 Azure를 분할 할 수있는 두 가지 가능한 기술은 무엇입니까? 각 정답은 완전한 해결책을 제시합니다. 여러 구독\n여러 Azure AD 디렉터리\n여러 지역\n여러 리소스 그룹\n1, 4\n다음 중 Azure 서비스에 대한 최신 수명주기 정책을 정확하게 설명하는 문은 무엇입니까? Microsoft는 5년 동안 서비스에 대한 일반 지원을 제공합니다.\nMicrosoft는 서비스 지원을 종료하기 전에 최소 12개월전에 통지합니다.\n서비스가 이반 공급 된 후 Microsoft는 최소 4년 동안 서비스에 대한 지원을 제공합니다.\n서비스가 만료되면 최대 5년까지 서비스에 대한 연장 지원을 구매할 수 있습니다.\n2\nAzure 구독에 대한 현재 청구 기간의 비용이 지정된 제한을 초과 할 때 Azure에서 Advisor 권장 사항을 사용하여 이메일 경고를 보낼 수 있습니다. 설명이 정확하면 “변경할 필요 없음\"을 선택하십시오. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경할 필요가 없습니다.\nIAM\n예산 알림\n규정 준수\n3\nAzure Standard 지원 플랜은 전화로 지원 엔지니어에게 연중 무휴 24 시간 액세스 할 수있는 가장 저렴한 옵션입니다. 설명이 정확하면 “변경할 필요 없음\"을 선택합니다. 설명이 정확하지 않은 경우 해당 설명을 올바르게 만드는 답을 선택하십시오. 변경이 필요하지 않습니다.\n개발자\nBasic\n프로페셔널","az-900-시험대비-문제풀이#\u003cstrong\u003eAz-900 시험대비 문제풀이\u003c/strong\u003e":""},"title":"Az-900 : 문제풀이"},"/system/azure/azuretraining/azure00/":{"data":{"":"Az-900 : CloudComputing 클라우드 컴퓨팅 클라우드 컴퓨팅은 스토리지 공간이나 CPU 주기와 같은 리소스를 다른 회사의 컴퓨터에 대여하는 서비스이며 사용한 만큼의 요금만을 지불합니다.\n클라우드 공급 기업은 작업을 싱행하는 데 필요한 실제 하드웨어와 이 하드웨어를 최신 상태로 유지할 책임이 존재하며, 제공되는 컴퓨팅 서비스는 클라우드 공급자에 따라 달라지며, 일반적으로는 하단의 항목이 기본적으로 포함됩니다.\nCompute power : 리눅스 서버나 웹 프로그램\nStorage : 저장 및 데이터베이스 역할\nNetworking : 클라우드 제공사와 우리 회사의 사이의 안전한 연결\nAnalytics : 이치 혹은 성능 데이터를 시각화\n클라우드 컴퓨팅 서비스 클라우드 컴퓨팅의 목적은 소규모 신생 기업이든 대기업이든 상관없이 비즈니스를 보다 쉽고 효율적으로 운영할 수 있도록 만드는 것으로, 이러한 요구를 충족시키기 위해 클라우드 컴퓨팅 공급자는 광범위한 서비스를 제공합니다. 컴퓨팅 기능 이메일을 보내거나, 인터넷에서 예약을 하거나, 온라인으로 지불을 하거나, 심지어 Microsoft 학습 모듈을 사용할 때도 각 요청을 처리하고 응답을 반환하는 클아우드 기반 서버와 상호작용하게 되며, 기본적으로 인터넷을 구성하는 다양한 클라우드 공급 기업이 제공하는 컴퓨팅 서비스를 사용합니다.\n클라우드 컴퓨팅을 사용하여 솔루션을 빌드하는 경우 리소스 및 요구 사항에 따라 작업을 수행하는 방법을 선택할 수 있습니다.\n여기에서 컴퓨팅 기능의 종류에는 기본적으로 VM, 컨테이너, 서버리스 컴퓨팅으로 분리할 수 있습니다.\nVM\nVM은 클라우드 기업의 물리적 서버에서 가상 머신을 생성하여 해당 서버를 직접 사용하여 생성하는 서비스입니다. 컨테이너\n컨테이너는 애플리케이션에 일관적이고 격리된 실행 환경을 제공하며, 게스트 운영체제가 필요하지 않은 점에서는 VM과 흡사하지만, 대신 애플리케이션과 몯느 해당 종속성이 컨테이너에 패키지된 다음, 표준 런타임 환경이 앱을 실행하는 데 사용됩니다.\n기본적으로 VM과의 가장 큰 차이는 운영체제의 유무에 대한 차이입니다.\n서버리스 컴퓨팅\n서버리스 컴퓨팅은 서버를 생성, 구성, 유지관리하지 않고 애플리케이션 코드를 실행하는 것이 가능합니다.\n서버리스 컴퓨팅의 핵심개념은 애플리케이션이 일부 작업에 의해 트리거될 때 실행되는 별도의 기능으로 분리된다는 점이며, 이는 자동화 작업에 이상적입니다.\n서버리스 모델은 기본적으로 사용하는 처리 시간에 대해서만 페이를 지불한다는 점에서 VM과 컨테이너와 성격이 다릅니다.\n컴퓨팅 서비스 비교 다이어그램 스토리지 대부분의 디바이스 및 애플리케이션은 데이터를 읽고 사용합니다. 이러한 모든 경우의 데이터는 데이터를 읽거나, 쓰여지며 이에 딸느 데이터이 유형 및 저장방식은 경우의 마다 다를 수 있습니다.\n일반적으로 클라우드 공급자는 이러한 모든 유형의 데이터를 처리할 수 있는 서비스를 제공하며, 클라우드 기반 데이터 스토리지는 사용자의 요구 사항에 맞게 확장할 수 있다는 장점이 있습니다.\n예를 들면 자동적으로 저장공간을 확장 및 줄일 수 있으며 백업 파일을 저장할 수도 있습니다.\n왜 클라우드 서비스인가? (클라우드 컴퓨팅의 혜택) 기본적으로 클라우드 서비스를 사용하는 것은 기존 비즈니스의 인프라 및 관리 비용을 점가하기 위해 사용하지만, 이는 순전히 선택사항입니다. 하단은 클라우드 서비스를 사용하는 이점을 나타냅니다. 비용효과적\n클라우드 컴퓨팅은 종량제 또는 사용량 기반 가격 책정 모델을 제공합니다.\n즉, 선불 인프라 비용이 없으며, 사용하지 않는 경우 비용이 청구되지 않아 비용을 절약할 수 있습니다.\n확장가능\n주말과 같은 특정 시간의 수요 또는 워크로드에 따라 사용되는 리소스와 서비스를 늘리거나 줄일 수 있습니다.\n클라우드 서비스는 수직적, 수평적 크기 조정을 둘다 지원하며 수직적 크기 조정은 Scale UP, 수평적 크기 조정은 Scale OUT이라 합니다.\nScale UP은 한 서버에 리소스 추가시키는 것을 의미하며, Scale OUT은 서버를 여러 대 생성하여 트래픽을 분산시키는 것을 의미합니다.\n이와 같은 기능들을 자동적으로 수행됩니다.\n탄력적\n수요 급증 또는 급감으로 인해 워크로드가 변경되면 클라우드 컴퓨팅 시스템은 자동으로 리소스를 추가하거나 제거하여 보정이 가능합니다. 최신상태\n클라우드를 사용하면 애플리케이션을 빌드하고 배포하는 중요한 작업에 집중할 수 있습니다. 클라우드를 사용하면 소프트웨어 패치, 하드웨어 설치, 업그레이드 및 기타 IT 관리 작업을 유지 관리해야하는 부담을 줄일 수 있습니다. 안정적\n클라우드는 컴퓨팅 공급자가 안정적으로 데이터를 유지할 수 있도록 백업, 재해 복구 및 데이터 복제 서비스를 제공하며, 장애가 발생하면 백업 구성 요소가 댓니 사용되며, 이를 내결함성이라 합니다. 전 세계 어디서든 사용가능\n클라우드 공급 기업은 전 세계의 다양한 지역에 완벽하게 증복된 데이터 센터를 갖추고 있으며, 이로 가능한 최적의 응답 시간을 제공할 수 있습니다. 안전함\n클라우드 공급자는 대부분의 조직에서 달성할 수 있는 것보다 더 낭느 보안을 제공할 수 있는 광범위한 정책, 기술, 제어 및 전문 기술을 제공합니다. 이를 온-프레미스의 환경과 비교해보면 하단의 장점들을 가지고 있습니다.\n온-프레미스 환경은 고정비용이지만 퍼블릭 환경은 유동적으로 비용우위를 가질 수 있다.\n온-프레미스 환경에서는 테스트 서버의 세팅에 많은 시간이 걸리지만, 퍼블릭 환경에서는 빠른 시간 내에 세팅이 가능한 생산성의 우위를 가질 수 있다.\n이미 퍼블릭환경에서는 패키지로 플랫폼을 제공하기 때문에 직접 설치할 필요가 없어 진입장벽이 온-프레미스 환경에 비해서 낮다.\n이와 같이 Azure는 여러 규정 준수 조건 및 요구 사항을 각 나라의 맞춰 설계되어있습니다.\n규정 준수 참조\n클라우드 컴퓨팅의 주요 개념 및 용어 위와 같이 클라우드 컴퓨팅의 가장 큰 특징은 탄력적이라는 면에 있으며, 과거 보안적인 문제도 점차 해결되어 가고 있습니다.\n하단의 개념들은 현재 클라우드 컴퓨팅의 핵심이 되는 개념들을 나열한 것입니다.\n내결함성 (Fault tolerance) : 특정 문제가 발생시에 에러를 복구하는 정도 고 가용성 (High availability) : 문제가 발생시에도 적은 다운타임으로 서비스를 제공할 수 있는 개념 재해 복구 (Disaster recovery) : 물리적으로 자연재해의 영향을 미치지 않는 장소에 서비스에 영향을 주지 않도록 구성하는 개념 확장성 (Scalability) : 서비스의 추가 자원이 필요할 시 보다 쉽게 자원을 추가하여 서비스를 제공할 수 있는 개념 민첩성 (Agility) : 특정 서비스의 필요 자원에 따라 자원을 추가\u0026 감소를 빠른 속도로 이루는 개념 탄력성 (Elasticity) : 확장성과 비슷하다 할 수 있지만, 확장성과의 차이는 여유분의 자원이 남는 서비스의 자원을 수축하여 자원을 효율적으로 관리할 수 있는 개념 글로벌 지원(Global reach) : 해외의 데이터 센터들을 활용하여 해외의 서비스를 제공할 수 있다는 개념 응답 속도(Customer latency) : 해외의 여러 엔드 유저들에게도 적절한 리소스를 배치시켜 빠른 속도로 서비스를 제공해 줄 수 있는 개념 예측 비용(Predictive cost): 클라우드는 기본적으로 사용한 만큼의 과금할 수 있는 구조로 이루어져있어, 적절하게 비용의 에측이 가능하다는 개념 보안 (Security) : 각 나라의 지역적인 규제나 정책, 준수사항 등을 만족하도록 구현한다는 개념 규모의 경제 규모 경제의 개념은 작은 규모로 운영하는 것에 비해 큰 규모로 운용할 때 효율적으로 작업을 수행할 수 있는 능력을 제공하는 개념입니다.\n클라우드는 큰 규모로 진행하기에, 낮은 비용 대비 고효율의 제공이 가능하는 것이 가능합니다.\nCapEx vs OpEx 자본 지출 : Capital Expenditure (CapEx)\n물리적 인프라에 대한 지출을 선불로 지불\n시간이 지남에 따라 세금 계산서에서 비용을 공제\n높은 초기 비용, 투자 가치는 시간이 지남에 따라 감소\n기본적으로 일반적으로 CapEx 온-프레미스 데이터 센터에는 다음과 같은 비용이 포합됩니다.\n서버 비용\n스토리지 비용\n네트워크 비용\n백업 및 보관 비용\n조직 연속성 및 재해 복구 비용\n데이터 센터 인프라 비용\n기술 인력\nOpEx 클라우드 컴퓨팅 비용\n임대 소프트에어 및 사용자 지정된 기능\n고정 하드웨어나 용량 대신 사용/ 수요에 따라 요금을 조정\n사용자 또는 조직 수준의 청구\n이와 같이 CapEx는 시작 단계에서 비용을 계획하기 때문에, 제한된 예산으로 인해 프로젝트를 시작하기 전에 비용을 예측해야 하는 경우에 유용합니다.\n운영 비용: Operational Expenditure (OpEx)\n수요의 따른 시간, 비용 그래프 CapEx와 달리 초기 비용이 아닌, 필요에 따라 서비스 또는 제품에 지출되고 즉시 청구\n같은 해에 세금 계산서에서 비용을 제공\n선 결제 비용이 없고, 종량제 사용\n클라우드의 소비 기반 모델 소비 기반 모델 : Consumption-based model\n선 결제 비용이 존재하지 않음\n비용이 많은 드는 인프라를 구매하고 관리할 필요가 없음음\n필요할 시에만 사용하며, 필요하지 않으면 서비스를 중지할 수 있음\n클라우드 서비스의 종류 퍼블릭 클라우드 클라우드 서비스 또는 호스팅 공급자가 소유\n여러 조직과 사용자에게 리소스와 서비스를 제공\n보안 네트워크 연결을 통해 접근 (일반적으로 인터넷으르 통해 접근)\n중요한 데이터 등의 대한 관리문제가 존재할 수 있음\n특징\nCapEx 없음\n응용 프로그램에 빠르게 엑세스가 가능한 민첩성을 가질 수 있음\n클라우드 공급자가 일정 부분의 책임을 짐\n소비 기반 모델\n제한적인 요소로 인해클라이언트의 요구사항을 충족할 수 없는 경우가 발생할 수 있음\n완전 자유롭게 관리하는 것이 사실상 불가능\n프라이빗 클라우드 클라우드 리소스를 사용하는 조직이 소유 및 운영을 관리\n조직은 자신들만의 데이터 센터에 클라우드 환경을 구축\n조직에게 본인들이 제공하는 서비스를 운영할 책이 존재\n특징\n제어력\n보안\n초기의 CapEx 비용이 존재하며, 유지 관리를 위해 하드웨어를 추가구매할 경우가 발생\n추가구매로 인한 민첩성이 제한을 받음\n프라이빗 클라우드는 기본적으로 전문적인 IT 기술 및 전문 지식이 필요\n하이브리드 클라우드 공용 및 사설 클라우드를 결합하여 응용 프로그램이 가장 적합한 위치에서 실행되도록 하는 클라우드 서비스\n비용적인 측면에서는 높을 수 있음\n특징\n유동성\n준수성\n설정 및 관리가 복잡해질 수 있으며 전문적인 인재가 필요\n클라우드 서비스의 유형 공동 관리 책임 (Shared responsibility model) 공동 관리 책임의 범위 On Premises 온-프레미스는 모든 책임을 사용자가 가짐\n이아스는 네트워킹, 스토리지, 컴퓨팅 외에는 모든 책임을 사용자가 가짐\nPaaS\n사스는 대부분 클라우드 벤더에서 책임을 지며 데이터와 접근권한만을 책임을 가짐\nInfrastructure as a Server (IaaS) 가장 기본적인 클라우드 컴퓨텅 서비스의 범주 (흔히 우리가 아는 Vm)\n클라우드 공급자로부터 가상머신, 스토리지, 네트워크 및 운영 체제를 대여\n네트워크를 통해 프로비저닝 받는 서비스\nPlatform as a Server (PaaS) 소프트웨어 응용 프로그램을 개발, 테스트 미 배포하기 위한 환경을 제공하는 서비스\n기본적으로 개발환경과 비슷하며 인프라 관리에 신경쓰지 않고, 응용 프로그램을 신속하게 만들 수 있도록 제공하는 서비스\nSoftware as a Server (SaaS) 최종 사용자를 위해 중앙에서 호스팅 되고 관리되는 소프트웨어\n인터넷을 통해 클라우드 기반 앱에 연결하여 사용\n각 클라우드 유형의 특징 IaaS(유동성)\nIaaS는 가장 유연한 클라우드 서비스\n애플리케이션을 실행하는 운영체제를 구성하고 관리할 수 있는 제어력을 가지고 있음\nPaaS(생산성)\n사용자는 응용 프로글매 개발에만 집중할 수 있음\n플랫폼 관리는 클라우드 공급자가 처리\nSaaS(종량제)\n사용자는 자신의 서비스에서 사용하는 소프트웨어에 대한 비용만 지불 ","#":"","az-900--cloudcomputing#\u003cstrong\u003eAz-900 : CloudComputing\u003c/strong\u003e":"","클라우드-서비스의-유형#\u003cstrong\u003e클라우드 서비스의 유형\u003c/strong\u003e":"","클라우드-서비스의-종류#\u003cstrong\u003e클라우드 서비스의 종류\u003c/strong\u003e":"","클라우드-컴퓨팅#\u003cstrong\u003e클라우드 컴퓨팅\u003c/strong\u003e":""},"title":"Az-900 : CloudComputing"},"/system/azure/azuretraining/azure01/":{"data":{"":"Azure 글로벌 인프라 아키텍처 Microsoft Geography Azure는 전 세계를 지정학적 경계 또는 국가 경계로 정의되는 지리적 위치로 분할합니다.\nAzure Geography란 일반적으로 데이터 상주성 및 규정 준수 경계를 유지하는 두 개 이상의 Azure 지역을 포함하고 있는 별도의 시장을 의미합니다.\nAzure Geography을 사용하면 아래와 같은 이점을 가질 수 있습니다.\n지리적 위치를 통해 특정 데이터 상주성 및 규정 준수 요구 사항이 있는 고객은 데이터와 애플리케이션을 가깝게 유지할 수 있습니다.\n지리적 위치는 지리적 경계 내에서 데이터 상주성, 주권, 규정 준수 및 복원력 요구 사항이 지켜지도록 보장합니다.\n지리적 위치는 전용 대용량 네트워킹 인프라 전체에 걸쳐 발생하는 Azure 지역 전체 장애를 견디는 내결함성을 갖고 있습니다.\nAzure의 Geography는 아래와 같이 4개로 분류됩니다.\n아메리카\n유럽\n아시아 태평양\n중동 및 아프리카\nAzure Region Azure Region은 서로 가까운 곳에 있고 대기 시간이 짧은 네트워크를 통해 연결된 데이터 센터를 하나 이상 포함하고 있는 지리적 영역을 의미합니다.\nAzure는 전 셰계에 위차한 데이터 센터로 구성\n최소 한 개 이상의 데이터 센터를 포함\n데이터 센터는 서로 근접한 위치에 있어 네트워크의 대기 시간이 짧은 특징을 가짐\nRegion은 각 지역에 맞는 Geography에 구성됩니다.\nRegion Pairs 각 Azure 지역은 300마일 이상 떨어져 있는 동일한 지리적 위치(예: 미국, 유럽 또는 아시아) 내의 다른 Azure 지역과 항상 쌍을 이룹니다.\nRegion Pairs를 통해 한 지리적 위치에서 가상 머신 스토리지 같은 리소스를 복제할 수 있으며, 이렇게 하면 두 Azure 지역에 동시에 영향을 주는 자연재해, 내전, 정전 또는 물리적 네트워크 중단 등의 이벤트 때문에 서비스가 중단될 가능성을 줄일 수 있습니다.\n각 Azure region은 다른 region과 페어링 연결되어 있습니다.\n특정 서비스는 paired region 사이의 자동 복제기능을 제공\nregion 장애 시, paired 된 리전을 복제하는 것이 우선시 되어짐\n특수 Azure Region\nUS DoD 중부, US Gov 버지니아, US Gov 아이오와 등 : 미국 정부 기관 및 파트너를 위한 물리적 및 논리적 네트워크로 격리된 Azure 인스턴스입니다. 이러한 데이터 센터는 선별된 미국인이 운영하며 추가 규정 준수 인증서를 포함하고 있습니다.\n중국 동부, 중국 북부 등 : 이러한 지역은 Microsoft 및 21Vianet 간의 고유한 파트너십을 통해 사용할 수 있으며, Microsoft에서 데이터 센터를 직접 관리하지 않습니다.\nRegion을 사용하는 이유\n보다 쉽게 확장이 가능하게 하는 확장성\n한 리전에 문제가 발생시 다른 리전에서 서비스가 가능토록 하는 가용성\n한국 중부, 한국 남부는 짝꿍 리전으로 한 리전이 오류가 발생 시 다른 리전에서 대신 서비스하도록 설정되어 있음\n가용성 옵션 ( Availability Options ) 가용성 영역은 Azure 지역 내에서 물리적으로 분리된 데이터 센터입니다.\nVm을 나눠서 생성함으로써, 끊어지지 않는 연속적인 서비스가 가능토록 하는 개념\n프로미엄 스토리지를 사용하면 99.9%의 연결성을 보장해주지만, 일반적인 경우에는 가용성 집합 혹은 가용성 영역을 생성하는 것이 권장되어짐\n가용성 집합 (Availability sets)\n유지 관리 혹은 하드웨어 오류 발생 시에도 응용 프로그램을 온라인 상태로 유지하는 서비스\nUpdate domains (US) : 예약된 유지관리, 성능 또는 보안 업데이트는 업데이트 도메인을 통해 순서가 정해짐\nFault domains (FD) : 데이터 센터 내에서 여러 하드웨어서 워크로드르 물리적으로 분\n가용성 영역 ( Availability zones )\nAzure Region 내에서의 물리적으로 분리된 영역\n가용성 영역보다 한 단게 확장된 개념\n하나 혹은 이상의 데이터센터로 구성되고 독립된 전원, 쿨링, 네트워크를 가짐\n독립된 영역으로 동작\nAzure SLA (서비스 수준 계약) Microsoft는 포괄적인 운영 정책, 표준 및 관례를 준수함으로써 고객에게 우수한 품질의 제품 및 서비스를 제공하기 위해 최선을 다하고 있습니다.\nAzure 제품 및 서비스의 SLA는 세 가지 주요 특징을 가지고 있습니다.\n성능 목표\nSLA는 Azure 제품 또는 서비스의 성능 목표를 정의합니다. SLA가 정의하는 성능 목표는 각 Azure 제품 및 서비스로 한정됩니다. 예를 들어 일부 Azure 서비스의 성능 목표는 작동 시간 보증 또는 연결률로 표현됩니다. 작동 시간 및 연결 보증\n- **일반적인 SLA는 해당하는 각 Azure 제품 또는 서비스의 성능 목표 약정을 99.9%(\"3개의 9\")에서 99.999%(\"5개의 9\") 사이에서 지정합니다. 이러한 목표는 서비스의 작동 시간 또는 응답 시간 같은 성능 기준에 적용할 수 있습니다.** - SLA % |\t주간 가동 중지 | 시간\t월간 가동 중지 시간 |\t연간 가동 중지 시간 — | — | — | — 99 | 1.68시간 | 7.2시간 | 3.65일 99.9 | 10.1분 | 43.2분 | 8.76시간 99.95 | 5분 | 21.6분 | 4.38시간 99.99 | 1.01분 | 4.32분 | 52.56분 99.999 | 6초 | 25.9초 | 5.26분\n서비스 크레딧\n또한 SLA는 Azure 제품 또는 서비스가 관련 SLA 사양을 수행하는 데 실패할 경우 Microsoft에서 어떻게 대응할 것인지를 설명합니다.\n99.9 -\u003e 10\n99 -\u003e 25\n95 -\u003e 100\nAzure SLA를 통한 앱 안정성 향상 SLA를 사용하여 Azure 솔루션이 클라이언트와 사용자의 비즈니스 요구 사항 및 수요를 얼마나 충족하는지 평가할 수 있습니다.\n고유한 SLA를 만들어서 고객의 특정 Azure 애플리케이션에 맞는 성능 목표를 설정할 수 있습니다. 이 접근 방식을 애플리케이션 SLA라고 합니다.\nSLA를 사용한 이점\n앱 요구 사항 이해\n효율적이고 신뢰할 수 있는 Azure 솔루션을 빌드하려면 워크로드 요구 사항을 알아야 합니다. 복원력\n복원력은 오류를 복구하여 계속 작동하는 시스템 기능입니다. 오류를 방지하는 것이 아니라 가동 중지 또는 데이터 손실을 방지하는 방법으로 오류에 대응하는 것입니다. 비용 및 복잡성과 고가용성\n가용성이란 시스템이 정상적으로 작동하는 시간을 말합니다. 가용성을 최대화하려면 가능한 서비스 오류를 방지하는 수단을 구현해야 합니다. 애플리케이션 SLA를 정의할 때 고려할 사항\n플리케이션 SLA에서 정의하는 성능 목표가 4개의 9(99.99%)인 경우 수동 작업으로 오류를 복구하면 SLA를 충족하기에 충분하지 않을 수 있습니다. Azure 솔루션이 자체적으로 진단하고 자체적으로 복구해야 합니다.\n4개의 9보다 높은 SLA 성능 목표를 충족하도록 신속하게 오류에 대응하기는 쉽지 않습니다.\n애플리케이션 SLA 성능 목표를 측정할 시간 범위를 신중하게 고민해야 합니다. 시간 범위가 짧을수록 허용 오차도 작습니다. 애플리케이션 SLA를 시간 단위 또는 일 단위 가동 시간으로 정의하는 경우 허용 오차가 작으면 성능 목표를 달성하지 못할 수 있다는 점을 이해해야 합니다.","#":"","azure-글로벌-인프라-아키텍처#\u003cstrong\u003eAzure 글로벌 인프라 아키텍처\u003c/strong\u003e":""},"title":"Az-900 : Region"},"/system/azure/azuretraining/azure02/":{"data":{"":"Azure 관리옵션 Azure 관리옵션의 다양한 종류 Azure의 관리는 다양한 도구 및 플랫폼을 사용하여 Azure을 구성하고 관리할 수 있습니다.\n명령줄, 언어별 SDK, 개발자 도구, 마이그레이션 도구 등에 제공되는 여러 도구가 있습니다.\n하단은 가장 일상적으로 사용되는 관리 및 조작에 주로 사용되는 도구들입니다.\nAzure Portal : GUI를 통해 Azure 조작\nAzure PowerShell 및 Azure CLI : 명령줄 및 자동화 기반으로 Azure 조작\nAzure Cloud Shell : 웹 기반 명령줄 인터페이스\n모바일 디바이스에서 리소스를 모니터링하고 관리하기 위한 Azure 모바일 앱\nAzure Portal Azure Portal은 몯느 웹 브라우저를 통해 엑세스할 수 있는 공용 웹 사이트로, Azure 계정으로 로그인하여 Azure의 서비스를 사용, 관리 및 모니터링이 가능합니다. Azure PowerShell Azure PowerShell은 Windows,Linux 또는 macOS에서 실행되는 PowerShell의 플랫폼 간 버전인 Windows PowerShell또는 PowerShell Core를 설치할 수 있는 모듈입니다.\nAzure PowerShell 명령어를 통해 Azure 서비스를 사용, 관리하는 것이 가능합니다.\nNew-AzVM ` -ResourceGroupName \"MyResourceGroup\" ` -Name \"TestVm\" ` -Image \"UbuntuLTS\" ` ... Azure CLI Azure CLI는 Azure에 연결하고 Azure 리소스에서 관리 명령을 실행하는 플랫폼 간 명령줄 프로그램입니다. 플랫폼 간이란 Windows, Linux 또는 macOS에서 실행합니다. az vm create \\ --resource-group MyResourceGroup \\ --name TestVm \\ --image UbuntuLTS \\ --generate-ssh-keys \\ ... Azure Cloud Shell Azure Cloud Shell은 Azure 리소스를 관리하기 위한 인증된 대화형 셸로, 브라우저에서 엑세스할 수 있습니다.\n기본적으로는 Azure CLI로 설정되어 있지만 pwsh를 통해 PowerShell Core로 전환이 가능합니다. 또한 owerShell 환경에는 두 CLI 도구가 사전 설치되어 있으며 여러 도구의 사용 또한 가능합니다.\n개발자 도구\n.NET Core\nPython\nJava\nNode.js\n편집기\n코드(Cloud Shell 편집기)\nvim\nnano\nemacs\n기타 도구\ngit\nmaven\nmake\nnpm\nAzure 모바일 앱 Microsoft Azure 모바일 앱을 사용하면 iOS나 Android 휴대폰 또는 태블릿에서 모든 Azure 계정과 리소스를 액세스, 관리 및 모니터링할 수 있습니다. 설치되면 다음 작업을 수행할 수 있습니다. ","#":"","azure-관리옵션#\u003cstrong\u003eAzure 관리옵션\u003c/strong\u003e":""},"title":"Az-900 : 관리옵션"},"/system/azure/azuretraining/azure03/":{"data":{"":"Azure 학생 계정생성 Azure 학생 계정생성 이번 장에서는 Azure 서비스의 사용을 위한 계정생성에 대해 알아보도록 하겠습니다.\n기본적으로 Azure의 서비스를 사용할 때에는 계정이 필요하며, 여기서 학생의 신분으로 가입을 진행할 경우 여러 혜택을 받을 수 있습니다.\n윈도우 10 edu\n윈도우 Server 2019\nVisual Studio 2017 Enterprise\nSQL Server 2017 Enterprise\nAzure 100$ 크레딧\n이와 같은 혜택을 받기 위해서는 학교 메일 (ac.kr or .edu)의 메일이 필요합니다.\n그럼 학생계정으로 Azure 계정을 생성해보도록 하겠습니다.\nAzure for Students Azure for Students에 접속합니다. 지금 구독하기를 클릭합니다. 이메일 학번과 패스워들를 입력하면 해당 이메일의 학교로 링크가 변경됩니다. ex) 학번@dankook.ac.kr 로그인을 진행합니다. 만약 학교계정이 없으면 일반 계정으로 진행하셔도 상관없읍니다. (학생 해택은 없음) 핸드폰 번호를 기입 후, 문자 혹은 전화를 통해 본인인증을 진행합니다. 회원가입을 진행합니다. 약관의 동의합니다. 하단과 같이 게정이 모두 설정되었다는 화면이 나오면, 생성이 완료된 것입니다. 하단과 같이 VM을 포함한 다수의 서비스를 이용할 수 있습니다. ","#":"","azure-학생-계정생성#\u003cstrong\u003eAzure 학생 계정생성\u003c/strong\u003e":"","azure-학생-계정생성-1#\u003cstrong\u003eAzure 학생 계정생성\u003c/strong\u003e":""},"title":"Azure 학생 계정생성"},"/system/azure/azuretraining/azure12/":{"data":{"":"","#":"Az-900 : 서비스 서비스는 Link를 참조하세요. IOT 솔루션 (Internet of Things) Azure IoT Central 보다 손 쉬운 IOT 서비스 구축을 위한 SaaS 형태의 관리형 서비스, (디바이스 연결, 모니터 그리고 관리 및 확장 지원) Azure IoT Hub 클라우드 기반의 IOT 관리 플랫폼 서비스 (중앙 메시지 허브, 양방향 통신 및 관리) 빅 데이터 분석 솔루션 (Big data and analytics) Azure Sysnapse Analytics 클라우드 기반의 수십 페타의 데이터를 MPP기반으로 빠르게 처리할 수 있는 온 디맨드 분석을 지원하여 인사이트를 찾아낼 수 있는 솔루션 Azure HDInsight 오픈소스 기반의 Hadoop을 관리되는 형태의 서비스를 제공, 쉽고 빠르고 비용 효율적으로 데이터 처리가 가능 인공지능 (Artificial Intelligence Azure Machin Learning service SDK 기반으로 손쉽게 code를 작성하고 train, test, deploy, manage 및 track 할 수 있는 플랫폼을 제공 Azure Machine Learning Studio GUI 기반으로 코드 없이도 손쉽게 ML model을 만들고 테스트를 배퐇라 수 있는 플랫폼을 제공 서버리스 (Serveless computing) Azure Functions 서비스를 위한 코드에 집중 (인프라 및 플랫폼으로 부터의 자유도 확보) Azure Logic Apps 작업 및 비즈니스 프로세스를 자동화하고 오케스트레이션 할 수 있도록 서비스 제공 엔터프라이즈 환경에서 앱, 데이터 긜고 시스템의 통합을 지원 Azure Event Grid 배포와 구독형태의 이벤트 소비를 위한 관리형 엔진 기반의 이벤트 라우팅 서비스 DevOps Azure DevOps services 클라우드 기반의 통합 개발 협업 서비스 (CI/CD) 제공 파이프라인, Git 저장소, 오픈 소스 연계 등 Azure DevTest Labs 쉽고 빠르게 재사용 가능한 템플릿을 통해 배포하여 최신 개발 코드를 배포 및 테스트 가능 Azure 관리 도구 Azure Advisor 배포된 Azure 리소스를 분석하고 가용성, 보안, 성능, 비용측면을 개선하는 방법을 제공 Azure quick start templates 사전에 code로 설정이 되어 있는 탬플릿을 토대로 리소스를 배포할 수 있게 도와주는 도구 JSON 형태를 가지고 있다. PowerShell Window 사용자를 위한 Shell로 Azure 서비스를 관리할 수 있다. Azure CLI BASH Shell을 사용하여 Azure 서비스를 관리할 수 있다. 네트워크 연결 보호 심층보호 (Denfense in depth)\n컴퓨터 시스템의 안전한 보호를 위한 대한 단계적 접근 방식을 구현\n여러 수준의 보호를 제공\n한 레이어에 대한 공격은 후속 레이어에서 격리\n공동 책임 (Shared security)\n고객이 제저아흔 데이터센터에서 클라우드 기반 데이터 센터로 마이그레이션하면 보안에 대한 책임 변경이 발생\n보안은 클라우드 공급자와 고객 간의 공통 관심사가 됨\n방화벽 (Azure Firewall)\n네트워크 리소스를 보호하기 위해 IP주소를 기반으로 서버 엑세스를 허용/ 거부하도록 하는 PaaS 형태의 방화벽 서비스\n인 바운드 및 아웃바운드 트래픽 필터링 규칙 적용\n고 가용성이 내장\n아웃바운드에서만 애플리케이션 레이어서의 프로텍션을 제공\n무제한 클라우드 확정성\nAzure 모니터 로깅을 사용\nAzure Distributed Denial of Service (DDoS)\nDDoS는 지속적인 공격으로 피해자의 리소스를 소모시켜 마비시키는 공격\n서비스 가용성에 영향을 미치기전에 원치 않는 네트워크 트래픽을 제공\n베이직은 기본적으로 제공\n스탠다드는 보다 나은 완화기능 (머신러닝 기반의 어댑티브 튜닝 등)\n네트워크 보안 그룹 (Network Security Groups : NSGs)\nAzure 가상 네트워크에서 Azure 리소스로의 네트어크 트래픽을 필터링\n소스 및 대상 IPㅈ소, 포트 및 프로토콜로 필터링하도록 인 바운드 및 아웃바운드 규칙의 설정이 가능\n필요에 따라 구독 한도 내에서 여러 규칙을 추가가능\nAzure는 세 NSG에 기본적인 기준, 보안 규칙을 적용\n우선 순위가 높은 규칙으로 기본 규칙 재정의 가능\nAzure 네트워크 보안 솔루션 선택 경계 레이어 (Perimeter layer) Azure DDoS 보호 및 Azure 방화벽을 통해 네트워크 경계를 보호 네트워크 레이어 (Networking layer) NSG(네트워크 보안 그룹) 인 바운드 및 아웃바운드 규칙을 사용하여 네트워크 리소스 간에 허용된 트래픽만 하용 핵심 Azure Identity 서비스 인증 및 권한 (Azure Active Deirctory : AAD)\nMicrosoft Azure의 클라우드 기반 신원 확인 및 접근 관리 서비스\n단일인증 (SSO)\n응용 프로그램 관리\nB2B (Federation)\nB2C (Consumer) ID 서비스\n디바이스 관리\n인증 (Authentication)\n리소스에 대한 엑세스를 원하는 사람 또는 서비스를 식별 권한부야 (Authoizotion)\n리소스에 대한 사용권한을 원하는 사람 또는 서비스에게 제공 다단계인증 (Azure Multi-Factor Authentication) 전체 인증을 위해 두 개 이상의 요소를 요구하여 신원 확인에 대한 추가 보안을 제공 보통 세가지 범류로 분류 당신이 알고 있는 것 당신이 가지고 있는 것 당신 임을 증명할 수 있는 것 보안 도구 보안센터 (Azure Security Center)\nAzure 온-프레미스 서비스에 대한 위협 보호 기능을 제공하는 모니터링 서비스\n구성된 설정, 리소스 및 네트워크에 따라 보안 권장사항을 제공\n보안 센터 시용 시나리오\n장애 시, 감지, 평가 및 진단 단계에서 보안 세터를 사용\n감지, 평가, 진단만 모니터링 나머지는 자기자신이 해결\n키 자격증명 (Azure Key Vault) 응용 프로그램 보안을 중앙 집중식 클라우드에 저장하여 액세스 권한을 안전하게 제어 (접근 등에 대한 로깅 및 관리) 비밀번호 관리 키 관리 인증서 관리 하드웨어 보안 모듈 (HSM)을 지원하는 장비에 저장된 비밀번호 정보 가져오기 지원 정보보호 (Azure Information Protecton : AIP)\n레이블을 적용하여 문서 및 전자 메일을 분류하고 보호\n관리자가 정의한 규칙 및 조건을 자동으로 사용 가능\n사용자가 수동으로 적용, 활용지원\n권장 사항에 따라 자동 및 수동 방법을 결합할 수 있음\n고급위협보호 (Azure Advanced Threat Protection : ATP)\n지능형 위협, 손상된 ID 및 악의적인 내부자 작업을 식별, 탐지 및 조사하기 위한 클라우드 기반 보안 솔루션\nPortal : 의심스러운 활동을 모니터링하고 대응하기 위한 전용 포털\nSensorts : 도메인 컨트롤러에 직접 설치\nCloud Service : Azure 인프라에서 실행\nAzure 커버넌스 방법론 정책 (Azure Policy)\nAzure 리소스에 대한 규칙을 적용하기 위해 정책을 사용하여 회사 표준 및 서비스 수준 계약(SLA)을 준수할 수 있음\n정책을 준수하지 않는 Azure 리소스를 평가하고 식별\n스토리지, 네트워킹, 컴퓨팅, 보안 센터 및 모니터링과 같은 범주에서 기본 제공 정책 및 이니셔티브 정의를 제공\n정챍 정의 –\u003e 리소스 정책 할당 –\u003e 평가 검토\n정책 이니셔티브 (Policy Initiatives) 이니셔티브는 Azure 정책과 함께 작동 역할 기반 액세스 제어 (RBAC)\nAzure 리소스에 대한 세분화된 엑세스 관리 제어 기능\n팀 내에 책임을 분리하여 그 작업을 수행하는 데 필요한 사용자에게만 적당한 권한을 부여\nAzure 포털 및 리스스의 접근을 허용 및 거부하도록 설정 지원\n잠금 (Resource locks)\n실수로 삭제하거나 수정하지 않도록 Azure 리소스를 보호\nAzure Portal 에서 구독, 리소스 구룹 또는 개별 리소스 수준에서 잠금을 관리 (상속 지원)\nAzure Blueprints\nAzure 리소스 및 정책들을 즉시 재생성 할 수 있도록 재사용 가능한 환경 정의를 만들 수 있다\n기본 제공 도구 및 아티팩트를 사용하여 배포를 감사하고 추적하고 규정 준수를 유지\nBlueprint를 특정 Azure DevOps 빌드 아디펙트 및 릴리스 파이프라인과 연결하여 엄격한 추적을 수행\n모니터링 및 리포트 태그 (Tags)\nAzure 리소스에 대한 메타 데이타 지원\n{키-값} 쌍으로 구성\n논리적으로 리소스를 분류하기 위해 활용\n청구 혹은 관리용 데이터 분석에 용이\n모니터 (Azure Monitor) 클라우드 및 온-프레미스 환경에서 원격 데이터를 수집, 분석 및 사용하여 애플리케이션의 가용성과 성능을 극대화 서비스건강 (Azure Service Health) 규정 준수 약관 및 요구사항 Microsoft는 다른 클라우드 서비스 공급자보다 가장 포괄적인 compliance offerings (인증 및 증명)을 제공 Azure 가격 책정 및 지원 Azure 구매 및 관리 https://azure.microsoft.com/ko-kr/support/plans/ https://azure.microsoft.com/ko-kr/resources/knowledge-center","az-900--서비스#\u003cstrong\u003eAz-900 : 서비스\u003c/strong\u003e":"","azure-관리-도구#\u003cstrong\u003eAzure 관리 도구\u003c/strong\u003e":""},"title":"Az-900 : 서비스"},"/system/azure/microsoftazure/":{"data":{"":"Azure Azure Docs\nAzure Computing\nAzure Networking\nAzure Mobile\nAzure DataBase\nAzure Storage\nAzure Web\nAzure IOT\nAzure BigData\nAzure AI\nAzure DevOps\nAzure HybridCloud\nAzure Training\nAz-900 : CloudComputing\nAz-900 : Region\nAzure","azure#\u003cstrong\u003eAzure\u003c/strong\u003e":""},"title":"Azure docs"},"/system/azure/microsoftazure/azure00/":{"data":{"":"Azure Computing Azure Computing은 클라우드 기반의 애플리케이션을 실행하기 이한 주문형 컴퓨팅 서비스로 가상 머신 및 컨테이너를 통해 멀티 코어 프로세서, 슈퍼 컴퓨팅 리소스를 제공합니다.\nAzure Computing은 애플리케이션 및 서비스를 호스팅하는 다양한 옵션을 제공합니다.\nAzure Virtual Machines 가상 머신 또는 VM은 무리적 컴퓨터의 소프트웨어 에뮬레이션입니다.\nVM에는 가상 프로세서, 메모리, 스토리지 및 네트워킹 리소스가 포함됩니다.\nAzure Virtual Machine Scale Sets Azure에서 호스팅되는 Windows 또는 Linux VM의 크기 조정 서비스로 부하 분사된 동일한 가상 머신 그룹을 만들고 관리할 수 있습니다.\nScale Sets를 사용하면 몇 분 안에 많은 수의 가상 머신을 중앙에서 관리, 구성 및 업데이트 하므로 고가용성 애플리케이션을 제공할 수 있습니다.\nAzure Kubernetes Service 컨테이너화된 서비스를 실행하는 VM 클러스터 관리를 사용하도록 설정 Azure Service Fabric 분산형 시스템 플랫폼. Azure 또는 온-프레미스에서 실행 Azure Batch Azure Batch를 통해 수십, 수백 또는 수천 개의 가상 머신으로 확장할 수 있을 뿐 아니라 대규모 작업을 예약하고 컴퓨팅을 관리하는 서비스 입니다.\n병렬 및 고성능 컴퓨팅 애플리케이션을 위한 관리 서비스\nAzure Container Instances 애플리케이션을 실행하기 위한 사상화 환경으로, 컨테이너는 가상 머신과 마찬가지로 호스트 운영 체제에서 실행됩니다.\n하지만 서버 또는 VM을 프로비저닝하지 않고 Azure에서 컨테이너화된 앱 실행하며 기존 호스트 OS를 사용하는 데 필요한 라이브러리와 구성 요소를 포함합니다.\nAzure Functions 이벤트 기반의 서버리스 컴퓨팅 서비스로 클라우드에 호스트된 실행 환경이지만, 기본 호스팅 환경을 완전히 추상화합니다.\n가장 큰 특징으로는 인프라 구성 또는 유지 관리가 필요하지 않거나 허용되지 않습니다.\nAzure App Service Azure App Service를 사용하면 인프라를 관리할 필요 없이 원하는 프로그래밍 언어로 웹앱, 백그라운드 작업, 모바일 백 엔드 및 RESTful API를 빌드하고 호스트할 수 있습니다. ","#":"","azure-computing#\u003cstrong\u003eAzure Computing\u003c/strong\u003e":""},"title":"Azure Computing"},"/system/azure/microsoftazure/azure01/":{"data":{"":"Azure Networking Azure Networking은 컴퓨팅 리소스를 연결하고 애플리케이션에 대한 액세스를 제공하는 것으로 Microsoft Azure 데이터 센터의 서비스 및 기능을 외부 환경에 연결하는 다양한 옵션이 포함되어 있습니다. Azure Virtual Network 수신 VPN(가상 사설망) 연결에 VM을 연결합니다. Azure Load Balancer Azure Load Balancer는 사용자를 위한 유지 관리를 Azure에서 담당하는 서비스 입니다.\n인바운 및 아웃바운드의 시나리오에 맟춰 해당 TCP, UDP 포트로 접근하는 각 트래픽을 부하분산합니다.\n단, 가상 머신에서 일반저인 부하 분산 장치를 소프트웨어를 수동으로 구성하는 경우 추가로 시스템을 유지 관리해야 하는 단점이 존재합니다.\nAzure Application Gateway Azure Applicatin Gateway는 모든 트래픽이 HTTP인 경우 URL 기반의 라우팅 규칙을 통해 부하 분산을 진행합니다.\nAAG의 장점은 하단과 같습니다..\n쿠키 선호도\n동일한 백 엔드 서버에서 사용자 세션을 유지하려는 경우에 유용합니다. SSL 지원\nSSL 인증서를 통해 암호화가 가능합니다. 웹 애플리케이션 방화벽\nWAF를 지원합니다. URL 규칙 기반 경로\nURL 패턴, 대상 IP 주소 및 포트에 해당하는 원본 IP 주소 및 포트에 따라 트래픽을 라우팅 할 수 있습니다. HTTP 헤더 수정\n각 요청의 인바운드 및 아웃바운드 HTTP 헤더에서 저옵를 추가하거나 제거가 가능합니다. Azure VPN Gateway 고성능 VPN 게이트웨이를 통한 Azure 가상 네트워크에 액세스합니다. Azure DNS Azure DNS는 사용자에게 친숙한 Domain을 통해 해당 IP 주소를 매핑하는 방법입니다.\nAzufre DNS는 매우 빠른 DNS 응답과 매우 높은 도메인 가용성을 제공합니다.\nAzure Content Delivery Network 전 세계 고객에게 고대역폭 콘텐츠를 제공합니다. Azure DDoS Protection Azure에서 호스트되는 애플리케이션을 DDoS(배포된 서비스 거부) 공격으로부터 보호합니다. Azure Traffic Manager Traffic Manager는 사용자에게 가장 가까운 DNS 서버를 사용하여 사용자 트래픽을 전역적으로 분산된 엔드포인트로 보냅니다.\n전 세계 Azure 지역에 네트워크 트래픽을 분산합니다.\nAzure ExpressRoute 고대역폭 전용 보안 연결을 통해 Azure에 연결합니다. Azure Network Watcher 시나리오 기반 분석을 사용하여 네트워크 문제를 모니터링하고 진단합니다. Azure Firewall 확장성에 제한이 없고 보안 수준이 높은 고가용성 방화벽을 구현합니다. Azure 가상 WAN 로컬 사이트와 원격 사이트를 연결하는 통합 WAN(광역 네트워크)을 구축합니다. ","#":"","azure-networking#\u003cstrong\u003eAzure Networking\u003c/strong\u003e":""},"title":"Azure Networking"},"/system/azure/microsoftazure/azure02/":{"data":{"":"Azure Mobile Azure Mobile은 개발자가 iOS, Android 및 Windows 앱용 모바일 백 엔드 서비스를 쉽고 빠르게 만들 수 있게 해줍니다. Azure Mobile의 기능은 다음과 같습니다. 오프라인 데이터 동기화\n온-프레미스 데이터 연결\n푸시 알림 브로드캐스트\n비즈니스 요구 사항과 일치하도록 자동 크기조정","azure-mobile#\u003cstrong\u003eAzure Mobile\u003c/strong\u003e":""},"title":"Azure Mobile"},"/system/azure/microsoftazure/azure03/":{"data":{"":"Azure Storage Azure Storage는 기본적인 스토리지 서비스를 제공하며 다음과 같은 특성을 가지고 있습니다..\n중복 및 복제 기능을 갖추고 있어 내구성과 가용성이 뛰어납니다.\n자동 암호화와 역할 기반 액세스 제어를 통해 보안을 유지합니다.\n사실상 스토리지에 제한이 없으므로 확장성이 뛰어납니다.\n유지 관리 및 사용자에 대한 중요한 문제를 관리하고 처리합니다.\nHTTP 또는 HTTPS를 통해 전 세계 어디에서든 액세스할 수 있습니다.\nAzure Blob Storage 비디오 파일이나 비트맵 같은 대규모 개체를 위한 스토리지 서비스 Azure File 스토리지 파일 서버처럼 액세스하고 관리할 수 있는 파일 공유 Azure Queue 스토리지 애플리케이션 간 메시지를 큐에 넣고 안정적으로 전달하기 위한 데이터 저장소 Azure Table 스토리지 스키마와 관계없이 비정형 데이터를 호스팅하는 NoSQL 스토리지 ","#":"","azure-storage#\u003cstrong\u003eAzure Storage\u003c/strong\u003e":""},"title":"Azure Storage"},"/system/azure/microsoftazure/azure04/":{"data":{"":"Azure Web Azure Web 에는 웹앱 및 HTTP 기반 웹 서비스의 빌드 및 호스트에 대한 최고 수준의 지원이 포함되어 있습니다. Azure App Service 강력한 클라우드 웹 기반 앱을 신속하게 만들기 Azure Notification Hubs 원하는 백 엔드에서 원하는 플랫폼으로 푸시 알림을 전송할 수 있습니다. Azure API Management 개발자, 파트너 및 직원에게 API를 안전하게 대규모로 게시할 수 있습니다. Azure Cognitive Search 완전 관리형 SaaS(Search-as-a-Service)입니다.\nAzure App Service의 Web Apps 기능 중요 업무용 웹앱을 대규모로 만들고 배포할 수 있습니다.\nAzure SignalR Service 실시간 웹 기능을 쉽게 추가할 수 있습니다. ","#":"","azure-web#\u003cstrong\u003eAzure Web\u003c/strong\u003e":""},"title":"Azure Web"},"/system/azure/microsoftazure/azure05/":{"data":{"":"Azure IoT Azure에서 IoT를 위한 엔드투엔드 솔루션을 지원하고 구동할 수 있는 여러 서비스가 있습니다. IoT Central 대규모 IoT 자산의 연결, 모니터링 및 관리를 도와주는, 완전히 관리되는 글로벌 IoT SaaS(Software-as-a-Service) 솔루션 Azure IoT Hub 수백만 개의 IoT 디바이스 간의 안전한 통신 및 모니터링을 제공하는 메시징 허브 IoT Edge 데이터 분석 모델을 IoT 디바이스로 직접 푸시하여 클라우드 기반 AI 모델을 참조할 필요 없이 상태 변경에 신속하게 대응할 수 있습니다. ","#":"","azure-iot#\u003cstrong\u003eAzure IoT\u003c/strong\u003e":""},"title":"Azure IoT"},"/system/azure/microsoftazure/azure06/":{"data":{"":"Azure BigData Microsoft Azure는 빅 데이터 및 분석 솔루션을 제공하기 위해 광범위한 기술 및 서비스를 지원합니다. Azure Synapse Analytics MPP(대규모 병렬 처리)를 활용하여 페타바이트 단위의 데이터에서 복잡한 쿼리를 빠르게 실행하는 클라우드 기반 EDW(Enterprise Data Warehouse)를 사용하여 대규모로 분석 실행 Azure HDInsight 클라우드의 관리형 Hadoop 클러스터를 사용하여 대량의 데이터 처리 Azure Databricks Azure의 다른 빅 데이터 서비스와 통합할 수 있는 Apache Spark 기반의 공동 작업용 분석 서비스입니다. ","#":"","azure-bigdata#\u003cstrong\u003eAzure BigData\u003c/strong\u003e":""},"title":"Azure BigData"},"/system/azure/microsoftazure/azure07/":{"data":{"":"Azure AI 클라우드 컴퓨팅과 관련된 AI는 광범위한 서비스에 기반을 두고 있으며, 그 중 Machine Learning이 핵심입니다. Machine Learning은 컴퓨터에서 기존 데이터를 사용하여 미래 동작, 결과 및 추세를 예측하는 데이터 과학 기술입니다. Machine Learning을 사용하면 컴퓨터에서 명시적으로 프로그래밍하지 않고 학습합니다. Azure Machine Learning 서비스 기계 학습 모델의 개발, 교육, 테스트, 배포, 관리 및 추적에 사용할 수 있는 클라우드 기반 환경입니다. 모델을 자동으로 생성하여 사용자에 맞게 조정할 수 있습니다. 이를 사용하면 로컬 머신의 학습을 시작한 다음, 클라우드로 확장할 수 있습니다. Azure Machine Learning Studio 미리 빌드된 기계 학습 알고리즘 및 데이터 처리 모듈을 사용하여 기계 학습 솔루션을 빌드, 테스트, 배포할 수 있는 끌어서 놓기 방식의 시각적 공동 작업 영역 Cognitive Services 밀접한 관련이 있는 제품 세트 Vision 사진과 동영상의 스마트한 식별, 캡션, 인덱싱, 중재를 수행하는 이미지 처리 알고리즘 Speech 음성을 텍스트로 변환하거나, 음성을 인증에 사용하거나, 앱에 화자 인식을 추가 지식 매핑 지능형 추천 및 의미 체계 검색 등의 작업을 해결하기 위해 복잡한 정보와 데이터를 매핑 Bing Search Add Bing Search API를 앱에 추가하고 단일 API 호출 기능을 활용하여 수십억 개의 웹 페이지, 이미지, 동영상 및 뉴스를 철저히 검색하는 기능을 활용가능 자연어 처리 미리 빌드된 스크립트를 사용하여 자연어를 처리하고, 감정을 평가하고, 사용자가 원하는 것을 인식 ","#":"","azure-ai#\u003cstrong\u003eAzure AI\u003c/strong\u003e":"","cognitive-services#\u003cstrong\u003eCognitive Services\u003c/strong\u003e":""},"title":"Azure AI"},"/system/azure/microsoftazure/azure08/":{"data":{"":"Azure DevOps Azure DevOps Services를 사용하여 애플리케이션에 연속 통합, 제공 및 배포를 제공하는 빌드 및 릴리스 파이프라인을 만들 수 있습니다. 리포지토리 및 애플리케이션 테스트를 통합하고, 애플리케이션 모니터링을 수행하고, 빌드 아티팩트로 작업할 수 있습니다. Azure DevOps Azure DevOps Services(이전 명칭: Visual Studio Team Services 또는 VSTS)는 고성능 파이프라인, 무료 비공개 Git 리포지토리, 구성 가능한 Kanban 보드, 광범위한 자동 및 클라우드 기반 부하 테스트를 비롯한 개발 협업 도구를 제공합니다. Azure DevTest Labs 배포 파이프라인에서 바로 애플리케이션을 테스트하거나 시연하는 데 사용할 수 있는 주문형 Windows 및 Linux 환경을 신속하게 만듭니다. ","#":"","azure-devops#\u003cstrong\u003eAzure DevOps\u003c/strong\u003e":""},"title":"Azure DevOps"},"/system/azure/microsoftazure/azure10/":{"data":{"":"Azure DataBase Azure DataBase는 기존 스토리지 문제를 해결하기 위해 클라우드에 데이터를 저장하는 것을 고려합니다.\n하지만 보안, 백업 및 재해 복구에 대한 우려가 존재합니다.\nAzure에서는 다양한 형식과 볼륨의 데이터를 저장하도록 여러 데이터베이스 서비스를 제공합니다.\nAzure Database의 특징 Azure Database의 이점\n자동화된 백업 및 복구\n전 세계에서 복제\n데이터 분석 지원\n암호화 기능\n다양한 데이터 형식\n가상 디스크의 데이터 스토리지\n스토리지 계층\n데이터 형식\n정형 데이터 (Structured data)\n정형 데이터는 스키마를 준수하는 데이터이므로 모든 데이터에 동일한 필드 또는 속성이 존재합니다.\n정형 데이터는 테이블의 한 행을 다른 테이블의 또 다른 행에 있는 데이터와 연결하는 방법을 나타내며, 이를 관계형 데이터라고도 합니다.\n반정형 데이터 (Semi-structured data)\n반정형 데이터는 테이블, 행, 열에 제약받지 않으며, 대신 태그 혹은 키 값을 사용합니다.\n반정형 데이터는 비관계형 데이터 혹은 NoSQL이라고도 합니다.\n비정형 데이터 (Unstructured data)\n비정형 데이터는 지정된 구조가 없는 데이터를 포괄합니다.\n데이터의 종류에 대한 제한이 없다는 것은 PDF, JPG, JSON와 같은 형식을 모두 포함하고 있습니다.\n스토리지 서비스 암호화\nAzure는 암호화 및 복제 기능을 통해 봉나 및 고가용성을 데이터에 제공합니다.\n미사용 데이터에 대한 Azure SSE (스토리지 서비스 암호화)를 사용하면 조직의 보안 및 규정준수를 충족하도록 데이터를 보호할 수 있습니다.\n클라이언트 쪽의 암호화를 통해 클라이언트 라이브러이에 데이턱 ㅏ이미 암호화되어 있어, 검색 중에 이를 해독합니다.\n스토리지 가용성 복제\n복제 유형은 스토리지 계정을 만들 때 설정됩니다.\n복제 기능은 데이터가 내구성이 있으며 항상 사용할 수 있는 지 확인합니다.\nAzure Cosmos DB (NoSQL) Azure Cosmos DB는 글로벌 분산형 데이터베이스 서비스로, 지속적으로 변경되는 데이터를 지원하기 위해 응답성이 뛰어난 Always On 애플리케이션을 빌드할 수 있는 스키마 없는 데이터 (NoSQL)을 지원합니다. Azure SQL Database Azure SQL Database는 안정적으로 Microsoft SQL Server 데이터베이스 엔진으로 관계형 DaaS (Databases as a Service)입니다.\n완전 관리형 데이터베이스로, 인프라를 관리할 필요 없이 선택한 프로그래밍 언어와 옵티마이저를 통해 최적의 빌드가 가능합니다.\n기존 local 환경의 데이터베이스와 마이그레이션이 가능하며 이 때 Microsoft Data Migration Assistant를 사용합니다.\n자동 크기 조정과 필수 인텔리전스, 강력한 보안을 통해 완벽하게 ### 관리되는 관계형 데이터베이스입니다.\nAzure Blob Storage Azure Blob Storage는 비정형 데이터베이스로 포함될 수 있는 데이터의 종류에 제한이 없습니다.\n수천 개의 동시 업로드, 대용량 비디오 데이터, 끊임없이 증가하는 로그 파일을 관리할 수 있으며, 어디서나 인터넷을 통해 연결할 수 있습니다.\n일반적으로 파일 형식으로 제한되지 않으며, 최대 8TB의 가상 머신용 데이터를 저장할 수 있습니다.\nAzure Data Lake Storage Azure Data Lake Storage는 개체 스토리지의 확정성 및 비용 혜택이 빅 데이터 파일 시스템 기능의 안정성 및 성능과 결합되어 있습니다. Azure Files Azure Files는 산업 표준 SMB 프로토콜을 통해 엑세스할 수 있는, 클라우드에서 완전 관리형 파일 공유를 제공합니다.\nAzure File 공유는 Windows, Linux 및 macOS의 클라우드 또는 온-프레미스 배포를 통해 동시에 탑재될 수 있으며 공유 또한 가능합니다.\nAzure Queue Storage Azure Queue Storage는 전 세계 어디에서나 엑세스할 수 있는 많은 수의 메시지를 저장하기 위한 서비스입니다.\n유연한 애플리케이션을 구축하고 기능을 분리하여 대용량 워크로드 전반에서 내구성을 향상 시킬 수 있습니다.\n비동기식 메시지 대기열 기능을 제공합니다.\n기본적으로 하나 이상의 송신기 구성 요소와 하나 이상의 수신기 구성 요소가 존재합니다\nQueue Storage를 사용하여 하단의 작업들의 수행이 가능합니다.\n작업의 백로그를 만들고 다른 Azure 웹 서버 간에 메시지를 전달합니다.\n여러 웹 서버/ 인프라 간에 로드를 배포하고 트래픽 증가를 관리합니다.\n여러 사용자가 동싱에 데이터에 엑세스할 때 구성 요소 오류에 대한 복원력을 빌드합니다.\nDisk Storage Disk Storage는 가상 머신, 애플리케이션 및 기타 서비스가 온-프레미스 시나리오와 마찬가지로 필요에 따라 엑세스하여 사용할 수 있는 디스크를 제공합니다.\nDisk Storage는 데이터를 연결된 가상 하드 디스크에서 영구적으로 저장 및 엑세스가 가능하며, 디스크는 Azure 혹은 사용자가 직접 관리가 가능합니다.\nDisk Storage의 종류에는 SSD, HDD 등 종류가 다양합니다.\nAzure Synapse Analytics 추가 비용 없이 모든 수준에서 필수 보안을 제공하며 완벽하게 관리되는 데이터 웨어하우스입니다. Azure Database Migration Service 애플리케이션 코드변경 없이 클라우드로 데이터베이스를 마이그레이션합니다. Azure Cache for Redis 자주 사용하는 정적 데이터를 캐시하여 데이터 및 애플리케이션 대기 ### 시간을 줄입니다. ","#":"","azure-database#\u003cstrong\u003eAzure DataBase\u003c/strong\u003e":"","disk-storage#\u003cstrong\u003eDisk Storage\u003c/strong\u003e":""},"title":"Azure DataBase"},"/system/ceph/":{"data":{"":"1️⃣ Ceph 개요 및 기본 개념 2️⃣ Ceph 기본 아키텍처 3️⃣ Ceph 설치 및 환경 구성 4️⃣ Ceph RADOS (Reliable Autonomic Distributed Object Store) 5️⃣ Ceph 블록 스토리지 (RBD) 실무 6️⃣ Ceph 파일 시스템 (CephFS) 실무 7️⃣ Ceph 오브젝트 스토리지 (RGW) 실무 8️⃣ Ceph 모니터링 및 성능 최적화 9️⃣ Ceph 장애 대응 및 트러블슈팅 🔟 Ceph과 Kubernetes (Rook-Ceph) 통합 1️⃣1️⃣ Ceph 확장 및 고급 설정 1️⃣2️⃣ Ceph을 활용한 실제 구축 사례 및 Best Practice"},"title":"Ceph"},"/system/ceph/ceph00/":{"data":{"":"","1-ceph란-무엇인가-역사-배경-발전-과정#1️⃣ \u003cstrong\u003eCeph란 무엇인가? (역사, 배경, 발전 과정)\u003c/strong\u003e":"","2-ceph의-주요-특징-고가용성-확장성-자가-복구-자율-관리#2️⃣ \u003cstrong\u003eCeph의 주요 특징 (고가용성, 확장성, 자가 복구, 자율 관리)\u003c/strong\u003e":"","3-기존-스토리지-시스템과의-비교-san-nas-object-storage-vs-ceph#3️⃣ \u003cstrong\u003e기존 스토리지 시스템과의 비교 (SAN, NAS, Object Storage vs Ceph)\u003c/strong\u003e":"","4-ceph의-주요-사용-사례-클라우드-스토리지-컨테이너-스토리지-백업-등#4️⃣ \u003cstrong\u003eCeph의 주요 사용 사례 (클라우드 스토리지, 컨테이너 스토리지, 백업 등)\u003c/strong\u003e":"","5-ceph의-기본-구성-요소-및-전체-아키텍처-개요#5️⃣ \u003cstrong\u003eCeph의 기본 구성 요소 및 전체 아키텍처 개요\u003c/strong\u003e":"1️⃣ Ceph란 무엇인가? (역사, 배경, 발전 과정) Ceph는 분산형 소프트웨어 정의 스토리지(SDS, Software Defined Storage)로서, 높은 확장성과 고가용성을 제공하는 오픈소스 스토리지 솔루션입니다.\n2003년 Sage Weil이 박사 논문 프로젝트로 시작하였으며, 이후 2012년 Inktank라는 회사가 Ceph를 상업적으로 지원하기 시작했습니다.\n2014년 Red Hat이 Inktank를 인수하면서 Ceph는 엔터프라이즈 환경에서도 주요 스토리지 솔루션으로 자리 잡게 되었습니다.\n1.1 Ceph의 주요 발전 과정 2003년: Sage Weil이 Ceph 프로젝트 시작 2012년: Inktank에 의해 상업적 지원 시작 2014년: Red Hat이 Inktank 인수 2016년: Ceph가 OpenStack의 기본 스토리지 솔루션으로 채택됨 현재: 클라우드, 컨테이너, 기업 데이터센터에서 널리 사용됨 2️⃣ Ceph의 주요 특징 (고가용성, 확장성, 자가 복구, 자율 관리) Ceph는 다음과 같은 주요 기능을 제공합니다.\n2.1 고가용성 (High Availability) 데이터를 여러 노드에 분산 저장하여 장애 발생 시에도 지속적인 데이터 접근 가능 OSD (Object Storage Daemon) 장애 시 자동으로 데이터 복구 수행 2.2 확장성 (Scalability) 노드 추가만으로 용량 및 성능 확장 가능 페타바이트(PB)급 데이터도 효율적으로 관리 가능 2.3 자가 복구 (Self-healing) 손상된 데이터를 자동으로 복제하여 원래 상태로 복구 클러스터 내부에서 데이터 무결성 유지 2.4 자율 관리 (Self-managing) CRUSH (Controlled Replication Under Scalable Hashing) 알고리즘을 사용하여 데이터 배치 자동화 별도의 중앙 제어 없이 스토리지 노드 간 균형을 유지 3️⃣ 기존 스토리지 시스템과의 비교 (SAN, NAS, Object Storage vs Ceph) 특징 SAN (Storage Area Network) NAS (Network Attached Storage) Object Storage Ceph 데이터 접근 방식 블록 스토리지 파일 스토리지 오브젝트 기반 블록, 파일, 오브젝트 모두 지원 확장성 제한적 제한적 뛰어남 뛰어남 고가용성 RAID 기반 제한적 뛰어남 뛰어남 비용 높은 초기 비용 중간 수준 저렴함 오픈소스 기반, 저렴 주요 사용처 엔터프라이즈 DB, 가상화 파일 공유 클라우드 스토리지 클라우드, 가상화, 컨테이너 Ceph는 SAN, NAS, Object Storage의 장점을 통합하여 하나의 솔루션에서 다양한 스토리지 요구사항을 충족할 수 있도록 설계되었습니다.\n4️⃣ Ceph의 주요 사용 사례 (클라우드 스토리지, 컨테이너 스토리지, 백업 등) 4.1 클라우드 스토리지 OpenStack 및 Kubernetes 환경에서 백엔드 스토리지로 활용 Amazon S3와 유사한 Object Storage 제공 4.2 컨테이너 스토리지 Kubernetes의 CSI (Container Storage Interface) 드라이버를 통해 컨테이너 환경에서 스토리지 제공 RBD (RADOS Block Device) 및 CephFS를 활용한 고성능 컨테이너 스토리지 구축 가능 4.3 백업 및 재해 복구 (Disaster Recovery) 분산 스토리지 기반의 백업 솔루션으로 활용 가능 다른 데이터센터로의 복제 기능 제공 5️⃣ Ceph의 기본 구성 요소 및 전체 아키텍처 개요 Ceph의 전체 아키텍처는 다음과 같은 주요 구성 요소로 이루어져 있습니다.\n5.1 구성 요소 Monitor (MON): 클러스터 상태 및 메타데이터 관리 Manager (MGR): 모니터링 및 관리 기능 제공 Object Storage Daemon (OSD): 실제 데이터를 저장하는 노드 Metadata Server (MDS): CephFS의 파일 메타데이터 관리 RADOS Gateway (RGW): S3 및 Swift API를 지원하는 오브젝트 스토리지 게이트웨이 5.2 Ceph의 전체 아키텍처 +--------------------+ +--------------------+ | Client | | CephFS Client | +--------------------+ +--------------------+ | | v v +------------------------------------------+ | Monitor (MON) Nodes | +------------------------------------------+ | | v v +------------------------------------------+ | Manager (MGR) Nodes | +------------------------------------------+ | | v v +------------------------------------------+ | OSD Nodes | Metadata Server (MDS) | +------------------------------------------+ Ceph는 클라이언트가 OSD 노드와 직접 통신하도록 설계되어 있어, 병목 없이 고성능 스토리지를 제공합니다."},"title":"Ceph 개요 및 기본 개념"},"/system/ceph/ceph01/":{"data":{"":"","1-ceph-cluster의-주요-컴포넌트#1️⃣ \u003cstrong\u003eCeph Cluster의 주요 컴포넌트\u003c/strong\u003e":"","2-monitor-mon-클러스터-상태-및-quorum-관리#2️⃣ \u003cstrong\u003eMonitor (MON): 클러스터 상태 및 Quorum 관리\u003c/strong\u003e":"","3-manager-mgr-클러스터-메트릭-ui-api-제공#3️⃣ \u003cstrong\u003eManager (MGR): 클러스터 메트릭, UI, API 제공\u003c/strong\u003e":"","4-object-storage-daemon-osd-데이터-저장-복제-및-분배#4️⃣ \u003cstrong\u003eObject Storage Daemon (OSD): 데이터 저장, 복제 및 분배\u003c/strong\u003e":"","5-metadata-server-mds-cephfs-메타데이터-관리#5️⃣ \u003cstrong\u003eMetadata Server (MDS): CephFS 메타데이터 관리\u003c/strong\u003e":"","6-rados-gateway-rgw-오브젝트-스토리지-인터페이스-제공#6️⃣ \u003cstrong\u003eRADOS Gateway (RGW): 오브젝트 스토리지 인터페이스 제공\u003c/strong\u003e":"","7-ceph의-데이터-분산-및-crush-알고리즘#7️⃣ \u003cstrong\u003eCeph의 데이터 분산 및 CRUSH 알고리즘\u003c/strong\u003e":"","8-placement-group-pg-개념과-역할#8️⃣ \u003cstrong\u003ePlacement Group (PG) 개념과 역할\u003c/strong\u003e":"","9-ceph의-데이터-복제-및-erasure-coding#9️⃣ \u003cstrong\u003eCeph의 데이터 복제 및 Erasure Coding\u003c/strong\u003e":"1️⃣ Ceph Cluster의 주요 컴포넌트 Ceph 클러스터는 다양한 컴포넌트로 구성되며, 각 컴포넌트는 역할을 나누어 분산 스토리지를 효율적으로 운영합니다.\n1. Ceph의 주요 컴포넌트 개요 Ceph 클러스터는 아래와 같은 핵심 구성 요소로 이루어져 있습니다.\nMonitor (MON) - 클러스터 상태 및 Quorum 관리 Manager (MGR) - 클러스터 메트릭, UI, API 제공 Object Storage Daemon (OSD) - 데이터 저장, 복제 및 분배 Metadata Server (MDS) - CephFS 메타데이터 관리 RADOS Gateway (RGW) - 오브젝트 스토리지 인터페이스 제공 2️⃣ Monitor (MON): 클러스터 상태 및 Quorum 관리 1. Monitor의 역할 클러스터의 상태를 유지 및 관리 클러스터 구성원(노드) 간 Quorum(과반수 동의) 유지 OSD 및 MDS의 상태 모니터링 2. Quorum이란? Quorum은 클러스터 내에서 과반수 이상의 MON 노드가 활성 상태여야 클러스터가 정상적으로 동작할 수 있도록 보장하는 메커니즘입니다.\n예시:\n3개의 MON이 있는 경우, 최소 2개 이상이 동작해야 함 5개의 MON이 있는 경우, 최소 3개 이상이 필요함 3️⃣ Manager (MGR): 클러스터 메트릭, UI, API 제공 1. Manager의 역할 클러스터 운영을 위한 메트릭 제공 Ceph Dashboard UI 제공 외부 API 연동 지원 (Prometheus, Grafana) 2. Ceph Dashboard 예시 ceph mgr module enable dashboard 위 명령어를 실행하면, Ceph Dashboard를 활성화하여 웹 UI를 통해 클러스터 상태를 모니터링할 수 있습니다.\n4️⃣ Object Storage Daemon (OSD): 데이터 저장, 복제 및 분배 1. OSD의 역할 실제 데이터를 저장하는 데몬 프로세스 데이터 복제 및 Erasure Coding 수행 CRUSH 맵을 사용하여 데이터 분산 2. OSD 프로세스 예제 systemctl status ceph-osd@1 이 명령어를 실행하면 특정 OSD의 상태를 확인할 수 있습니다.\n5️⃣ Metadata Server (MDS): CephFS 메타데이터 관리 1. MDS의 역할 CephFS의 파일 시스템 메타데이터 관리 파일 및 디렉터리 구조 유지 POSIX 호환 파일 시스템 제공 2. MDS 확인 명령어 ceph fs status 이 명령어를 실행하면 CephFS의 상태 및 MDS 상태를 확인할 수 있습니다.\n6️⃣ RADOS Gateway (RGW): 오브젝트 스토리지 인터페이스 제공 1. RGW의 역할 Amazon S3 및 OpenStack Swift 호환 API 제공 Ceph를 오브젝트 스토리지로 사용 가능하게 함 2. RGW 예제 radosgw-admin user create --uid=\"testuser\" --display-name=\"Test User\" 위 명령어를 실행하면 S3와 같은 방식으로 Ceph RGW에서 사용할 수 있는 사용자를 생성할 수 있습니다.\n7️⃣ Ceph의 데이터 분산 및 CRUSH 알고리즘 1. CRUSH (Controlled Replication Under Scalable Hashing) CRUSH는 데이터를 효율적으로 분산 저장하기 위한 Ceph의 핵심 알고리즘입니다.\nCRUSH 알고리즘 특징:\n중앙 집중식 메타데이터 서버 없이 데이터 위치 결정 하드웨어 장애 시 자동으로 데이터 복구 수행 정책 기반 데이터 배치 가능 (SSD, HDD 분리 등) 2. CRUSH 맵 예시 ceph osd crush tree 위 명령어를 실행하면 현재 클러스터의 CRUSH 맵을 확인할 수 있습니다.\n8️⃣ Placement Group (PG) 개념과 역할 1. PG란? PG(Placement Group)는 데이터의 논리적 그룹이며, CRUSH 알고리즘을 통해 OSD에 분배됩니다.\n예시:\n데이터를 직접 OSD에 저장하는 것이 아니라 PG를 통해 저장 PG가 여러 OSD에 복제되어 데이터 보호 2. PG 상태 확인 ceph pg stat 이 명령어를 실행하면 PG의 현재 상태를 확인할 수 있습니다.\n9️⃣ Ceph의 데이터 복제 및 Erasure Coding 1. 데이터 복제 방식 Ceph는 기본적으로 데이터를 여러 OSD에 복제하여 저장 기본 복제 수치는 3개이며, 3개의 OSD에 동일한 데이터를 저장하여 내구성을 확보 2. Erasure Coding (EC) Erasure Coding은 RAID-5, RAID-6과 유사한 방식으로 데이터를 저장하여 저장 효율성을 높이는 방식입니다.\n예제:\nceph osd pool create mypool 8 erasure 위 명령어를 실행하면 Erasure Coding을 적용한 새로운 스토리지 풀을 생성할 수 있습니다."},"title":"Ceph 기본 아키텍처"},"/system/ceph/ceph02/":{"data":{"":"","1-ceph-클러스터-배포-방식-비교#1️⃣ \u003cstrong\u003eCeph 클러스터 배포 방식 비교\u003c/strong\u003e":"","2-ceph-설치-전-사전-준비#2️⃣ \u003cstrong\u003eCeph 설치 전 사전 준비\u003c/strong\u003e":"","3-cephadm을-이용한-클러스터-구축#3️⃣ \u003cstrong\u003eCephadm을 이용한 클러스터 구축\u003c/strong\u003e":"","4-ceph-설정-파일-cephconf-이해-및-튜닝#4️⃣ \u003cstrong\u003eCeph 설정 파일 (ceph.conf) 이해 및 튜닝\u003c/strong\u003e":"1️⃣ Ceph 클러스터 배포 방식 비교 Ceph을 구축하는 방법은 여러 가지가 있으며, 환경에 따라 적절한 배포 방식을 선택해야 합니다.\n1. Manual 설치 개별 노드에 직접 패키지를 설치하고 설정 파일을 수동으로 구성 설치 과정에 대한 완전한 제어 가능 복잡하고 유지보수 부담이 큼 2. Ceph-Ansible Ansible을 사용하여 자동화된 설치 및 설정 적용 대규모 환경에서 반복적인 설치 작업을 단순화 설정이 잘못되면 디버깅이 어려울 수 있음 3. Cephadm 최신 Ceph 배포 방식으로, 컨테이너 기반 관리 지원 ceph orch 명령을 통해 클러스터를 쉽게 관리 가능 지속적인 유지보수 및 확장에 유리 💡 추천: 신규 설치 환경에서는 Cephadm을 사용하는 것이 가장 쉽고 관리가 편리합니다.\n2️⃣ Ceph 설치 전 사전 준비 1. 하드웨어 요구 사항 Ceph 클러스터의 성능과 안정성을 위해 적절한 하드웨어가 필요합니다.\n구성 요소 최소 요구 사항 권장 사양 CPU 4코어 이상 8코어 이상 RAM OSD당 2GB 이상 OSD당 4GB 이상 디스크 HDD, SSD 지원 OSD: SSD + HDD 조합 네트워크 1Gbps 최소 10Gbps 이상 💡 NVMe SSD를 WAL/DB 용도로 활용하면 성능이 향상됩니다.\n2. 네트워크 구성 Ceph에서는 데이터 트래픽과 관리 트래픽을 분리하는 것이 중요합니다.\nPublic Network: 클라이언트와의 통신 (예: 192.168.1.0/24) Private Network: OSD 간 데이터 복제 (예: 10.0.0.0/24) 💡 고가용성을 위해 VLAN 또는 BGP를 활용한 네트워크 구성 고려\n3. 클러스터 사이징 및 스토리지 설계 Ceph 클러스터를 설계할 때 고려해야 할 사항:\n데이터 복제(Redundancy): 2-way vs 3-way 복제 Erasure Coding 사용 여부 OSD당 PG 개수 설정 (권장 PG 수 계산법: OSD 수 × 100 / 복제 수) 💡 데이터 신뢰성을 위해 최소 3개의 MON 노드 필요\n3️⃣ Cephadm을 이용한 클러스터 구축 Cephadm은 최신 Ceph 설치 방식으로, 컨테이너 기반으로 Ceph을 쉽게 배포하고 관리할 수 있습니다.\n1. Cephadm 다운로드 및 초기화 curl --silent --remote-name --location https://download.ceph.com/cephadm/cephadm chmod +x cephadm ./cephadm add-repo --release quincy ./cephadm install ceph-common 2. 클러스터 부트스트랩 cephadm bootstrap --mon-ip 192.168.1.100 💡 --mon-ip 옵션은 초기 Monitor 노드의 IP 주소를 지정하는 데 사용됩니다.\n3. 노드 추가 추가적인 노드를 Ceph 클러스터에 포함하려면:\nceph orch host add node1 192.168.1.101 ceph orch host add node2 192.168.1.102 💡 Cephadm을 이용하면 모든 클러스터 노드를 자동으로 관리할 수 있음\n4️⃣ Ceph 설정 파일 (ceph.conf) 이해 및 튜닝 Ceph 설정 파일 ceph.conf는 클러스터 동작을 제어하는 주요 구성 요소입니다.\n1. 기본 ceph.conf 예제 [global] fsid = 7f4e14b4-5f6a-4c6b-a9d5-ef14a7b6b9c8 mon_initial_members = mon1, mon2, mon3 mon_host = 192.168.1.100,192.168.1.101,192.168.1.102 public_network = 192.168.1.0/24 cluster_network = 10.0.0.0/24 osd_pool_default_size = 3 2. 튜닝 옵션 설정 옵션 설명 권장 값 osd_pool_default_size 복제본 개수 3 osd_pool_default_min_size 최소 복제본 개수 2 ms_bind_ipv6 IPv6 활성화 여부 false bluestore_cache_size 블루스토어 캐시 크기 OSD RAM의 50% 💡 네트워크 튜닝 예제\nethtool -K eth0 tx off rx off sysctl -w net.core.rmem_max=67108864 sysctl -w net.core.wmem_max=67108864 "},"title":"Ceph 설치 및 환경 구성"},"/system/ceph/ceph03/":{"data":{"":"","1-rados-reliable-autonomic-distributed-object-store-개념-및-핵심-기능#1️⃣ \u003cstrong\u003eRADOS Reliable Autonomic Distributed Object Store) 개념 및 핵심 기능\u003c/strong\u003e":"","2-ceph-클라이언트와-rados의-관계#2️⃣ \u003cstrong\u003eCeph 클라이언트와 RADOS의 관계\u003c/strong\u003e":"","3-ceph-osd의-데이터-저장-구조#3️⃣ \u003cstrong\u003eCeph OSD의 데이터 저장 구조\u003c/strong\u003e":"","4-pg-placement-group와-osd-매핑#4️⃣ \u003cstrong\u003ePG (Placement Group)와 OSD 매핑\u003c/strong\u003e":"","5-데이터-복제-및-erasure-coding의-차이#5️⃣ \u003cstrong\u003e데이터 복제 및 Erasure Coding의 차이\u003c/strong\u003e":"","6-osd-failure-대응-및-자동-복구-과정#6️⃣ \u003cstrong\u003eOSD Failure 대응 및 자동 복구 과정\u003c/strong\u003e":"","7-ceph-osd-heartbeat-및-failure-detection#7️⃣ \u003cstrong\u003eCeph OSD Heartbeat 및 Failure Detection\u003c/strong\u003e":"1️⃣ RADOS Reliable Autonomic Distributed Object Store) 개념 및 핵심 기능 RADOS(Reliable Autonomic Distributed Object Store)는 Ceph의 핵심 스토리지 엔진으로, Ceph 클러스터 내에서 데이터를 안정적으로 분산 및 관리하는 역할을 합니다.\n1. RADOS의 핵심 기능 분산 스토리지: 데이터를 여러 OSD에 자동 분산 자가 복구(Self-Healing): 장애 발생 시 데이터 복제 및 복구 자동 수행 확장성: OSD 추가 시 자동으로 데이터 재분배 자율 관리(Self-Managing): CRUSH 알고리즘 기반으로 데이터 배치 자동 조정 🔹 💡 RADOS는 Ceph의 가장 중요한 구성 요소이며, 모든 Ceph 서비스가 RADOS 위에서 동작합니다.\n2️⃣ Ceph 클라이언트와 RADOS의 관계 Ceph 클라이언트는 다양한 인터페이스를 통해 RADOS와 직접 통신합니다.\n1. 클라이언트 유형 클라이언트 설명 Librados 직접 RADOS API를 사용하는 애플리케이션 RBD 블록 스토리지 인터페이스 제공 CephFS POSIX 호환 파일 시스템 인터페이스 제공 RGW S3 호환 오브젝트 스토리지 제공 2. 클라이언트와 RADOS의 데이터 흐름 클라이언트 요청 -\u003e CRUSH 알고리즘을 통해 PG 결정 -\u003e 해당 PG가 포함된 OSD로 요청 전달 📌 💡 클라이언트는 데이터를 직접 OSD에 저장하며, MON 노드는 데이터 전송에 개입하지 않습니다.\n3️⃣ Ceph OSD의 데이터 저장 구조 OSD(Object Storage Daemon)는 Ceph 클러스터에서 데이터를 저장하는 핵심 구성 요소입니다.\n1. OSD의 데이터 저장 방식 Bluestore: 최신 OSD 저장 방식 (메타데이터는 RocksDB, 데이터는 원본 디스크) Filestore(Deprecated): 기존 파일시스템 기반 저장 방식 2. OSD의 내부 구성 block.db (Metadata 저장, SSD 권장) block.wal (Write-Ahead Logging 저장, SSD 권장) block (실제 데이터 저장, HDD/SSD 가능) 📌 💡 Bluestore는 SSD를 block.db로 활용하면 성능이 크게 향상됩니다.\n4️⃣ PG (Placement Group)와 OSD 매핑 1. Placement Group (PG) 개념 PG(Placement Group)는 데이터를 OSD에 효과적으로 분산하기 위한 중간 계층입니다.\n객체 -\u003e Placement Group (PG) -\u003e OSD 🔹 💡 PG는 CRUSH 알고리즘을 기반으로 OSD에 매핑됩니다.\n2. PG 수 설정 예제 PG 수 계산 공식:\nPG 수 = (OSD 수 × 100) / 복제본 수 💡 예를 들어, OSD가 10개이고 복제본이 3개라면:\n(10 × 100) / 3 ≈ 333개 📌 💡 PG 수를 너무 작게 설정하면 데이터 불균형이 발생할 수 있습니다.\n5️⃣ 데이터 복제 및 Erasure Coding의 차이 Ceph은 두 가지 방식으로 데이터를 보호할 수 있습니다.\n1. 데이터 복제 (Replication) 데이터를 지정된 개수(예: 3개)만큼 복제하여 여러 OSD에 저장 장애 발생 시 빠른 복구 가능 스토리지 사용량이 증가 (복제본 개수만큼 용량 필요) 2. Erasure Coding (EC) RAID와 유사한 방식으로 데이터 조각을 여러 OSD에 저장 저장 공간 효율이 높음 복구 시 계산 부하가 크며, 성능이 다소 저하될 수 있음 📌 💡 성능이 중요한 환경에서는 Replication을, 저장 공간이 중요한 환경에서는 Erasure Coding을 사용합니다.\n6️⃣ OSD Failure 대응 및 자동 복구 과정 Ceph은 OSD 장애가 발생하면 데이터를 자동으로 복구합니다.\n1. OSD 장애 감지 MON이 OSD Heartbeat(하트비트)를 감지하여 장애 발생 여부 판단 장애 발생 시 CRUSH 맵에서 해당 OSD를 제거 2. 데이터 복구 과정 Ceph은 복제본을 기준으로 데이터 누락 확인 Healthy OSD들이 누락된 데이터를 자동 복구 복구 완료 후 클러스터 상태가 HEALTH_OK로 변경 💡 복구 진행 중에는 클러스터 상태가 HEALTH_WARN 또는 HEALTH_ERR로 나타날 수 있음\n7️⃣ Ceph OSD Heartbeat 및 Failure Detection OSD 간 Heartbeat(하트비트) 검사를 통해 장애를 감지합니다.\n1. Heartbeat 방식 OSD는 주기적으로 다른 OSD에 신호를 보냄 일정 시간(기본 20초) 내 응답이 없으면 장애로 간주 2. Heartbeat 튜닝 예제 ceph config set osd osd_heartbeat_grace 10 # 기본 20초 -\u003e 10초로 단축 ceph config set osd osd_heartbeat_interval 3 # 기본 6초 -\u003e 3초로 단축 📌 💡 Heartbeat 값을 너무 낮게 설정하면 네트워크 지연으로 인한 오탐이 발생할 수 있음"},"title":"Ceph RADOS"},"/system/ceph/ceph04/":{"data":{"":"","1-ceph-rbd-개념-및-사용-사례#1️⃣ \u003cstrong\u003eCeph RBD 개념 및 사용 사례\u003c/strong\u003e":"","2-rbd-이미지-생성-관리#2️⃣ \u003cstrong\u003eRBD 이미지 생성, 관리\u003c/strong\u003e":"","3-rbd-snapshots-및-cloning#3️⃣ \u003cstrong\u003eRBD Snapshots 및 Cloning\u003c/strong\u003e":"","4-rbd-striping과-성능-튜닝#4️⃣ \u003cstrong\u003eRBD Striping과 성능 튜닝\u003c/strong\u003e":"","5-rbd-cache-및-성능-최적화#5️⃣ \u003cstrong\u003eRBD Cache 및 성능 최적화\u003c/strong\u003e":"","6-kvmproxmoxopenstack에서-ceph-rbd-사용법#6️⃣ \u003cstrong\u003eKVM/Proxmox/OpenStack에서 Ceph RBD 사용법\u003c/strong\u003e":"","7-rbd를-kubernetes에서-활용하기-rook-ceph#7️⃣ \u003cstrong\u003eRBD를 Kubernetes에서 활용하기 (Rook-Ceph)\u003c/strong\u003e":"1️⃣ Ceph RBD 개념 및 사용 사례 1. Ceph RBD란? RBD(RADOS Block Device)는 Ceph의 블록 스토리지 솔루션으로, 클라우드 및 가상화 환경에서 활용됩니다.\n🔹 주요 특징\n확장성과 고가용성을 갖춘 분산 블록 스토리지 스냅샷 및 클론 기능 제공 동적 크기 조정 가능 고성능을 위한 캐시 및 스트라이핑 기능 제공 2. RBD 사용 사례 사용 사례 설명 KVM/Proxmox VM 디스크 백엔드로 사용 OpenStack Cinder 블록 스토리지로 통합 Kubernetes CSI 기반의 블록 스토리지 활용 Docker 컨테이너 볼륨으로 사용 가능 📌 💡 RBD는 Ceph 클러스터의 분산 스토리지 기능을 활용하여, 안정성과 성능을 극대화할 수 있습니다.\n2️⃣ RBD 이미지 생성, 관리 1. RBD 이미지 생성 Ceph의 RBD는 rbd 명령어를 이용하여 생성할 수 있습니다.\n# 10GB 크기의 RBD 이미지 생성 rbd create mydisk --size 10G --pool rbd 2. RBD 이미지 목록 조회 rbd ls --pool rbd 3. RBD 이미지 크기 조정 (확장) rbd resize mydisk --size 20G 4. RBD 이미지 삭제 rbd rm mydisk 📌 💡 RBD 이미지는 VM, 컨테이너, 데이터 저장용 디스크 등으로 활용할 수 있습니다.\n3️⃣ RBD Snapshots 및 Cloning 1. RBD Snapshot 생성 및 관리 # 스냅샷 생성 rbd snap create mydisk --snap snap1 # 스냅샷 목록 확인 rbd snap ls mydisk # 스냅샷 삭제 rbd snap rm mydisk@snap1 2. RBD Clone 생성 # RBD 스냅샷을 읽기 전용 클론으로 생성 rbd clone mydisk@snap1 myclone 📌 💡 RBD 스냅샷과 클론 기능을 활용하면, VM의 빠른 배포 및 백업이 가능합니다.\n4️⃣ RBD Striping과 성능 튜닝 1. Striping이란? 데이터를 여러 OSD에 균등하게 배치하여 성능을 향상시키는 기법 디스크 I/O 부하를 분산하여 병목 현상을 방지 2. Striping 활성화 예제 rbd create mydisk --size 10G --stripe-unit 64K --stripe-count 4 📌 💡 stripe-unit: 한 번에 기록할 데이터 크기, stripe-count: 병렬 저장할 OSD 수\n5️⃣ RBD Cache 및 성능 최적화 1. RBD 성능 벤치마크 # RBD 벤치마크 실행 rbd bench --io-type write --io-size 4K --io-threads 4 mydisk 2. RADOS 벤치마크 테스트 # RADOS 스토리지 성능 테스트 rados bench -p rbd 10 write --no-cleanup 📌 💡 벤치마크 테스트를 통해 블록 스토리지의 성능을 최적화할 수 있습니다.\n6️⃣ KVM/Proxmox/OpenStack에서 Ceph RBD 사용법 1. KVM에서 RBD 스토리지 사용 qemu-img convert -O raw vm-disk.img rbd:rbd/mydisk 📌 💡 KVM은 RBD를 VM의 디스크로 직접 마운트할 수 있습니다.\n2. Proxmox에서 RBD 스토리지 추가 pveceph pool create rbd pvesm add ceph rbd --monhost 10.0.0.1 --pool rbd 3. OpenStack에서 Ceph RBD 연동 [ceph] rbd_store_pool = volumes rbd_user = openstack rbd_secret_uuid = \u003cUUID\u003e 📌 💡 Proxmox와 OpenStack에서 Ceph RBD를 활용하면, VM 디스크를 분산 스토리지로 사용할 수 있습니다.\n7️⃣ RBD를 Kubernetes에서 활용하기 (Rook-Ceph) 1. Kubernetes에 Rook-Ceph 설치 kubectl create -f common.yaml kubectl create -f operator.yaml kubectl create -f cluster.yaml 2. Ceph RBD 기반 PVC 생성 예제 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-rbd-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: rook-ceph-block 3. PVC를 Pod에서 사용 apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: - name: my-container image: ubuntu volumeMounts: - mountPath: \"/mnt/data\" name: ceph-volume volumes: - name: ceph-volume persistentVolumeClaim: claimName: my-rbd-pvc 📌 💡 Rook-Ceph을 활용하면 Kubernetes 환경에서도 Ceph RBD를 쉽게 사용할 수 있습니다."},"title":"Ceph 블록 스토리지 (RBD) 실무"},"/system/ceph/ceph05/":{"data":{"":"","1-cephfs-개념-및-동작-방식#1️⃣ \u003cstrong\u003eCephFS 개념 및 동작 방식\u003c/strong\u003e":"","2-cephfs-구축-및-마운트-ceph-fuse-mountceph#2️⃣ \u003cstrong\u003eCephFS 구축 및 마운트 (ceph-fuse, mount.ceph)\u003c/strong\u003e":"","3-cephfs-metadata-server-mds-설정-및-관리#3️⃣ \u003cstrong\u003eCephFS Metadata Server (MDS) 설정 및 관리\u003c/strong\u003e":"","4-cephfs-snapshot-및-데이터-보호#4️⃣ \u003cstrong\u003eCephFS Snapshot 및 데이터 보호\u003c/strong\u003e":"","5-cephfs-성능-튜닝-mds-cache-journaling-client-performance-tuning#5️⃣ \u003cstrong\u003eCephFS 성능 튜닝 (mds cache, journaling, client performance tuning)\u003c/strong\u003e":"","6-nfs-ganesha를-통한-nfs-지원#6️⃣ \u003cstrong\u003eNFS-Ganesha를 통한 NFS 지원\u003c/strong\u003e":"","7-kubernetes-환경에서-cephfs-활용#7️⃣ \u003cstrong\u003eKubernetes 환경에서 CephFS 활용\u003c/strong\u003e":"1️⃣ CephFS 개념 및 동작 방식 1. CephFS란? CephFS(Ceph File System)는 Ceph의 분산 파일 시스템으로, Ceph 클러스터 내에서 파일 시스템을 제공합니다. CephFS는 여러 서버 간에 데이터를 분산 저장하면서도, 일반적인 파일 시스템처럼 동작할 수 있습니다.\n🔹 주요 특징\n확장성: 데이터가 여러 OSD에 분산되어 있어 스토리지 용량 확장이 용이함 고가용성: 클러스터 내에서 자동 복구 및 복제 기능을 제공 성능: CephFS는 대규모 데이터 처리에 최적화되어 있으며, 병렬 처리 및 데이터 캐싱을 지원 2. CephFS 동작 원리 CephFS는 **Metadata Server (MDS)**와 **OSD (Object Storage Daemon)**로 구성되어 있습니다. MDS는 파일 시스템의 메타데이터를 관리하고, OSD는 실제 데이터를 저장합니다.\n📌 💡 CephFS는 Ceph의 분산 아키텍처를 활용하여 높은 성능과 확장성을 제공합니다.\n2️⃣ CephFS 구축 및 마운트 (ceph-fuse, mount.ceph) 1. CephFS 설치 CephFS를 사용하기 위해서는 먼저 Ceph 클러스터에서 CephFS를 활성화해야 합니다. CephFS를 활성화하려면, ceph CLI에서 다음 명령을 실행합니다.\n# CephFS 활성화 ceph fs new cephfs ceph-balance ceph-metadata 2. CephFS 마운트 (ceph-fuse 사용) CephFS를 클라이언트 시스템에 마운트하려면 ceph-fuse 또는 mount.ceph 명령을 사용할 수 있습니다. ceph-fuse는 Ceph 클러스터에서 CephFS를 사용하기 위한 FUSE 기반 마운트 유틸리티입니다.\n# CephFS 마운트 (ceph-fuse 사용) ceph-fuse /mnt/cephfs 3. mount.ceph 사용하여 CephFS 마운트 # CephFS 마운트 (mount.ceph 사용) mount -t ceph :/ /mnt/cephfs -o name=admin,secret=\u003csecret-key\u003e 📌 💡 CephFS는 클라이언트 시스템에서 표준 파일 시스템처럼 마운트되어 사용할 수 있습니다.\n3️⃣ CephFS Metadata Server (MDS) 설정 및 관리 1. MDS 구성 CephFS의 메타데이터는 MDS가 관리합니다. MDS는 Ceph 클러스터에서 파일과 디렉터리의 정보를 저장하고 처리합니다.\n# MDS 모니터링 상태 확인 ceph mds stat 2. MDS 설치 및 시작 # MDS 데몬 시작 ceph-deploy mds create \u003cmds-node\u003e 3. MDS 서비스 관리 MDS 데몬의 상태 및 로그를 확인하고, 필요시 재시작을 할 수 있습니다.\n# MDS 서비스 상태 확인 systemctl status ceph-mds@\u003cmds-node\u003e 📌 💡 MDS는 CephFS에서 파일 시스템의 메타데이터를 효율적으로 관리하여 성능을 최적화합니다.\n4️⃣ CephFS Snapshot 및 데이터 보호 1. CephFS 스냅샷 생성 CephFS는 ceph CLI를 사용하여 스냅샷을 생성할 수 있습니다. 스냅샷은 특정 시점의 데이터를 저장하여 복구할 수 있도록 합니다.\n# CephFS 스냅샷 생성 rados snap create cephfs_data@snapshot1 2. 스냅샷 목록 확인 및 삭제 # CephFS 스냅샷 목록 확인 rados snap ls cephfs_data # CephFS 스냅샷 삭제 rados snap rm cephfs_data@snapshot1 📌 💡 스냅샷을 통해 CephFS 데이터를 보호하고, 손실된 데이터를 복구할 수 있습니다.\n5️⃣ CephFS 성능 튜닝 (mds cache, journaling, client performance tuning) 1. MDS 캐시 성능 튜닝 MDS의 성능을 최적화하기 위해 캐시 크기를 조정할 수 있습니다.\n# MDS 캐시 크기 설정 ceph config set mds.\u003cmds-node\u003e mds_cache_size 1024 2. Journaling 활성화 Journaling은 CephFS의 데이터 무결성을 보장하는 중요한 기능입니다.\n# Journaling 활성화 ceph config set mds.\u003cmds-node\u003e mds_journal_size 100MB 3. 클라이언트 성능 튜닝 클라이언트 성능을 향상시키기 위해 여러 매개변수를 조정할 수 있습니다.\n# 클라이언트 캐시 크기 설정 ceph config set client ceph_client_cache_size 128M 📌 💡 성능 튜닝을 통해 CephFS를 최적화하고, 높은 처리 성능을 유지할 수 있습니다.\n6️⃣ NFS-Ganesha를 통한 NFS 지원 1. NFS-Ganesha 설치 NFS-Ganesha는 CephFS를 NFS 서버로 활용할 수 있도록 해주는 솔루션입니다. 이를 통해 CephFS를 NFS 클라이언트에서 마운트할 수 있습니다.\n# NFS-Ganesha 설치 apt-get install nfs-ganesha-ceph 2. NFS-Ganesha 설정 # /etc/ganesha/ganesha.conf 파일 수정 EXPORT { Export_Id = 1; Path = /mnt/cephfs; Pseudo = /cephfs; Access_Type = RW; Squash = No_Root_Squash; } 3. NFS 서버 시작 # NFS-Ganesha 서비스 시작 systemctl start nfs-ganesha 📌 💡 NFS-Ganesha를 활용하면 CephFS를 NFS 서버로 제공하여 다양한 시스템과의 호환성을 확보할 수 있습니다.\n7️⃣ Kubernetes 환경에서 CephFS 활용 1. Kubernetes에서 CephFS 사용을 위한 설정 Kubernetes에서 CephFS를 사용하려면 먼저 PersistentVolume(PV)과 PersistentVolumeClaim(PVC)을 정의해야 합니다.\napiVersion: v1 kind: PersistentVolume metadata: name: cephfs-pv spec: capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain cephfs: monitors: - \u003cmon-ip\u003e:6789 user: admin secretRef: name: ceph-secret path: /mnt/cephfs readOnly: false 2. CephFS를 PVC로 연결 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cephfs-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: cephfs 3. Pod에서 CephFS 사용 apiVersion: v1 kind: Pod metadata: name: cephfs-pod spec: containers: - name: cephfs-container image: ubuntu volumeMounts: - mountPath: \"/mnt/data\" name: cephfs-volume volumes: - name: cephfs-volume persistentVolumeClaim: claimName: cephfs-pvc 📌 💡 Kubernetes 환경에서 CephFS를 활용하면, 여러 Pod 간에 동일한 파일 시스템을 공유할 수 있습니다."},"title":"Ceph 파일 시스템 (CephFS) 실무"},"/system/ceph/ceph06/":{"data":{"":"","1-ceph-rados-gateway-개념-및-역할#1️⃣ \u003cstrong\u003eCeph RADOS Gateway 개념 및 역할\u003c/strong\u003e":"","2-rgw와-s3-api-연동#2️⃣ \u003cstrong\u003eRGW와 S3 API 연동\u003c/strong\u003e":"","3-rgw-multi-site-replication-및-데이터-동기화#3️⃣ \u003cstrong\u003eRGW Multi-site Replication 및 데이터 동기화\u003c/strong\u003e":"","4-ceph-rgw-성능-튜닝-civetweb-nginx--rgw#4️⃣ \u003cstrong\u003eCeph RGW 성능 튜닝 (civetweb, nginx + RGW)\u003c/strong\u003e":"","5-openstack-swift-api와의-호환성#5️⃣ \u003cstrong\u003eOpenStack Swift API와의 호환성\u003c/strong\u003e":"","6-ceph-rgw-인증-및-권한-관리-keystone-ldap#6️⃣ \u003cstrong\u003eCeph RGW 인증 및 권한 관리 (Keystone, LDAP)\u003c/strong\u003e":"","7-rgw를-kubernetes에서-활용하기-ceph-object-storage-crd#7️⃣ \u003cstrong\u003eRGW를 Kubernetes에서 활용하기 (Ceph Object Storage CRD)\u003c/strong\u003e":"1️⃣ Ceph RADOS Gateway 개념 및 역할 1. RADOS Gateway란? Ceph RADOS Gateway(RGW)는 Ceph 클러스터에 오브젝트 스토리지 서비스(S3, Swift API)를 제공하는 서비스입니다. RGW는 AWS S3와 OpenStack Swift와 호환되며, 분산 파일 시스템의 오브젝트 저장소를 지원합니다.\n🔹 주요 기능\n데이터 저장: 오브젝트 스토리지 서비스 제공 S3/Swift 호환: 다양한 클라우드 애플리케이션과 호환 가능 확장성: Ceph의 기본적인 분산 시스템을 활용하여 매우 높은 확장성을 제공 2. RADOS Gateway의 아키텍처 RADOS Gateway는 기본적으로 Ceph OSD와 통합되어 데이터를 오브젝트 형태로 저장합니다. 이를 통해 클라이언트는 HTTP(S)를 통해 데이터를 업로드하고 다운로드할 수 있습니다.\n📌 💡 RGW는 S3와 Swift API를 제공하여 다양한 애플리케이션이 Ceph 오브젝트 스토리지와 쉽게 통합될 수 있습니다.\n2️⃣ RGW와 S3 API 연동 1. S3 API 소개 S3 API는 Amazon Web Services(AWS)에서 제공하는 오브젝트 스토리지 서비스의 API입니다. Ceph RGW는 이 S3 API를 구현하여, AWS S3와 호환되는 환경을 제공합니다.\n2. S3 API 연동 설정 RGW에서 S3 API를 활성화하려면, S3 호환 도메인을 설정하고, radosgw 서비스가 S3 요청을 처리할 수 있도록 해야 합니다.\n# radosgw.conf에서 S3 관련 설정 활성화 [client.radosgw.gateway] rgw_frontend = civetweb rgw_frontend_port = 80 rgw_backend = ceph 3. S3 API 테스트 AWS CLI를 사용하여 Ceph RGW의 S3 API와 연동된 스토리지를 테스트할 수 있습니다.\n# AWS CLI로 S3 API 요청 보내기 aws --endpoint-url=http://\u003cRGW-URL\u003e:80 s3 ls 📌 💡 S3 API를 사용하면, Ceph RGW를 AWS S3와 유사한 방식으로 사용할 수 있습니다.\n3️⃣ RGW Multi-site Replication 및 데이터 동기화 1. Multi-site Replication 개념 Ceph RGW는 다수의 RGW 인스턴스를 사용하여 데이터 복제를 수행할 수 있습니다. 이 기능은 서로 다른 지리적 위치에 있는 데이터 센터 간에 데이터를 동기화하는 데 유용합니다.\n2. Multi-site Replication 설정 Ceph RGW에서 Multi-site Replication을 활성화하려면, radosgw-admin 명령어로 복제 사이트를 설정합니다.\n# 첫 번째 사이트 설정 radosgw-admin zonegroup create --name=main --rgw-zonegroup=main radosgw-admin zone create --name=main --rgw-zone=site1 --rgw-zonegroup=main # 두 번째 사이트 설정 radosgw-admin zonegroup create --name=main --rgw-zonegroup=main radosgw-admin zone create --name=main --rgw-zone=site2 --rgw-zonegroup=main 3. 데이터 동기화 상태 확인 # 복제 상태 확인 radosgw-admin sync status 📌 💡 Multi-site Replication을 통해 Ceph RGW는 여러 사이트 간에 데이터를 효율적으로 동기화할 수 있습니다.\n4️⃣ Ceph RGW 성능 튜닝 (civetweb, nginx + RGW) 1. civetweb 사용하여 성능 최적화 civetweb는 RGW의 기본 웹 서버로, 경량화된 성능을 제공합니다. 기본적으로 civetweb를 사용하여 성능을 최적화할 수 있습니다.\n# civetweb 설정 파일 예시 rgw_frontend = civetweb rgw_frontend_port = 8080 2. nginx를 활용한 성능 최적화 nginx는 고성능 리버스 프록시 서버로, RGW와 연동하여 더 나은 성능을 제공합니다. 다음과 같이 nginx를 활용하여 성능을 최적화할 수 있습니다.\n# nginx 설정 예시 server { listen 80; server_name radosgw.example.com; location / { proxy_pass http://127.0.0.1:80; } } 3. 성능 테스트 # S3 API 성능 테스트 s3cmd du --recursive s3://bucketname 📌 💡 civetweb 또는 nginx를 사용하여 Ceph RGW의 성능을 최적화할 수 있습니다.\n5️⃣ OpenStack Swift API와의 호환성 1. Swift API 개요 OpenStack Swift는 오픈소스 오브젝트 스토리지 시스템으로, Ceph RGW는 이 Swift API를 통해 Swift와 호환됩니다.\n2. Swift API와 RGW 연동 Ceph RGW는 Swift API를 구현하여 OpenStack 환경에서도 Ceph를 오브젝트 스토리지로 사용할 수 있게 해줍니다. Swift의 기본 설정은 radosgw.conf 파일에서 관리할 수 있습니다.\n# radosgw.conf에서 Swift 호환 설정 rgw_swift = true 3. Swift 클라이언트 연동 Swift 클라이언트를 사용하여 Ceph RGW에 접근할 수 있습니다.\n# Swift 클라이언트로 Ceph RGW 연결 swift -v -A http://\u003cRGW-URL\u003e:8080/auth/v1.0 -U \u003cuser\u003e -K \u003ckey\u003e stat 📌 💡 Swift API를 사용하여 OpenStack 환경에서 Ceph RGW를 쉽게 활용할 수 있습니다.\n6️⃣ Ceph RGW 인증 및 권한 관리 (Keystone, LDAP) 1. Keystone 연동 Keystone은 OpenStack의 인증 시스템으로, Ceph RGW에서 Keystone을 사용하여 인증을 처리할 수 있습니다.\n# Keystone 인증을 위한 설정 rgw_keystone_url = http://\u003ckeystone-url\u003e:5000/v3 2. LDAP 연동 Ceph RGW는 LDAP와 연동하여 사용자 인증 및 권한 관리를 할 수 있습니다.\n# LDAP 인증 설정 예시 rgw_ldap_uri = ldap://\u003cldap-server\u003e rgw_ldap_bind_dn = cn=admin,dc=example,dc=com 3. 권한 관리 # 사용자 권한 설정 radosgw-admin user create --uid=\"user1\" --display-name=\"User One\" --email=\"user1@example.com\" 📌 💡 Keystone 및 LDAP와의 연동을 통해 Ceph RGW의 인증 및 권한 관리를 효율적으로 수행할 수 있습니다.\n7️⃣ RGW를 Kubernetes에서 활용하기 (Ceph Object Storage CRD) 1. Ceph Object Storage CRD 정의 Kubernetes에서 Ceph RGW를 사용하려면 CustomResourceDefinition(CRD)을 정의하여 Ceph 오브젝트 스토리지의 PersistentVolume을 관리할 수 있습니다.\napiVersion: ceph.rook.io/v1 kind: CephObjectStore metadata: name: ceph-object-store namespace: rook-ceph spec: gateway: port: 80 securePort: 443 2. Ceph Object Storage 활용 Kubernetes에서 Ceph Object Storage CRD를 사용하여 Ceph RGW를 클러스터 내에서 활용할 수 있습니다.\n# Kubernetes에서 Ceph RGW 접근 kubectl apply -f ceph-object-store.yaml 3. RGW 데이터 관리 # Kubernetes Pod에서 RGW 데이터 확인 kubectl logs ceph-object-store-\u003cpod-name\u003e 📌 💡 Kubernetes에서 Ceph RGW를 활용하면, 클라우드 네이티브 환경에서 효율적인 오브젝트 스토리지를 구현할 수 있습니다."},"title":"Ceph 오브젝트 스토리지 (RGW) 실무"},"/system/ceph/ceph07/":{"data":{"":"","1-ceph-dashboard-활용#1️⃣ \u003cstrong\u003eCeph Dashboard 활용\u003c/strong\u003e":"","2-prometheus--grafana를-이용한-ceph-모니터링#2️⃣ \u003cstrong\u003ePrometheus \u0026amp; Grafana를 이용한 Ceph 모니터링\u003c/strong\u003e":"","3-ceph-osd-및-pg-상태-점검-ceph-osd-tree-ceph-pg-dump#3️⃣ \u003cstrong\u003eCeph OSD 및 PG 상태 점검 (ceph osd tree, ceph pg dump)\u003c/strong\u003e":"","4-성능-진단-및-벤치마킹-rados-bench-fio-iostat#4️⃣ \u003cstrong\u003e성능 진단 및 벤치마킹 (rados bench, fio, iostat)\u003c/strong\u003e":"","5-ceph-네트워크-튜닝-msgr-v2-rdma-jumbo-frames#5️⃣ \u003cstrong\u003eCeph 네트워크 튜닝 (msgr v2, RDMA, Jumbo Frames)\u003c/strong\u003e":"","6-ceph의-데이터-밸런싱-ceph-balancer-pg_#6️⃣ \u003cstrong\u003eCeph의 데이터 밸런싱 (ceph balancer, pg_autoscaler)\u003c/strong\u003e":"","7-osd-recovery-및-backfill-최적화#7️⃣ \u003cstrong\u003eOSD Recovery 및 Backfill 최적화\u003c/strong\u003e":"1️⃣ Ceph Dashboard 활용 1. Ceph Dashboard란? Ceph Dashboard는 Ceph 클러스터의 상태를 실시간으로 모니터링하고, 관리할 수 있는 웹 기반의 인터페이스입니다. 이를 통해 클러스터의 성능 지표를 시각적으로 확인하고, 시스템을 관리할 수 있습니다.\n2. Ceph Dashboard 설치 Ceph Dashboard를 설치하려면, 클러스터의 관리자 노드에서 다음 명령어를 실행하여 Ceph Dashboard를 활성화합니다.\n# Ceph Dashboard 활성화 ceph mgr module enable dashboard # 관리자 사용자 생성 ceph dashboard ac-user-create admin admin-password administrator # Dashboard 포트 설정 ceph config set mgr mgr/dashboard/server_port 7000 이제, 웹 브라우저에서 http://\u003cceph-manager-node\u003e:7000 주소로 접속하여 Ceph Dashboard에 접근할 수 있습니다.\n3. Dashboard 주요 기능 클러스터 상태: OSD, MON, MGR, MDS 등의 상태를 실시간으로 확인 성능 모니터링: I/O, 용량, 네트워크 등 성능 지표 모니터링 알림 및 이벤트: 클러스터에서 발생하는 주요 이벤트 및 알림을 제공 📌 💡 Ceph Dashboard를 통해 쉽게 클러스터의 상태를 모니터링하고 관리할 수 있습니다.\n2️⃣ Prometheus \u0026 Grafana를 이용한 Ceph 모니터링 1. Prometheus 설치 Prometheus는 메트릭 수집 및 모니터링 시스템으로, Ceph 클러스터의 상태를 실시간으로 추적할 수 있습니다. Ceph에서는 ceph-mgr 모듈을 사용하여 Prometheus와 연동할 수 있습니다.\n# Prometheus exporter 활성화 ceph mgr module enable prometheus 2. Grafana 설정 Grafana는 Prometheus에서 수집한 데이터를 시각화하는 도구입니다. Ceph의 모니터링 데이터를 Grafana 대시보드로 시각화하려면, Grafana에서 Prometheus 데이터 소스를 설정해야 합니다.\n# Grafana에 Prometheus 데이터 소스 추가 datasource: name: Prometheus type: Prometheus url: http://\u003cprometheus-server\u003e:9090 3. Grafana 대시보드 Ceph 클러스터에 대한 다양한 대시보드를 제공하는 Grafana 대시보드를 설치하고, Ceph 성능 모니터링을 시각적으로 제공합니다.\n📌 💡 Prometheus와 Grafana를 결합하여 Ceph 클러스터의 실시간 상태를 시각적으로 모니터링할 수 있습니다.\n3️⃣ Ceph OSD 및 PG 상태 점검 (ceph osd tree, ceph pg dump) 1. OSD 상태 점검 Ceph에서 OSD는 데이터를 저장하고, 읽고, 복제하는 역할을 합니다. OSD 상태를 점검하려면 다음 명령어를 사용합니다.\n# OSD 트리 확인 ceph osd tree 이 명령어는 OSD 노드의 트리 구조를 보여주며, 각 OSD가 어떤 상태에 있는지 확인할 수 있습니다.\n2. PG 상태 점검 PG(Placement Group)는 Ceph에서 데이터를 저장하는 논리적인 단위입니다. PG 상태를 점검하려면 ceph pg dump 명령어를 사용합니다.\n# PG 상태 확인 ceph pg dump 이 명령어는 각 PG의 상태와 관련된 정보를 출력합니다.\n📌 💡 OSD 및 PG 상태 점검은 클러스터의 안정성과 성능을 확인하는 데 중요한 작업입니다.\n4️⃣ 성능 진단 및 벤치마킹 (rados bench, fio, iostat) 1. rados bench rados bench는 Ceph 클러스터의 성능을 측정하는 도구입니다. 이를 통해 클러스터의 읽기/쓰기 성능을 벤치마킹할 수 있습니다.\n# rados bench 명령어 예시 rados bench -p ceph_pool 10 write 위 명령어는 ceph_pool에서 10초 동안 쓰기 성능을 측정합니다.\n2. fio fio는 디스크 I/O 성능을 측정하는 툴입니다. Ceph 클러스터의 성능을 테스트하기 위해 fio를 사용하여 벤치마킹할 수 있습니다.\n# fio 명령어 예시 fio --name=ceph_bench --ioengine=rados --size=1G --runtime=30s --numjobs=1 3. iostat iostat는 I/O 성능 모니터링 도구입니다. Ceph 클러스터의 OSD 성능을 점검할 때 유용합니다.\n# iostat 명령어 예시 iostat -x 1 📌 💡 rados bench, fio, iostat를 사용하여 Ceph 클러스터의 성능을 진단하고 벤치마킹할 수 있습니다.\n5️⃣ Ceph 네트워크 튜닝 (msgr v2, RDMA, Jumbo Frames) 1. msgr v2 사용 msgr v2는 Ceph 클러스터의 네트워크 성능을 개선하기 위한 새로운 메시지 프로토콜입니다. 이를 활성화하면 Ceph의 내부 통신 성능이 향상됩니다.\n# msgr v2 활성화 ceph config set global msgr2_enabled true 2. RDMA 지원 RDMA(원격 직접 메모리 접근)는 고속 네트워크 통신을 지원하여 Ceph 클러스터의 성능을 향상시킬 수 있습니다. RDMA를 활성화하려면, 적절한 하드웨어 및 네트워크 구성이 필요합니다.\n3. Jumbo Frames 사용 Jumbo Frames는 더 큰 데이터 패킷을 전송할 수 있어 네트워크 효율성을 향상시킵니다. 이를 사용하려면 네트워크 인터페이스 카드(NIC)와 Ceph OSD의 MTU 설정을 변경해야 합니다.\n# NIC MTU 설정 예시 ifconfig eth0 mtu 9000 📌 💡 네트워크 튜닝을 통해 Ceph 클러스터의 성능을 최적화할 수 있습니다.\n6️⃣ Ceph의 데이터 밸런싱 (ceph balancer, pg_autoscaler) 1. Ceph Balancer 사용 Ceph Balancer는 클러스터의 데이터 분포를 자동으로 조정하여 성능을 최적화하는 도구입니다.\n# Ceph Balancer 활성화 ceph balancer on 2. pg_autoscaler 사용 pg_autoscaler는 Ceph 클러스터에서 PG(Placement Group)의 수를 자동으로 조정하여 성능을 최적화합니다.\n# pg_autoscaler 활성화 ceph config set mon pg_autoscaler_enabled true 📌 💡 데이터 밸런싱을 통해 Ceph 클러스터의 성능을 최적화하고, 자동으로 균형을 맞출 수 있습니다.\n7️⃣ OSD Recovery 및 Backfill 최적화 1. OSD Recovery OSD Recovery는 장애가 발생한 OSD를 복구하는 과정입니다. ceph osd recovery_delay 및 ceph osd recovery_max_active 등을 통해 복구 작업을 최적화할 수 있습니다.\n# 복구 지연 시간 설정 ceph osd set recovery_delay 15 # 동시에 활성화할 복구 작업 수 설정 ceph osd set recovery_max_active 3 2. Backfill 최적화 Backfill은 OSD가 데이터를 복구하는 과정으로, 이 작업의 성능을 최적화하려면 ceph osd backfillfull_ratio와 같은 매개변수를 조정할 수 있습니다.\n# Backfill 최적화 ceph osd set backfillfull_ratio 0.75 📌 💡 OSD Recovery 및 Backfill 최적화를 통해 장애 복구와 데이터 복구 작업을 효율적으로 수행할 수 있습니다."},"title":"Ceph 모니터링 및 성능 최적화"},"/system/ceph/ceph08/":{"data":{"":"","1-osd-장애-복구-ceph-osd-repair-ceph-osd-lost#1️⃣ \u003cstrong\u003eOSD 장애 복구 (ceph osd repair, ceph osd lost)\u003c/strong\u003e":"","2-mon-quorum-장애-해결-ceph-quorum_#2️⃣ \u003cstrong\u003eMON Quorum 장애 해결 (ceph quorum_status)\u003c/strong\u003e":"","3-rbd-및-cephfs-데이터-복구#3️⃣ \u003cstrong\u003eRBD 및 CephFS 데이터 복구\u003c/strong\u003e":"","4-placement-group-pg-stuck-해결-ceph-pg-repair#4️⃣ \u003cstrong\u003ePlacement Group (PG) Stuck 해결 (ceph pg repair)\u003c/strong\u003e":"","5-osd-full-상태-해결-ceph-osd-reweight-ceph-df#5️⃣ \u003cstrong\u003eOSD Full 상태 해결 (ceph osd reweight, ceph df)\u003c/strong\u003e":"","6-network-partitioning-발생-시-대응-방법#6️⃣ \u003cstrong\u003eNetwork Partitioning 발생 시 대응 방법\u003c/strong\u003e":"","7-ceph-로그-분석-ceph-log-ceph-crash#7️⃣ \u003cstrong\u003eCeph 로그 분석 (ceph log, ceph crash)\u003c/strong\u003e":"1️⃣ OSD 장애 복구 (ceph osd repair, ceph osd lost) 1. OSD 장애 복구란? Ceph에서 OSD(Object Storage Device)는 중요한 데이터를 저장하는 역할을 하며, 장애가 발생하면 해당 OSD의 데이터 복구가 필요합니다. ceph osd repair 명령어를 사용하여 OSD 복구 작업을 수행할 수 있습니다.\n2. ceph osd repair 사용 예시 # OSD 장애가 발생한 경우 OSD 복구 ceph osd repair \u003cosd-id\u003e 이 명령어는 지정된 OSD에 대한 복구 작업을 실행하여 데이터를 다시 복구합니다.\n3. OSD가 사라진 경우 (ceph osd lost) OSD가 물리적으로 손상되거나 서버가 고장나서 데이터가 손실되었을 경우, ceph osd lost 명령어로 해당 OSD에 대한 상태를 점검하고 복구를 시도할 수 있습니다.\n# OSD 손실된 상태 확인 ceph osd lost 📌 💡 장애가 발생한 OSD를 빠르게 복구하여 클러스터의 가용성을 유지하는 것이 중요합니다.\n2️⃣ MON Quorum 장애 해결 (ceph quorum_status) 1. MON Quorum이란? Ceph 클러스터에서 MON(Monitor)은 클러스터 상태를 추적하고, 클러스터의 상태를 조정하는 중요한 역할을 합니다. Quorum은 MON 서버가 장애 없이 정상적으로 동작하는지 확인하는 상태입니다. MON Quorum이 깨지면 클러스터의 동작에 영향을 미칠 수 있습니다.\n2. ceph quorum_status 사용 예시 Quorum 장애를 확인하려면 ceph quorum_status 명령어를 사용합니다.\n# MON Quorum 상태 확인 ceph quorum_status 이 명령어를 통해 MON의 상태와 Quorum이 정상적으로 유지되고 있는지 확인할 수 있습니다.\n3. Quorum 복구 방법 MON 서버가 장애가 나면, 다른 MON 서버를 활성화하여 Quorum을 복구할 수 있습니다. 추가로 MON 서버를 늘려서 가용성을 높이는 방법도 고려할 수 있습니다.\n📌 💡 MON Quorum 장애를 빠르게 해결하여 클러스터의 일관성과 안정성을 유지할 수 있습니다.\n3️⃣ RBD 및 CephFS 데이터 복구 1. RBD(RADOS Block Device) 복구 RBD는 블록 스토리지 서비스로, Ceph에서 제공하는 기능 중 하나입니다. RBD에서 장애가 발생하면 데이터를 복구하는 방법은 rados 명령어를 사용하는 것입니다.\n2. RBD 복구 예시 # RBD 복구 작업 radosgw-admin object stats --bucket=\u003cbucket-name\u003e --object=\u003cobject-name\u003e 3. CephFS 데이터 복구 CephFS는 Ceph의 파일 시스템 서비스입니다. CephFS에서 데이터를 복구할 때는 ceph fs 명령어를 사용하여 파일 시스템 상태를 점검하고 복구 작업을 수행합니다.\n# CephFS 상태 점검 ceph fs status 📌 💡 RBD 및 CephFS의 데이터를 신속하게 복구하여 사용자 데이터를 보호할 수 있습니다.\n4️⃣ Placement Group (PG) Stuck 해결 (ceph pg repair) 1. PG란? Placement Group(PG)는 Ceph 클러스터에서 데이터를 분배하고 관리하는 기본 단위입니다. PG가 stuck 상태에 빠지면, 데이터를 정상적으로 읽고 쓸 수 없게 됩니다.\n2. ceph pg repair 사용 예시 PG가 stuck 상태에 빠졌을 때는 ceph pg repair 명령어를 사용하여 복구를 시도할 수 있습니다.\n# PG 복구 작업 ceph pg repair \u003cpg-id\u003e 이 명령어는 stuck된 PG를 복구하여 클러스터의 데이터 가용성을 회복합니다.\n📌 💡 PG가 stuck 상태에 빠지지 않도록 주기적으로 모니터링하고, 복구 작업을 통해 빠르게 문제를 해결할 수 있습니다.\n5️⃣ OSD Full 상태 해결 (ceph osd reweight, ceph df) 1. OSD Full 상태란? OSD Full 상태는 OSD가 데이터를 더 이상 저장할 수 없을 때 발생합니다. 이 상태가 발생하면 데이터를 더 이상 쓰지 못하고 클러스터 성능에 영향을 줄 수 있습니다.\n2. ceph osd reweight 사용 예시 OSD Full 상태를 해결하려면 ceph osd reweight 명령어를 사용하여 특정 OSD의 가중치를 조정합니다.\n# OSD 가중치 조정 ceph osd reweight \u003cosd-id\u003e \u003cweight\u003e 3. ceph df 사용 예시 ceph df 명령어를 사용하여 디스크의 사용 상태를 점검하고, OSD Full 상태의 원인을 파악합니다.\n# 디스크 사용량 확인 ceph df 📌 💡 OSD Full 상태를 빠르게 해결하여 데이터를 정상적으로 저장할 수 있도록 합니다.\n6️⃣ Network Partitioning 발생 시 대응 방법 1. Network Partitioning이란? Network Partitioning은 클러스터의 일부 노드가 네트워크 연결이 끊겨서 다른 노드와 통신할 수 없는 상태입니다. 이 상태에서는 데이터의 가용성과 일관성에 문제가 생길 수 있습니다.\n2. 대응 방법 Network Partitioning이 발생하면, 네트워크를 재구성하여 노드들을 재연결하고, ceph osd crush reweight 명령어를 사용하여 데이터를 다시 균형 맞추는 작업을 수행할 수 있습니다.\n# 네트워크 재연결 후, OSD 가중치 조정 ceph osd crush reweight \u003cosd-id\u003e \u003cweight\u003e 📌 💡 Network Partitioning 발생 시 빠르게 문제를 해결하여 클러스터의 가용성을 유지할 수 있습니다.\n7️⃣ Ceph 로그 분석 (ceph log, ceph crash) 1. Ceph 로그란? Ceph는 다양한 로그 파일을 생성하여 시스템의 상태와 동작을 기록합니다. 로그를 통해 장애 원인과 시스템 상태를 파악할 수 있습니다.\n2. ceph log 사용 예시 ceph log 명령어를 사용하여 클러스터의 로그를 실시간으로 확인할 수 있습니다.\n# 실시간 로그 확인 ceph log last 3. ceph crash 사용 예시 ceph crash 명령어를 사용하여 Ceph에서 발생한 크래시 정보를 확인하고, 문제를 분석합니다.\n# 크래시 정보 확인 ceph crash stat 📌 💡 Ceph 로그와 크래시 정보를 분석하여 문제를 빠르게 해결할 수 있습니다."},"title":"Ceph 장애 대응 및 트러블슈팅"},"/system/ceph/ceph09/":{"data":{"":"","1-rook-ceph-개요-및-아키텍처#1️⃣ \u003cstrong\u003eRook-Ceph 개요 및 아키텍처\u003c/strong\u003e":"","2-rook-ceph-설치-및-설정#2️⃣ \u003cstrong\u003eRook-Ceph 설치 및 설정\u003c/strong\u003e":"","3-rook을-이용한-ceph-클러스터-자동-배포#3️⃣ \u003cstrong\u003eRook을 이용한 Ceph 클러스터 자동 배포\u003c/strong\u003e":"","4-ceph-rbd-storageclass-생성-및-사용#4️⃣ \u003cstrong\u003eCeph RBD StorageClass 생성 및 사용\u003c/strong\u003e":"","5-cephfs를-pvc로-마운트하는-방법#5️⃣ \u003cstrong\u003eCephFS를 PVC로 마운트하는 방법\u003c/strong\u003e":"","6-rook-ceph-모니터링-및-성능-튜닝#6️⃣ \u003cstrong\u003eRook-Ceph 모니터링 및 성능 튜닝\u003c/strong\u003e":"1️⃣ Rook-Ceph 개요 및 아키텍처 1. Rook-Ceph이란? Rook-Ceph는 Kubernetes 환경에서 Ceph 스토리지 클러스터를 쉽게 관리할 수 있도록 지원하는 오픈소스 프로젝트입니다. Rook는 Ceph의 클러스터를 자동으로 배포하고, 관리할 수 있는 기능을 제공합니다. 이를 통해 Kubernetes에서 Ceph를 손쉽게 설치하고, 운영할 수 있습니다.\n2. Rook-Ceph 아키텍처 Rook-Ceph 아키텍처는 여러 구성 요소로 이루어져 있습니다. 각 구성 요소가 Kubernetes의 클러스터에서 어떻게 상호작용하는지 이해하는 것이 중요합니다.\nCeph Monitors (MON): 클러스터의 상태를 추적하고 클러스터의 일관성을 유지합니다. Ceph OSDs: 데이터를 저장하는 역할을 합니다. Ceph Managers (MGR): 클러스터의 관리 및 대시보드를 담당합니다. Rook Operator: Ceph 클러스터를 설치하고 관리하는 컨트롤러 역할을 합니다. 📌 💡 Rook-Ceph를 사용하면 Kubernetes 환경에서 Ceph를 손쉽게 배포하고 관리할 수 있습니다.\n2️⃣ Rook-Ceph 설치 및 설정 1. Rook-Ceph 설치 준비 Rook-Ceph를 설치하기 위해서는 먼저 Kubernetes 클러스터가 준비되어 있어야 하며, Helm이 설치되어 있어야 합니다.\n2. Rook-Ceph 설치 과정 # Rook 설치 kubectl create -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/crds.yaml kubectl create -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/operator.yaml kubectl create -f https://raw.githubusercontent.com/rook/rook/master/deploy/examples/cluster.yaml 3. 설치 후 확인 # Ceph 클러스터 상태 확인 kubectl -n rook-ceph get cephcluster 이 명령어로 클러스터가 제대로 설치되었는지 확인할 수 있습니다.\n📌 💡 설치 과정에서 필요한 리소스가 자동으로 생성되며, 클러스터 상태를 모니터링할 수 있습니다.\n3️⃣ Rook을 이용한 Ceph 클러스터 자동 배포 1. Rook Operator 활용 Rook Operator는 Ceph 클러스터를 Kubernetes에서 자동으로 배포하고 관리하는 역할을 합니다. 이를 통해 수동으로 설정할 필요 없이 Ceph 클러스터를 Kubernetes 환경에서 손쉽게 배포할 수 있습니다.\n2. 자동 배포 설정 # Ceph 클러스터 배포 YAML 파일 예시 apiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph namespace: rook-ceph spec: cephVersion: image: ceph/ceph:v15 storage: useAllNodes: true useAllDevices: true mon: count: 3 manager: count: 2 이 설정은 Ceph 클러스터를 자동으로 배포하며, 배포가 완료되면 클러스터가 Kubernetes 내에서 관리됩니다.\n📌 💡 Ceph 클러스터를 자동으로 배포하여 운영 효율성을 높일 수 있습니다.\n4️⃣ Ceph RBD StorageClass 생성 및 사용 1. RBD StorageClass란? RBD(RADOS Block Device)는 Ceph 클러스터에서 블록 스토리지를 제공하는 기능입니다. Kubernetes에서 RBD를 사용하려면 StorageClass를 생성해야 합니다.\n2. RBD StorageClass 생성 예시 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: rook-ceph-block provisioner: rook.io/block parameters: blockPool: replicapool fsType: ext4 이 StorageClass는 rook-ceph-block이라는 이름으로 RBD를 사용할 수 있게 설정합니다.\n3. StorageClass 사용 예시 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: rook-ceph-block 이 PersistentVolumeClaim(PVC)은 rook-ceph-block StorageClass를 사용하여 RBD 스토리지를 요청합니다.\n📌 💡 RBD를 사용하여 Kubernetes 내에서 블록 스토리지를 효율적으로 사용할 수 있습니다.\n5️⃣ CephFS를 PVC로 마운트하는 방법 1. CephFS란? CephFS는 Ceph의 파일 시스템 기능을 제공합니다. 이를 통해 파일 시스템 기반으로 데이터를 저장하고 Kubernetes에서 활용할 수 있습니다.\n2. CephFS PVC 생성 예시 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: cephfs-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi storageClassName: rook-cephfs 3. CephFS 마운트 예시 PVC로 요청한 CephFS를 Pod에 마운트합니다.\napiVersion: v1 kind: Pod metadata: name: cephfs-pod spec: containers: - name: cephfs-container image: busybox volumeMounts: - mountPath: /mnt/cephfs name: cephfs-volume volumes: - name: cephfs-volume persistentVolumeClaim: claimName: cephfs-pvc 이 설정은 CephFS를 Kubernetes Pod에 마운트하여 사용할 수 있게 합니다.\n📌 💡 CephFS를 PVC로 마운트하여 Kubernetes에서 파일 시스템을 활용할 수 있습니다.\n6️⃣ Rook-Ceph 모니터링 및 성능 튜닝 1. 모니터링 툴 활용 Rook-Ceph 클러스터의 상태와 성능을 모니터링하기 위해 Prometheus와 Grafana를 사용할 수 있습니다.\n2. Prometheus와 Grafana 설정 # Prometheus 및 Grafana 설정 파일 예시 apiVersion: v1 kind: ConfigMap metadata: name: rook-prometheus data: prometheus.yml: | scrape_configs: - job_name: 'rook-ceph' static_configs: - targets: ['rook-ceph-monitor:9100'] 3. 성능 튜닝 Ceph 클러스터의 성능을 최적화하려면 여러 파라미터를 조정할 수 있습니다. 예를 들어, OSD의 I/O 성능을 개선하려면 다음과 같이 설정을 수정할 수 있습니다.\napiVersion: ceph.rook.io/v1 kind: CephCluster metadata: name: rook-ceph spec: cephVersion: image: ceph/ceph:v15 storage: osds: resources: limits: memory: \"4Gi\" cpu: \"2\" 📌 💡 모니터링 툴을 활용하여 클러스터의 성능을 주기적으로 점검하고, 성능 튜닝을 통해 안정성을 유지할 수 있습니다."},"title":"Ceph과 Kubernetes (Rook-Ceph) 통합"},"/system/ceph/ceph10/":{"data":{"":"","11-ceph-클러스터-노드-추가-및-확장-ceph-osd-add#1️⃣1️ \u003cstrong\u003eCeph 클러스터 노드 추가 및 확장 (ceph osd add)\u003c/strong\u003e":"","2-erasure-coding-적용-및-활용-ceph-osd-pool-set#2️⃣ \u003cstrong\u003eErasure Coding 적용 및 활용 (ceph osd pool set \u0026hellip; erasure)\u003c/strong\u003e":"","3-ceph-bluestore-vs-filestore-차이점-및-마이그레이션#3️⃣ \u003cstrong\u003eCeph BlueStore vs Filestore 차이점 및 마이그레이션\u003c/strong\u003e":"","4-ceph-multi-cluster-설정-multi-site-stretch-cluster#4️⃣ \u003cstrong\u003eCeph Multi-cluster 설정 (Multi-site, Stretch Cluster)\u003c/strong\u003e":"","5-ceph-public-cloud-배포-aws-gcp-azure#5️⃣ \u003cstrong\u003eCeph Public Cloud 배포 (AWS, GCP, Azure)\u003c/strong\u003e":"","6-ceph과-ldapkerberos-인증-연동#6️⃣ \u003cstrong\u003eCeph과 LDAP/Kerberos 인증 연동\u003c/strong\u003e":"","7-cephx-보안-모델-및-액세스-제어#7️⃣ \u003cstrong\u003eCephX 보안 모델 및 액세스 제어\u003c/strong\u003e":"1️⃣1️ Ceph 클러스터 노드 추가 및 확장 (ceph osd add) 1. Ceph 클러스터 노드 추가의 필요성 Ceph 클러스터는 수평적으로 확장이 가능하며, 필요한 성능이나 용량을 충족하기 위해 새로운 노드를 추가할 수 있습니다. 노드를 추가하면 클러스터의 스토리지 용량을 확장하고, 더 많은 OSD(객체 저장 장치)를 추가하여 I/O 성능을 개선할 수 있습니다.\n2. ceph osd add로 노드 추가하기 새로운 OSD를 클러스터에 추가하려면 ceph osd add 명령어를 사용합니다. 먼저 OSD 디스크를 준비한 후, 아래와 같이 명령어를 실행합니다.\n# OSD를 추가하는 명령어 ceph osd create ceph osd crush add osd.\u003cosd-id\u003e \u003cweight\u003e root=default 이 명령은 새로운 OSD를 클러스터에 추가하고, CRUSH 맵에서 해당 OSD의 위치를 정의합니다.\n3. 확장 후 상태 점검 확장된 OSD의 상태를 점검하려면 아래 명령어로 확인할 수 있습니다.\n# OSD 상태 확인 ceph osd tree 📌 💡 Ceph 클러스터를 확장하여 더 많은 용량과 성능을 추가할 수 있습니다.\n2️⃣ Erasure Coding 적용 및 활용 (ceph osd pool set … erasure) 1. Erasure Coding이란? Erasure Coding은 데이터를 분할하여 여러 위치에 저장하고, 일부 데이터가 손실되더라도 복원할 수 있도록 하는 기술입니다. Ceph에서는 Erasure Coding을 사용하여 스토리지 효율성을 높이고, 데이터 복원성을 유지할 수 있습니다.\n2. Erasure Coding 적용 방법 Erasure Coding을 활성화하려면 ceph osd pool set 명령어를 사용하여 풀에 대해 Erasure Coding을 설정합니다.\n# Erasure Coding pool 생성 예시 ceph osd pool create mypool 64 64 erasure ceph osd pool set mypool crush_rule \u003crule\u003e 이 명령어는 새로운 풀을 생성하고, Erasure Coding을 적용합니다.\n3. Erasure Coding 장점 Erasure Coding은 데이터 중복을 최소화하면서도 데이터의 안전성을 높이는 장점이 있습니다. 다만, 데이터 복구 시간은 Replication보다 느릴 수 있습니다.\n📌 💡 Erasure Coding을 활용하여 데이터 중복을 최소화하고 스토리지 효율성을 개선할 수 있습니다.\n3️⃣ Ceph BlueStore vs Filestore 차이점 및 마이그레이션 1. BlueStore와 Filestore의 차이점 Ceph는 두 가지 주요 스토리지 백엔드인 BlueStore와 Filestore를 제공합니다.\nFilestore는 기존의 블록 장치에 데이터를 저장하는 방식으로, 오래된 방식입니다. BlueStore는 Ceph의 최신 스토리지 백엔드로, 성능과 효율성을 크게 개선하였습니다. 데이터를 직접 OSD 디스크에 저장하며, 내부적으로 RocksDB를 사용하여 메타데이터를 관리합니다. 2. Filestore에서 BlueStore로 마이그레이션 Filestore에서 BlueStore로 마이그레이션하려면 OSD를 다시 포맷하고 새로 생성해야 합니다.\n# 기존 OSD를 BlueStore로 마이그레이션 ceph osd out \u003cosd-id\u003e ceph osd crush remove \u003cosd-id\u003e ceph osd create \u003cosd-id\u003e --blue-store 이 명령어는 기존 OSD를 BlueStore로 변경하고, 새로운 OSD를 생성합니다.\n3. BlueStore 사용 시 장점 더 빠른 성능과 적은 CPU 사용 스토리지 공간 최적화 RocksDB로 메타데이터 관리, 더 나은 I/O 성능 📌 💡 BlueStore로 마이그레이션하면 성능과 효율성을 크게 향상시킬 수 있습니다.\n4️⃣ Ceph Multi-cluster 설정 (Multi-site, Stretch Cluster) 1. Multi-cluster란? Ceph Multi-cluster는 여러 Ceph 클러스터를 연동하여 운영할 수 있는 기능입니다. 이를 통해 다른 데이터 센터나 지역에 걸쳐 Ceph 클러스터를 분산하여 고가용성과 장애 복구 기능을 제공합니다.\n2. Stretch Cluster 설정 Stretch Cluster는 서로 다른 데이터 센터에 위치한 Ceph 클러스터를 하나로 묶는 방식입니다. 이를 통해 장애가 발생하더라도 데이터를 손실 없이 복구할 수 있습니다.\n# Stretch Cluster 설정 예시 ceph cluster ring add \u003cnew-cluster-ip\u003e ceph osd crush rule create-replicated \u003cnew-rule-name\u003e \u003cnew-crush-rule\u003e 3. Multi-site 설정 Multi-site 구성은 여러 Ceph 클러스터를 연결하여 데이터를 동기화하고, 다른 지역에서 데이터를 읽고 쓸 수 있도록 설정하는 방식입니다. 이를 통해 데이터 복제와 백업을 손쉽게 관리할 수 있습니다.\n📌 💡 Multi-cluster 및 Stretch Cluster 설정을 통해 Ceph 클러스터의 고가용성을 높이고, 장애 발생 시 빠르게 복구할 수 있습니다.\n5️⃣ Ceph Public Cloud 배포 (AWS, GCP, Azure) 1. Ceph 클러스터를 Public Cloud에 배포하기 Ceph 클러스터를 Public Cloud(AWS, GCP, Azure)에 배포하여 클라우드에서 제공하는 유연성을 활용할 수 있습니다. Cloud 환경에서 Ceph를 설치하면, 스토리지 요구 사항에 맞게 쉽게 확장할 수 있습니다.\n2. AWS에서 Ceph 배포 # AWS에 Ceph 클러스터 배포 ceph-deploy new \u003caws-nodes\u003e ceph-deploy install \u003caws-nodes\u003e 이 명령어로 AWS에서 Ceph 클러스터를 설치하고 관리할 수 있습니다.\n3. GCP/Azure에서 Ceph 배포 GCP와 Azure에서의 배포는 AWS와 유사하며, 각 클라우드 제공자의 인프라에 맞게 설정을 조정해야 합니다.\n📌 💡 Public Cloud에서 Ceph를 배포하여 클라우드 환경에서도 고가용성 및 확장성 있는 스토리지를 구축할 수 있습니다.\n6️⃣ Ceph과 LDAP/Kerberos 인증 연동 1. LDAP와 Ceph 인증 연동 Ceph는 LDAP 인증을 통해 사용자의 인증과 권한 관리를 중앙에서 처리할 수 있습니다. LDAP 서버와 Ceph를 연동하여 인증을 강화할 수 있습니다.\n2. Kerberos 인증 연동 Kerberos를 사용하여 Ceph에 대한 보안을 더욱 강화할 수 있습니다. Kerberos는 클라이언트와 서버 간의 인증을 처리하고, Ceph 클러스터에 대한 액세스를 제어합니다.\n3. 연동 설정 예시 # LDAP 설정 예시 ceph auth add client.\u003cusername\u003e mon 'allow r' osd 'allow rwx' 이 명령어를 통해 LDAP 인증과 Ceph의 연동을 설정할 수 있습니다.\n📌 💡 Ceph와 LDAP/Kerberos 인증 연동을 통해 보다 안전하게 클러스터에 액세스할 수 있습니다.\n7️⃣ CephX 보안 모델 및 액세스 제어 1. CephX 보안 모델 CephX는 Ceph의 보안 모델로, 클러스터 내에서의 액세스를 인증하고 제어하는 역할을 합니다. CephX는 사용자 및 클라이언트의 인증을 처리하고, 접근 제어 목록(ACL)을 기반으로 리소스에 대한 접근을 제어합니다.\n2. 액세스 제어 설정 CephX를 사용하여 클러스터의 각 리소스에 대한 접근 권한을 설정할 수 있습니다.\n# CephX 사용자 추가 예시 ceph auth add client.\u003cusername\u003e mon 'allow r' osd 'allow rwx' 3. CephX의 장점 세분화된 액세스 제어 클러스터와 데이터 보호 인증 및 암호화 지원 📌 💡 CephX를 통해 클러스터 내에서의 액세스를 안전하게 관리할 수 있습니다."},"title":"Ceph 확장 및 고급 설정"},"/system/ceph/ceph11/":{"data":{"":"","1-기업-환경에서의-ceph-구축-사례#1️⃣ \u003cstrong\u003e기업 환경에서의 Ceph 구축 사례\u003c/strong\u003e":"","2-openstack과-ceph-연동-사례#2️⃣ \u003cstrong\u003eOpenStack과 Ceph 연동 사례\u003c/strong\u003e":"","3-kubernetes-환경에서-ceph-활용-사례#3️⃣ \u003cstrong\u003eKubernetes 환경에서 Ceph 활용 사례\u003c/strong\u003e":"","4-ceph-고가용성-구성-haproxy-keepalived-활용#4️⃣ \u003cstrong\u003eCeph 고가용성 구성 (HAProxy, Keepalived 활용)\u003c/strong\u003e":"","5-ceph-배포-후-운영-및-유지보수-전략#5️⃣ \u003cstrong\u003eCeph 배포 후 운영 및 유지보수 전략\u003c/strong\u003e":"","6-실무에서-ceph-구축-시-고려해야-할-사항#6️⃣ \u003cstrong\u003e실무에서 Ceph 구축 시 고려해야 할 사항\u003c/strong\u003e":"1️⃣ 기업 환경에서의 Ceph 구축 사례 1. 기업 환경에서 Ceph을 사용하는 이유 기업 환경에서는 데이터 저장소에 대한 요구가 매우 높습니다. Ceph는 고가용성, 확장성, 비용 효율성을 제공하는 스토리지 시스템으로서, 특히 대규모 데이터를 처리하는 기업에서 많이 사용됩니다. 예를 들어, 금융, 제조업, 클라우드 서비스 제공업체 등에서는 Ceph을 사용하여 데이터를 안정적으로 저장하고 관리합니다.\n2. 구축 사례: 대형 금융기업 대형 금융기업에서는 대규모 트랜잭션 데이터를 처리하고, 빠르게 확장 가능한 스토리지를 필요로 합니다. Ceph을 도입하여 분산형 저장소를 구축하고, 데이터를 여러 노드에 분산시켜 성능을 최적화하였습니다.\n요구 사항: 높은 트랜잭션 처리 성능과 데이터 복제. 구성: Ceph 클러스터를 사용하여 OSD 노드, MON, MDS를 분리하고, CephFS와 RBD를 활용하여 다양한 데이터 서비스를 제공. 성공적인 요소: 고가용성 및 데이터 복제 전략을 통해 장애 발생 시 빠른 복구가 가능. 3. 구축 사례: 클라우드 서비스 제공업체 클라우드 서비스 제공업체는 Ceph을 사용하여 클라우드 스토리지 서비스의 확장성과 유연성을 확보했습니다. Ceph의 오브젝트 스토리지(RGW)는 S3 호환 API를 제공하여 다양한 클라우드 기반 애플리케이션과 통합됩니다.\n요구 사항: 클라우드 환경에서의 스토리지 통합. 구성: RBD, CephFS, RGW를 활용하여 저장소 서비스를 제공하고, 멀티 테넌시 지원. 성공적인 요소: 유연한 확장과 자동화된 스토리지 관리. 📌 💡 Ceph은 기업 환경에 맞게 다양한 요구 사항을 충족시키는 강력한 스토리지 솔루션입니다.\n2️⃣ OpenStack과 Ceph 연동 사례 1. OpenStack과 Ceph 연동 개요 OpenStack은 클라우드 인프라를 관리하는 플랫폼이며, Ceph은 오브젝트, 블록, 파일 시스템을 모두 지원하는 분산 스토리지 솔루션입니다. Ceph을 OpenStack과 연동하면 유연하고 고가용성 있는 클라우드 스토리지를 구축할 수 있습니다.\n2. 연동 사례: 퍼블릭 클라우드 환경 퍼블릭 클라우드 환경에서 Ceph과 OpenStack을 연동하여 사용한 사례로, Ceph은 OpenStack의 Cinder(블록 스토리지)와 Glance(이미지 서비스)와 통합되었습니다.\n요구 사항: 퍼블릭 클라우드 환경에서 스토리지 관리 효율성 증대. 구성: Ceph RBD를 OpenStack Cinder와 연동하여 블록 스토리지 제공, CephFS를 Glance와 통합하여 이미지 저장. 성공적인 요소: OpenStack과의 원활한 통합을 통해 유연한 스토리지 서비스 제공. 3. 연동 사례: 하이브리드 클라우드 환경 하이브리드 클라우드 환경에서는 Ceph을 온프레미스 데이터 센터와 퍼블릭 클라우드에 걸쳐 사용하여 데이터 이동 및 백업을 효율적으로 관리합니다.\n요구 사항: 온프레미스와 클라우드 환경 간 데이터 동기화. 구성: Ceph을 OpenStack에 연동하고, 멀티 클라우드 환경에서 데이터를 관리. 성공적인 요소: 클라우드 간 데이터 이동을 최적화하고 비용을 절감. 📌 💡 Ceph과 OpenStack 연동은 클라우드 환경에서 스토리지 관리를 통합하고 효율성을 극대화합니다.\n3️⃣ Kubernetes 환경에서 Ceph 활용 사례 1. Kubernetes와 Ceph 통합 Kubernetes는 컨테이너 오케스트레이션 플랫폼이며, Ceph은 컨테이너화된 애플리케이션에 대한 스토리지 백엔드를 제공합니다. Ceph은 Kubernetes 환경에서 Persistent Volume(PV)으로 사용할 수 있습니다.\n2. 활용 사례: CI/CD 파이프라인 구축 CI/CD 파이프라인에서 Ceph RBD를 사용하여 빌드 아티팩트를 저장하고, Kubernetes에서 이 데이터를 Persistent Volume으로 마운트하여 애플리케이션이 빌드 및 배포 과정에서 데이터를 사용할 수 있게 합니다.\n요구 사항: 지속적인 데이터 관리 및 클라우드 네이티브 애플리케이션의 데이터 저장. 구성: Ceph RBD를 Kubernetes의 Persistent Volume으로 사용, Helm을 이용해 Ceph 클러스터 배포. 성공적인 요소: CI/CD 파이프라인에서의 효율적인 데이터 처리. 3. 활용 사례: 동적 스토리지 프로비저닝 Kubernetes에서 Ceph을 사용하여 동적으로 스토리지를 프로비저닝하고, Kubernetes의 StatefulSet 및 Deployment에서 이를 마운트하여 데이터를 저장합니다.\n요구 사항: 자동화된 스토리지 프로비저닝 및 동적 리소스 할당. 구성: Ceph RBD와 CephFS를 Kubernetes의 StorageClass와 연동하여 스토리지 제공. 성공적인 요소: 자동화된 스토리지 관리 및 Kubernetes 환경에서의 효율적인 데이터 처리. 📌 💡 Kubernetes 환경에서 Ceph을 활용하여 컨테이너화된 애플리케이션에 대한 효율적인 스토리지 솔루션을 제공할 수 있습니다.\n4️⃣ Ceph 고가용성 구성 (HAProxy, Keepalived 활용) 1. Ceph 고가용성(HA) 구성 필요성 Ceph 클러스터는 고가용성을 제공할 수 있지만, MON(Monitor) 노드에 장애가 발생하면 클러스터가 영향을 받을 수 있습니다. HAProxy와 Keepalived를 사용하여 Ceph의 MON을 고가용성으로 구성할 수 있습니다.\n2. HAProxy와 Keepalived를 이용한 MON 고가용성 구성 HAProxy는 로드 밸런서를 제공하고, Keepalived는 MON 노드의 장애 조치를 처리하여 Ceph 클러스터의 MON 서비스를 고가용성으로 만듭니다.\n# Keepalived 예시 설정 interface eth0 virtual_ipaddress { 192.168.0.100 } 3. 구성 후 모니터링 구성 후 HAProxy와 Keepalived를 이용하여 Ceph MON 노드가 자동으로 장애를 감지하고, 클러스터가 중단되지 않도록 보장할 수 있습니다.\n📌 💡 HAProxy와 Keepalived를 활용하여 Ceph MON의 고가용성을 구성하고, 클러스터의 안정성을 높일 수 있습니다.\n5️⃣ Ceph 배포 후 운영 및 유지보수 전략 1. 운영 및 유지보수 개요 Ceph 클러스터를 운영하려면 주기적인 모니터링과 유지보수가 필요합니다. Ceph은 분산 시스템으로 복잡할 수 있으므로, 주기적인 점검과 성능 최적화 작업이 중요합니다.\n2. 운영 전략: 성능 모니터링 ceph -s와 ceph health 명령어를 사용하여 클러스터 상태를 주기적으로 점검하고, 성능 저하가 있는지 확인합니다. 또한, Ceph Dashboard를 이용하여 실시간 모니터링을 할 수 있습니다.\n3. 유지보수 전략: 패치 적용 및 업그레이드 Ceph 클러스터는 보안 패치와 버전 업그레이드가 필요합니다. ceph-deploy나 ceph upgrade 명령어를 사용하여 클러스터를 업그레이드하고 최신 기능을 사용합니다.\n📌 💡 주기적인 모니터링과 업그레이드를 통해 Ceph 클러스터를 안정적으로 운영할 수 있습니다.\n6️⃣ 실무에서 Ceph 구축 시 고려해야 할 사항 1. 스케일링 고려사항 Ceph 클러스터는 확장이 용이하지만, 확장을 진행할 때는 성능 요구 사항과 하드웨어 리소스를 충분히 고려해야 합니다.\n2. 장애 복구 및 백업 전략 Ceph 클러스터는 내장된 복제 및 복구 기능을 제공하지만, 중요한 데이터에 대해 별도의 백업 전략을 수립해야 합니다.\n3. 보안 고려사항 Ceph 클러스터의 보안 설정을 강화하기 위해 CephX 인증 및 액세스 제어를 활용하고, SSL/TLS 암호화를 적용하는 등의 보안 작업이 필요합니다.\n📌 💡 Ceph 클러스터 구축 시에는 성능, 확장성, 보안 등을 종합적으로 고려해야 합니다."},"title":"Ceph을 활용한 실제 구축 사례 및 Best Practice"},"/system/hardware/board/":{"data":{"":"Main Board main board란 서버의 주요 구성 부품(CPU, Ram, GPU, Nic 등)들을 설치, 연결, 조절하는 주 회로 기판(main circuit board)을 메인보드(main board)라고 한다.\n각 부품들이 하나로 연결되어 온전한 기능을 할 수 있게 해주는 회로가 존재하는 데, 이 회로들을 통해서 전기를 각 부품에게 배분해주는 역할을 하는 부품이 메인보드이다.\nmain board가 없으면 cpu가 램으로 처리된 정보를 보낼 수 없다.\n메인보드의 ROM에는 bios라는 일종의 작은 운영체제가 들어있다.\n연결된 모든 장치는 메인보드의 bus를 통하여 정보를 교류하며 그 흐름을 메인보드가 관리한다.\n전원공급장치(PSU)에서 공급된 전기는 메인보드에서 다시 전압을 조절하여 프로세서와 메인보드 및 각종 전자 장치에 공급됨\n메인보드에는 ram을 꽂을 수 있는 ram slot과 cpu slot, 내장 사운드 카드와 내장 랜카드, 외장 그래픽 카드, 사운드 카드, tv 수신 카드 등을 꽂을 수 있는 pci 슬롯을 제공함\n즉, main board란 각 파트의 브릿지 역할을 하는 토대이다.\nMain board Part PSU (Power Supply Unit) : 전원공급장치\nNIC (Network Interface Controller) : 외장 네트워크 인터페이스 카드 / IPMI는 내장\nPCI (Peripheral Component Interconnect Bus) : GPU, M.2 등 다른 파트를 장착하기 위한 slot\nCPU : 중앙 처리장치\nMemory : 메인 메모리로 주로 RAM(Random Access Memory)를 의미\nR/C (Raid Controller) : R/C는 각 Disk의 raid 정보를 가지고 있으며, 이를 관리하는 파트\nFAN : 전방에서 후방으로 공기를 순환시키는 선풍기\nDisk : 비휘발성 메모리로 데이터를 저장\nCase : 내부의 파트를 보호하기 위한 Case\nMemory slot은 하기 사진과 같이 채널을 이루고 있다. ","#":"","main-board#Main Board":""},"title":"What is M/B"},"/system/hardware/cpu/":{"data":{"":"CPU CPU란 (Central Processing Unit / main processor)으로 컴퓨터 시스템을 통제하고 프로그램(프로세스)의 연산을 실행하는 핵심적인 컴퓨터의 중앙 제어장치로 사람으로 따지면 뇌의 역할은 하는 내장형 칩을 의미한다.\nCPU는 기계어로 쓰인 명령어를 해석하며 실행하며, 외부의 정보를 입력받아 이를 기억하고, 연산하며 외부로 출력하는 역할을 수행한다.\nCPU는 크게 하기 3가지 구분할 수있다.\n처리할 명령어를 저장하는 프로세서와 레지스터 연산을 담당하는 산술논리연산장치(ALU / 연산장치), 명령어의 해석과 올바른 실행을 위해 CPU를 제어하는 제어장치(Control Unit) CPU의 구조 Core : CPU의 Core 즉 CPU의 핵심적인 역할을 수행해내는 중심부 역할을 말하며 이 코어에서 시스템의 모든 연산처리를 수행한다.\nThread : Core는 H/W 적인 관점에서 본 관점이며, Thread는 논리적인 작업 관점에서 처리 단위를 나뉘어 연산처리(HyperThreading)를 수행한다고 할 수 있다.\nClock : Clock은 동작 주파수로 CPU의 속도를 나타내는 단위이며, CPU가 데이터 양에 따라 속도가 달라질 수 있다. 단, 클럭이 높을 수록 발열 및 전력사용이 증가하기에, 최근에는 Multi Core 및 Multi Thread로 일정량의 Clock을 유지시켜 성능 향상을 꾀하고 있다.\nCache Memory : CPU 내부에서 임시로 사용하는 버퍼(Buffer) 메모리 중 하나로, 자주 사용하는 파일 등을 따로 모아두고, 나중에 해당 파일을 다시 실행하면 컴퓨터에서 바로 불러오는 파일을 의미하며, CPU가 하나의 데이터를 처리하는 동안 메인 메모리로부터 다음에 처리할 데이터를 불러와 대기하는 시킬 수 있어, 처리 속도를 높일 수 있다.\n정말 간단하게 요악하자면 Core는 몸통, Thread는 몸통에 달린 팔이라고 할 수 있지만, Thread가 무조건 많다고 좋은 것이 아니며, 대부분 Thread가 높을 수록 Clock이 낮기 때문에 개개인에 맞게 사용하는 것이 옳다고 할 수 있다.\nCPU 아키텍처 커널 및 프로세스 : 유저와 CPU 사이에서 서로의 언어를 해석하고 전달(OS, 프로그램)\nProtection ring : 유저가 사용하는 커널과 시스템이 사용하는 커널을 나눔으로써 시스템 자체를 보호(System call)\nVirtaul memroy / Page table : 유저가 사용하는 프로세스가 메모리에 접근하기 전에 사용되는 공간으로 실제 메모리 사용량의 변동성을 억제시켜 이를 통해 시스템의 안전성을 향상시키는 역할을 수행\nContext Switching : 실제 CPU는 특정 프로세스만을 지속적으로 점유시키고 있지 않으며, 다른 프로세스를 스왑하면서 사용되는 데, 이를 Context Switching이라 한다.\nPage Table Entry의 Protechtion bit : 유저가 사용하는 프로세스의 가상 메모리의 특정 주소는 커널 데이터를 담고 있어, sysc all을 호출하여 커널의 도움을 받으러 갈 때 Page Table을 커널 것으로 교체할 필요가 없어져, 파일의 Read/Write\n명령어(instrcution) : CPU 명령어 수행 동작\nPipeline : 파이프라인은 CPU가 하나의 명령어를 처리하는 과정도 너무 복잡하고 많기 때문에, 이를 잘게 쪼개서 여러 가지 작은 단계로 나누어 처리하는 방식\nCache : 주메모리에서 값을 읽는 동작은 CPU의 명령어 처리 속도에 비하면 한참 느리다. 따라서 이 갭을 줄이기 위해 매우 빠르지만 작은 저장 공간이 CPU에 있는데, 이를 캐시라고 한다.\n분기(Branch) 명령어는 어떤 조건이 맞으면 다음에 실행할 명령어의 위치를 임의로 지정할 수 있게 해준다. 이는 같은 명령어들을 반복해서 실행하거나 조건에 따라 다른 일을 하고 싶을 때 사용하는 매우 기본적인 명령어다.\n비순차적 명령어 처리(OoOE)는 파이프라인의 송출(Issue) → 실행(Execute) → 회신(Writeback) 단계에 한해서 늦게 온 명령어가 일찍 온 명령어를 새치기할 수 있는 기술\nSpeculative execution : 어떤 명령어가 특정 파이프라인 단계에 필요한 정보가 없어서 진행이 막혔을 때, 필요한 정보를 예측해서 높은 확률로 맞힌다면 틀렸을 때의 다소 큰 손해를 넘어서는 이익을 취할 수 있다. 고성능의 CPU는 이러한 예측에 기반한 갖가지 기술들을 적극 활용하고 있다.\nCPU 처리과정 CPU의 처리과정을 간단하게 나열하면 아래와 같다.\nFetch: 실행할 명령어들을 가져온다.\nDecode: 이후 처리를 돕기 위해 명령어의 종류를 선택 및 결정한다. / CISC, RISC\nRename: 명령어가 가리키는 레지스터를, 내부에 숨어있는 물리적 레지스터로 매핑한다.\nDispatch: 명령어가 실행하기 위해 기다리는 대기열에 넣는다.\nIssue: 대기열에 있는 명령어가 실행될 수 있으면[12] 실행하기 위한 장치로 보낸다.\nExecute: 실행한다.\nWriteback: 결과값을 레지스터에 써야 한다면 쓴다. 결과값을 기다리고 있던 명령어가 있다면 결과가 생겼다고 알려준다.\nCommit: 명령어 수행을 완료하고, 명령어 실행을 위해 할당받은 자원을 모두 토해낸다. 명령어의 실행 결과를 사용자에게 노출시킨다.","#":"","cpu#CPU":""},"title":"What is CPU"},"/system/hardware/disk/":{"data":{"":"Disk Disk란 보조기억장치로 RAM(주기억장치)이 전원이 없어지면 데이터가 삭제되는 반면, Disk는 전원이 없어진 이후에도 데이터가 유지되는 비휘발성 메모리이다. HDD HDD (Hard Disk drive) 하드 디스크는 플로피 디스크와 같은 자기 기록 매체이나, 플로피 디스크와 다르게 금속 재질의 플래터에 데이터를 기록하기 때문에 플로피디스크와 구분짓기 위해 재질적으로 단단하다는 뜻으로 하드디스크라 한다. HDD의 원리와 구조 저장되는 모든 데이터는 원리적으로 0 or 1 두 디지털 신호로 의해 이루어진다. ","#":"","disk#Disk":"","hdd#HDD":""},"title":"What is Disk"},"/system/hardware/memory/":{"data":{"":"What is RAM RAM이란 Random Access Memory로 ROM Read Only Memotry과는 반대로 휘발성 메모리를 의미한다.\nRandom Access라는 말은 어느 위치에든 똑같은 속도로 접근하여 읽고 쓸 수 있음을 의미하며, 사용자가 자유롭게 내용을 읽고 쓰고 지울 수 있는 기억장치로, 컴퓨터가 켜지면 CPU는 연산 및 동작에 필요한 내용이 전원이 유지되는 동안에 RAM에 저장된다.\n주로 주기억장치로 불리며 보조기억장치로는 흔이 우리가 사용하는 Disk(HDD, SDD 등)가 있다.\n단, RAM의 범주에는 ‘임의 접근’할 수 있는 메모리이므로 HDD 등의 장치들도 RAM의 범주에 들어가기도 하지만, HDD 등의 기억장치와 같이 어느 위치에나 직접 접근할 수 있으나 데이터의 물리적 위치에 따라 읽고 쓰는 시간에 차이가 발생하게 되는 기억장치들은 Direct Access Memory 또는 Direct Access Data Storage라고 부른다.\n휘발성과 비휘발성 휘발성 메모리 SRAM (Static Random Access Memory) : 전원이 차단되자마자 즉시 데이터가 삭제되는 메모리 DRAM (Dynamic Random Access Memory) : 내부에 전류를 일시적으로 저장하는 역할을 하여 약 5분 정도 데이터가 유지 될 수 있는 메모리 메모리 구조 프로그램이 실행되기 위해서는 프로그램이 메모리에 로드(load)되어야 하며, 이에따라 변수들이 저장될 메모리가 필요하다. 즉, 위 사진과 같이 프로그램의 실행을 위해 Code, Data, Heap, Stack 영역을 할당 받을 수 있다. Code 영역 실행할 프로그램의 코드가 저장되는 영역으로 텍스트(code) 영역이라고도 부른다. 상수 및 컴파일 될 기계어가 들어가며, 프로그램이 시작하고 종료될 때까지 메모리를 점유한다. CPU에서는 Code 영역에 저장된 명령어를 순차적으로 가져가 처리한다. Data 영역 프로그램의 전역 변수와 정적(Static) 변수가 저장되는 영역으로 Data 영역 또한 프로그램이 시작하고 종료될 때까지 메모리를 점유한다. Heap 영역 사용자가 직접 관리할 수 있는 영역으로, 사용자에 의해 동적으로 메모리 공간이 할당되고 해제된다. (malloc() 또는 new 연산자를 통해 할당 / free() 또는 delete 연산자를 통해서 해제) 메모리의 낮은 주소에서 높은 주소의 방향으로 할당되며, 런타임 시에 크기가 결정된다. 프로그램에 필요한 개체의 개수나 크기를 미리 알 수 없는 경우 사용 가능 Stack 영역 함수의 호출과 관계되는 지역 변수와 매개변수가 저장되는 영역으로, 함수의 호출과 함께 할당되며 함수의 호출이 완료되면 소멸한다. 함수의 호출 정보를 스택 프레임(Stack frame)이라고 한다. 메모리의 높은 주소에서 낮은 주소의 방향으로 할당된다. ","#":""},"title":"What is RAM"},"/system/hardware/nic/":{"data":{"":"","network-interface-card#Network Interface Card":" NIC(Network Interface Controller)이란 서버가 네트워크에 연결하여 통신하기 위해 사용하는 하드웨어 장치이다.\nOSI 계층 1(물리 계층)과 계층 2(데이터 링크 계층) 장치를 가지는데, MAC 주소를 사용하여 낮은 수준의 주소 할당 시스템을 제공하고 네트워크 매개체로 물리적인 접근을 가능하게 한다."},"title":"What is NIC"},"/system/hardware/rc/":{"data":{"":"","raidcontroller#RaidController":" R/C(Raid Controller)란 RAID 인프라에서 디스크 드라이브를 관리하는 스토리지 구성 요소 유형입니다. 즉, RAID 디스크를 관리하는 서버에 물리 디스크 드라이브를 논리 장치로 제공한다. RAID 컨트롤러는 디스크 어레이 컨트롤러를 의미힌다. "},"title":"What is RC"},"/system/openstack/openstack/":{"data":{"":"\nOpenStack\nOpenStack의 개요\nKeyston : 인증을 관리하는 서비스\nGlance : 이미지를 관리하는 서비스\nNova : 가상의 서버를 생성하는 서비스\nNeutron : 네트워크를 관리하는 서비스\nCinder : 블록 스토리지 서비스\nCeilometer : 리소스의 사용량과 부하를 관리하는 서비스\nHorizon : 외부 인터페이스 대시보드 서비스\nSwift : 오브젝트 스토리지 관리 서비스\nHeat : 오케트스트레션 서비스\nTrove : 데이터베이스 서비스\nSahara : 데이터 프로세싱 서비스\nIronic : 베어메탈 서비스\nOpenStack Training\nOpenStack Ussuri\nOpenStack Ussuri : Overview\nOpenStack Ussuri : 환경설정\nOpenStack Ussuri : Keystone\nOpenStack Ussuri : Glance\nOpenStack Ussuri : Nova\nOpenStack Ussuri : Neutron\nOpenStack Ussuri : Cinder\nOpenStack Ussuri : Horizon\nOpenStack Ussuri : Swift\nOpenStack Ussuri : Heat\nOpenStack Ussuri : Gnocch\nOpenStack Ussuri : Trove\nOpenStack Ussuri : Designate\nOpenStack Ussuri : Brabican\nOpenStack Ussuri : Rally\nOpenStack Ussuri : Manila\nOpenStack Stain DevStack PackStack "},"title":"OpenStack docs"},"/system/openstack/openstack/ceilometer/":{"data":{"":"리소스의 사용량과 부하를 관리하는 서비스 : Ceilometer 리소스의 사용량과 부하를 관리하는 서비스 : Ceilometer Ceilometer은 각 서비스들의 예상 부하에 따라 추가 작업과 노드를 관리하는 역할을 수행 클라우드에서 배포된 자원의 사용량과 성능을 측정해 사용자가 자원 상태를 모니터링 할 수 있는 기능을 제공 Ceilomete는 리버티 버전에서 기존에 알람 서비스를 하던 부분을 aodh로 분리 핵심 서비스 역할 Polling agent OpenStack 서비스를 폴링하고 미터를 빌드하도록 설계된 데몬 프로세스 Notification agent 메시지 큐에서 알림을 듣고 이벤트 및 샘플로 변환하며 파이프 라인 조치를 적용하도록 설계된 데몬 프로세스 Ceilometer로 표준화 및 수집 된 데이터는 다양한 대상으로 전송 Gnocchi는 이러한 스토리지 및 쿼리를 최적화하기 위해 시계열 형식으로 측정 데이터를 캡처하도록 개발 Aodh는 사용자 정의 규칙이 깨졋을 때 경고를 보낼 수 있는 경보 서비스 Ceilomter은 이와 같이 리소스를 감시 및 예상하여 다른 작업을 수행할 수 있도록 하는 서비스를 의미 데이터 수집 위의 그림과 같이 Polling agents에서 각 서비스들의 API의 리소스를 읽어 데이터를 수집 Notification agents는 Polling agents에서 수집한 데이터들을 토대로 Ceilomter 서비스 혹은 Events로 변환하는 역할을 수행 데이터 처리 수집된 데이터를 토대로 Notification bus를 통해 엔드 포인트로 재분배하여 이벤트 및 샘플로 처리하느 역할을 수행 데이터 요청 데이터의 요청은 수집된 자료들을 토대로 서로 데이터를 주고 받으며, Polling agents라는 클라우드 컨트롤러에서 처리되며, 여러 플러그인을 사용하여 데이터를 통신합니다. 데이터 처리 및 변형 위의 그림은 수집된 데이터를 수집, 분석 및 변형 배포하는 과정을 나타낸 그림으로 Ceilomter은 각 서비스들의 데이터를 수집하여, 변형시키는 여러 소스를 제공 OpenStack에서의 Ceilomter의 위치 구성요소 역할 Ceilometer-colletcor 중앙 관리서버에서 실행되며, Queue 모니터링 하는 서비스 Ceilometer-agent-notification ceilometer-collector와 함꼐 중앙 관리서버에서 실행, Queue를 이용해 이벤트와 미러링 데이터를 기록 Ceilometer-agent-compute 각 컴퓨팅 노드에 설치해서 실행, 자원 활용 통계로 사용 Ceilometer-account-central 중앙 관리 서버에서 실행, 인스턴스에 연결되지 않은 자원이나 컴퓨터 노드의 활용 가능한 자원 통계를 폴링 Ceilometer-alarm-evaluator 하나 이상의 중앙 관리 서버에서 실행, 슬라이딩 시간대에 임계 값을 추가할 때 발생하는 경보 시점을 결정 Ceilometer-alarm-nottifier 하나 이상의 중앙 관리 서버에서 실행되며, 샘플을 수집하는 임계 값 평가에 따라 알람을 설정 Ceilometer database 수집한 데이터를 저장할 Ceilometer 데이터 베이스 Ceilometer-api 하나 또는 그 이상의 중앙 관리 서버에서 실행되며 데이터베이스에서 데이터 엑세스를 제공 ","#":"","리소스의-사용량과-부하를-관리하는-서비스--ceilometer#\u003cstrong\u003e리소스의 사용량과 부하를 관리하는 서비스 : Ceilometer\u003c/strong\u003e":"","리소스의-사용량과-부하를-관리하는-서비스--ceilometer-1#\u003cstrong\u003e리소스의 사용량과 부하를 관리하는 서비스 : Ceilometer\u003c/strong\u003e":""},"title":"Ceilometer"},"/system/openstack/openstack/cinder/":{"data":{"":"블록 스토리지 서비스 : Cinder 블록 스토리지 서비스 : Cinder Cinder은 bloack storage 서비스로 storage를 가상화 시켜 여러 스토리지로 사용하거나, 공유할 수 있는 서비스 Cinder은 하나 이상의 노드에서 실행되도록 설계되었으며, SQL 기반 중앙 데이터베이스를 사용 Cinder은 집계 시스템을 사용하여 여러 데이터 저장소로 이동이 가능 Cinder 구성요소 구성요소 역할 DB 데이터저장을 위한 SQL 데이터베이스로, 모든 구성요소에서 사용 Web Dashboard API와 통신할 수 있는 외부 인터페이스 api http 요청을 받고 명령을 변환하여 대기열 또는 http를 통해 다른 구성요소와 통신 Auth Manager 사용자/ 프로젝트/ 역할에 따라, 리소스의 대한 허용을 결정 Scheduler 볼륨을 가져올 호스트를 결정 volume 동적으로 부착 가능한 블록 장치를 관리 backup 블록 저장 장치의 백업을 관리 외부 인터페이스인 dash-board에서 요청을 보내면, Cinder-api가 keyston에게 인증을 확인하기 위해 요청을 보낸다. 인증이 완료되면 DB를 읽어 알맞은 리소스를 생성, 혹은 할당하는 프로세스를 가진다. Cinder의 논리 아키텍처 구성요소 역할 Cinder-api 요청에 따라 storage를 할당, 확장하는 기능을 수행 Queue 각 구성요소 간의 통신기능을 수행 Cinder database Cinder 서비스를 수행하기 위한 일련의 정보들은 보관, 관리하는 DB cinder voulme Cinder 서비스를 통해 가상화되어진 Storage voulme, voulme을 관리 및 업데이트 volume provider storage volume을 생성, 확장하는 서비스 cinder scheduler 요청이 다수인 경우 큐를 통해 받은 메시지를 수행 Nova에서 생성된 인스턴스에서 확장해서 사용할 수 있는 저장 공간을 생성, 삭제하고 인스턴스에 연결 할 수 있는 기능을 제공 Cinder는 물리 하드 디스크를 LVM(Logical Volume Manager)으로 설정 설정된 LVM은 cinder-conf와 nova.conf의 환경을 설정해서 cinder-volume을 할당 cinder-api로 생성된 볼륨은 단일 인스턴스 또는 여러 인스턴스에 할당 할 수 있음 Cinder driver type Cinder 기본 블록 스토리지 드라이버는 iSCSI 기반의 LVM LVM이란 하드 디스크를 파티션 대신 논리 볼륨으로 할당하고, 디스크 여러 개를 좀 더 효율적이고 유연하게 관리할 수 있는 방식을 의미 블록 장치는 물리 볼륨으로 초기화해야 하며, 논리 볼륨으로 생성하려면 물리 볼륨을 볼륨 그룹으로 통합해야 함 ","#":"","블록-스토리지-서비스--cinder#\u003cstrong\u003e블록 스토리지 서비스 : Cinder\u003c/strong\u003e":"","블록-스토리지-서비스--cinder-1#\u003cstrong\u003e블록 스토리지 서비스 : Cinder\u003c/strong\u003e":""},"title":"Cinder"},"/system/openstack/openstack/glance/":{"data":{"":"이미지를 관리하는 서비스 : Glance 이미지를 관리하는 서비스 : Glance Cloud Computing을 사용하기 위해서는 Virtual Machine을 생성하기 위한 이미지가 필요로 하며, Glance는 Nova에서 생성하는 인스턴스의 운영체제에 해당하는 이미지를 관리하는 서비스 Glance의 구성요소 Glance는 위의 그림과 같이 3가지의 구성요소로 이루어져 있다 구성요소 역할 Glance-api 이미지를 확인/ 복구/ 저장하는 등의 질의를 하기 이한 api 요청/ 응답을 담당 Glance-registry 이미지에 대한 메타데이터를 저장하고 처리하는 역할을 담당 및 Glance database에 저장된 데이터를 불러들이는 역할을 수행 Glance-database 이미지의 관련 정보들을 보관 논리 아키텍처의 Glance Glance 사용자들은 glance-api로 이미지를 등록, 삭제, 관리 glance-api는 glance-registry와 Glance database에서 이미지를 관리 이미지를 등록할 때는 glance-registry로 Glance database에 저장 등록된 이미지를 사용할 때는 Glance database에 바로 사용을 요청 관리자는 운영하려는 운영체제의 이미지를 glance-registry로 Glance database에 등록 가상 머신 이미지 포맷 aki: 아마존 커널 이미지 ami: 아마존 머신 이미지 ari: 아마존 ram 디스크 이미지 iso: 광학 디스크나 CD-ROM의 데이터 콘텐츠를 지원하는 아카이브 포맷 qcow2: QEMU 에뮬레이터가 지원하는 포맷, 동적으로 확장할 수 있으며, Copy on Write를 지원 raw: 구조화되지 않은 디스크 포맷 vdi: VirtalBox 모니터와 QEMU 에뮬레이터가 지원하는 디스크 포맷 vhd: VHD 디스크 포맷은 VMware, Xen 마이크로소프트, VirtualBox 같은 가상 머신 모니터가 사용하는 일반적인 디스크 포맷 vhdx: VHDX 디스크 포맷은 큰 디스크 크기를 지원하는 VHD 형식의 향상된 버전 vmdk: 일반적인 디스크 포맷으로 여러 가상 머신 모니터가 지원 컨테이너 포맷(container Format) aki: 아마존 커널 이미지 ami: 아마존 머신 이미지 bare: 아마존 ram 디스크 이미지 docker: Docker 컨테이너 포맷 ova: tar 파일의 OVF 패키지 ovf: OVF 컨테이너 포맷 Glance 명령어 현재 이미지 목록 확인 openstack image list 특정 이미지의 자세한 정보 확인 openstack image show [이미지 이름] 이미지 삭제 openstack image delete [이미지 이름] 이미지 추가 openstack image create --public --container-format bare --disk-format qcow2 --file [경로를 포함한 이미지 파일 이름] [이미지 이름] 커스텀 이미지 생성 xming 윈도우에 설치 CentOS 준비 후 CentOS에 가상머신 프로그램 설치 및 실행 $ yum install qemu kvm qemu-kvm libvirt virt-install bridge-utils virt-manager dejavu-lgc-sans-fonts virt-viewer $ systemctl restart libvirtd ISO 파일로 qcow2 각 이미지에 맞는 파일 생성 qemu-img create -f qcow2 [이미지 파일 위치] [이미지 파일 크기] qemu-img create -f qcow2 /test/centos7.qcow2 10G ISO로 가상머신 생성 $ virt-install --name centos \\ --ram 1024 --disk \\ [비어있는 이미지 파일 위치],format=qcow2 \\ --network network=default \\ --graphics vnc,listen=0.0.0.0 \\ --noautoconsole \\ --os-type=linux \\ --os-variant=centos7.0 \\ --location=[ISO 위치] 본체 윈도우에서 putty x11 설정 Putty -\u003e SSH -\u003e X11 -\u003e Enable X11 Forwarding 체크 -\u003e X display location : localhost:0 설정 후 접속 virt-manager 생성한 QEMU 가상머신 설정 SELINUX 끄기 acpid 설치 및 설정 cloud-init 및 cloud-utils 설치 및 설정 /etc/sysconfig/network qemu-guest-agent 설치 및 설정 grub 수정 생성한 가상머신에서 이미지 작업 ( 커스터 마이징 ) 설치 후 설정 yum install -y /usr/bin/virt-sysprep virt-sysprep -d centos \u003c-네트워크 장치의 MAC주소와 같은 정보를 삭제하는 작업 virsh undefine centos \u003c-가상머신 삭제하는 작업 ","#":"","glance-명령어#\u003cstrong\u003eGlance 명령어\u003c/strong\u003e":"","이미지를-관리하는-서비스--glance#\u003cstrong\u003e이미지를 관리하는 서비스 : Glance\u003c/strong\u003e":"","이미지를-관리하는-서비스--glance-1#\u003cstrong\u003e이미지를 관리하는 서비스 : Glance\u003c/strong\u003e":"","커스텀-이미지-생성#\u003cstrong\u003e커스텀 이미지 생성\u003c/strong\u003e":""},"title":"Glance"},"/system/openstack/openstack/heat/":{"data":{"":"오케스트레이션 서비스 : Heat Heat Heat는 탬블릿과 Stack을 사용하여 자동으로 인스턴스의 리소스를 추가하거나 줄이는 서비스 오케스트레이션은 자원 관리, 배치, 정렬을 자동화하는 것 오케스트레이션은 인스턴스 생성에 대한 일련의 과정을 자동화해서 인프라를 쉽게 배포할 수 있도록 하는 탬플릿 기반 엔진 오케스트레이션에서 사용되는 템플릿 언어는 인프라, 서비스, 응용프로그램, 프로비저닝, 자동화 컴퓨팅, 스토리지, 네트워킹, 자동 스케일링 등에 사용 가능 Heat의 논리 아키텍처 구성요소 역할 heat-api RPC heat 엔진에 전송해서 요청된 API를 처리한 REST API를 제공 heat-api-cfn AWS CloudFormation과 호환되는 AWS 타입의 Query API를 제공 heat-engine 템플릿을 생성하고, Consumer(API를 사용하려고 접근하는 애플리케이션이나 서비스)를 다시 이벤트로 제공하는 오케스트레이션의 주 작업을 수행 queue 각 서비스들이 통신을 하기 위한 서비스 ","#":"","heat#\u003cstrong\u003eHeat\u003c/strong\u003e":"","오케스트레이션-서비스--heat#\u003cstrong\u003e오케스트레이션 서비스 : Heat\u003c/strong\u003e":""},"title":"Heat"},"/system/openstack/openstack/horizon/":{"data":{"":"외부 인터페이스 대시보드 서비스 : Horizon 외부 인터페이스 대시보드 서비스 : Horizon 사용자가 웹 UI로 인스턴스 생성, 삭제, 관리 등을 쉽고 빠르게 처리할 수 있게 지원 Horizon은 아파치 웹 서버를 사용 및 Python, Django 프레임워크로 구현 논리 아키텍처의 Horizon 논리 아키텍처에서 보이는 Horizon은 단순히 Horizon 자체 모듈만 존재 모든 서비스의 API와 연동해서 사용자에게 웹 서비스를 제공 ","#":"","외부-인터페이스-대시보드-서비스--horizon#\u003cstrong\u003e외부 인터페이스 대시보드 서비스 : Horizon\u003c/strong\u003e":"","외부-인터페이스-대시보드-서비스--horizon-1#\u003cstrong\u003e외부 인터페이스 대시보드 서비스 : Horizon\u003c/strong\u003e":""},"title":"Horizon"},"/system/openstack/openstack/ironic/":{"data":{"":"","#":"Ironic 베어메탈 서비스 Ironic 물리적인 컴퓨터를 관리하고 자원을 제공하는 구성요소의 모음\nIronic은 구성에 따라 다음과 같은 다른 여러 오픈스택 서비스와 상호 작용할 수 있음\nIPMI 메트릭을 사용하는 오픈스택 텔레미터 모듈(Ceilometer)\n인증 요청 및 다른 오픈스택 서비스를 인증하는 오픈스택 인증 서비스(Keystone)\n이미지 및 이미지 메타데이터를 검색할 수 있는 오픈스택 이미지 서비스(Glance)\nDHCP 및 네트워크를 구성하는 오픈스택 네트워크 서비스(Neutron)\n오픈스택 네트워크 서비스인 Nova는 베어메탈 서비스와 함꼐 작동하고, 인스턴스를 관리하는 사용자용 API를 제공\n오픈스택 컴퓨트 서비스는 베어메탈 서비스가 제공하지 않는 예약 기능, 테넌트 할당량, IP 할당, 기타 서비스를 제공\n오픈스택 오브젝트 스토리지 서비스인 Swift는 드라이브 설정, 사용자 이미지, 배포 로그 및 점검 데이터 임시 저장 장소를 제공\nIronic은 다음 요소로 구성\nironic-api: 응용프로그램 요청을 원격 프로시저 호출(RPC)을 이용해서 ironic-conductor로 전송한 후 응용프로그램 요청을 처리하는 RESTful API\nironic-conductor: 노드를 추가, 편집, 삭제하며 IPMI 또는 SSH를 사용해 노드를 켜고 끌수 있음, 베어메탈 노드를 프로비저닝, 배치 정리 수행\nironic-python-agent: 원격 엣세스, 하드웨어 제어, 하드웨어 기본 스펙으로 ironic-conductor 및 ironic-inspector 서비스를 제공하려고 임시 RAM 디스크에서 실행되는 python 서비스","ironic#\u003cstrong\u003eIronic\u003c/strong\u003e":""},"title":"Ironic"},"/system/openstack/openstack/keystone/":{"data":{"":"인증을 관리하는 서비스 : Keyston 인증을 관리하는 서비스 : Keystone Keystone은 인증 토큰을 비롯해 테넌트 및 사용자 관리, 서비스의 엔드포인트 URL 등을 관리하는 서비스 Keystone은 openstack의 백엔드에서 RBAD ( Role Based Access Control )을 통해 사용자의 접근을 제어하는 등의 인증 ( Identify ) 서비스로 사용되며 다음과 같은 기능으로 이루어져 있음 Keystone의 구성요소 구성요소 역할 user 사람 또는 오픈 스택 서비스를 이용하는 서비스 ( nova, neutron, cinder 등 )을 의미 User은 특정 프로젝트에 할당할 수 있으며, 증복을 허용하지 않음 Authentication 사용자의 신분을 확인하는 절차로, 특정 값을 통해 Keystone이 이를 검증 보통 인증을 위한 자료로는 ID, PW가 사용되며 Keystone은 인증확인 시 인증토큰을 방행 Token RBAD의 신분을 증명하기 위해 사용되는 텍스트 데이터 Token type fernet, uuid, pki, pkiz 어떤 자원에 접근이 가능한지 범위가 지정되어 있음 ( 시간 제한 있음 ) Project Keystone V2까지 Tenant라는 이름으로 사용 ( V3 이후 Project ) 어떤 자원이나 어플리케이션에 대한 권리를 가진 보안그룹 프로젝트는 특정 도메인에 의해 소유 Endpoint 사용자가 서비스를 이용하기 위해 연결정보를 제공하는 접근 가능한 네트워크 주소 ( URL ) EndPoint type admin, internal, public Role 사용자가 어떤 동작을 수행하도록 허용하는 규칙 사용자가 가지는 역할은 사용자에게 발행된 토큰을 통해 확인 사용자가 서비스를 호출하면, 서비스는 토큰에 저장된 사용자의 역할을 해석하여 허용유무 결정 Domain 구성요소를 효과적으로 관리하기 위한 사용자, 그룹, 프로젝트의 집합 사용자들은 한 도메인의 관리자의 권한 등을 부여받는 방식으로 역할을 부여가능 Domain, Project, Group, User, Rule 개념과 관계 Keystone은 위에도 언급하였 듯이 사용자 인증 부분과 서비스 인증 부분을 관리 사용자일 때는 사용자 ID와 패스워드, 사용자 권한의 롤( Roll )을 등록 서비스일 때는 서비스를 등록하고 해당 서비스의 엔드포인트 URL을 등록 도메인(Domain)은 서로 분리되어 있음 각 도메인에는 프로젝트와 사용자가 있음 프로젝트는 사용자를 가질 수 있음 사용자에게는 롤이 있으며, 여러 프로젝트의 구성원이 될 수 있음 관리자 롤(Admin Role)을 가진 사용자끼리, 일반 사용자롤(Member Role)을 가진 사용자간의 그룹핑(Grouping)을 할 수 있음 Keystone의 논리 아키텍처 Keystone의 논리 아키텍처는 토큰(Token), 카탈로그(Catalog), 정책(Poliy), 인증(Identity) 으로 구성 구성요소 역할 Token Backend 사용자별 토큰을 관리 Catalog Backend 오픈스택에서 모든 서비스의 엔드포인트 URL을 관리 Policy Backend 테넌트, 사용자 계정, 롤 등을 관리 Identity Backend 사용자 인증을 관리 Openstack에서 Keystone 위치 Openstack Keystone은 모든 서비스를 관장하는 위치 모든 User, Service는 Keystone의 인증을 통해서만 요청, 응답이 가능 Keystone은 타인이나 해커에게서 시스템을 안전하게 보호하고 사용자 등록, 삭제, 권한 관리, 사용자가 접근할 수 있는 서비스 포인트 관리와 다른 API들의 인증 등의 전체적인 인증 프로세스를 관리하는 역할을 수행 ","#":"","인증을-관리하는-서비스--keyston#\u003cstrong\u003e인증을 관리하는 서비스 : Keyston\u003c/strong\u003e":"","인증을-관리하는-서비스--keystone#\u003cstrong\u003e인증을 관리하는 서비스 : Keystone\u003c/strong\u003e":""},"title":"Keystone"},"/system/openstack/openstack/neutron/":{"data":{"":"네트워크를 관리하는 서비스: Neutron 네트워크를 관리하는 서비스: Neutron Neutron은 네트워크 서비스로 여러 노드에 여러 프로세스를 배치하는 독립형 서비스 프로세스는 서로 및 다른 OpenStack의 서비스와 상화 작용 Neutron의 논리 아키텍처 구성요소 기능 neutron-server network api의 기능 및 네트워크 확장 기능을 서비스하며, 각 포트의 대한 모델 및 Pfmf 지정, AMQP를 사용하여 데이터베이스와 통신하는 플러그인을 통해 수행 neutron-L2-agent OVS 가상 Bridge 사이에서데이터 패킷을 전달하기 위한 중계장치 neutron-l3-agent 태넌트 네트워크에서 VM의 외부 네트워크 엑세서를 위한 L3/ NAT 전달을 제공 neutron-dhcp-agent 테넌트 네트워크에 DHCP 서비스를 제공, DHCP agent는 메시지 큐에 엑세스할 수 있는 권한이 필요 Queue 다른 서비스 간의 통신의 역할을 수행 Neutron Database Neutron 서비스를 수행하기 위한 일련의 정보들은 보관, 관리하는 DB Neutron 3rd Party Plugin Neutron 서비스의 안정적인 통신 역할을 수행 plugin agent 각 compute node에서 실행되며 로컬 vswitch을 구성 및 관리 network provider services 테넌트 네트워크에 추가 네트워킹 서비스를 제공 Neutron은 다양한 네트워크 플러그인이나 네트워크 모델을 지원 사용자는 Neutron API를 이용해 neutron-server로 IP 할당을 요청 neutron-server 들어온 요청을 Queue로 다시 요청 Queue는 neutron-dhcp-agent와 Neutron 3rd Party plugin으로 IP 할당 지시를 내림 neutron-dhcp-agent와 Neutron 3rd Party Plugin은 지시 받은 작업 수행을 시작 neutron-server는 수시로 작업 상태를 Neutron database에 저장 할당된 IP를 인스턴스에서 사용 가능 Neutron의 네트워킹 프로세스 neutron-server에 의해 명령을 요청을 받음 plugin을 토대로 Messae queue를 통해 각 agent의 기능을 수행 이와 함께 SDN 서비스를 수행 Neutron network의 종류 네트워크의 종류 기능 Management network OpenStack 구성 요소 간의 내부 통신에 사용, 기본적으로 IP 주소는 데이터 센터 내에서만 사용이 가능 Guest network 클라우드 배포 내에서 인스턴스 데이터 통신에 사용되며, 네트워킹 플러그인 및 테넌트가 만든 가상 네트워크의 구성 선택에 따라 변동 External network 외부에서 인스턴스에 대한 엑세스를 위해 제공되는 네트워크 API network OpneStack API를 외부에 노출시키는 네트워크 Neutron과 VRRP, DVR VRRP(Virtual Router Redundancy Protocl)로 랜에서 정적으로 설정된 기본 라우터를 사용할 때, 하나 이상의 백업 라우터를 사용하는 방법을 제공하는 인터넷 프로토콜\nDVR(Distributed Virtual Router)이란 VRRP 기능을 향상시키고, 분산 라우팅 기능과 HA(High Availability), 로드밸런싱 기능을 사용할 수 있음\n기존 레거시 HA 라우터와 마찬가지로 DVR/ SNAT(Static NAT), HA 라우터는 다른 노드에서 실행되는 L3 Agent의 백업 DVR/ SNAT 라우터에서 SNAT 서비스 장애를 빠르게 해결 가능\n네트워크 관련 명령어 $ openstack network list # 네트워크 확인 $ openstack network show [네트워크 이름] # 네트워크 정보 조회 $ ip netns # 라우터 정보 조회 $ ip netns exec [라우터이름] [리눅스 명령어] netstat -r arp -an ifconfig ping # 라우터의 자세한 정보 조회 $ openstack network create --provider-network-type [타입] [네트워크 이름] # 네트워크 생성 # openstack subnet create --network [네트워크 이름] --gateway [GW주소] --subnet-range [서브넷 범위] [서브넷 이름 # 서브넷 생성 $ openstack router list # 라우터 목록 확인 $ openstack router show [라우터 이름] # 라우터 정보 조회 $ openstack router add subnet [라우터 이름] [서브넷 이름] # 라우터에 서브넷 추가 $ openstack port create --network [네트워크 이름] --fixed-ip subnet=[서브넷 이름] [포트 이름] # 포트 생성 $ openstack router add port [라우터 이름] [포트 이름] # 라우터에 포트 추가 fixed-ip, floating-ip IP 역할 Fixed IP 가상머신에 할당되는 내부 IP를 의미 Floating IP 클라우드 내의 가상머신이 인터넷 외부망과 연결되기 위해 배정 받는 IP를 의미 Security Group 인스턴스에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상의 네트워크 방화벽 하나의 인스턴스에 여러 개의 보안 그룹 적용도 가능 ","#":"","네트워크-관련-명령어#\u003cstrong\u003e네트워크 관련 명령어\u003c/strong\u003e":"","네트워크를-관리하는-서비스-neutron#\u003cstrong\u003e네트워크를 관리하는 서비스: Neutron\u003c/strong\u003e":"","네트워크를-관리하는-서비스-neutron-1#\u003cstrong\u003e네트워크를 관리하는 서비스: Neutron\u003c/strong\u003e":""},"title":"Neutron"},"/system/openstack/openstack/nova/":{"data":{"":"가상의 서버를 생성하는 서비스 : Nova 가상의 서버를 생성하는 서비스 : Nova Nova는 compute 서비스의 핵심 compute 서비스란, 가상머신이 필요한 자원을 할당하고, 관리하는 서비스로 하이퍼바이저, 메시지 Queue, 인스턴스 접속을 하는 콘솔 등의 다양한 기능이 유기적으로 연결되어 가상 서버를 생성할 수 있는 시스템을 구성하는 시스템 Nova 서비스의 고려사항 고려사항 설명 CPU compute 서비스가 동작할 호스트 시스템의 cpu가 기본적으로 자체 하드웨어 가상화를 지원이 필수 Hypervisor 서비스에 사용할 하이퍼바이저를 맞게 설정해야 하며, 기본적으로 사용하는 Hypervisor은 KVM/QEMU Storage compute 서비스를 통해 인스턴스가 생성되면서 시스템의 디스크 용량의 제한을 가할 수 있음, 이를 위해 넉넉한 공간이 필요 Overcommit 기본적으로 자원을 할당하는 경우 1:1이 아닌 CPU는 16:1, Memory는 1.5:1로 할당 되어짐 네트워킹 생성된 인스턴스의 경우 nova가 독자적으로 구현하는 것이 아닌 다른 network 서비스를 연게해서 사용해야하며, 주로 Neutron 네트워크 서비스와 함께 사용 Nova의 논리 아키텍처 서비스 역할 nova-api 최종 사용자즈이 API콜을 통해 서비스 간 질의 응답을 담당 nova-compute 가상화 API를 이용하여 가상 머신 인스턴스를 생성하고 종료하는 역할을 수행 nova-scheduler compute host가 다수인 경우 큐를 통해 받은 메시지를 누구에게 명령할 것인지를 결정 nova-conductor 코디네이션과 데이터베이스 쿼리를 지원하는 서버 데몬 nova-cert X509 인증서에 대한 Nova Cert서비스를 제공하는 서버 데몬 nova-consoleauth 데몬, 콘솔 프록시를 제공하는 사용자에 대한 인증 토큰 제공 Guest Agent 실제 compute 시스템 상에 구축된 인스턴스로 Nova-compute 서비스에 의해 제어되어짐 nova-api-metadata 인스턴스의 메타데이터의 요청을 처리 nova-novncproxy VNC 콘솔화면을 제공 nova-novaclient: nova REST API를 사용하는 클라이언트 프로그램 nova-network | 인스턴스의 네트워크 기능을 수행 nova-compute-kvm | 인스턴스(가상 머신)와 관련된 모든 프로세스를 처리 python-guestfs | 파일 생성 기능을 지원하는 Python 라이브러리 qemu-kvm | KVM 하이퍼바이저\n위와 같이 많은 서비스들이 존재 Nova는 대시보드나 콘솔에서 호출하는 nova-api에서 시작 Queue를 이용해 nova-compute에 인스턴스를 생성하라는 명령을 전달 nova-compute는 하이퍼바이저 라이브러리를 이용해 하이퍼바이저에 인스턴스를 생성하려는 명령어를 전달 Hypervisor을 통해 인스턴스를 생성 생성된 인스턴스는 nova-api로 접근할 수 있으며 Nova의 모든 기능은 메시지 Queue로 처리할 수 있음 Nova가 지원하는 하이퍼바이저의 종류 기본 하이퍼바이저는 KVM과 QEMU 프로바이더가 테스트하는 Hyper-V, VMware, XenServer, Sen via libvirt 몇 번의 테스트만 하는 하이퍼바이저 드라이버인 베어메탈, Docker, LXC via libvirt ","#":"","가상의-서버를-생성하는-서비스--nova#\u003cstrong\u003e가상의 서버를 생성하는 서비스 : Nova\u003c/strong\u003e":"","가상의-서버를-생성하는-서비스--nova-1#\u003cstrong\u003e가상의 서버를 생성하는 서비스 : Nova\u003c/strong\u003e":""},"title":"Nova"},"/system/openstack/openstack/openstack/":{"data":{"":"인프라 환경 변화의 시작, 클라우드 클라우드 컴퓨팅의 정의와 종류 클라우드 컴퓨팅(Cloud Computing) 인터넷이 가능한 디바이스(스마트폰, 스마트패드, 스마트TV 등)로 클라우드에서 데이터를 처리하며, 저장 및 관리하는 컴퓨팅 시스템\n클라우드 서비스의 종류\nIaaS(Infrastrcture as a Service): 서버, 스토리지, 네트워크를 가상화 환경으로 만 들어 필요에 따라 인프라 자원을 제공하는 서비스\nPaaS(Platform as a Service): 웹에서 개발 플랫폼을 제공하는 서비스\nSaaS(Software as a Service): 온디맨드 소프트웨어(On-demand Software)라고도 하며, 중앙에서 호스팅 되는 소프트웨어를 웹 브라우저 등 클라우이언트로 이용하는 서비스\nDaas(Desktop as a Service): 클라우드 인프라를 이용해 os가 설치된 인스턴스를 제공하는 서비스\nBaaS(Backend as a Service): 모바일 환경에 맞춰 구현하기 힘든 백엔드 부분을 제공하는 서비스\nPublic Cloud: 언제든지 접근이 가능한 클라우드 서비스\nPrivate Cloud: 외부에서는 접근이 불가능한 사내 클라우드 서비스\nHybrid Cloud Management System: 퍼블릭 클라우드와, 프라이빗 클라우드를 혼용하는 클라우드 서비스\n클라우드 핵심 서비스 컴퓨트와 스토리지 컴퓨트 서비스(Compute Service)\n사용자가 원하는 운영체제가 탑재된 컴퓨터나 서버를 인터넷에서 사용할 수 있게 제공하는 서비스 스토리지 서비스(Storage Service)\n사용자가 소유한 데이터나 음악, 동영상, 문서 파일을 인터넷에 있는 스토리지에 저장, 삭제 공유할 수 있는 서비스 하이퍼바이저의 정의와 종류 하이퍼바이저의 정의 하이퍼바이저(Hypervisor)\n가상 머신 모니터라고도 하며, 호스트 컴퓨터 한 대에서 운영체제 다수를 동시에 실행하는 논리적 플랫폼을 의미 하이퍼바이저의 분류\nNative, 베어메탈 방식: 하드웨어에 직접 설치해서 실행되는 방식 Hosted 방식: 애플리케이션처럼 프로그램으로 설치되는 방식 가상화 방식에 따른 하이퍼바이저의 분류\n전가상화 방식(Full Virtualization): 하드웨어를 모두 가상화하는 방식으로, 게스트 운영체제를 변경하지 않고, 다양한 운영체제로 이용할 수 있음. Native 방식이 이에 해당\n반가상화 방식(Para Virtualization): 하이퍼바이저로만 제어가 가능한 방식으로, 높은 성능의 유지가 가능하지만, 오픈 소스가 아니면 운영이 불가능\n하이퍼바이저의 종류 KVM(for Kerne-based VirtualMachine):\n오픈스택의 거본 하이퍼바이저로 전가상화 방식을 지원 반드시 Inter VT나 AMD-V가 있어야만 사용이 가능 리눅스, 윈도 이미지를 수정하지 않고 여러 가상 머신으로 실행이 가능 Xen과 Xen Server:\nCenter를 이용한 관리 기능, 스토리지 지원과 실시간 마이그레이션, 고가용성 기능처럼 데이터센터에서 요구하는 확장 기능을 제공 Hyper-V:\n디바이스 드라이버가 부모 파티션 위에서 동작하며, 콘솔 OS의 역할을 부모 파티션이 수행 다른 하이퍼바이저의 비해 크기가 작아 오류 코드가 포함될 확류이 낮음 Inter VT, AMD-V x64를 지원하는 하드웨어가 있어야 가상화가 가능 VMware vSphere ESX:\n적은 하드웨어서도 애플리케이션을 통합할 수 있도록 서버를 가상화해주는 무료 베어메탈 하이퍼바이저 ESX는 가상 머신의 업무를 지원하는 역할을 수행, 가상 머신이 발생시킨 명령어를 하이퍼바이저가 받아 재작업 후, 가상 환경에서 잘 구동하도록 바이너리 변환 방식을 사용 Inter, VT, AMD-V 같은 가상화를 지원하는 디바이스가 없어도 가상화를 구현할 수 있음 Docker:\n리눅스 기반의 컨테이너 런타임 오픈 소스로, 가상 머신과 기능이 유사하며, 가상 머신보다 훨씬 가벼운 형태로 배포가 가능 컨테이너의 개념으로 가상 머신처럼 Docker Engine을 호스트 웨어서 수행하며, 리눅스 기반의 운영체제만 수행이 가능 가상 머신처럼 하드웨어를 가상화하는 것이 아니라, 게스트 OS를 분리시켜 제공 호스트 운영체제의 프로세스 공간을 공유한다고 할 수 있음 VirtualBox:\n리눅스, OS X, 솔라리스, 윈도를 게스트 운영체제로 가상화하는 x86 가상화 소프트웨어 다른 하이퍼바이저와 비교했을 때는 기능이 부족 원격 데스크톱 프로토콜(RDP), iSCSI 지원, RDP를 거치는 원격 디바이스의 USB 지원처럼 원격 가상 컴퓨터를 제어할 수 있는 기능이 있음 Inter VT와 AMD-V를 지원 VMware Workstation:\n게스트 운영체제에 설치할 수 있는 다리이버 및 기타 소프트웨어의 묶음 게스트 머신이 고해상도 화면에 접근할 수 있게 하는 VESA호한 그래픽, 네트워크 인터페이스 카드용 네트워크 드라이버, 호스트와 게스트 간 클립보드 공유, 시간 동기화 기능 등을 제공 Parallels Desktop:\n맥용 인텔 프로세서가 있는 매킨토시 컴퓨터에 하드웨어 가상화를 제공하려고 만든 소프트웨어 MS-DOS, 윈도, 맥, 리눅스, 솔라시스 등 다양한 운영체제를 가상화 할 수 있음 하이퍼바이저별 이미지 포맷 KVM: img, qcow2, vmdk VMWARE: vmdk 오라클 VirtualBOx: vdi, vmdk, qcow2, vhd 마이크로소프트 Hyper-V: vhd, vmdk, vdi Xen, Xen Server: qcow2, vhd 이미지포맷 설명 qcow2: QEMU Copy On Write 2 vdi: Virtual Disk Image vmdk: VMware Virtual Disk DevelopmentKit vhd: Virtual Hard Disk 클라우드에서 알아야 할 네트워크 상식 고정 IP, 유동 IP\n고정IP (Fixed IP): 인터넷 공유기를 연결해 고정으로 할당받는 IP 유동IP (Floating IP): 가상 인스턴스가 외부에서 접근할 수 있도록 할당하는 인터넷이 가능한 IP 클래스의 범위\nA 클래스: 1 ~ 126 B 클래스: 128 ~ 191 C 클래스: 192 ~ 223 D 클래스: 224 ~ 239 E 클래스: 240 ~ 254 멀티캐스트는 D 클래스, E 연구 개발 목적으로 예약된 클래스 CIDR(Classless Inter-Domain Routing) 클래스가 없는 도메인간 라우팅 기법으로 기존 IP할당 방식인 네트워크 클래스를 대체 급격히 부족해지는 IPv4 주소를 좀 더 효율적으로 사용 접두어를 이용한 주소 지정 방식의 계층적 구조를 사용해 인터넷 라우팅의 부담을 덜어 줌 SDN(Software Defined Networking) 네트워크 제어 기능이 물리적 네트워크와 분리되도록 프로그래밍한 네트워크 구조를 뜻함 네트워크 제어 기능을 데이터 전달 기능과 분리해서 구현해야 한다. 네트워크 제어 기능이 개발되고 실행될 수 있는 환경을 분리해 낮은 성능의CPU가 있는 하드위어 위에 스위치에 더 이상 위치시키지 않는다. 오픈플로(OpenFlow) SDN의 근간이 되는 기술로 SDN 아키텍처의 컨트롤 레이어와 인프라스트럭처 레이어 사이에 정의된 최초의 표준 통신 인터페이스 흐름정보로 패킷의 전달 경로와 방식을 제어 오픈플로는 오픈플로 컨트롤러와 오픈플로로 지원 네트워크 장비(라우터, 스위치) 사이에서 커뮤니케이션 역할을 담당 일반적인 네트워크 장비(라우터, 스위치)는 플로 테이블을 이용해서 네트워크 트래픽을 처리하는 반면, 오픈플로는 소프트웨어 컨트롤러로 플로테이블을 조작하고 데이터 경로를 설정 네트워크 장비 라우터(Router):\n인터넷 등 서로 다른 네트워크를 연결할 때 사용하는 장비 데이터 패킷이 목적지까지 갈 수 있는 경로를 검사하여 최적의 경로를 탐색하는 것을 라우팅이라 함 경로가 결정되면 결정된 길로 데이터 패킷하는 것을 스위칭이라고 함 허브(Hub):\n인터넷이 등장하기 이전, 컴퓨터와 컴퓨터를 연결해 네트워크를 구성하는 장비 멀티포트(Multiport) 또는 리피터(Repeater)라고도 할 수 있습니다. CSMA/CD(Carrier Sense Multiple Access/Collision Detect):\n이더넷 전송 프로토콜로 IEEE 802.3 표준에 규격화되어 있습니다. 브리지(Bridge):\n콜리전(충돌) 도메인을 나누어 서로 통신이 가능하도록 다리처럼 연결해 주는 네트워크 장비 분리된 콜리전 도메인을 세그먼트라고 한다. 스위치(Switch)\n브리지와 역할이 동일하지만, 소프트웨어적으로 처리하는 스위치가 소프트웨어적으로 처리하는 브리지보다 속도가 더 빠르다. 스위치가 브리지 보다 많은 포트 개수를 제공(20~ 100) 브리지는 Store-and-forward라는 프레임 처리 방식만 지원하지만, 스위치는 Cut-through, Store-and-forward라는 프레임 처리 방식을 지원 스위치 관련 용어\n프레임: 데이터를 주고받을 때 데이터를 적절한 크기로 묶어 놓은 것 프레임 처리 방식: 입력되는 프레임을 스위칭하는 방식입니다. Store-and-forward: 들어오는 프레임 전부를 일단 버퍼에 담아 두고, CRC 등 오류 검출을 완전히 처리한 후 전달(포워딩)하는 스위칭 기법 Cut-through: 스위칭 시스템에서 수신된 패킷 부분만 검사해 이를 곧바로 스위칭하는 방식 블록 스토리지와 오브젝트 스토리지 블록 스토리지(Block Storage)와 오브젝트 스토리지(Object Storage) 블록 스토리지: 컴퓨터의 용량을 추가하는 것처럼 클라우드 상의 하드 디스크를 블록 스토리지라고 함 오브젝트 스토리지: 사용자 계정별로 저장 공간을 할당할 수 있는 스토리지 시스템으로 블록 스토리지와는 다르게 단독으로 구성이 가능하며, 계정의 컨테이너 파일이나 데이터를 저장할 수 있는 저장 공간 대표적인 스토리지 서비스 아마존의 EBS와 S3:\nEBS(Elastic Block Store)는 블록 스토리지에 해당하는 서비스 EC2(Elastic Compute Cloud)은 생성한 인스턴스에 확장해서 사용할 수 있는 스토리지 서비스 S3는 오브젝트 스토리지에 해당하는 서비스로 사용자 계정에 해당하는 Owner, 컨테이너에 해당하는 Bucket, 파일이나 해당데이터에 해당하는 오브젝트로 구성되어있다. 오픈스택의 Cinder와 Swift\nCinder는 오픈스택의 기본 서비스 중 하나로 블록 스토리지 서비스를 제공한다. Cinder는 cinder-volume, cinder-backup, cinder-scheduler, Volume Provider, cinder-api로 구성 Nova에서 제공하는 인스턴스의 확장 스토리지로 사용할 수 있다. Swift는 오픈스택의 기본 서비스 중 하나로 오브젝트 스토리지 서비스를 제공한다. Swift는 proxy-server, account-server, container-server, object-server, swift-api로 구성된다. proxy-server는 여러 대의 스토리지 노드로 구성된 account-server, container-server, object-server을 관리한다. Ceph의 RBD와 RADOS\nCeph는 모든 종류의 스토리지 서비스를 모아 놓은 오픈 소스 서비스라고 할 수 있다. RADOS라는 스토리지 노드 위에 LIBRADOS라는 RADOS 라이브러리가 있다. 아마존의 S3, 오픈스택의 Swift와 연동하는 RADOSGW(게이트웨이)가 있다 QEMU나 KVM에서 생성한 인스턴스를 블록 스토리지로 사용하는 RBD(Rados Block Device), 사용자의 편의성을 제공하려고 POSIX(표준 운영체제 인터페이스)를 제공하는 Ceph FS로 구성되어 있다. OpenStack 오픈스택과 아키텍처 오픈스택 오픈스택은 컴퓨트, 오브젝트 스토리지, 이미지, 인증 서비스 등이 유기적으로 연결되어 하나의 커다한 클라우드 컴퓨팅 시스템을 구축하는 것.\n개념 아키텍처로 살펴보는 오픈스택의 변화\n백사버전부터는 컴퓨트 서비스에는 Nova 추가\n스토리지 서비스에는 Swift 추가\n이미지 관리 서비스에는 Glance 추가\nNova, Swift, Glance의 인증을 담당하는 Keystone 추가\n서비스를 보다 쉽게 이용하려고 사용자에게 대시보드를 제공하는 Horizon 추가\n폴섬 버전부터는 네트워크 서비스와 블록 스로리지 서비스를 Quantum와 Cinder 로 분류함\nQuantum은 기존 nova-network와 다르게 OpenFlow를 사용해서 여러 네트워크 컨트롤러의 지원이 가능\n하바나버전부터는 오케스트레이션 서비스인 Heat와 텔레미터 서비스인 Ceilometer가 있습니다.\n킬로 이후 버전부터는 빅데이터 프로세싱 프레임워크인 Sahara 추가\n데이터베이스 서비스인 Trove 추가\nPXE나 IPMI를 사용해 베어메탈을 프로비저닝하는 Ironic 추가\n코어 서비스 6개와 이를 지원하는 많은 서비스를 표현한 빅텐트(Big-tent)라는 개념 추가\n클라우드 서비스(오픈스택을 기준으로) 시스템 관련 클라우드 서비스\nNova 스토리지 관련 클라우드 서비스\nSwift : 객체 스토리지 Cinder : 블록 스토리지 네트워크 관련 클라우드 서비스\nNeutron 데이터 관련 클라우드 서비스\nGlance Trove 기타 클라우드 서비스\nHorizon Keystone 논리 아키텍처로 살펴보는 오픈스택의 변화\n상황별 오픈스택 구성 요소\n사내 클라우드 컴퓨팅 환경을 구축할 때나 퍼블릭 클라우드 서비스를 구성할 때 오픈스택을 주로 채택 회사의 클라우드 환경을 어떤 목적으로 사용하느냐에 따라 선택해야할 서비스가 달라질 수 있음 HTC(High Throughput Computing): HTC 사용자는 종종 Nova 컴퓨트로 전환해 Horizon 대시보드로 단일 API 엔드포인트를 사용자에게 제공함. Keystone은 일반적으로 사용자 계정이 저장되는 LDAP 백엔드를 연결하는 데 사용 이런 종류의 프로젝트를 구성하려면 다음이 서비스가 필요함 대시보드 서비스 Horizon 텔레미터 서비스 Ceilometer 블록 스토리지 서비스 Cinder 오케스트레이션 서비스 Heat 이미지 서비스 Glance 인증 서비스 keystone 컴퓨트 서비스 Nova 웹 호스팅: 웹 호스팅 회사 중 하나로 수백만 개의 호스팅 사용하는 데 오픈스택을 활용 Nova, Neutron, Keystone, Glance, Horizon 같은 일반적인 코어 서비스를 이용 사용자 계정 데이터를 수집하고 요금을 청구할 때 일부 기술로 Ceilometer를 활용 네트워크 서비스 Neutron 대시보드 서비스 Horizon 텔레미터 서비스 Ceilometer 이미지 서비스 Glance 인증 서비스 keystone 컴퓨트 서비스 Nova 퍼블릭 클라우드: 오픈스택은 전 세계 사용자에에 IaaS를 제공하는 퍼블릭 클라우드를 지원 Nova, Glance, Keystone, Cinder, Neutron 같은 서비스를 제공 Swift를 사용해 오브젝트 스토리지 서비스를 제공, Designate는 DNSaaS(DNS as a Service)를 제공함 네트워크 서비스 Neutron 도메인 네임 서비스 Designate 블록 스토리지 서비스 Cinder 오브젝트 스토리지 서비스 Swift 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova 웹 서비스, 전자상거래: 이베이, 오버스톡닷컴, 베스트바이 등 많은 회사가 오픈스택을 이용해 웹 서비스도 하고 전자상거래의 백엔드로도 사용 상황에 맞춰 오픈스택 클라우드는 PCI 표준처럼 구성하기도 함 Trove는 내부 고객에게 데이터베이스 서비스인 DaaS를 제공, 네트워크 정의 소프트웨어 SDN은 Neutron을 제공 네트워크 서비스 Neutron 대시보드 서비스 Horizon 데이터베이스 서비스 Trove 블록 스토리지 서비스 Cinder 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova 컴퓨트 스타터 키트(Compute Starter Kit): 더 많은 사람이 오픈스택을 사용할 수 있도록 하는 것이 컴퓨터 스타터 키트라고 함 스타터 키트는 추가 기능으로 클라우드 확장할 수 있는 방법을 문서화로 제공하는 단순한 프로젝트를 의미함 네트워크 서비스 Neutron 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova 빅데이터: 다양한 리소스 데이터를 분석하는 빅데이터에도 활동 됨 빅데이터 분석 서비스인 Sahara 프로젝트는 오픈스택 위에 빅데이터 응용프로그램(Hadoop, Spark)을 간단하게 제공할 수 있음 네트워크 서비스 Neutron 대시보드 서비스 Horizon 베이메탈 서비스 Ironic 빅데이터 서비스 Sahara 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova DBaaS: 대부분의 회사는 응용프로그램을 백업하려 데이터베이스에 크게 의존하며 일반적인 관리 자동화 및 스케일 아웃을 최우선으로 생각 오픈스택 Trove 프로젝트는 이 기능을 제공 및 여러 SQL 및 NoSQL 백엔드를 지원 Ironic 프로젝트는 데이터베이스의 성능을 극대화하려고 베어메탈 프로비저닝을 제공 네트워크 서비스인 Neutron 대시보드 서비스 Horizon 데이터베이스 서비스 Trove 도메인 네임 서비스 Designate 베어메탈 서비스 Ironic 블록 스토리지 서비스 Cinder 오브젝트 스토리지 서비스 Swift 이미지 서비스 Glance 인증 서비스 Keystone 컴퓨트 서비스 Nova 비디오 처리와 콘텐츠 전달: 제작 스튜디오나 주요 케이블 서비스 제공 업체 같은 곳의 비디오 처리(Video Processing), 콘텐츠 전달(Contents Delivery)은 오픈스택의 보편적인 사용 예시임 Keystone에서 선보인 인증 표준은 이제 동일한 대시보드와 인증을 사용해 프라이빗 클라우드 및 퍼블릭 클라우드에서 비디오 콘텐츠를 원할하게 이동시킬 수 있음 네트워크 서비스 Neutron 오브젝트 스토리지 서비스 Swift 인증 서비스 Keystone 컴퓨트 서비스 Nova 컨테이너 서비스: 가상머신, 컨테이너, 베어메탈에서 실행되는 워크로드를 단일 클라우드에서 운영할 수 있도록 개발되었음 Kubernetes, Mesos, Docker 같은 새로운 컨테이너 오케스트레이션 엔징(COE, container Orchestration Engices)와 통합하려고 Magnum 프로젝트에 엑세스 할 수 있음 네트워크 서비스 Neutron 대시보드 서비스 Horizon 베어메탈 서비스 Ironic 블록 스토리지 서비스 Cinder 이미지 서비스 Glance 인증 서비스 Keystone 컨테이너 서비스 Management 컴퓨트 서비스 Nova 오픈스택 적용사례 업종별·목적별 클라우드 활용 사례 웹 사이트에서 클라우드 활용 소셜 게임의 클라우드 활용 애플리케이션 개발/테스트 환경에서의 클라우드 활용 스타트업 기업에서의 클라우드 활용 BCP(비지니스 연속성 계획)의 클라우드 활용 ERP(통합 기간 업무 시스템)에서의 클라우드 활용 제조업의 클라우드 활용 지자체 클라우드 교육 분야의 클라우드 활용 농업 분야의 클라우드 활용 빅 데이터 이용을 위한 클라우드의 활용 IoT에서 클라우드 활용 인공 지능 등의 새로운 산업 영역에서의 클라우드 활용 참고 홈페이지 오픈스택 릴리스 웹 사이트 Nalee의 IT 이야기 오픈스택 파운데이션과 커뮤니티 오픈스택 파운데이션: 나라별로 오픈스택 사용자 그룹을 운영하고 있다. 사용자 그룹은 공식 사용자 그룹과 일반 사용자 그룹으로 나뉨 오픈스택 사용자 그룹은 총 112개, 이중 공시기 사용자 그룹은 18개이며, 엠버서더로 활동하는 구성원은 총 12명, 아시아는 6명이다. 오픈스택은 버전별 컨트리뷰터 활동을 그래프와 표로 보여주는 http://stackalytics.com/을 운영한다. ","#":"","openstack#\u003cstrong\u003eOpenStack\u003c/strong\u003e":"","블록-스토리지와-오브젝트-스토리지#\u003cstrong\u003e블록 스토리지와 오브젝트 스토리지\u003c/strong\u003e":"","오픈스택-적용사례#\u003cstrong\u003e오픈스택 적용사례\u003c/strong\u003e":"","오픈스택과-아키텍처#\u003cstrong\u003e오픈스택과 아키텍처\u003c/strong\u003e":"","인프라-환경-변화의-시작-클라우드#\u003cstrong\u003e인프라 환경 변화의 시작, 클라우드\u003c/strong\u003e":"","참고-홈페이지#\u003cstrong\u003e참고 홈페이지\u003c/strong\u003e":"","클라우드-컴퓨팅의-정의와-종류#\u003cstrong\u003e클라우드 컴퓨팅의 정의와 종류\u003c/strong\u003e":"","클라우드에서-알아야-할-네트워크-상식#\u003cstrong\u003e클라우드에서 알아야 할 네트워크 상식\u003c/strong\u003e":"","하이퍼바이저의-정의와-종류#\u003cstrong\u003e하이퍼바이저의 정의와 종류\u003c/strong\u003e":""},"title":"OpenStack 개요"},"/system/openstack/openstack/sahara/":{"data":{"":"","#":"Sahara 데이터 프로세싱 서비스 Sahara 오픈스택 위 빅데이터를 다루기 위한 Hadoop이나 Spark를 쉽게 제공할 수 있게 도와주는 서비스 Sahara는 다음 요소로 구성 Auth: 클라이언트 인증 및 권한을 부여, 오픈스택 인증 서비스 Keystone과 통신 DAL: Data Access Layer의 약어로 데이터 엑세스 계층을 의미, DB의 내부 모델을 유지 Secure Storage Access Layer: 암호 및 개인 키 같은 인증 데이터를 안전한 저장소에 보관 Provisioning Engine: 오픈스택 컴퓨트 서비스 Nova, Heat, Cinder, Glance, Designate와 통신을 담당하는 구성 요소 Vendor Plugins: 프로비저닝된 VM에서 데이터 처리 프레임워크를 구성하고 시작하는 기능을 담당하는 플러그 가능한 메커니즘 EDP: Elastic Data Processing의 약어로 Sahara가 제공하는 클러스테에서 데이터 처리 작업을 예약하고 관리 REST API: REST HTTP 인터페이스로 Sahara 기능을 호출 Python Sahara Client: 다른 오픈스택 구성 요소와 마찬가지로 Sahara에는 자체 Python 클라이언트가 있음 Sahara Pages: Sahara용 GUI로 오픈스택 대시보드인 Horizon에 있음 ","sahara#\u003cstrong\u003eSahara\u003c/strong\u003e":""},"title":"Sahara"},"/system/openstack/openstack/service/":{"data":{"":"","#":"**** 옵셔널 서비스 컴퓨트, 오브젝트 스토리지, 이미지, 인증, 네트워크, 블록 스토리지, 대시보드 서비스만으로도 오픈스택을 구축할 수 있음 텔레미터, 오케스트레이션, 데이터베이스 같은 서비스를 제대로 사용한다면 효율적인 클라우드 관리와 운영에 많은 도움을 많을 수 있음 메시징 서비스 Zaqar 공유 파일 시스템 서비스 Manila DNS 서비스 Designate ","heading#****":""},"title":"Service"},"/system/openstack/openstack/swift/":{"data":{"":"오브젝트 스토리지 관리 서비스 : Swift 다른 서비스와는 다르게 단독으로 구성되며, 클라우드 스토리지 서비스 ( ex : 네이버 클라우드 ) Swift 서비스는 Object Storage 서비스 중 하나 분산 구조의 Object 데이터의 저장 스토리지 체계로 구성 빠른 성능 및 대용량 저장공간이 필요 할 때 사용 동영상, 이미지, 디스크 이미지 등의 대용량, 비정형 데이터를 파일과 메타데이터로 저장하여 분산 관리 오브젝트 스토리지 관리 서비스 : Swift Swift의 논리 아키텍처 구성요소 역할 swift-proxy-server 사용자가 서비스를 다루기 위한 REST API 서비스 swift-account-server 계정 정보 및 사용자 정보를 관리하고 각 컨테이너 별 정보를 관리하기 위한 데몬 프로세스 swift-container-server 사용자 게정의 컨테이너를 간리하는 서비스 swift-object-server 실제 데이터를 저장하고 관리하는 서비스 어카운트, 컨테이너는 별도의 메타데이터가 데이터베이스로 관리 오브젝트는 저장 공간에 직접 저장되는 방식으로 구성 swift-proxy-server는 오픈스택의 Object API를 제공 사용자는 API를 사용해 데이터를 사용 Swift의 논리적 구성 요소 Swift서비스는은 스토리지 공간 여러 개를 합쳐 하나의 커다란 공간으로 가상화하고, 그 안에서 사용자만의 별도 스토리지 공간이 있는 것처럼 다시 가상화\nswift-proxy-server는 스토리지 노드 여러 개를 관리하며 사용자 인증을 담당, 최근에는 Keystone으로 인증을 처리하며, 프록시 서버와 함께 설치\n기본적으로 스토리지 노드에는 swift-account-server, swift,compute-server, swift-object-server가 실행되며 실제 메타데이터파일이나 오브젝트에 해당하는 데이터 파일을 저장\nSwift 역시도 Nova를 구성할 떄와 마찬가지로 스토리지 노드가 여러 호스트로 구성이 가능\n각 스토리지 노드에는 swift-account-server, swift-container-server, swift-object-server가 실행\n서버들은 관리자가 설정한 해당 포트로 서로 통신\n스토리지 노드 중 하나라도 손상이 되면 데이터를 잃지 않도록 데이터 복제(Replica)프로세스가 함께 실행\nSwift Ring 스토리지 파일은 자신이 관리하는 데이터를 서로 공유하려고 Ring인 파일이 어느 노드에, 어떤 데이터가 들어 있는 지를 인지 이를 확인하기 위해 사용도는 것이 Ring 파일 Ring파일은 프록시 노드에서 생성해 모든 스토리지 노드가 동일하게 소유 Ring 파일에는 디바이스 정보 디바이스를 구분하는 ID 존(Zone) 번호 스토리지 노드 IP 포트 디바이스 이름 디바이스 비중 기타 디바이스 정보 Swift의 데이터 관리 방법 Swift는 사용자 게정을 관리하는 어카운트, 디렉터리 개념의 컨테이너, 실제 파일을 표현하는 오브젝트로 구성\nSwift는 어카운트가 컨테이너를 포함하고, 컨테이너가 오브젝트를 포함하도록 관리\n기본적으로 Swift에서는 프록시 노드 한 대에 스토리지 노드 다섯 대를 구성하기를 권장\n프록시 노드들은 로드밸런서로 묶여 있어 사용자는 특정 URL 하나만 호출해도 스토리지 서비스를 자유롭게 사용가능\n파일을 올릴 때는 설정된 리플리카로 여러 스토리지 노드로 분산해서 저장, 다운로드 시 그 중 한 곳을 사용\nSwift와 Keystone Swift에는 SwAuth를 이용하는 인증 방법과 Keystone을 이용\n최근에는 Keystone을 이용해서 주로 인증하며, Keystone에는 프로젝트, 사용자, 롤이 있음\n관리자(admin, swiftoperator)는 사용자와 컨테이너를 생성, 삭제할 수 있음\n관리자는 오브젝트도 올리기, 내려받기, 삭제를 할 수 있음\n일반 사용자(member)은 사용자와 컨테이너를 생성할 수 없음\n일반 사용자는 관리자가 미리 생성해서 권한을 준 컨테이너만 사용할 수 있음\n일반 사용자는 관리자가 설정한 권한으로 오브젝트 목록을 확인할 수 있음\n일반 사용자는 관리자가 설장한 권한으로 데이터를 올리고 내릴 수 있음\n특정 사용자에게 관리자(admin) 권한을 부여하려면 리셀러어드민(ResellerAdmin) 롤을 주어야 함\n해당 사용자는 관리자가 할 수 있는 기능을 모두 사용할 수 있음\nSwift의 이레이저 코딩(Eraure Coding) 기능과 스토리지 정책 스토리지 저장 공간을 효율적으로 관리하는 것이 이레이져 코딩\n다양한 물리 스토리지 디바이스를 정책별로 사용할 수 있게 지원하는 스토리지 정책(Storage Policy)기능이 있음\n이레이져 코딩은 HDFS, RAID 외의 스토리지에서 데이터 저장 공간의 효율성을 높이려고 설계된 데이터 복제 방식\n이레이져 코딩은 이레이져 코드를 사용해 데이터를 인코딩하고, 데이터가 손실되면 디코딩 과정을 거쳐 원본 데이터를 복구하는 기법 중 하나\n이레이저 코드와 각 코드마다 사용하는 알고리즘은 다양한데 이런 알고리즘에 Reed-Solom on Code, Tahoe-Lafs, Weaver Code 등이 있음\n스토리지 정책은 여러 오브젝트링을 생성해 다양한 목적으로 클러스터를 세그먼트화 할 수 있음\n수정된 해시링을 통해 클러스터에서 데이터가 있어야 할 위치를 결정\n이레이저 코드를 사용해 콜드 스토리지(Cold Storage: 저전력 스토리지)도 정의 할 수 있음","#":"","오브젝트-스토리지-관리-서비스--swift#\u003cstrong\u003e오브젝트 스토리지 관리 서비스 : Swift\u003c/strong\u003e":"","오브젝트-스토리지-관리-서비스--swift-1#\u003cstrong\u003e오브젝트 스토리지 관리 서비스 : Swift\u003c/strong\u003e":""},"title":"Swift"},"/system/openstack/openstack/trove/":{"data":{"":"데이터베이스 서비스 : Trove Trove는 관계형 데이터베이스 기능을 활용 클라우드 사용자와 데이터 베이스 관리자는 필요에 따라 Trove를 통해 데이터베이스 인스턴스를 제공, 관리 서비스 Trove의 논리 아키텍처 구성요소 역할 python-troveclient 클라이언트에서 콘솔로 trove-api를 실행할 수 있게 지원 trove-api RESTful API 방식의 JSON을 지원, Trove인스턴스를 관리하고 프로비저닝 trove-taskmanager 인스턴스 프로비저닝을 지원, 라이프 사이클 관리 및 운영하는 작업을 수행 trove-conductor 호스트에서 실행되는 서비스로 호스트 정보를 업데이트 및 게스트 인스턴스 메시지를 수신 trove-guestagent 게스트 인스턴스 안에서 실행, 데이터 베이스 작업을 실행, 관리 ","#":"","데이터베이스-서비스--trove#\u003cstrong\u003e데이터베이스 서비스 : Trove\u003c/strong\u003e":""},"title":"Trove"},"/system/openstack/openstacktraining/":{"data":{"":"\nOpenStack Training\nOpenStack Ussuri\nOpenStack Ussuri : Overview\nOpenStack Ussuri : 환경설정\nOpenStack Ussuri : Keystone\nOpenStack Ussuri : Glance\nOpenStack Ussuri : Nova\nOpenStack Ussuri : Neutron\nOpenStack Ussuri : Cinder\nOpenStack Ussuri : Horizon\nOpenStack Ussuri : Swift\nOpenStack Ussuri : Heat\nOpenStack Ussuri : Gnocch\nOpenStack Ussuri : Trove\nOpenStack Ussuri : Designate\nOpenStack Ussuri : Brabican\nOpenStack Ussuri : Rally\nOpenStack Ussuri : Manila\nOpenStack Stain DevStack PackStack OpenStack\nOpenStack의 개요\nKeyston : 인증을 관리하는 서비스\nGlance : 이미지를 관리하는 서비스\nNova : 가상의 서버를 생성하는 서비스\nNeutron : 네트워크를 관리하는 서비스\nCinder : 블록 스토리지 서비스\nCeilometer : 리소스의 사용량과 부하를 관리하는 서비스\nHorizon : 외부 인터페이스 대시보드 서비스\nSwift : 오브젝트 스토리지 관리 서비스\nHeat : 오케트스트레션 서비스\nTrove : 데이터베이스 서비스\nSahara : 데이터 프로세싱 서비스\nIronic : 베어메탈 서비스"},"title":"OpenStack Training"},"/system/openstack/openstacktraining/devstack/":{"data":{"":"DevStack Stein 설치DevStack Devbian 계열 ( ex : Ubuntu )의 OpenStack 자동화 설치 툴 DevStack 설치 Update Ubuntu System $ sudo apt -y update $ sudo apt -y upgrade $ sudo apt -y dist-upgrade # Ubuntu의 시스템 및 패키지를 업데이트 합니다. $ sudo init 6 # 시스템을 재시작합니다. Add Stack User $ sudo useradd -s /bin/bash -d /opt/stack -m stack # devstack 설치를 위해 stack 유저를 생성합니다. $ echo \"stack ALL=(ALL) NOPASSWD: ALL\" | sudo tee /etc/sudoers.d/stack # 암호 없이 접근할 권한을 부여합니다. Download DevStack loacl.conf 파일의 추가적인 설정은 DevStack을 참조해주세요. $ su - stack $ sudo apt -y install git $ git clone https://git.openstack.org/openstack-dev/devstack # stack user로 진입하여, devstack을 다운받습니다. $ vi local.conf [[local|localrc]] ADMIN_PASSWORD=[ PW ] DATABASE_PASSWORD=$ADMIN_PASSWORD RABBIT_PASSWORD=$ADMIN_PASSWORD SERVICE_PASSWORD=$ADMIN_PASSWORD HOST_IP=[ 현재 호스트의 IP ] # 설치를 위한 설정 파일을 생성합니다. # PW, IP에는 사용자가 원하는 PW, 설치 호스트 네트워크의 IP를 기입합니다. Start OpenStack Deployment On Ubuntu 18.04 with DevStack $ cd devstack ./stack.sh Access OpenStack Dashboard http://[ HOST IP ]/dashboard로 접속합니다. User Name = admin Password = local.conf의 PW ","#":"","devstack#\u003cstrong\u003eDevStack\u003c/strong\u003e":"","devstack-stein-설치#\u003cstrong\u003eDevStack Stein 설치\u003c/strong\u003e":""},"title":"DevStack"},"/system/openstack/openstacktraining/openstack-stein/":{"data":{"":"Openstack Stain Manual 설치 1. 시스템 및 네트워크 구성 여기서는 Nat 네트워크를 외부, host1 대역을 내부로 사용하여 Openstack을 구축해보도록 하겠습니다. 운영체제 및 네트워크 구성 Hypervisor : Vmware Workstation 15 OS : CentOS7 노드 구성 OS Hostname Network Interface Network Interface2 CPU RAM DISK CentOS7 controller Nat ( 192.168.10.100 ) HOST1 ( 10.10.10.10 ) 2cpu 4thread 8 RAm 30G CentOS7 natwork Nat ( 192.168.10.101 ) HOST1 ( 10.10.10.20 ) 1cpu 2thread 2 RAm 20G CentOS7 compute Nat ( 192.168.10.102 ) HOST1 ( 10.10.10.30 ) 1cpu 4thread 4 RAm 100G 기본적인 업데이트 및 설정을 모든 노드에 진행합니다. $ yum -y update # 업데이트 $ vi /etc/hosts 10.10.10.10 controller 10.10.10.20 network 10.10.10.30 compute # known host 등록 설정이 완료되면 기본 구성을 모든 노드에 진행합니다. $ yum -y install chrony # 시간 동기화를 위한 chrony 설치 $ vi /etc/chrony.conf #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server ntp1.jst.mfeed.ad.jp iburst server ntp2.jst.mfeed.ad.jp iburst server ntp3.jst.mfeed.ad.jp iburst allow 10.10.10.0/24 # 시간동기화 $ firewall-cmd --add-service=ntp --permanent $ firewall-cmd --reload # ntp 방화벽 허용 및 리로딩 $ init 6 # 시스템 재시작 $ chronyc sources # 확인 2. Openstack 기본 패키지 구성 Openstack의 기본 패키지 구성은 먼저 contorller 노드만을 통해 진행됨을 유의해주시길 바랍니다. controller 노드에는 다음의 패키지가 설치됩니다. MariaDB: OpenStack 서비스 및 VM 관련 설정들을 보관하기 위해 사용 RabbitMQ: OpenStack 서비스 간 상호 메시지를 주고 받기 위하나 메시지 큐로 사용 Memcached: 범용 분산 메모리 캐시 시스템으로, 자주 외부 데이터에 접근해야 하는 경우에 발생하는 오버헤드를 줄이기 위해 메모리르르 캐싱하고 읽어들이는 역할을 담당, OpenStack 서비스에서는 주로 인증 메커니즘에서 토큰 캐싱을 위해 사용됩니다. Openstack 패키지 설치 및 레포지토리 구성 $ yum -y install centos-release-openstack-stein $ sed -i -e \"s/enabled=1/enabled=0/g\" /etc/yum.repos.d/CentOS-OpenStack-stein.repo # stein 패캐지를 등록합니다. MariaDB를 설치합니다. $ yum --enablerepo=centos-openstack-stein -y install mariadb-server $ vi /etc/my.cnf [mysqld] character-set-server=utf8 # charset을 utf-8으로 변경합니다 $ systemctl start mariadb $ systemctl enable mariadb # mariadb을 시작 및 자동시작을 등록합니다. $ mysql_secure_installation # 패스워드 설정을 진행합니다. $ firewall-cmd --add-service=mysql --permanent $ firewall-cmd --reload RabbitMQ 및 Memcached를 설치합니다. $ yum --enablerepo=centos-openstack-stein -y install rabbitmq-server $ yum --enablerepo=centos-openstack-stein -y install memcached $ vi /etc/my.cnf.d/mariadb-server.cnf [mysqld] ... character-set-server=utf8 max_connections=500 # Mariadb의 위에 내용을 추가합니다. $ vi /etc/sysconfig/memcached OPTIONS=\"-l 0.0.0.0,::\" # mamcached를 모든 리스닝 상태로 전환시킵니다. $ systemctl restart mariadb rabbitmq-server memcached $ systemctl enable mariadb rabbitmq-server memcached # Mariadb와 함께 RabbitMQ 및 Memcached를 시작 및 자동시작을 등록합니다. $ rabbitmqctl add_user [ id ] [ pw ] # rabbitmq 유저를 생성합니다. 여기서는 openstack/qwer1234를 사용하도록 하겠습니다. $ rabbitmqctl set_permissions [ id ] \".*\" \".*\" \".*\" # 생성한 사용자에게 모든 권한을 부여합니다. $ firewall-cmd --add-port={11211/tcp,5672/tcp} --permanent $ firewall-cmd --reload 3. Keystone ( 인증 서비스 ) 구성 Keystone 또한 controller의 설치를 진행합니다. keystone에 대한 설명은 keystone을 참조해주세요. keyston DB 생성 $ mysql -u root -p MariaDB [(none)]\u003e create database keystone; MariaDB [(none)]\u003e grant all privileges on keystone.* to keystone@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on keystone.* to keystone@'%' identified by 'pw'; MariaDB [(none)]\u003e flush privileges; # keystone 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) keystone 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-keystone openstack-utils python-openstackclient httpd mod_wsgi # keystone 및 관련 패키지를 설치합니다. $ vi /etc/keystone/keystone.conf [cache] ... memcache_servers = controller:11211 [database] ... connection = mysql+pymysql://keystone:qwer1234@controller/keystone [token] ... provider = fernet # keystone 구성을 위해 설정파일 수정합니다. # hosts에 등록한 IP 혹은 controller의 IP를 기입하셔도 무관합니다. $ su -s /bin/bash keystone -c \"keystone-manage db_sync\" # 설정 값을 토대로 db의 설정을 저정합니다. $ keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone $ keystone-manage credential_setup --keystone-user keystone --keystone-group keystone # 토큰 및 자격 증명 암호화를 위해 사용되는 키 저장소를 생성합니다. $ export controller=10.10.10.10 $ keystone-manage bootstrap --bootstrap-password qwer1234 \\ --bootstrap-admin-url http://$controller:5000/v3/ \\ --bootstrap-internal-url http://$controller:5000/v3/ \\ --bootstrap-public-url http://$controller:5000/v3/ \\ --bootstrap-region-id RegionOne # controlelr의 IP로 keystone을 부트스트랩합니다. $ setsebool -P httpd_use_openstack on $ setsebool -P httpd_can_network_connect on $ setsebool -P httpd_can_network_connect_db on $ firewall-cmd --add-port=5000/tcp --permanent $ firewall-cmd --reload # Selinux와 방화벽으르 설정합니다. $ ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ $ systemctl start httpd $ systemctl enable httpd # keystone 설정 활성화 및 httpd 를 시작합니다. 정상 동작 확인을 위한 토큰 파일 생성 $ vi ~/admin export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=qwer1234 export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 $ chmod 600 ~/admin $ source ~/admin project 생성 $ cd ~ $ . admin $ openstack project create --domain default --description \"Service Project\" service +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Service Project | | domain_id | default | | enabled | True | | id | 3f0b3ef5b8c94a0a9cca8e34ea2fdbd6 | | is_domain | False | | name | service | | parent_id | default | | tags | [] | +-------------+----------------------------------+ # project 생성 $ openstack project list +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 3f0b3ef5b8c94a0a9cca8e34ea2fdbd6 | service | | ec1a4336cfa64d04bbc8f908b26a6cda | admin | +----------------------------------+---------+ 이것으로 keystone에 대한 설치가 끝났습니다. 혹시 오류가 발생할 경우 /var/log/keystone/ 혹은 /var/log/httpd/에서 error 로그, keystone 로그를 검색하여 오류를 찾아내시면 보다 쉽게 문제를 해결하실 수 있습니다. 4. Glance ( 이미지 서비스 ) 구성 Glance 또한 controller에서만 설치를 진행합니다. 에 대한 설명은 Glance을 참조해주세요. glance 사용자 추가 $ source ~/admin # 전에 생성했던 토큰 값을 적용합니다. $ openstack user create --domain default --project service --password qwer1234 glance # glance 게정을 추가합니다. $ openstack role add --project service --user glance admin # glance에 admin의 권한을 부여합니다. $ openstack service create --name glance --description \"OpenStack Image service\" image # glance 서비스 엔트리를 생성합니다. $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne image public http://$controller:9292 $ openstack endpoint create --region RegionOne image internal http://$controller:9292 $ openstack endpoint create --region RegionOne image admin http://$controller:9292 # glance 서비스의 endpoint를 추가합니다 ( public, internal, admin ) $ openstack user list +----------------------------------+--------+ | ID | Name | +----------------------------------+--------+ | bd36365f2459468a9c480cb48bab3ac0 | glance | | e19db9d5ec2c4c30b7a85d18b8b0e589 | admin | +----------------------------------+--------+ $ openstack endpoint list +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+ | 00b38774cef048ee9950eda6938accc3 | RegionOne | keystone | identity | True | public | http://10.10.10.10:5000/v3/ | | 4591b06391374fe888380fa23b8f5121 | RegionOne | glance | image | True | admin | http://10.10.10.10:9292 | | 53dd31fecf2d44949c141149a13c673b | RegionOne | keystone | identity | True | admin | http://10.10.10.10:5000/v3/ | | 555f3d900f7e416bb783120f7ce74fe8 | RegionOne | glance | image | True | internal | http://10.10.10.10:9292 | | 5b3ac620bb7d4d9aabdf0f33229ee346 | RegionOne | glance | image | True | public | http://10.10.10.10:9292 | | bdd7df7c8cba46f6ada2c12155a9f1d6 | RegionOne | keystone | identity | True | internal | http://10.10.10.10:5000/v3/ | +----------------------------------+-----------+--------------+--------------+---------+-----------+-----------------------------+ Glance DB 생성 $ mysql -u root -p MariaDB [(none)]\u003e create database glance; MariaDB [(none)]\u003e grant all privileges on glance.* to glance@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on glance.* to glance@'%' identified by 'pw'; MariaDB [(none)]\u003e flush privileges; # 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) glance 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-glance # glance 패키지를 설치합니다. $ vi /etc/glance/glance-api.conf [DEFAULT] bind_host = 0.0.0.0 [glance_store] stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ # 이미지 경로 지정 [database] # database 연동 connection = mysql+pymysql://glance:qwer1234@controller/glance [keystone_authtoken] # keystone 인증 www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = qwer1234 [paste_deploy] flavor = keystone # glance.conf를 수정합니다. $ su -s /bin/bash glance -c \"glance-manage db_sync\" # glance db를 동기화 시킵니다. $ systemctl start openstack-glance-api $ systemctl enable openstack-glance-api # glance를 시작 및 실행시 자동시작을 등록합니다. $ setsebool -P glance_api_can_network on $ firewall-cmd --add-port=9292/tcp --permanent $ firewall-cmd --reload # Selinux 및 firewall을 설정합니다. 확인을 위한 이미지 생성 $ wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img # 확인을 위해 cirros 이미지를 다운 받습니다. $ openstack image create \"Cirros\" --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2 # image 등록 $ openstack image list +--------------------------------------+--------+--------+ | ID | Name | Status | +--------------------------------------+--------+--------+ | 38e15009-022b-49ce-bcdf-b220eb3c5b12 | Cirros | active | +--------------------------------------+--------+--------+ # 확인 5. Nova ( 컴퓨트 서비스 ) 구성 Nova 서비스는 controller 노드와 compute노드에 구성됩니다. 설치는 contoller \u003e compute 순으로 진행하도록 하겠습니다. Nova에 대한 설명은 Nova을 참조해주세요. Nova, Placement 추가 및 등록 $ source ~/admin $ openstack user create --domain default --project service --password qwer1234 nova $ openstack role add --project service --user nova admin $ openstack user create --domain default --project service --password qwer1234 placement $ openstack role add --project service --user placement admin # nova 유저와 placement유저를 생성합니다. $ openstack service create --name nova --description \"OpenStack Compute Service\" compute # nova 서버 엔트리 저장 $ openstack service create --name placement --description \"OpenStack Compute Placement Service\" placement # placement 서버 엔트리 저장 $ openstack user list # 확인 +----------------------------------+-----------+ | ID | Name | +----------------------------------+-----------+ | 18bdf3e68a754aa182f93196a918ba65 | nova | | 18ff8b52493a408d9933596ed20cca9c | glance | | bfd0cf6d358e49bf88f183a463c689a2 | placement | | e19db9d5ec2c4c30b7a85d18b8b0e589 | admin | +----------------------------------+-----------+ $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne compute public http://$controller:8774/v2.1/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne compute internal http://$controller:8774/v2.1/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne compute admin http://$controller:8774/v2.1/%\\(tenant_id\\)s # nova 서비스의 endpoint를 추가합니다 $ openstack endpoint create --region RegionOne placement public http://$controller:8778 $ openstack endpoint create --region RegionOne placement internal http://$controller:8778 $ openstack endpoint create --region RegionOne placement admin http://$controller:8778 $ placement의 endpoint를 추가합니다. $ openstack endpoint list # 확인 -------+-----------+--------------------------------------------+ | ID | Region | Service Name | Service Type | Enabled | Interface | URL | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------------+ | 00b38774cef048ee9950eda6938accc3 | RegionOne | keystone | identity | True | public | http://10.10.10.10:5000/v3/ | | 04ca5fb6701348089777d68a68ca7cd2 | RegionOne | placement | placement | True | public | http://10.10.10.10:8778 | | 53ad55ce8897463b86ea616a8ba64d95 | RegionOne | glance | image | True | public | http://10.10.10.10:9292 | | 53dd31fecf2d44949c141149a13c673b | RegionOne | keystone | identity | True | admin | http://10.10.10.10:5000/v3/ | | 595a2045543b42c2bb6f23e2dd30a3bb | RegionOne | glance | image | True | internal | http://10.10.10.10:9292 | | 6820b49138d54b63ac34cd52f1be08f6 | RegionOne | placement | placement | True | internal | http://10.10.10.10:8778 | | 6ad740445fca4a0fb684d913909fe129 | RegionOne | nova | compute | True | admin | http://10.10.10.10:8774/v2.1/%(tenant_id)s | | 9863826e093943cf97a05dfc6e3c159a | RegionOne | nova | compute | True | internal | http://10.10.10.10:8774/v2.1/%(tenant_id)s | | b9f9701a57ec40e487ce493a63903cae | RegionOne | placement | placement | True | admin | http://10.10.10.10:8778 | | bd787b85b3124f0ab15854998624cb19 | RegionOne | nova | compute | True | public | http://10.10.10.10:8774/v2.1/%(tenant_id)s | | bdd7df7c8cba46f6ada2c12155a9f1d6 | RegionOne | keystone | identity | True | internal | http://10.10.10.10:5000/v3/ | | d394eaf13ac840b3b2e69e074c2c1c20 | RegionOne | glance | image | True | admin | http://10.10.10.10:9292 | +----------------------------------+-----------+--------------+--------------+---------+-----------+--------------------------------------------+ Nova DB 생성 $ mysql -u root -p MariaDB [(none)]\u003e create database nova; MariaDB [(none)]\u003e grant all privileges on nova.* to nova@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on nova.* to nova@'%' identified by 'pw'; MariaDB [(none)]\u003e create database nova_api; MariaDB [(none)]\u003e grant all privileges on nova_api.* to nova@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on nova_api.* to nova@'%' identified by 'pw'; MariaDB [(none)]\u003e create database nova_placement; MariaDB [(none)]\u003e grant all privileges on nova_placement.* to nova@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on nova_placement.* to nova@'%' identified by 'pw'; MariaDB [(none)]\u003e create database nova_cell0; MariaDB [(none)]\u003e grant all privileges on nova_cell0.* to nova@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on nova_cell0.* to nova@'%' identified by 'pw'; MariaDB [(none)]\u003e flush privileges; # nova 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) nova 서비스를 설치 및 수정합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-nova # nova 패키지를 설치합니다. $ vi /etc/nova/nova.conf [DEFAULT] my_ip = 10.10.10.10 state_path = /var/lib/nova enabled_apis = osapi_compute,metadata log_dir = /var/log/nova [api] auth_strategy = keystone [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = $state_path/tmp [api_database] connection = mysql+pymysql://nova:qwer1234@controller/nova_api [database] connection = mysql+pymysql://nova:qwer1234@controller/nova [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = qwer1234 [placement] auth_url = http://controller:5000 os_region_name = RegionOne auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [placement_database] connection = mysql+pymysql://nova:qwer1234@controller/nova_placement [wsgi] api_paste_config = /etc/nova/api-paste.ini # nova의 설정 파일을 수정합니다. Selinux 및 firewalld을 설정합니다. $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ semanage port -a -t http_port_t -p tcp 8778 $ firewall-cmd --add-port={6080/tcp,6081/tcp,6082/tcp,8774/tcp,8775/tcp,8778/tcp} --permanent $ firewall-cmd --reload nova 서비스를 DB에 저장합니다. $ su -s /bin/bash nova -c \"nova-manage api_db sync\" $ su -s /bin/bash nova -c \"nova-manage cell_v2 map_cell0\" $ su -s /bin/bash nova -c \"nova-manage db sync\" $ su -s /bin/bash nova -c \"nova-manage cell_v2 create_cell --name cell1\" nova 서비스를 시작 및 자동시작을 설정합니다. $ systemctl restart httpd $ chown nova. /var/log/nova/nova-placement-api.log $ for service in api consoleauth conductor scheduler novncproxy; do systemctl start openstack-nova-$service systemctl enable openstack-nova-$service done 이상으로 controller 노드에서의 구성을 마치겠습니다. 하단부터의 패키지 설치는 compute노드에서 진행해주세요 Stein 레포지터리를 활성화합니다. $ yum -y install centos-release-openstack-stein $ sed -i -e \"s/enabled=1/enabled=0/g\" /etc/yum.repos.d/CentOS-OpenStack-stein.repo # stein 패캐지를 등록합니다. KVM 하이퍼바이저를 구성합니다. $ yum -y install qemu-kvm libvirt virt-install bridge-utils # KVM 구성에 필요한 가상화 및 네트워크 도구들을 설치합니다. $ lsmod | grep kvm # 확인 $ systemctl start libvirtd $ systenctk ebable libvirtd compute 노드에 nova 서비스를 설치 및 수정합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-nova # nova 패키지를 설치합니다. $ vi /etc/nova/nova.conf [DEFAULT] my_ip = 10.10.10.30 state_path = /var/lib/nova enabled_apis = osapi_compute,metadata log_dir = /var/log/nova transport_url = rabbit://openstack:qwer1234@controller [api] auth_strategy = keystone [vnc] enabled = True server_listen = 0.0.0.0 server_proxyclient_address = 192.168.10.102 novncproxy_base_url = http://192.168.10.102/vnc_auto.html # vnc 화면으르 활성화 합니다. 추후 오픈스택 대시보드 혹은 vnc 클라이언트 프로그램으로 접속할 때 사용합니다. [glance] api_servers = http://controller:9292 [oslo_concurrency] lock_path = $state_path/tmp [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = qwer1234 [placement] auth_url = http://controller:5000 os_region_name = RegionOne auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [wsgi] api_paste_config = /etc/nova/api-paste.ini # nova의 설정 파일을 수정합니다. Selinux 및 firewall 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ firewall-cmd --add-port=5900-5999/tcp --permanent $ firewall-cmd --reload nova 서비스 시작 $ systemctl start openstack-nova-compute $ systemctl enable openstack-nova-compute \u0026 controller# openstack compute service list # 확인 +----+------------------+------------+----------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +----+------------------+------------+----------+---------+-------+----------------------------+ | 4 | nova-consoleauth | controller | internal | enabled | up | 2020-07-19T02:47:16.000000 | | 5 | nova-conductor | controller | internal | enabled | up | 2020-07-19T02:47:12.000000 | | 8 | nova-scheduler | controller | internal | enabled | up | 2020-07-19T02:47:12.000000 | | 9 | nova-compute | compute | nova | enabled | up | 2020-07-19T02:47:08.000000 | +----+------------------+------------+----------+---------+-------+----------------------------+ 6. Neutron ( 네트워크 서비스 ) 구성 Neutron 서비스르르 구성하는 과정에서는 모든 노드에 설치가 진행됩니다. 기본적으로 openvswithch를 중심으로 진행하며, 경우에 따라서는 linuxbridge로 서비스를 대체하는 것이 가능합니다. 설치 과정은 controller, compute, network 노드 순으로 진행하겠습니다. Neutron에 대한 설명은 Neutron을 참조해주세요. Neutron 사용자 추가 $ openstack user create --domain default --project service --password qwer1234 neutron $ openstack role add --project service --user neutron admin $ openstack service create --name neutron --description \"OpenStack Networking service\" network # Netutron 사용자를 추가 및 서비스를 등록합니다. $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne network public http://$controller:9696 $ openstack endpoint create --region RegionOne network internal http://$controller:9696 $ openstack endpoint create --region RegionOne network admin http://$controller:9696 # neutron의 endpoint를 생성합니다. Neutron DB 생성 $ mysql -u root -p MariaDB [(none)]\u003e create database neutron_ml2; MariaDB [(none)]\u003e grant all privileges on neutron_ml2.* to neutron@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on neutron_ml2.* to neutron@'%' identified by 'pw'; MariaDB [(none)]\u003e flush privileges; # neutron 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) Neutron 설치 및 설정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 # neutron 패키지 설치 $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron dhcp_agent_notification = True allow_overlapping_ips = True notify_nova_on_port_status_changes = True notify_nova_on_port_data_changes = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [database] connection = mysql+pymysql://neutron:qwer1234@controller/neutron_ml2 [nova] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = nova password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/metadata_agent.ini [DEFAULT] nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret memcache_servers = controller:11211 # metadata_agent.ini 파일을 수정합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security # ml2_conf.ini 파일에 설정을 수정합니다. 이어 nova.conf 파일에 설정을 추가합니다. $ vi /etc/nova/nova.conf [DEFAULT] ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P daemons_enable_cluster_mode on $ firewall-cmd --add-port=9696/tcp --permanent $ firewall-cmd --reload # Selinux 및 방화벽을 설정합니다. Neutron DB를 생성 및 서비스를 시작합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ su -s /bin/bash neutron -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\" # Neutron DB를 생성합니다. $ systemctl start neutron-server neutron-metadata-agent $ systemctl enable neutron-server neutron-metadata-agent $ systemctl restart openstack-nova-api 이제 다음으로는 network 노드에 구현해보도록 하겠습니다. $ yum -y install centos-release-openstack-stein $ sed -i -e \"s/enabled=1/enabled=0/g\" /etc/yum.repos.d/CentOS-OpenStack-stein.repo # stein 패캐지를 등록합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs # neutron 패키지를 설치합니다. neutron 설정합니다. $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/l3_agent.ini [DEFAULT] ... interface_driver = openvswitch # l3_agent.ini 파일을 수정합니다. $ vi /etc/neutron/dhcp_agent.ini [DEFAULT] ... interface_driver = openvswitch dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true # dhcp_agent.ini 파일을 수정합니다. $ vi /etc/neutron/metadata_agent.ini [DEFAULT] nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret # metadata_agent.ini 파일을 수정합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security # ml2_conf.ini 파일에 설정을 수정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true # openvswitch_agent.ini 파일의 하단에 추가합니다. Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P haproxy_connect_any on $ setsebool -P daemons_enable_cluster_mode on $ vi my-ovsofctl.te # create new module my-ovsofctl 1.0; require { type neutron_t; class capability sys_rawio; } #============= neutron_t ============== allow neutron_t self:capability sys_rawio; $ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다. 시스템을 재시작 합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ systemctl start openvswitch $ systemctl enable openvswitch $ ovs-vsctl add-br br-int $ systemctl restart openstack-nova-compute $ systemctl start neutron-openvswitch-agent $ systemctl enable neutron-openvswitch-agent 이어서 compute 노드에서의 설정을 진행하겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch # neutron 패키지를 설치합니다. neutron 설정합니다. $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security # ml2_conf.ini 파일에 설정을 수정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true # openvswitch_agent.ini 파일의 하단에 추가합니다. 이어서 Nova.conf 파일을 수정합니다. $ vi /etc/nova/nova.conf [DEFAULT] ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver vif_plugging_is_fatal = True vif_plugging_timeout = 300 [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P haproxy_connect_any on $ setsebool -P daemons_enable_cluster_mode on $ vi my-ovsofctl.te # create new module my-ovsofctl 1.0; require { type neutron_t; class capability sys_rawio; } #============= neutron_t ============== allow neutron_t self:capability sys_rawio; $ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다. 시스템을 재시작 합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ systemctl start openvswitch $ systemctl enable openvswitch $ ovs-vsctl add-br br-int $ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl start neutron-$service systemctl enable neutron-$service done 이제 이어 compute 노드에서 neutron 서비스를 설치하도록 하겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch # neutron 서비스를 설치합니다. $ vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone stae_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = mechanism_drivers = openvswitch extension_drivers = port_security $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true 이어서 nova.conf 파일을 수정합니다. $ vi /etc/nova/nova.conf [DEFAULT] ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver vif_plugging_is_fatal = True vif_plugging_timeout = 300 [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret Selinux 및 방화벽 설정 $ yum --enablerepo=centos-openstack-stein -y install openstack-selinux $ setsebool -P neutron_can_network on $ setsebool -P haproxy_connect_any on $ setsebool -P daemons_enable_cluster_mode on $ vi my-ovsofctl.te # create new module my-ovsofctl 1.0; require { type neutron_t; class capability sys_rawio; } #============= neutron_t ============== allow neutron_t self:capability sys_rawio; $ checkmodule -m -M -o my-ovsofctl.mod my-ovsofctl.te $ semodule_package --outfile my-ovsofctl.pp --module my-ovsofctl.mod $ semodule -i my-ovsofctl.pp # Selinux 및 방화벽을 추가설정합니다. 서비스를 재시작 및 등록합니다. $ ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ systemctl start openvswitch $ systemctl enable openvswitch $ ovs-vsctl add-br br-int $ systemctl restart openstack-nova-compute $ systemctl start neutron-openvswitch-agent $ systemctl enable neutron-openvswitch-agent 이제 다음으로는 본격적으로 neutron 네트워크를 구현해보도록 하겠습니다. 먼저 controller 노드에서 ml2_conf 파일을 수정 및 추가합니다. 위에서 tenant 타입을 비워둔 이유는, 타입에 따라 사용하는 네트워크 구조가 달라지기 때문입니다. 여기서는 vxlan을 사용해 구성해보도록 하겠습니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] tenant_network_types = vxlan [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # ml2.conf 파일을 수정합니다 $ systemctl restart neutron-server # neutron 서비스를 재시작 합니다. 이제 Network 노드에서의 설치를 진행해보도록 하겠습니다. $ ovs-vsctl add-br br-eth1 $ ovs-vsctl add-port br-eth1 ens33 # 네트워크 브릿지를 생성하고, 네트워크 노드의 외부대역의 인터페이스 번호를 바인딩합니다. neutron 서비스 사용을 위한 설정을 진행합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # ml2_conf.ini 파일에 설정을 추가 설정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [agent] tunnel_type = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.20 bridge_mappings = physnet1:br-eth1 # openvswitch_agent.ini 파일의 하단에 추가합니다. $ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl restart neutron-$service done # neutron 서비스를 재시작합니다. $ systemctl stop firewalld $ systemctl disable firewalld # 방화벽을 해제합니다. 바인딩 오류를 해결하기 위해 설정을 진행합니다. $ vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=Ethernet BOOTPROTO=static DEFROUTE=yes NAME=ens33 DEVICE=ens33 ONBOOT=yes $ vi /var/tmp/create_interface.sh #!/bin/bash ip link set up br-eth1 ip addr add 192.168.10.101/24 dev br-eth1 route add default gw 192.168.10.2 dev br-eth1 echo \"nameserver 8.8.8.8\" \u003e /etc/resolv.conf $ chmod 755 /var/tmp/create_interface.sh $ vi /etc/systemd/system/set_interface.service [Unit] Description=Description for sample script goes here After=network.target [Service] Type=simple ExecStart=/var/tmp/create_interface.sh TimeoutStartSec=0 [Install] WantedBy=default.target $ systemctl enable set_interface $ init 6 이어 compute 노드에서의 설정을 진행합니다. $ vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # ml2_conf.ini 파일에 설정을 추가 설정합니다. $ vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [agent] tunnel_type = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.30 # openvswitch_agent.ini 파일의 하단에 추가합니다. $ for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl restart neutron-$service done # neutron 서비스를 재시작합니다. $ systemctl stop firewalld $ systemctl disable firewalld # 방화벽을 해제합니다. 확인 $ openstack network agent list +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | ID | Agent Type | Host | Availability Zone | Alive | State | Binary | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ | 261bbd8f-ece9-4818-91c3-be75b928fa54 | Open vSwitch agent | network | None | :-) | UP | neutron-openvswitch-agent | | 26376b7b-e4d0-413c-85b9-521994c41bf6 | Open vSwitch agent | compute | None | :-) | UP | neutron-openvswitch-agent | | 8b520189-c500-47ec-b330-b84bc0a3b622 | Metadata agent | controller | None | :-) | UP | neutron-metadata-agent | | ba443e32-a931-465f-acff-05621dac0424 | Metadata agent | network | None | :-) | UP | neutron-metadata-agent | | be878ec2-b8c9-4923-8d01-111d7c11c8f1 | L3 agent | network | nova | :-) | UP | neutron-l3-agent | | cb74c09d-7ec5-4457-a384-8303235adc97 | DHCP agent | network | nova | :-) | UP | neutron-dhcp-agent | +--------------------------------------+--------------------+------------+-------------------+-------+-------+---------------------------+ $ openstack router create router01 $ openstack network create int --provider-network-type vxlan $ openstack subnet create int_sub --network int \\ --subnet-range 1.1.1.0/24 --gateway 1.1.1.2 \\ --dns-nameserver 8.8.8.8 # 라우터와 내부대역을 생성합니다. $ openstack router add subnet router01 int_sub # 라우터와 내부대벽을 연결시킵니다. $ openstack network create \\ --provider-physical-network physnet1 \\ --provider-network-type flat --external ext $ openstack subnet create subnet2 \\ --network ext_net --subnet-range 192.168.10.0/24 \\ --allocation-pool start=192.168.10.150,end=192.168.10.200 \\ --gateway 192.168.10.2 --dns-nameserver 8.8.8.8 # 외부대역을 생성합니다. 외부대역의 IP는 바인딩한 br-eth1의 IP 대역과 동일해야합니다. $ openstack router set router01 --external-gateway ext # 생성한 라우터의 게이트웨이를 생성한 외부대역에 바운딩시킵니다. $ openstack network list +--------------------------------------+------+--------------------------------------+ | ID | Name | Subnets | +--------------------------------------+------+--------------------------------------+ | 2875f833-2d46-4740-bdd4-09c75c53e2b1 | int | 698d35ae-8d7c-436f-be1b-fcf4319eb5fe | | 4a25933d-ed21-4a5c-a87b-4e782e93c14c | ext | 47b0ee11-b628-4260-9185-71d1dab401ea | +--------------------------------------+------+--------------------------------------+ $ openstack subnet list +--------------------------------------+---------+--------------------------------------+-----------------+ | ID | Name | Network | Subnet | +--------------------------------------+---------+--------------------------------------+-----------------+ | 47b0ee11-b628-4260-9185-71d1dab401ea | ext-sub | 4a25933d-ed21-4a5c-a87b-4e782e93c14c | 192.168.10.0/24 | | 698d35ae-8d7c-436f-be1b-fcf4319eb5fe | int-sub | 2875f833-2d46-4740-bdd4-09c75c53e2b1 | 1.1.1.0/24 | +--------------------------------------+---------+--------------------------------------+-----------------+ $ wget http://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.img -P /var/kvm/images $ openstack image create \"Ubuntu1804\" --file /var/kvm/images/ubuntu-18.04-server-cloudimg-amd64.img --disk-format qcow2 --container-format bare --public # 이미지를 다운로드 및 등록합니다. $ openstack flavor create --ram 1024 --disk 10 --vcpus 1 m1.small # flavor를 생성합니다. $ ssh-keygen -q -N \"\" $ openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey # keypair를 생성합니다. $ openstack floating ip create ext # floating ip를 생성합니다. $ openstack create server --image Ubuntu1804 --flavor m1.small --key mykey --network int Ubuntu $ openstack server add floating ip Ubuntu 192.168.10.170 # 인스턴스를 생성하고 floating ip를 추가합니다. $ openstack server list +--------------------------------------+--------+--------+-------------------------------+------------+----------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+--------+--------+-------------------------------+------------+----------+ | 75fa0186-ab63-4aaa-a27c-3f2126e5d31d | Ubuntu | ACTIVE | int=1.1.1.248, 192.168.10.170 | Ubuntu1804 | m1.small | +--------------------------------------+--------+--------+-------------------------------+------------+----------+ $ openstack security group create open $ openstack security group rule create --protocol icmp --ingress open $ openstack security group rule create --protocol tcp --dst-port 22:22 open $ openstack security group rule create --protocol tcp --dst-port 80:80 open $ openstack server add security group Ubuntu open # 보안그룹을 생성하고 적용시킵니다. $ ssh ubuntu@192.168.10.170 $ ping 8.8.8.8 $ sudo apt -y install apache2 $ sudo service apache2 start # 본체 Host에서 접속해서 확인 이것으로 기본적인 openstack-stein 버전의 설치를 완료하였습니다. 7. Horizon ( 대시보드 서비스 ) 구성 Horizon은 controller 노드에서 설치가 진행됩니다. 에 대한 설명은 Horizone을 참조해주세요. Horizon 패키지 설치 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-dashboard # Horizon 패키지를 설치합니다. 대시보드를 설정합니다. $ vi /etc/openstack-dashboard/local_settings ALLOWED_HOSTS = ['*'] # 수정 OPENSTACK_API_VERSIONS = { \"identity\": 3, \"image\": 3, \"volume\": 3, \"compute\": 2, } # 주석 제거 및 수정 OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True # 주석 해제 및 수정 OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default' # 주석 제거 CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': '127.0.0.1:11211', }, } # 주석제거 OPENSTACK_HOST = \"controller\" # IP 변경 OPENSTACK_KEYSTONE_DEFAULT_ROLE = \"member\" # 수정 $ vi /etc/httpd/conf.d/openstack-dashboard.conf WSGIDaemonProcess dashboard WSGIProcessGroup dashboard WSGISocketPrefix run/wsgi WSGIApplicationGroup %{GLOBAL} # 추가 Selinux 및 방화벽 설정 $ setsebool -P httpd_can_network_connect on $ firewall-cmd --add-service={http,https} --permanent $ firewall-cmd --reload $ systemctl restart httpd ** DB 생성** $ mysql -u root -p MariaDB [(none)]\u003e create database keystone; MariaDB [(none)]\u003e grant all privileges on .* to @'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on .* to @'%' identified by 'pw'; MariaDB [(none)]\u003e flush privileges; # 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) 8. Cinder ( 오브젝트 스토리지 및 블록 스토리지 구성 ) Cinder는 기본적으로 독립적으로 storage 노드를 구성하거나 혹은 compute 노드에 추가하여 사용합니다. 여기서는 compute 노드에 포함하여 구성하도록 하겠습니다/ 구성 순서는 controller \u003e compute 노드 순으로 진행하겠습니다. Cinder에 대한 설명은 Cinder을 참조해주세요. Cinder 서비스 등록 $ source ~/admin $ openstack user create --domain default --project service --password qwer1234 cinder $ openstack role add --project service --user cinder admin $ openstack service create --name cinderv3 --description \"OpenStack Block service\" volumev3 # cinder 사용자를 추가 및 서비스를 등록합니다. $ export controller=10.10.10.10 $ openstack endpoint create --region RegionOne volumev3 public http://$controller:8776/v3/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne volumev3 internal http://$controller:8776/v3/%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne volumev3 admin http://$controller:8776/v3/%\\(tenant_id\\)s # cinder의 endpoint를 생성합니다. Cinder DB 생성 $ mysql -u root -p MariaDB [(none)]\u003e create database cinder; MariaDB [(none)]\u003e grant all privileges on cinder.* to cinder@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on cinder.* to cinder@'%' identified by 'pw'; MariaDB [(none)]\u003e flush privileges; # cinder 구성을 위한 db를 생성합니다. ( 저는 편의를 위해 모든 pw로 qwer1234 설정하였습니다. ) cinder 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-cinder $ vi /etc/cinder/cinder.conf [DEFAULT] my_ip = 10.10.10.10 log_dir = /var/log/cinder state_path = /var/lib/cinder auth_strategy = keystone transport_url = rabbit://openstack:qwer1234@controller enable_v3_api = True [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = qwer1234 [database] connection = mysql+pymysql://cinder:qwer1234@controller/cinder [oslo_concurrency] lock_path = $state_path/tmp # cinder.conf 파일을 수정합니다. $ su -s /bin/bash cinder -c \"cinder-manage db sync\" # cinder db를 동기화시킵니다. $ systemctl start openstack-cinder-api openstack-cinder-scheduler $ systemctl enable openstack-cinder-api openstack-cinder-scheduler # cinder 시작 및 자동시작을 등록합니다. $ echo \"export OS_VOLUME_API_VERSION=3\" \u003e\u003e ~/admin $ source ~/admin # 볼륨 버전을 API 3로 지정합니다. $ firewall-cmd --add-port=8776/tcp --permanent $ firewall-cmd --reload 이어서 compute 노드에 설치를 진행하겠습니다. cinder 패키지 설치 및 수정 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-cinder python2-crypto targetcli $ vi /etc/cinder/cinder.conf [DEFAULT] my_ip = 10.10.10.30 log_dir = /var/log/cinder state_path = /var/lib/cinder auth_strategy = keystone transport_url = rabbit://openstack:qwer1234@controller glance_api_servers = http://controller:9292 enable_v3_api = True [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = qwer1234 [database] connection = mysql+pymysql://cinder:qwer1234@controller/cinder [oslo_concurrency] lock_path = $state_path/tmp # cinder.conf 파일을 수정합니다. $ systemctl start openstack-cinder-volume $ systemctl enable openstack-cinder-volume # cinder 서비스를 시작 및 자동시작을 등록합니다. $ controller $openstack volume service list # 확인 +------------------+------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-07-20T04:02:31.000000 | +------------------+------------+------+---------+-------+----------------------------+ 8-2. LVM으로 블록 스토리지 백엔드 구성 compute 노드에 cinder 서비스를 설치한 것에 이어 LVM 백엔드를 설정해보도록 하겠습니다. VG 생성 참조 $ fdisk /dev/sd[ n ] # 만약 디스크 파티션이 없으시면 새로 생성 후 등록합니다. # 저는 간단하게 100G 하드를 추가한 후, cinder 이름으로 vg를 생성하였습니다. $ vi /etc/cinder/cinder.conf [DEFAULT] ... enabled_backends = lvm [lvm] target_helper = lioadm target_protocol = iscsi target_ip_address = 10.10.10.30 volume_group = cinder volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_dir = $state_path/volumes # cinder.conf의 상단에 내용을 추가설정합니다. $ firewall-cmd --add-service=iscsi-target --permanent $ firewall-cmd --reload # 방화벽 설정을 추가합니다. $ systemctl restart openstack-cinder-volume # 서비스를 재시작합니다. 이어서 compute 노드의 nova.conf 파일을 수정합니다. $ vi /etc/nova/nova.conf [cinder] os_region_name = RegionOne # nova.conf의 하단에 상단의 내용을 추가합니다. $ systemctl restart openstack-nova-compute # nova 서비스를 재시작합니다. $ controller $ openstack volume service list # 생성을 확인합니다. +------------------+-------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+-------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-07-20T04:54:52.000000 | | cinder-volume | compute@lvm | nova | enabled | up | 2020-07-20T04:54:52.000000 | +------------------+-------------+------+---------+-------+----------------------------+ $ controller $ openstack volume cretae --size 1 test # 확인용 1G volume을 생성합니다. +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2020-07-20T05:00:21.000000 | | description | None | | encrypted | False | | id | f09ee80f-3ec8-4eaf-a4a5-af13cccbd5ae | | migration_status | None | | multiattach | False | | name | test | | properties | | | replication_status | None | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | type | None | | updated_at | None | | user_id | 296ce49d1dc94931b62a726fb64712e9 | +---------------------+--------------------------------------+ $ openstack volume list # 생성한 volume을 확인합니다. +--------------------------------------+------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+------+-----------+------+-------------+ | f09ee80f-3ec8-4eaf-a4a5-af13cccbd5ae | test | available | 1 | | +--------------------------------------+------+-----------+------+-------------+ 8-3. LBaaS 설치 로드밸런싱을 위해서는 LBaaS를 사용해야 합니다. LBaaS에 대해서는 LBaaS를 참조해주세요. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron-lbaas net-tools # LBaaS 서비스를 설치합니다. $ vi /etc/neutron/neutron.conf service_plugins = router,lbaasv2 # lbaasv2 서비스를 추가합니다. $ vi /etc/neutron/neutron_lbaas.conf [service_providers] service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default $ vi /etc/neutron/lbaas_agent.ini [DEFAULT] interface_driver = openvswitch $ su -s /bin/bash neutron -c \"neutron-db-manage --subproject neutron-lbaas --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\" $ systemctl restart neutron-server network 노드와 compute 노드는 동일하게 진행합니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-neutron-lbaas haproxy net-tools $ vi /etc/neutron/neutron.conf service_plugins = router,lbaasv2 $ vi /etc/neutron/neutron_lbaas.conf [service_providers] service_provider = LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default $ vi /etc/neutron/lbaas_agent.ini [DEFAULT] interface_driver = openvswitch $ systemctl start neutron-lbaasv2-agent $ systemctl enable neutron-lbaasv2-agent 8-4. LFS, LVM 기반 다중 스토리지 노드 구성 9. Swift ( 오브젝트 스토리지 서비스 ) 구성 Swift란 오브젝트 스토리지 서비스로, 흔히 우리가 생각하는 네이버 클라우드와 거의 동일한 맥락이라 할 수 있습니다. swift는 기본적으로 controller에 설치하나 여기서는 비교적 자원소모가 적은 network 노드에 proxy-sever를, compute 노드를 storage로 사용하여 설치하여 진행하겠습니다. swift에 대한 설명은 swift을 참조해주세요.** swift 서비스 생성 controlloer 노드에는 swift 관련 패키지를 설치하지는 않지만 서비스의 관리를 위해 유저, 엔드포인트, url을 생성합니다. $ openstack user create --domain default --project service --password qwer1234 swift $ openstack role add --project service --user swift admin $ openstack service create --name swift --description \"OpenStack Object Storage\" object-store # swfit 유저를 생성하고 관리자의 권한을 부여합니다. $ export swift_proxy=10.10.10.20 $ openstack endpoint create --region RegionOne object-store public http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne object-store internal http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne object-store admin http://$swift_proxy:8080/v1/AUTH_%\\(tenant_id\\)s # swift의 endpoint를 등록합니다. 여기서 proxy 서버는 네트워크 노드를 등록합니다. 이어서 network 노드에서의 swift 설치 및 설정을 진행하겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-swift-proxy python-memcached openssh-clients # swift 서비스에 필요한 패키지를 설치합니다. $ vi /etc/swift/proxy-server.conf [filter:cache] use = egg:swift#memcache #memcache_servers = 127.0.0.1:11211 memcache_servers = controller:11211 [filter:authtoken] paste.filter_factory = keystonemiddleware.auth_token:filter_factory #admin_tenant_name = %SERVICE_TENANT_NAME% #admin_user = %SERVICE_USER% #admin_password = %SERVICE_PASSWORD% #admin_host = 127.0.0.1 #admin_port = 35357 #admin_protocol = http #admin_ /tmp/keystone-signing-swift # paste.filter_factory를 제외한 기존 정보는 주석처리 www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = swift password = qwer1234 delay_auth_decision = true # 위에 내용을 대신 주석 추가 # proxy-server.conf 파일을 수정합니다. # memcache_servers의 IP는 controller 노드의 IP로 수정합니다. $ vi /etc/swift/swift.conf [swift-hash] #swift_hash_path_suffix = %SWIFT_HASH_PATH_SUFFIX% swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path swift 서비스의 사용을 위해 account, container, object를 생성합니다. $ swift-ring-builder /etc/swift/account.builder create 12 1 1 $ swift-ring-builder /etc/swift/container.builder create 12 1 1 $ swift-ring-builder /etc/swift/object.builder create 12 1 1 # account, container, object를 생성합니다. # 12 = 한 클러스터 스토리지에서 생성 가능한 파티션의 수 # 1 = 오브젝트 복수 수 ( 스토리지의 개수 ) # 1 = 데이터 이동, 복제, 파티션 이동 등이 진행될 때 잠기는 최소 시간, 데이터 손실을 방지하기 위한 기능 $ swift-ring-builder /etc/swift/account.builder add r0z0-10.10.10.30:6202/device0 100 $ swift-ring-builder /etc/swift/container.builder add r0z0-10.10.10.30:6201/device0 100 $ swift-ring-builder /etc/swift/object.builder add r0z0-10.10.10.30:6200/device0 100 $ swift-ring-builder /etc/swift/account.builder rebalance $ swift-ring-builder /etc/swift/container.builder rebalance $ swift-ring-builder /etc/swift/object.builder rebalance # compute 노드의 builder에 region과 zone을 추가 후 반영시킵니다. # r = region, z = zone $ chown swift. /etc/swift/*.gz # swift 관련 파일의 소유권을 변경합니다. $ systemctl start openstack-swift-proxy $ systemctl enable openstack-swift-proxy # 프록시 서비스를 시작합니다. $ firewall-cmd --add-port=8080/tcp --permanent $ firewall-cmd --reload # 방화벽을 사용 중이라면 방화벽을 등록합니다. 이제 이어 storage를 구성하기 위해 compute 노드에서의 설치를 진행해보도록 하겠습니다. compute 노드는 이미 cinder 서비스가 동작하고 있어 기본적인 네트워크, 시간 설정, 레포지터리 지정 등은 구성이 마친 상태의 노드입니다. 만약 다른 노드에 구성하시거나 swift 서비스를 다중 노드로 구성하시는 경우 위와 같은 설정을 먼저 진행해주시길 바랍니다. 여기서는 swift 서비스를 위해 100G의 버츄얼 디스크( dev/sdc )를 추가하여 진행하였습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-swift-account openstack-swift-container openstack-swift-object xfsprogs rsync openssh-clients # swift 서비스를 설치합니다. $ scp root@network:/etc/swift/*.gz /etc/swift/ $ chown swift. /etc/swift/*.gz # network 노드에서의 설정파일을 복사옵니다. $ vi /etc/swift/swift.conf [swift-hash] #swift_hash_path_suffix = %SWIFT_HASH_PATH_SUFFIX% swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path $ swift.conf 파일을 설정합니다. $ vi /etc/swift/account-server.conf bind-ip = 0.0.0.0 bind_port = 6202 $ vi /etc/swift/container-server.conf bind-ip = 0.0.0.0 bind_port = 6201 $ vi /etc/swift/object-server.conf bind-ip = 0.0.0.0 bind_port = 6200 $ vi /etc/rsyncd.conf pid file = /var/run/rsymcd.pid log file = /var/log/rsymcd.log uid = swift gid = swift address = compute [account] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/account.lock [container] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/container.lock [object] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/object.lock [swift_server] path = /etc/swift read only = true write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 5 lock file = /var/lock/swift_server.lock # swift 서비스관련 파일을 수정합니다. compute 노드에서 disk 설정을 진행합니다. $ mkfs.xfs -i size=1024 -s size=4096 /dev/sdb1 meta-data=/dev/sdc1 isize=1024 agcount=4, agsize=6553536 blks = sectsz=4096 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=26214144, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=12799, version=2 = sectsz=4096 sunit=1 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 # 디스크의 xfs의 유형으로 포맷시킵니다. $ mkdir -p /srv/node/device0 $ mount -o noatime,nodiratime,nobarrier /dev/sdc1 /srv/node/device0 $ chown -R swift. /srv/node # device0 디렉토리를 생성하고 해당 디렉토리에 sdb1 볼륨을 마운트시킨 후, swift로 소유권을 변경시킵니다. $ vi /etc/fstab /dev/sdc1 /srv/node/device0 xfs noatime,nodiratime,nobarrier 0 0 # 재부팅할 경우를 대비하여 생성한 볼륨을 fstab에 등록합니다. selinux 및 방화벽 관련 서비스를 설정합니다. $ semanage fcontext -a -t swift_data_t /srv/node/device0 $ restorecon /srv/node/device0 $ firewall-cmd --add-port={873/tcp,6200/tcp,6201/tcp,6202/tcp} --permanent $ firewall-cmd --reload swift 관련 서비스를 재시작합니다. $ systemctl restart rsyncd openstack-swift-account-auditor openstack-swift-account-replicator openstack-swift-account openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object $ systemctl enable rsyncd openstack-swift-account-auditor openstack-swift-account-replicator openstack-swift-account openstack-swift-container-auditor openstack-swift-container-replicator openstack-swift-container-updater openstack-swift-container openstack-swift-object-auditor openstack-swift-object-replicator openstack-swift-object-updater openstack-swift-object 확인을 위해 controller 노드에 httpd를 재시작합니다. $ systemctl restart httpd # 대시보드 접속 후 프로젝트에서 오브젝트 스토리지가 메뉴에 있는 지를 확인합니다. $ openstack container create test +---------------------------------------+-----------+------------------------------------+ | account | container | x-trans-id | +---------------------------------------+-----------+------------------------------------+ | AUTH_2ac06290d2d943d5a768fe3daa53b118 | test | tx22f3dd125f134a189602c-005f24cef1 | +---------------------------------------+-----------+------------------------------------+ $ echo Hello \u003e test.txt $ swift upload test test.txt $ swift list test $ swift list test test.txt 10. Heat ( Orchestration ) 설치 클라우딩 컴퓨팅이 꽃인 Orchestaration 기능을 수행하는 Heat 서비스를 설치해보도록 하겠습니다. Heat 설치는 controller, network 노드 순으로 우리어집니다. Heat*에 대한 설명은 Heat을 참조해주세요. Heat 서비스 생성 $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-heat-common python-heatclient # heat 서비스 관련 패키지를 다운로드 합니다. $ openstack user create --domain default --project service --password qwer1234 heat $ openstack role add --project service --user heat admin $ openstack role create heat_stack_owner $ openstack role create heat_stack_user $ openstack role add --project admin --user admin heat_stack_owner $ openstack service create --name heat --description \"Openstack Orchestration\" orchestration $ openstack service create --name heat-cfn --description \"Openstack Orchestration\" cloudformation # heat 유저를 생성하고 관리자의 권한을 부여합니다. $ export heat_api=10.10.10.20 $ openstack endpoint create --region RegionOne orchestration public http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne orchestration internal http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne orchestration admin http://$heat_api:8004/v1/AUTH_%\\(tenant_id\\)s $ openstack endpoint create --region RegionOne cloudformation public http://$heat_api:8000/v1 $ openstack endpoint create --region RegionOne cloudformation internal http://$heat_api:8000/v1 $ openstack endpoint create --region RegionOne cloudformation admin http://$heat_api:8000/v1 # heat 서비스의 endpoint를 등록합니다. 여기서 proxy 서버는 네트워크 노드를 등록합니다. $ openstack domain create --description \"Stack projects and users\" heat $ openstack user create --domain heat --password qwer1234 heat_domain_admin $ openstack role add --domain heat --user heat_domain_admin admin # heat domain을 생성하고 heat 유저에게 권한을 부여합니다. heat의 DB를 생성합니다. $ mysql -u root -p MariaDB [(none)]\u003e create database heat; MariaDB [(none)]\u003e grant all privileges on heat.* to heat@'localhost' identified by 'pw'; MariaDB [(none)]\u003e grant all privileges on heat.* to keystone@'%' identified by 'pw'; MariaDB [(none)]\u003e flush privileges; # heat DB를 생성합니다. 여기서 pw는 qwer1234으로 모두 통일하였습니다. 이어서 network 노드에서 heat 서비스를 설치해보겠습니다. $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine python-heatclient # heat 서비스를 위한 패키지를 설치합니다. $ vi /etc/heat/heat.conf [DEFAULT] deferred_auth_method = trusts trusts_delegated_roles = heat_stack_owner # Heat installed server heat_metadata_server_url = http://network:8000 heat_waitcondition_server_url = http://network:8000/v1/waitcondition heat_watch_server_url = http://network:8003 heat_stack_user_role = heat_stack_user # Heat domain name stack_user_domain_name = heat # Heat domain admin name stack_domain_admin = heat_domain_admin # Heat domain admin's password stack_domain_admin_password = qwer1234 # RabbitMQ connection info transport_url = rabbit://openstack:qwer1234@controller # MariaDB connection info [database] connection = mysql+pymysql://heat:qwer1234@controller/heat # Keystone auth info [clients_keystone] auth_uri = http://controller:5000 # Keystone auth info [ec2authtoken] auth_uri = http://controller:5000 [heat_api] bind_host = 0.0.0.0 bind_port = 8004 [heat_api_cfn] bind_host = 0.0.0.0 bind_port = 8000 # Keystone auth info [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = qwer1234 [trustee] auth_plugin = password auth_url = http://controller:5000 username = heat password = qwer1234 user_domain_name = default # heat.conf 파일을 수정합니다. $ su -s /bin/bash heat -c \"heat-manage db_sync\" $ systemctl start openstack-heat-api openstack-heat-api-cfn openstack-heat-engine $ systemctl enable openstack-heat-api openstack-heat-api-cfn openstack-heat-engine # DB의 데이터를 삽입하고, 서비스슬 등록합니다. 방화벽을 사용중이면 방화벽을 설정합니다. $ firewall-cmd --add-port={8000/tcp,8004/tcp} --permanent $ firewall-cmd --reload $ yum --enablerepo=centos-openstack-stein,epel -y install openstack-designate-api openstack-designate-central openstack-designate-worker openstack-designate-producer openstack-designate-mdns python-designateclient bind bind-utils # 서비스 관련 패키지를 설치합니다. $ rndc-confgen -a -k designate -c /etc/designate.key -r /dev/urandom $ chown named:designate /etc/designate.key $ chmod 640 /etc/designate.key # key를 생성합니다. $ vi /etc/named.conf # create new options { listen-on port 53 { any; }; listen-on-v6 port 53 { none; }; directory \"/var/named\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; # replace query range to your own environment allow-query { localhost; 10.10.10.0/24; }; allow-new-zones yes; request-ixfr no; recursion no; bindkeys-file \"/etc/named.iscdlv.key\"; managed-keys-directory \"/var/named/dynamic\"; pid-file \"/run/named/named.pid\"; session-keyfile \"/run/named/session.key\"; }; include \"/etc/designate.key\"; controls { inet 0.0.0.0 port 953 allow { localhost; } keys { \"designate\"; }; }; logging { channel default_debug { file \"data/named.run\"; severity dynamic; }; }; zone \".\" IN { type hint; file \"named.ca\"; }; $ chown -R named. /var/named $ systemctl start named $ systemctl enable naemd $ vi /etc/designate/designate.conf [DEFAULT] log_dir = /var/log/designate transport_url = rabbit://openstack:qwer1234@controller root_helper = sudo designate-rootwrap /etc/designate/rootwrap.conf [database] connection = mysql+pymysql://heat:qwer1234@controller/heat [service:api] listen = 0.0.0.0:9001 auth_strategy = keystone api_base_uri = http://controller:9001 enable_api_v2 = True enabled_extensions_v2 = quotas, reports [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = qwer1234 [service:worker] enabled = True notify = True [storage:sqlalchemy] connection = mysql+pymysql://heat:qwer1234@controller/heat $ su -s /bin/sh -c \"designate-manage database sync\" designate $ systemctl start designate-central designate-api $ systemctl enable designate-central designate-api $ vi /etc/designate/pools.yaml # create new (replace hostname and IP address to your own environment) - name: default description: Default Pool attributes: {} ns_records: - hostname: network.srv.world. priority: 1 nameservers: - host: 10.10.10.20 port: 53 targets: - type: bind9 description: BIND9 Server masters: - host: 10.10.10.20 port: 5354 options: host: 10.10.10.20 port: 53 rndc_host: 10.10.10.20 rndc_port: 953 rndc_key_file: /etc/designate.key $ su -s /bin/sh -c \"designate-manage pool update\" designate Updating Pools Configuration $ systemctl start designate-worker designate-producer designate-mdns $ systemctl enable designate-worker designate-producer designate-mdns 이어서 selinux와 방화벽을 설정합니다. $ setsebool -P named_write_master_zones on $ firewall-cmd --add-service=dns --permanent $ firewall-cmd --add-port={5354/tcp,9001/tcp} --permanent $ firewall-cmd --reload controller\u003e $openstack dns service list # 확인 11. Openstack 대시보드 메인 로고 및 링크 변경 12. Neutron 기반 Service Functon Chaining ( SFC ) 기능 구성 ","#":"","openstack-stain-manual-설치#\u003cstrong\u003eOpenstack Stain Manual 설치\u003c/strong\u003e":""},"title":"Openstack Stain Manual 설치"},"/system/openstack/openstacktraining/openstack-ussuri-01/":{"data":{"":"OpenStack Ussuri : OverviewOpenStack Ussuri : Overview OpenStack Ussuri 설치는 위의 그림과 표에 맞춰 설치가 진행됩니다. minimal 기본 설치는 keystone, glance, nova, neutron, cinder, horizon이며 여기서는 가능한 모든 서비스를 설치하도록 하겠습니다. OS HOST NAME CPU/thead RAM DISK Network Interface-1 Network Interface-2 CentOS8 controller 4/8 6144 100G Nat host1 CentOS8 network 2/4 2048 40G Nat host1 CentOS8 compute 4/8 4096 40G host1 CentOS8 storage1 1/2 1024 50G host1 CentOS8 storage2 1/2 1024 50G host1 CentOS8 storage3 1/2 1024 50G host1 Service Code Name Description Identity Service Keystone User Management Compute Service Nova Virtual Machine Management Image Service Glance Manages Virtual image like kernel image or disk image Dashboard Horizon Provides GUI console via Web browser Object Storage Swift Provides Cloud Storage Block Storage Cinder Storage Management for Virtual Machine Network Service Neutron Virtual Networking Management Orchestration Service Heat Provides Orchestration function for Virtual Machine Metering Service Ceilometer Provides the function of Usage measurement for accounting Database Service Trove Database resource Management Data Processing Service Sahara Provides Data Processing function Bare Metal Provisioning Ironic Provides Bare Metal Provisioning function Messaging Service Zaqar Provides Messaging Service function Shared File System Manila Provides File Sharing Service DNS Service Designate Provides DNS Server Service Key Manager Service Barbican Provides Key Management Service ","openstack-ussuri--overview#\u003cstrong\u003eOpenStack Ussuri : Overview\u003c/strong\u003e":"","openstack-ussuri--overview-1#\u003cstrong\u003eOpenStack Ussuri : Overview\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Overview"},"/system/openstack/openstacktraining/openstack-ussuri-02/":{"data":{"":"OpenStack Ussuri : 기본 환경설정 ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached | ----------------------- OpenStack Ussuri : 기본 환경설정 앞 기본 환경설정을 모든 노드에서 진행한 후, DB, RabbitMQ, Memcached는 controller에서만 설치를 진행합니다. $ all\u003e $ controller\u003e $ controller ~(keystone)\u003e $ compute\u003e $ network\u003e # 위와 같은 호스트를 주의헤 주세요 ! # (keystone)은 keystone 설치 후 인증 받은 터미널입니다. Ussuri repository 등록 OpenStack 구현을 위해 Ussuri repository를 구현합니다. $ all\u003e dnf -y install centos-release-openstack-ussuri $ all\u003e sed -i -e \"s/enabled=1/enabled=0/g\" /etc/yum.repos.d/CentOS-OpenStack-ussuri.repo $ all\u003e dnf --enablerepo=centos-openstack-ussuri -y upgrade NTP ( Network Time Protocol ) Server 설치 NTP Server는 모든 Node에서 설정을 진행합니다. $ all\u003e dnf --enablerepo=centos-openstack-ussuri -y install openstack-selinux # SELinux를 설치합니다. $ all\u003e dnf install -y wget # wget을 설치합니다. $ all\u003e dnf --enablerepo=powertools -y install epel-release # epel 레포지터리를 등록합니다. $ all\u003e dnf --enablerepo=powertools -y install checkpolicy # 만약 Checkpolicy가 설치되어 있지 않으면, 패키지를 다운 받습니다. $ all\u003e dnf -y install chrony $ all\u003e vi /etc/chrony.conf # pool 2.centos.pool.ntp.org iburst pool ntp.nict.jp iburst allow 10.10.10.0/24 $ all\u003e systemctl enable --now chronyd # chrony 파일을 수정합니다. allow에는 사용대역을 기입합니다. $ all\u003e firewall-cmd --add-service=ntp --permanent $ all\u003e firewall-cmd --reload $ all\u003e init 6 $ all\u003e chronyc sources ^+ ntp-k1.nict.jp 1 6 17 10 -588us[-2093us] +/- 28ms ^+ ntp-a3.nict.go.jp 1 6 17 10 -2468us[-3973us] +/- 30ms ^* ntp-b3.nict.go.jp 1 6 17 10 +1015us[ -490us] +/- 22ms ^- ntp-b2.nict.go.jp 1 6 17 10 +2720us[+2720us] +/- 22ms # 방화벽을 등록 후 확인합니다. Controller MariaDB 설치 $ controller\u003e dnf module -y install mariadb:10.3 $ controller\u003e vi /etc/my.cnf.d/charaset.cnf [mysqld] character-set-server = utf8mb4 [client] default-character-set = utf8mb4 # mariadb를 설치 후, charaset 설정을 변경하기 위해 파일을 수정합니다. $ controller\u003e systemctl restart --now mariadb $ controller\u003e systemctl enable --now mariadb # DB를 재시작 합니다. $ controller\u003e firewall-cmd --add-service=mysql --permanent $ controller\u003e firewall-cmd --reload # 방화벽을 설정합니다. $ controller\u003e mysql_secure_installation $ controller\u003e mysql -u root -p # 설정을 초기화 후, 비빌번호를 생성합니다. RabbitMQ, Memcached 설치 RabbitMQ는 오픈 소스 메시지 브로커 소프트웨어이며, AMQP를 구현합니다. RabbitMQ는 OpenStack에서는 서로간의 통신을 위해 사용됩니다. Memcached이란 Memcached 는 범용 분산 캐시 시스템로, OpenStack에서 캐시값을 관리합니다. RabbitMq, Memcached는 Controller에서만 설치를 진행합니다. $ controller\u003e dnf --enablerepo=powertools -y install rabbitmq-server memcached $ controller\u003e vi /etc/my.cnf.d/mariadb-server.cnf [mysqld] ..... ..... max_connections=500 # 인증허용 시간 값을 추가합니다. $ controller\u003e vi /etc/sysconfig/memcached OPTIONS=\"-l 0.0.0.0,::\" # 모두가 사용할 수 있도록 값을 수정합니다. $ controller\u003e systemctl restart mariadb rabbitmq-server memcached $ controller\u003e systemctl enable mariadb rabbitmq-server memcached # RabbitMQ, Memcached 서비스를 등록합니다. $ controller\u003e rabbitmqctl add_user openstack qwer1234 $ controller\u003e rabbitmqctl set_permissions openstack \".*\" \".*\" \".*\" # rabbitmq를 사용할 openstack 유저를 패스워드 qwer1234로 생성하고 모든 권한을 줍니다. $ controller\u003e vi rabbitmqctl.te module rabbitmqctl 1.0; require { type rabbitmq_t; type rabbitmq_var_log_t; type rabbitmq_var_lib_t; type etc_t; type init_t; class file write; class file getattr; } #============= rabbitmq_t ============== allow rabbitmq_t etc_t:file write; #============= init_t ================== allow init_t rabbitmq_var_lib_t:file getattr; allow init_t rabbitmq_var_log_t:file getattr; $ controller\u003e checkmodule -m -M -o rabbitmqctl.mod rabbitmqctl.te $ controller\u003e semodule_package --outfile rabbitmqctl.pp --module rabbitmqctl.mod $ controller\u003e semodule -i rabbitmqctl.pp # rabbitmq의 설정을 추가 후 등록시킵니다. $ controller\u003e firewall-cmd --add-service=memcache --permanent $ controller\u003e firewall-cmd --add-port=5672/tcp --permanent $ controller\u003e firewall-cmd --reload # 방화벽을 설정합니다. ","#":"","openstack-ussuri--기본-환경설정#\u003cstrong\u003eOpenStack Ussuri : 기본 환경설정\u003c/strong\u003e":"","openstack-ussuri--기본-환경설정-1#\u003cstrong\u003eOpenStack Ussuri : 기본 환경설정\u003c/strong\u003e":""},"title":"OpenStack Ussuri : 환경설정"},"/system/openstack/openstacktraining/openstack-ussuri-03/":{"data":{"":"OpenStack Ussuri : Keystone ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached Keystone | | httpd | ----------------------- OpenStack Ussuri : Keystone Keystone은 OpenStack에서 인증 서비스를 구성하고 있습니다. Keystone에 대한 자세한 설명은 Keystone을 참조해주세요. Keystone 유저와 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database keystone; $ MariaDB\u003e grant all privileges on keystone.* to keystone@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on keystone.* to keystone@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Keystone을 설치합니다. $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,epel,powertools -y install openstack-keystone python3-openstackclient httpd mod_ssl python3-mod_wsgi python3-oauth2client # keystone 및 관련 모듈을 설치합니다. $ controller\u003e vi /etc/keystone/keystone.conf [cache] memcache_servers = controller:11211 [database] connection = mysql+pymysql://keystone:qwer1234@controller/keystone [token] provider = fernet $ controller\u003e su -s /bin/bash keystone -c \"keystone-manage db_sync\" $ controller\u003e keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone $ controller\u003e keystone-manage credential_setup --keystone-user keystone --keystone-group keystone # Keystone DB를 임포트 시킵니다. $ controller\u003e keystone-manage bootstrap --bootstrap-password qwer1234 \\ --bootstrap-admin-url http://controller:5000/v3/ \\ --bootstrap-internal-url http://controller:5000/v3/ \\ --bootstrap-public-url http://controller:5000/v3/ \\ --bootstrap-region-id RegionOne $ controller\u003e setsebool -P httpd_use_openstack on $ controller\u003e setsebool -P httpd_can_network_connect on $ controller\u003e setsebool -P httpd_can_network_connect_db on $ controller\u003e vi keystone-httpd.te module keystone-httpd 1.0; require { type httpd_t; type keystone_log_t; class file create; class dir { add_name write }; } #============= httpd_t ============== allow httpd_t keystone_log_t:dir { add_name write }; allow httpd_t keystone_log_t:file create; $ controller\u003e checkmodule -m -M -o keystone-httpd.mod keystone-httpd.te $ controller\u003e semodule_package --outfile keystone-httpd.pp --module keystone-httpd.mod $ controller\u003e semodule -i keystone-httpd.pp $ controller\u003e firewall-cmd --add-port=5000/tcp --permanent $ controller\u003e firewall-cmd --reload # 방화벽 및 SELinux를 설정합니다. $ controller\u003e vi /etc/httpd/conf/httpd.conf ServerName controller:80 # 99번 줄에 추가합니다. $ controller\u003e ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/ $ controller\u003e systemctl enable --now httpd # httpd 서비스를 등록합니다. Keystone Project 생성 $ controller\u003e vi ~/admin_key export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=qwer1234 export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 export PS1='[\\u@\\h \\W~(keystone)]\\$ ' $ controller\u003e chmod 600 ~/admin_key $ controller\u003e source ~/admin_key $ controller\u003e echo \"source ~/admin_key \" \u003e\u003e ~/.bash_profile # keystone 인증파일 생성 후 시작시 등록되게 등록시킵니다. $ controller ~(keystone)\u003e openstack project create --domain default --description \"Service Project\" service +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Service Project | | domain_id | default | | enabled | True | | id | 7c10c02365be496fb47f12bfd40fe4a7 | | is_domain | False | | name | service | | options | {} | | parent_id | default | | tags | [] | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack project list +----------------------------------+---------+ | ID | Name | +----------------------------------+---------+ | 7c10c02365be496fb47f12bfd40fe4a7 | service | | c76211c24a1f460ca67274d655d46725 | admin | +----------------------------------+---------+ ","#":"","openstack-ussuri--keystone#\u003cstrong\u003eOpenStack Ussuri : Keystone\u003c/strong\u003e":"","openstack-ussuri--keystone-1#\u003cstrong\u003eOpenStack Ussuri : Keystone\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Keystone"},"/system/openstack/openstacktraining/openstack-ussuri-04/":{"data":{"":"OpenStack Ussuri : Glance ----------------------- | [ Controller Node ] | | | | MariaDB RabbitMQ | | Memcached Keystone | | httpd Glance | ----------------------- OpenStack Ussuri : Glance Glance는 OpenStack에서 이미지 생성에 필요한 Iamge 관리 서비스를 구성하고 있습니다. Glance에 자세한 설명은 Glance를 참조해주세요. Glance service 및 User 생성 $ contoller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 glance +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 03f5b16a7be84cb688617d1943c8fe8c | | name | glance | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ contoller ~(keystone)\u003e openstack role add --project service --user glance admin $ contoller ~(keystone)\u003e openstack service create --name glance --description \"OpenStack Image service\" image +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Image service | | enabled | True | | id | af365771c17a4a25ae1d0c659e2dc0eb | | name | glance | | type | image | +-------------+----------------------------------+ $ contoller ~(keystone)\u003e openstack endpoint create --region RegionOne image public http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | cc65faecd7b042ffafd0f262cd7547df | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | af365771c17a4a25ae1d0c659e2dc0eb | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ $ contoller ~(keystone)\u003e openstack endpoint create --region RegionOne image internal http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | ea41c7b17c844e658ac83c547eddcf6d | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | af365771c17a4a25ae1d0c659e2dc0eb | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ $ contoller ~(keystone)\u003e openstack endpoint create --region RegionOne image admin http://controller:9292 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 1393a64ef0ec428ba437602ac5b390f6 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | af365771c17a4a25ae1d0c659e2dc0eb | | service_name | glance | | service_type | image | | url | http://controller:9292 | +--------------+----------------------------------+ Glance 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database glance; $ MariaDB\u003e grant all privileges on glance.* to glance@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on glance.* to glance@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Glance 설치 $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-glance # Glacne 및 관련 모듈을 설치합니다. $ controller\u003e vi /etc/glance/glance-api.conf [DEFAULT] bind_host = 0.0.0.0 [glance_store] stores = file,http default_store = file filesystem_store_datadir = /var/lib/glance/images/ [database] connection = mysql+pymysql://glance:qwer1234@controller/glance [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = glance password = qwer1234 [paste_deploy] flavor = keystone # glacne 파일을 수정합니다. $ controller\u003e su -s /bin/bash glance -c \"glance-manage db_sync\" $ controller\u003e systemctl enable --now openstack-glance-api # Glance DB를 임포트 시킨후 서비스를 등록합니다. $ controller\u003e setsebool -P glance_api_can_network on $ controller\u003e vi glanceapi.te module glanceapi 1.0; require { type glance_api_t; type httpd_config_t; type iscsid_exec_t; class dir search; class file { getattr open read }; } #============= glance_api_t ============== allow glance_api_t httpd_config_t:dir search; allow glance_api_t iscsid_exec_t:file { getattr open read }; $ controller\u003e checkmodule -m -M -o glanceapi.mod glanceapi.te $ controller\u003e semodule_package --outfile glanceapi.pp --module glanceapi.mod $ controller\u003e semodule -i glanceapi.pp $ controller\u003e firewall-cmd --add-port=9292/tcp --permanent $ controller\u003e firewall-cmd --reload Glance Image 생성 $ controller ~(keystone)\u003e mkdir -p /var/kvm/images $ controller ~(keystone)\u003e wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img # 이미지를 다운받습니다. $ controller ~(keystone)\u003e openstack image create \"cirros\" --file cirros-0.5.1-x86_64-disk.img --disk-format qcow2 +------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+ | Field | Value | +------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+ | checksum | 1d3062cd89af34e419f7100277f38b2b | | container_format | bare | | created_at | 2020-08-06T11:08:39Z | | disk_format | qcow2 | | file | /v2/images/dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16/file | | id | dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16 | | min_disk | 0 | | min_ram | 0 | | name | Cirros | | owner | c76211c24a1f460ca67274d655d46725 | | properties | os_hash_algo='sha512', os_hash_value='553d220ed58cfee7dafe0 03c446a9f197ab5edf8ffc09396c74187cf83873c877e7ae041cb80f3b91489acf687183adcd689b 53b38e3ddd22e627e7f98a09c46', os_hidden='False', owner_specified.openstack.md5=' 1d3062cd89af34e419f7100277f38b2b', owner_specified.openstack.object='images/Cirr os', owner_specified.openstack.sha256='c4110030e2edf06db87f5b6e4efc27300977683d5 3f040996d15dcc0ad49bb5a', self='/v2/images/dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16' | | protected | False | | schema | /v2/schemas/image | | size | 16338944 | | status | active | | tags | | | updated_at | 2020-08-06T11:08:39Z | | visibility | shared | +------------------+------------------------------------------------------------ -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -+ # 이미지를 등록합니다. $ controller ~(keystone)\u003e openstack image list +--------------------------------------+--------+--------+ | ID | Name | Status | +--------------------------------------+--------+--------+ | dc7c2474-8ec9-4f74-a1c3-7cf6a9ad3d16 | Cirros | active | +--------------------------------------+--------+--------+ # 이미지를 확인합니다. ","#":"","openstack-ussuri--glance#\u003cstrong\u003eOpenStack Ussuri : Glance\u003c/strong\u003e":"","openstack-ussuri--glance-1#\u003cstrong\u003eOpenStack Ussuri : Glance\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Glance"},"/system/openstack/openstacktraining/openstack-ussuri-05/":{"data":{"":"OpenStack Ussuri : Nova ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | | | Libvirt | | MariaDB RabbitMQ | | Nova-compute | | Memcached Keystone | | Open vSwitch | | httpd nova | | L2 Agent | | Nova-API | ----------------------- ----------------------- OpenStack Ussuri : Nova Nova는 OpenStack에서 인스턴스를 생성하는 서비스입니다. Nova에 대한 자세한 설명은 Nova를 참조해주세요. Nova, ceilometer service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 nova +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | f26027517d5e4b5b984b5db8d42398c8 | | name | nova | | options | {} | | qwer1234_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 placement +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 2394500b4512456f9d9d5066a5ecb1f7 | | name | placement | | options | {} | | qwer1234_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user nova admin $ controller ~(keystone)\u003e openstack role add --project service --user placement admin $ controller ~(keystone)\u003e openstack service create --name nova --description \"OpenStack Compute service\" compute +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Compute service | | enabled | True | | id | 28d495eca718439f9dc6ce395e0720dc | | name | nova | | type | compute | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack service create --name placement --description \"OpenStack Compute Placement service\" placement +-------------+-------------------------------------+ | Field | Value | +-------------+-------------------------------------+ | description | OpenStack Compute Placement service | | enabled | True | | id | 8515d3d046834de9b71b2938aae89898 | | name | placement | | type | placement | +-------------+-------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne compute public http://controller:8774/v2.1/%\\(tenant_id\\)s --------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | f13ca97a20eb46a3a1c1dfab546a00cc | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 28d495eca718439f9dc6ce395e0720dc | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne compute internal http://controller:8774/v2.1/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 1bc41c829f2f47e7962cba46f0da8ddc | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 28d495eca718439f9dc6ce395e0720dc | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne compute admin http://controller:8774/v2.1/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 8022a415f22c400c92989320a2be3133 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 28d495eca718439f9dc6ce395e0720dc | | service_name | nova | | service_type | compute | | url | http://controller:8774/v2.1/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne placement public http://controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 5e988f2be72242f0b3923e27e9db009c | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 8515d3d046834de9b71b2938aae89898 | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne placement internal http://controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | a68cf8b6eeb043c2aa1ec95d7711cb50 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 8515d3d046834de9b71b2938aae89898 | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne placement admin http://controller:8778 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 63e47fcbfd7841dd95bb4d9d9a910ab5 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 8515d3d046834de9b71b2938aae89898 | | service_name | placement | | service_type | placement | | url | http://controller:8778 | +--------------+----------------------------------+ Nova 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database nova; $ MariaDB\u003e grant all privileges on nova.* to nova@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on nova.* to nova@'%' identified by 'qwer1234'; $ MariaDB\u003e create database nova_api; $ MariaDB\u003e grant all privileges on nova_api.* to nova@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on nova_api.* to nova@'%' identified by 'qwer1234'; $ MariaDB\u003e create database nova_cell0; $ MariaDB\u003e grant all privileges on nova_cell0.* to nova@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on nova_cell0.* to nova@'%' identified by 'qwer1234'; $ MariaDB\u003e create database placement; $ MariaDB\u003e grant all privileges on placement.* to placement@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on placement.* to placement@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Nova 설치 $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-nova openstack-placement-api # nova 및 관련 모듈을 설치합니다. $ controller\u003e vi /etc/nova/nova.conf [DEFAULT] my_ip = 10.10.10.10 # my_ip는 반드시 IP로 적어주세요 ! state_path = /var/lib/nova enabled_apis = osapi_compute,metadata log_dir = /var/log/nova transport_url = rabbit://openstack:qwer1234@controller [api] auth_strategy = keystone [glance] api_servers = http://controller:9292 [vnc] enabled = true server_listen = $my_ip server_proxyclient_address = $my_ip [oslo_concurrency] lock_path = $state_path/tmp [api_database] connection = mysql+pymysql://nova:qwer1234@controller/nova_api [database] connection = mysql+pymysql://nova:qwer1234@controller/nova [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = nova password = qwer1234 [placement] auth_url = http://controller:5000 os_region_name = RegionOne auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [wsgi] api_paste_config = /etc/nova/api-paste.ini $ controller\u003e vi /etc/placement/placement.conf [DEFAULT] debug = false [api] auth_strategy = keystone [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = placement password = qwer1234 [placement_database] connection = mysql+pymysql://placement:qwer1234@controller/placement $ controller\u003e vi /etc/httpd/conf.d/00-placement-api.conf \u003cDirectory /usr/bin\u003e Require all granted \u003c/Directory\u003e # 15번 줄에 추가시킵니다. $ controller\u003e su -s /bin/bash placement -c \"placement-manage db sync\" $ controller\u003e su -s /bin/bash nova -c \"nova-manage api_db sync\" $ controller\u003e su -s /bin/bash nova -c \"nova-manage cell_v2 map_cell0\" $ controller\u003e su -s /bin/bash nova -c \"nova-manage db sync\" $ controller\u003e su -s /bin/bash nova -c \"nova-manage cell_v2 create_cell --name cell1\" # nova DB에 임포트 시킵니다. $ controller\u003e semanage port -a -t http_port_t -p tcp 8778 $ controller\u003e firewall-cmd --add-port={6080/tcp,6081/tcp,6082/tcp,8774/tcp,8775/tcp,8778/tcp} --permanent $ controller\u003e firewall-cmd --reload $ controller\u003e systemctl restart httpd $ controller\u003e chown placement. /var/log/placement/placement-api.log $ controller\u003e for service in api conductor scheduler novncproxy; do systemctl enable --now openstack-nova-$service done # Selinux 및 방화벽을 설정합니다. $ controller ~(keystone)\u003e openstack compute service list +----+----------------+------------+----------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +----+----------------+------------+----------+---------+-------+----------------------------+ | 4 | nova-conductor | controller | internal | enabled | up | 2020-08-06T12:10:34.000000 | | 5 | nova-scheduler | controller | internal | enabled | up | 2020-08-06T12:10:38.000000 | +----+----------------+------------+----------+---------+-------+----------------------------+ Conpute node Nova 설치 nova 설차 Nova 서비스를 설치하기 전에 가상화를 위한 KVM + QEMU를 설치합니다. 이를 위해서는 Inter VT나 AMD-V가 필요합니다. ( CPU ) $ lsmod | grep kvm kvm_amd 110592 0 ccp 98304 1 kvm_amd kvm 786432 1 kvm_amd irqbypass 16384 1 kvm $ compute\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install qemu-kvm libvirt virt-install libguestfs-tools # KVM 관련 모듈을 설치합니다. $ compute\u003e systemctl enable --now libvirtd # libvirtd 서비스를 등록 및 시작합니다. $ compute\u003e nmcli connection add type bridge autoconnect yes con-name br0 ifname br0 # br0의 가상 브릿지를 추가합니다. $ compute\u003e nmcli connection modify br0 ipv4.addresses 10.10.10.30/24 ipv4.method manual # 가상 브리지의 IP를 추가합니다. ( compute node ip ) $ compute\u003e nmcli connection modify br0 ipv4.gateway 10.10.10.10 # 가상 브리지의 GATEWAY를 등록합니다. $ compute\u003e nmcli connection modify br0 ipv4.dns 8.8.8.8 # 가상 브릿지의 DNS를 등록합니다. $ compute\u003e nmcli connection del ens34 # 본래의 네트워크 인터페이스를 삭제합니다. $ compute\u003e nmcli connection add type bridge-slave autoconnect yes con-name ens34 ifname ens34 master br0 # 삭제한 네트워크 인터페이스 대신 브릿지를 매핑시키고 네트워크를 재시작 시킵니다. # 제 compute node의 내부대역 IP는 10.10.10.30/24 ens34입니다 햇갈리지 마세요 ! $ compute\u003e init 6 $ compute\u003e ipfconfig br0: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 10.10.10.30 netmask 255.255.255.0 broadcast 10.10.10.255 inet6 fe80::6765:fe91:a94b:5529 prefixlen 64 scopeid 0x20\u003clink\u003e ether 00:0c:29:80:33:15 txqueuelen 1000 (Ethernet) RX packets 465 bytes 56335 (55.0 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 348 bytes 66663 (65.1 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ens34: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 ether 00:0c:29:80:33:15 txqueuelen 1000 (Ethernet) RX packets 471 bytes 63205 (61.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 450 bytes 75797 (74.0 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 재시작후 네트워크 인터페이스를 확인하면 위와 같이 생성된 것을 확인할 수 있습니다. $ compute\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-nova-compute # nova 및 관련 모듈을 설치합니다. $ controller\u003e scp /etc/nova/nova.conf compute:/etc/nova/nova.conf # nova의 기본설정파일을 복사합니다. $ compute\u003e vi /etc/nova/nova.conf [default] my_ip = 10.10.10.30 # my_ip는 반드시 IP로 적어주세요 ! [libvirt] virt_type = qemu [vnc] enabled = True server_listen = 0.0.0.0 server_proxyclient_address = $my_ip novncproxy_base_url = http://controller:6080/vnc_auto.html # nova관련 설정을 추가합니다. $ compute\u003e firewall-cmd --add-port=5900-5999/tcp --permanent $ compute\u003e firewall-cmd --reload $ compute\u003e systemctl enable --now libvirtd $ compute\u003e systemctl enable --now openstack-nova-compute Nova 설치 확인 $ controller\u003e su -s /bin/bash nova -c \"nova-manage cell_v2 discover_hosts\" # DB에 compute의 대한 설정을 업데이트 합니다. $ controller\u003e nova-manage cell_v2 discover_hosts --verbose # compute 노드가 검색이 안되었을 시 추가적으로 검색합니다. $ controller ~(keystone)\u003e openstack compute service list +----+----------------+------------+----------+---------+-------+----------------------------+ | ID | Binary | Host | Zone | Status | State | Updated At | +----+----------------+------------+----------+---------+-------+----------------------------+ | 4 | nova-conductor | controller | internal | enabled | up | 2020-08-06T21:40:34.000000 | | 5 | nova-scheduler | controller | internal | enabled | up | 2020-08-06T21:40:37.000000 | | 8 | nova-compute | compute | nova | enabled | up | 2020-08-06T21:40:36.000000 | +----+----------------+------------+----------+---------+-------+----------------------------+ ","#":"","conpute-node-nova-설치#\u003cstrong\u003eConpute node Nova 설치\u003c/strong\u003e":"","openstack-ussuri--nova#\u003cstrong\u003eOpenStack Ussuri : Nova\u003c/strong\u003e":"","openstack-ussuri--nova-1#\u003cstrong\u003eOpenStack Ussuri : Nova\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Nova"},"/system/openstack/openstacktraining/openstack-ussuri-06/":{"data":{"":"OpenStack Ussuri : Neutron ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Neutron | | L2 Agent | | metadata agent | | Nova-API Compute | ----------------------- ----------------------- | L2 agent L3 agent | | metadata agent | | Neutron Server | ----------------------- OpenStack Ussuri : Neutron Neutron는 OpenStack에서 네트워크 전반을 관리하는 서비스입니다. Neutron에 대한 자세한 설명은 Neutron를 참조해주세요. Neutron service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 neutron +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 943fbb4370164c77ae6bf7fa455292f8 | | name | neutron | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user neutron admin $ controller ~(keystone)\u003e openstack service create --name neutron --description \"OpenStack Networking service\" network +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Networking service | | enabled | True | | id | 055e5f6e38004338b0ae4a86e77932ae | | name | neutron | | type | network | +-------------+----------------------------------+ # neutron service 및 user을 생성합니다. $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne network public http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 350c666f597a41e59234b09f534aa72f | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 055e5f6e38004338b0ae4a86e77932ae | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne network internal http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | b9cad959e1634ff797e27f00d50e9578 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 055e5f6e38004338b0ae4a86e77932ae | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne network admin http://controller:9696 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 72fc145deb1d4d508e3691b3bf77708e | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 055e5f6e38004338b0ae4a86e77932ae | | service_name | neutron | | service_type | network | | url | http://controller:9696 | +--------------+----------------------------------+ # neutron endpoint를 등록합니다. neutron 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database neutron_ml2; $ MariaDB\u003e grant all privileges on neutron_ml2.* to neutron@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on neutron_ml2.* to neutron@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Neutron 설치 $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-neutron openstack-neutron-ml2 # neutron 및 관련 모듈을 설치합니다. $ controller\u003e vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone state_path = /var/lib/neutron dhcp_agent_notification = True allow_overlapping_ips = True notify_nova_on_port_status_changes = True notify_nova_on_port_data_changes = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [database] connection = mysql+pymysql://neutron:qwer1234@controller/neutron_ml2 [nova] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = nova password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ controller\u003e vi /etc/neutron/metadata_agent.ini [DEFAULT] nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret [cache] memcache_servers = controller:11211 $ controller\u003e vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 $ controller\u003e vi /etc/nova/nova.conf [default] ... ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret $ controller\u003e setsebool -P neutron_can_network on $ controller\u003e setsebool -P daemons_enable_cluster_mode on $ controller\u003e firewall-cmd --add-port=9696/tcp --permanent $ controller\u003e firewall-cmd --reload # 방화벽 및 SELinux를 설정합니다. $ controller\u003e ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ controller\u003e su -s /bin/bash neutron -c \"neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugin.ini upgrade head\" $ controller\u003e systemctl enable --now neutron-server neutron-metadata-agent $ controller\u003e systemctl restart openstack-nova-api # neutron DB를 임포트 시킨 후, 서비스를 등록 합니다. neutron Network Node 설치 Neutron 설치 $ network\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch libibverbs # neutron 및 관련 모듈을 설치합니다. $ network\u003e vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone state_path = /var/lib/neutron allow_overlapping_ips = True # RabbitMQ connection info transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/lock $ network\u003e vi /etc/neutron/dhcp_agent.ini [DEFAULT] interface_driver = openvswitch dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq enable_isolated_metadata = true $ network\u003e vi /etc/neutron/metadata_agent.ini nova_metadata_host = controller metadata_proxy_shared_secret = metadata_secret [cache] memcache_servers = controller:11211 $ network\u003e vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # 끝에 추가합니다. $ network\u003e vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true [agent] tunnel_types = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.20 bridge_mappings = physnet1:br-eth1 # 끝에 추가합니다. # 여기는 IP 를 반드시 적어야 해요 ! $ network\u003e setsebool -P neutron_can_network on $ network\u003e setsebool -P haproxy_connect_any on $ network\u003e setsebool -P daemons_enable_cluster_mode on $ network\u003e vi ovsofctl.te module ovsofctl 1.0; require { type neutron_t; type neutron_exec_t; type neutron_t; type dnsmasq_t; class file execute_no_trans; class capability { dac_override sys_rawio }; } #============= neutron_t ============== allow neutron_t self:capability { dac_override sys_rawio }; allow neutron_t neutron_exec_t:file execute_no_trans; #============= dnsmasq_t ============== allow dnsmasq_t self:capability dac_override; $ network\u003e checkmodule -m -M -o ovsofctl.mod ovsofctl.te $ network\u003e semodule_package --outfile ovsofctl.pp --module ovsofctl.mod $ network\u003e semodule -i ovsofctl.pp $ network\u003e systemctl disable --now firewalld # Selinux 및 방화벽을 설정합니다. $ network\u003e ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ network\u003e systemctl enable --now openvswitch $ network\u003e ovs-vsctl add-br br-int $ network\u003e ovs-vsctl add-br br-eth1 $ network\u003e ovs-vsctl add-port br-eth1 ens32 $ network\u003e vi /etc/sysconfig/network-scripts/ifcfg-ens32 TYPE=Ethernet BOOTPROTO=static NAME=ens32 DEVICE=ens32 ONBOOT=yes $ network\u003e vi /var/tmp/network_interface.sh #!/bin/bash ip link set up br-eth1 ip addr add 192.168.10.20/24 dev br-eth1 route add default gw 192.168.10.2 dev br-eth1 echo \"nameserver 8.8.8.8\" \u003e /etc/resolv.conf $ network\u003e chmod 755 /var/tmp/network_interface.sh $ network\u003e vi /etc/systemd/system/set_interface.service [Unit] Description=Description for sample script goes here After=network.target [Service] Type=simple ExecStart=/var/tmp/network_interface.sh TimeoutStartSec=0 [Install] WantedBy=default.target $ systemctl enable set_interface $ init 6 # network 인터페이스 주의 !!! ( ex : ens32 ) $ network\u003e for service in dhcp-agent l3-agent metadata-agent openvswitch-agent; do systemctl enable --now neutron-$service done # neutron 서비스를 등록합니다. neutron compute Node 설치 Neutron 설치 $ compute\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-neutron openstack-neutron-ml2 openstack-neutron-openvswitch # neutron 및 관련 모듈을 설치합니다. $ compute\u003e vi /etc/neutron/neutron.conf [DEFAULT] core_plugin = ml2 service_plugins = router auth_strategy = keystone state_path = /var/lib/neutron allow_overlapping_ips = True transport_url = rabbit://openstack:qwer1234@controller [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = neutron password = qwer1234 [oslo_concurrency] lock_path = $state_path/lock $ compute\u003e vi /etc/neutron/plugins/ml2/ml2_conf.ini [ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = vxlan mechanism_drivers = openvswitch extension_drivers = port_security [ml2_type_flat] flat_networks = physnet1 [ml2_type_vxlan] vni_ranges = 1:1000 # 끝에 추가합니다. $ compute\u003e vi /etc/neutron/plugins/ml2/openvswitch_agent.ini [securitygroup] firewall_driver = openvswitch enable_security_group = true enable_ipset = true [agent] tunnel_types = vxlan prevent_arp_spoofing = True [ovs] local_ip = 10.10.10.20 # 끝에 추가합니다. # 여기는 반드시 IP로 적어야 해요 ! $ compute\u003e vi /etc/nova/nova.conf [default] ... ... use_neutron = True linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver vif_plugging_is_fatal = True vif_plugging_timeout = 300 [neutron] auth_url = http://controller:5000 auth_type = password project_domain_name = default user_domain_name = default region_name = RegionOne project_name = service username = neutron password = qwer1234 service_metadata_proxy = True metadata_proxy_shared_secret = metadata_secret $ compute\u003e setsebool -P neutron_can_network on $ compute\u003e setsebool -P daemons_enable_cluster_mode on $ compute\u003e vi ovsofctl.te module ovsofctl 1.0; require { type neutron_t; type neutron_exec_t; type neutron_t; type dnsmasq_t; class file execute_no_trans; class capability { dac_override sys_rawio }; } #============= neutron_t ============== allow neutron_t self:capability { dac_override sys_rawio }; allow neutron_t neutron_exec_t:file execute_no_trans; #============= dnsmasq_t ============== allow dnsmasq_t self:capability dac_override; $ network\u003e checkmodule -m -M -o ovsofctl.mod ovsofctl.te $ network\u003e semodule_package --outfile ovsofctl.pp --module ovsofctl.mod $ network\u003e semodule -i ovsofctl.pp $ systemctl disable --now firewalld # Selinux 및 방화벽을 설정합니다. $ compute\u003e ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini $ compute\u003e systemctl enable --now openvswitch $ compute\u003e ovs-vsctl add-br br-int $ compute\u003e systemctl restart openstack-nova-compute $ compute\u003e systemctl enable --now neutron-openvswitch-agent # neutron 서비스를 등록합니다. 확인 $ controller ~(keystone)\u003e openstack router create router +-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+ | Field | Value | +-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2020-08-07T00:05:40Z | | description | | | distributed | False | | external_gateway_info | null | | flavor_id | None | | ha | False | | id | f40d6130-a01c-486a-b088-3f27c9f57607 | | location | cloud='', project.domain_id=, project.domain_name='d efault', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', re gion_name='', zone= | | name | router | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 1 | | routes | | | status | ACTIVE | | tags | | | updated_at | 2020-08-07T00:05:40Z | +-------------------------+----------------------------------------------------- -------------------------------------------------------------------------------- --------------------+ $ controller ~(keystone)\u003e openstack network create int --provider-network-type vxlan +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2020-08-07T00:05:58Z | | description | | | dns_domain | None | | id | 0edec63e-cc62-4e93-8962-d0ad2df27bc8 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | False | | is_vlan_transparent | None | | location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= | | mtu | 1450 | | name | int | | port_security_enabled | True | | project_id | c76211c24a1f460ca67274d655d46725 | | provider:network_type | vxlan | | provider:physical_network | None | | provider:segmentation_id | 1 | | qos_policy_id | None | | revision_number | 1 | | router:external | Internal | | segments | None | | shared | False | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2020-08-07T00:05:58Z | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack subnet create int-sub --network int \\ --subnet-range 1.1.1.0/24 --gateway 1.1.1.1 \\ --dns-nameserver 8.8.8.8 +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | allocation_pools | 1.1.1.2-1.1.1.254 | | cidr | 1.1.1.0/24 | | created_at | 2020-08-07T00:06:25Z | | description | | | dns_nameservers | 8.8.8.8 | | dns_publish_fixed_ip | None | | enable_dhcp | True | | gateway_ip | 1.1.1.1 | | host_routes | | | id | 800bc5af-45e9-4719-8969-4c154bc111d6 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= | | name | int-sub | | network_id | 0edec63e-cc62-4e93-8962-d0ad2df27bc8 | | prefix_length | None | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2020-08-07T00:06:25Z | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack router add subnet router int-sub $ controller ~(keystone)\u003e openstack network create \\ --provider-physical-network physnet1 \\ --provider-network-type flat --external ext +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | admin_state_up | UP | | availability_zone_hints | | | availability_zones | | | created_at | 2020-08-07T00:06:47Z | | description | | | dns_domain | None | | id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | | ipv4_address_scope | None | | ipv6_address_scope | None | | is_default | False | | is_vlan_transparent | None | | location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= | | mtu | 1500 | | name | ext | | port_security_enabled | True | | project_id | c76211c24a1f460ca67274d655d46725 | | provider:network_type | flat | | provider:physical_network | physnet1 | | provider:segmentation_id | None | | qos_policy_id | None | | revision_number | 1 | | router:external | External | | segments | None | | shared | False | | status | ACTIVE | | subnets | | | tags | | | updated_at | 2020-08-07T00:06:47Z | +---------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack subnet create ext-sub \\ --network ext --subnet-range 192.168.10.0/24 \\ --allocation-pool start=192.168.10.150,end=192.168.10.200 \\ --gateway 192.168.10.2 --dns-nameserver 8.8.8.8 +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | allocation_pools | 192.168.10.150-192.168.10.200 | | cidr | 192.168.10.0/24 | | created_at | 2020-08-07T00:07:21Z | | description | | | dns_nameservers | 8.8.8.8 | | dns_publish_fixed_ip | None | | enable_dhcp | True | | gateway_ip | 192.168.10.2 | | host_routes | | | id | 31a92331-f102-4c4e-8c02-f97baa9eab28 | | ip_version | 4 | | ipv6_address_mode | None | | ipv6_ra_mode | None | | location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= | | name | ext-sub | | network_id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | | prefix_length | None | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 0 | | segment_id | None | | service_types | | | subnetpool_id | None | | tags | | | updated_at | 2020-08-07T00:07:21Z | +----------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack router set router --external-gateway ext $ controller ~(keystone)\u003e openstack network rbac list +--------------------------------------+-------------+--------------------------------------+ | ID | Object Type | Object ID | +--------------------------------------+-------------+--------------------------------------+ | 4e8ebe0b-60f0-485c-8696-74378068c844 | network | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | +--------------------------------------+-------------+--------------------------------------+ $ controller ~(keystone)\u003e wget http://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.img -P /var/kvm/images $ controller ~(keystone)\u003e openstack image create \"Ubuntu1804\" --file /var/kvm/images/ubuntu-18.04-server-cloudimg-amd64.img --disk-format qcow2 --container-format bare --public # Ubuntu18.04 이미지를 다운로드 후, 등록합니다. $ controller ~(keystone)\u003e openstack security group create all-port +-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:10:31Z | | description | all-port | | id | 97224218-b304-4076-9645-d68092a9366a | | location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= | | name | all-port | | project_id | c76211c24a1f460ca67274d655d46725 | | revision_number | 1 | | rules | created_at='2020-08-07T00:10:32Z', direction='egress', ethertype='IPv6', id='333de7e9-5c1b-4b2f-bb0e-2da1b878abb6', updated_at='2020-08-07T00:10:32Z' | | | created_at='2020-08-07T00:10:32Z', direction='egress', ethertype='IPv4', id='644e18e1-4f4e-42ad-bef8-937e47254a27', updated_at='2020-08-07T00:10:32Z' | | stateful | True | | tags | [] | | updated_at | 2020-08-07T00:10:32Z | +-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack security group rule create --protocol icmp --ingress all-port +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:13:31Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | 27688481-047b-4fc0-948c-de109e46d7f5 | | location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= | | name | None | | port_range_max | None | | port_range_min | None | | project_id | c76211c24a1f460ca67274d655d46725 | | protocol | icmp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | 97224218-b304-4076-9645-d68092a9366a | | tags | [] | | updated_at | 2020-08-07T00:13:31Z | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack security group rule create --protocol tcp --dst-port 22:22 all-port +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:13:36Z | | description | | | direction | ingress | | ether_type | IPv4 | | id | da2afd20-818a-4bfe-9017-c837b2bf30ec | | location | cloud='', project.domain_id=, project.domain_name='default', project.id='c76211c24a1f460ca67274d655d46725', project.name='admin', region_name='', zone= | | name | None | | port_range_max | 22 | | port_range_min | 22 | | project_id | c76211c24a1f460ca67274d655d46725 | | protocol | tcp | | remote_group_id | None | | remote_ip_prefix | 0.0.0.0/0 | | revision_number | 0 | | security_group_id | 97224218-b304-4076-9645-d68092a9366a | | tags | [] | | updated_at | 2020-08-07T00:13:36Z | +-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e ssh-keygen -q -N \"\" $ controller ~(keystone)\u003e openstack keypair create --public-key ~/.ssh/id_rsa.pub MyKey +-------------+-------------------------------------------------+ | Field | Value | +-------------+-------------------------------------------------+ | fingerprint | a3:8f:44:f6:e1:4e:da:a0:90:f1:5d:dc:6a:8b:ad:76 | | name | MyKey | | user_id | 57ce8f772e374a7c9282f2674fda1ba7 | +-------------+-------------------------------------------------+ $ controller ~(keystone)\u003e openstack flavor create --ram 1024 --disk 10 --vcpus 1 m1.small +----------------------------+--------------------------------------+ | Field | Value | +----------------------------+--------------------------------------+ | OS-FLV-DISABLED:disabled | False | | OS-FLV-EXT-DATA:ephemeral | 0 | | disk | 10 | | id | dabfebd4-cd05-4cec-9567-78b8c9e3d6b6 | | name | m1.small | | os-flavor-access:is_public | True | | properties | | | ram | 1024 | | rxtx_factor | 1.0 | | swap | | | vcpus | 1 | +----------------------------+--------------------------------------+ $ controller ~(keystone)\u003e openstack server create --image Ubuntu1804 --flavor m1.small --key Mykey --network int --security-group all-port Ubuntu $ controller ~(keystone)\u003e openstack floating ip create ext +---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Field | Value | +---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | created_at | 2020-08-07T00:16:15Z | | description | | | dns_domain | None | | dns_name | None | | fixed_ip_address | None | | floating_ip_address | 192.168.10.191 | | floating_network_id | 68e5adb0-a8c4-473b-88a9-fdaaf6f12ec2 | | id | 409a4724-1e13-4150-a2e1-6b3a205c4ff6 | | location | Munch({'cloud': '', 'region_name': '', 'zone': None, 'project': Munch({'id': 'c76211c24a1f460ca67274d655d46725', 'name': 'admin', 'domain_id': None, 'domain_name': 'default'})}) | | name | 192.168.10.191 | | port_details | None | | port_id | None | | project_id | c76211c24a1f460ca67274d655d46725 | | qos_policy_id | None | | revision_number | 0 | | router_id | None | | status | DOWN | | subnet_id | None | | tags | [] | | updated_at | 2020-08-07T00:16:15Z | +---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack server add floating ip Ubuntu 192.168.10.191 $ controller ~(keystone)\u003e ","#":"","neutron-compute-node-설치#\u003cstrong\u003eneutron compute Node 설치\u003c/strong\u003e":"","neutron-network-node-설치#\u003cstrong\u003eneutron Network Node 설치\u003c/strong\u003e":"","openstack-ussuri--neutron#\u003cstrong\u003eOpenStack Ussuri : Neutron\u003c/strong\u003e":"","openstack-ussuri--neutron-1#\u003cstrong\u003eOpenStack Ussuri : Neutron\u003c/strong\u003e":"","확인#\u003cstrong\u003e확인\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Neutron"},"/system/openstack/openstacktraining/openstack-ussuri-07/":{"data":{"":"OpenStack Ussuri : Cinder ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | ----------------------- | L2 agent L3 agent | | NFS | | metadata agent | ----------------------- | Neutron Server | ----------------------- OpenStack Ussuri : Cinder Cinder는 OpenStack에서 전체적인 볼륨, 디스크를 관리하는 서비스입니다. Cinder 서비스는 다른 Storage Node들과 함께 사용하도록 NFS 서버 또한 구축하여 백업 서비스를 활성하할 수 있게 구성핟록 하겠습니다. Cinder에 대한 자세한 설명은 Cinder를 참조해주세요. Cinder service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 cinder +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | 7c10c02365be496fb47f12bfd40fe4a7 | | domain_id | default | | enabled | True | | id | 1f9dbcbb529a45c28b5bb8b035ea277a | | name | cinder | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user cinder admin $ controller ~(keystone)\u003e openstack service create --name cinderv3 --description \"OpenStack Block Storage\" volumev3 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Block Storage | | enabled | True | | id | 225ceadb699d4e79adf30769cd872fef | | name | cinderv3 | | type | volumev3 | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne volumev3 public http://controller:8776/v3/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 6bf917232caa43eab3b83959fb19cb45 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 225ceadb699d4e79adf30769cd872fef | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne volumev3 internal http://controller:8776/v3/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | c5987fc3d9eb4fb79a2e8cf73a274936 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 225ceadb699d4e79adf30769cd872fef | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne volumev3 admin http://controller:8776/v3/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | eff2398584944c0fa7575d1991d725fe | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 225ceadb699d4e79adf30769cd872fef | | service_name | cinderv3 | | service_type | volumev3 | | url | http://controller:8776/v3/%(tenant_id)s | +--------------+-----------------------------------------+ # Cinder의 Endpoint를 생성합니다. Cinder 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database cinder; $ MariaDB\u003e grant all privileges on cinder.* to cinder@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on cinder.* to cinder@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Cinder 설치 $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-cinder # cinder 및 관련 모듈을 설치합니다. $ controller\u003e vi /etc/cinder/cinder.conf [DEFAULT] my_ip = controller log_dir = /var/log/cinder state_path = /var/lib/cinder auth_strategy = keystone transport_url = rabbit://openstack:qwer1234@controller enable_v3_api = True [database] connection = mysql+pymysql://cinder:qwer1234@controller/cinder [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = cinder password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ controller\u003e su -s /bin/bash cinder -c \"cinder-manage db sync\" $ controller\u003e systemctl enable --now openstack-cinder-api openstack-cinder-scheduler # cinder DB를 임포트 시키고, 서비스를 등록합니다. $ controller\u003e echo \"export OS_VOLUME_API_VERSION=3\" \u003e\u003e ~/admin_key $ controller\u003e source ~/admin_key # key파일을 수정합니다. $ controller\u003e firewall-cmd --add-port=8776/tcp --permanent $ controller\u003e firewall-cmd --reload # 방화벽을 설정합니다. Cinder compute node 설치 $ compute\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-cinder targetcli # cinder 및 관련 모듈을 설치합니다. $ compute\u003e fdisk ... # LVM의 타입으로 파티션을 추가합니다. # cinder 이름으로 vg를 생성합니다. $ controller\u003e scp /etc/cinder/cinder.conf compute:/etc/cinder/cinder.conf $ compute\u003e vi /etc/cinder/cinder.conf [default] my_ip = compute ... ... glance_api_servers = http://controller:9292 enabled_backends = lvm [lvm] target_helper = lioadm target_protocol = iscsi target_ip_address = compute volume_group = cinder volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volumes_dir = $state_path/volumes $ compute\u003e vi /etc/nova/nova.conf [cinder] os_region_name = RegionOne $ compute\u003e systemctl restart openstack-nova-compute $ compute\u003e systemctl enable --now openstack-cinder-volume $ compute\u003e vi iscsiadm.te module iscsiadm 1.0; require { type iscsid_t; class capability dac_override; } #============= iscsid_t ============== allow iscsid_t self:capability dac_override; $ compute\u003e checkmodule -m -M -o iscsiadm.mod iscsiadm.te $ compute\u003e semodule_package --outfile iscsiadm.pp --module iscsiadm.mod $ compute\u003e semodule -i iscsiadm.pp $ compute\u003e firewall-cmd --add-service=iscsi-target --permanent $ compute\u003e firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. 확인 $ controller ~/(keystone)\u003e openstack volume service list +------------------+-------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+-------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-08-07T01:29:22.000000 | | cinder-volume | compute@lvm | nova | enabled | up | 2020-08-07T01:29:22.000000 | +------------------+-------------+------+---------+-------+----------------------------+ $ controller ~/(keystone)\u003e openstack volume create --size 1 test +---------------------+--------------------------------------+ | Field | Value | +---------------------+--------------------------------------+ | attachments | [] | | availability_zone | nova | | bootable | false | | consistencygroup_id | None | | created_at | 2020-08-07T01:46:06.000000 | | description | None | | encrypted | False | | id | aa07bf85-424d-478c-ae52-648ddc588465 | | migration_status | None | | multiattach | False | | name | test | | properties | | | replication_status | None | | size | 1 | | snapshot_id | None | | source_volid | None | | status | creating | | type | __DEFAULT__ | | updated_at | None | | user_id | 57ce8f772e374a7c9282f2674fda1ba7 | +---------------------+--------------------------------------+ $ controller ~/(keystone)\u003e openstack volume list +--------------------------------------+------+-----------+------+-------------+ | ID | Name | Status | Size | Attached to | +--------------------------------------+------+-----------+------+-------------+ | aa07bf85-424d-478c-ae52-648ddc588465 | test | available | 1 | | +--------------------------------------+------+-----------+------+-------------+ 오류가 있어 수정 중입니다 !Cinder 백업 서비스 구성 NFS 구성참조 $ compute\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install nfs-utils # nfs-utils을 설치합니다. $ compute\u003e vi /etc/exports /nfs 10.10.10.0/24(rw,no_root_squash) # ro: 마운트 된 볼륨의 데이터를 읽기만 가능 # rw: 마운트 된 볼륨에 쓰기 또한 가능 # no_root_squash: 루트 자격을 가진 사용자만 쓰기 가능 # noaccess: 디렉터리 접근 불가 $ compute\u003e systemctl enable --now rpcbind nfs-server # NFC-server 서비스를 등록 및 시작합니다. $ vi /etc/cinder/cinder.conf [default] ... ... enabled_backends = lvm,nfs [nfs] volume_driver = cinder.volume.drivers.nfs.NfsDriver volume_backend_name = NFS nfs_shares_config = /etc/cinder/nfs_shares nfs_mount_point_base = $state_path/mnt_nfs # cinder.conf 파일의 nfs를 추가합니다. $ compute\u003e vi /etc/cinder/nfs_shares compute:/nfs # 공유될 디렉토리를 지정합니다. $ compute\u003e chmod 640 /etc/cinder/nfs_shares $ compute\u003e chgrp cinder /etc/cinder/nfs_shares $ compute\u003e systemctl restart openstack-cinder-volume $ compute\u003e chown -R cinder. /var/lib/cinder/mnt_nfs # cinder nfs 파일의 권한을 변경하고 cinder 서비스를 재시작합니다. $ compute\u003e firewall-cmd --add-service=nfs --permanent $ compute\u003e firewall-cmd --reload # 방화벽을 설정합니다. $ compute\u003e vi iscsiadm.te module iscsiadm 1.0; require { type iscsid_t; class capability dac_override; } #============= iscsid_t ============== allow iscsid_t self:capability dac_override; $ compute\u003e checkmodule -m -M -o iscsiadm.mod iscsiadm.te $ compute\u003e semodule_package --outfile iscsiadm.pp --module iscsiadm.mod $ compute\u003e semodule -i iscsiadm.pp $ compute\u003e systemctl restart openstack-nova-compute # SELinux를 설정하고 compute 서비스를 재시작합니다. Cinder 백업 서비스 구성 $ compute\u003e vi /etc/cinder/cinder.conf [default] ... ... backup_driver = cinder.backup.drivers.nfs.NFSBackupDriver backup_mount_point_base = $state_path/backup_nfs backup_share = compute:/var/lib/cinder-backup # ciner 백업 서비스를 활성화하기 이해 cinder.conf 파일의 설정을 추가합니다. $ compute\u003e systemctl enable --now openstack-cinder-backup $ compute\u003e chown -R cinder. /var/lib/cinder/backup_nfs $ cinder backup 서비스를 활성화합니다. 확인 $ controller ~(keystone)\u003e openstack volume service list +------------------+-------------+------+---------+-------+----------------------------+ | Binary | Host | Zone | Status | State | Updated At | +------------------+-------------+------+---------+-------+----------------------------+ | cinder-scheduler | controller | nova | enabled | up | 2020-08-12T04:31:53.000000 | | cinder-volume | compute@lvm | nova | enabled | up | 2020-08-12T04:31:46.000000 | | cinder-volume | compute@nfs | nova | enabled | up | 2020-08-12T04:31:46.000000 | +------------------+-------------+------+---------+-------+----------------------------+ $ controller ~(keystone)\u003e $ controller ~(keystone)\u003e $ controller ~(keystone)\u003e $ controller ~(keystone)\u003e ","#":"","cinder-compute-node-설치#\u003cstrong\u003eCinder compute node 설치\u003c/strong\u003e":"","cinder-백업-서비스-구성#\u003cstrong\u003eCinder 백업 서비스 구성\u003c/strong\u003e":"","openstack-ussuri--cinder#\u003cstrong\u003eOpenStack Ussuri : Cinder\u003c/strong\u003e":"","openstack-ussuri--cinder-1#\u003cstrong\u003eOpenStack Ussuri : Cinder\u003c/strong\u003e":"","오류가-있어-수정-중입니다-#\u003cstrong\u003e오류가 있어 수정 중입니다 !\u003c/strong\u003e":"","확인#\u003cstrong\u003e확인\u003c/strong\u003e":"","확인-1#\u003cstrong\u003e확인\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Cinder"},"/system/openstack/openstacktraining/openstack-ussuri-08/":{"data":{"":"OpenStack : HorizonOpenStack : Horizon Horizon은 openstack에서 GUI 환경을 제공해주는 서비스입니다. Horizon에 대한 자세한 설명은 Horizon을 참조해주세요. $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-dashboard $ controller\u003e vi /etc/openstack-dashboard/local_settings ALLOWED_HOSTS = ['*',''] # 모든 host의 접속이 가능하게 설정합니다. CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': 'controller:11211', }, } SESSION_ENGINE = \"django.contrib.sessions.backends.cache\" OPENSTACK_HOST = \"controller\" OPENSTACK_KEYSTONE_URL = \"http://controller:5000/v3\" # openstack host와 SESSION 서버의 host를 지정합니다. TIME_ZONE = \"Asia/Seoul\" # 시간을 지정합니다. WEBROOT = '/dashboard/' LOGIN_URL = '/dashboard/auth/login/' LOGOUT_URL = '/dashboard/auth/logout/' LOGIN_REDIRECT_URL = '/dashboard/' OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = 'Default' OPENSTACK_API_VERSIONS = { \"identity\": 3, \"volume\": 3, \"compute\": 2, } # 끝에 추가합니다. $ controller\u003e vi /etc/httpd/conf.d/openstack-dashboard.conf .... .... WSGIApplicationGroup %{GLOBAL} # 상단에 추가합니다. $ controller\u003e systemctl restart httpd # httpd를 재 시작합니다. $ controller\u003e setsebool -P httpd_can_network_connect on $ controller\u003e firewall-cmd --add-service={http,https} --permanent $ controller\u003e firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. 확인 접속확인 http://[ controller의 IP ]/dashboard/ ","openstack--horizon#\u003cstrong\u003eOpenStack : Horizon\u003c/strong\u003e":"","openstack--horizon-1#\u003cstrong\u003eOpenStack : Horizon\u003c/strong\u003e":"","확인#\u003cstrong\u003e확인\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Horizon"},"/system/openstack/openstacktraining/openstack-ussuri-09/":{"data":{"":"OpenStack Ussuri : Swift ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | ----------------------- | metadata agent | ----------------------- | Neutron Server | ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- OpenStack Ussuri : Swift Swift는 우리가 흔히 사용하는 네이버 클라우드, 구글 드라이브와 같은 오브젝트 스토리지 서비스 입니다. Swift 설치는 network, Storage 순으로 이루어집니다. Swift*에 대한 설명은 Swift을 참조해주세요. $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 swift +--------------------------------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | dd2f0225406249b195e4feff91eca393 | | name | swift | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user swift admin $ controller ~(keystone)\u003e openstack service create --name swift --description \"OpenStack Object Storage\" object-store +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Object Storage | | enabled | True | | id | d9d7bc4b99774d3ba701e2eae93edfe2 | | name | swift | | type | object-store | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne object-store public http://network:8080/v1/AUTH_%\\(tenant_id\\)s +--------------+------------------------------------+ | Field | Value | +--------------+------------------------------------+ | enabled | True | | id | a70e1ac16a9144529ea49132cd7dd39e | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | d9d7bc4b99774d3ba701e2eae93edfe2 | | service_name | swift | | service_type | object-store | | url | http://network:8080/v1/AUTH_%(tenant_id)s | +--------------+------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne object-store internal http://network:8080/v1/AUTH_%\\(tenant_id\\)s +--------------+------------------------------------+ | Field | Value | +--------------+------------------------------------+ | enabled | True | | id | 6b5ea7b028f94035aef5601cf35d3a29 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | d9d7bc4b99774d3ba701e2eae93edfe2 | | service_name | swift | | service_type | object-store | | url | http://network:8080/v1/AUTH_%(tenant_id)s | +--------------+------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne object-store admin http://network:8080/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 08c18a5313f642d59de980f51666f830 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | d9d7bc4b99774d3ba701e2eae93edfe2 | | service_name | swift | | service_type | object-store | | url | http://network:8080/v1 | +--------------+----------------------------------+ Network Node Swift-Proxy 설치 $ network\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-swift-proxy python3-memcached openssh-clients # swift-proxy 및 관련 모듈을 설치합니다. $ network\u003e vi /etc/swift/proxy-server.conf [filter:cache] use = egg:swift#memcache memcache_servers = controller:11211 [filter:authtoken] paste.filter_factory = keystonemiddleware.auth_token:filter_factory # admin_tenant_name = %SERVICE_TENANT_NAME% # admin_user = %SERVICE_USER% # admin_password = %SERVICE_PASSWORD% # auth_host = 127.0.0.1 # auth_port = 35357 # auth_protocol = http # signing_dir = /tmp/keystone-signing-swift # 주석처리 후, 하단의 아래의 항모들을 추가합니다.합니다. www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = swift password = qwer1234 delay_auth_decision = true $ network\u003e vi /etc/swift/swift.conf [swift-hash] swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path # 파일 안에 내용들을 삭제 후, 생성합니다. $ network\u003e swift-ring-builder /etc/swift/account.builder create 12 3 1 $ network\u003e swift-ring-builder /etc/swift/container.builder create 12 3 1 $ network\u003e swift-ring-builder /etc/swift/object.builder create 12 3 1 $ network\u003e swift-ring-builder /etc/swift/account.builder add r0z0-10.10.10.50:6202/device 100 $ network\u003e swift-ring-builder /etc/swift/container.builder add r0z0-10.10.10.50:6201/device 100 $ network\u003e swift-ring-builder /etc/swift/object.builder add r0z0-10.10.10.50:6200/device 100 $ network\u003e swift-ring-builder /etc/swift/account.builder add r1z1-10.10.10.51:6202/device 100 $ network\u003e swift-ring-builder /etc/swift/container.builder add r1z1-10.10.10.51:6201/device 100 $ network\u003e swift-ring-builder /etc/swift/object.builder add r1z1-10.10.10.51:6200/device 100 $ network\u003e swift-ring-builder /etc/swift/account.builder add r2z2-10.10.10.52:6202/device 100 $ network\u003e swift-ring-builder /etc/swift/container.builder add r2z2-10.10.10.52:6201/device 100 $ network\u003e swift-ring-builder /etc/swift/object.builder add r2z2-10.10.10.52:6200/device 100 $ network\u003e swift-ring-builder /etc/swift/account.builder rebalance $ network\u003e swift-ring-builder /etc/swift/container.builder rebalance $ network\u003e swift-ring-builder /etc/swift/object.builder rebalance $ network\u003e chown swift. /etc/swift/*.gz $ network\u003e systemctl enable --now openstack-swift-proxy $ network\u003e firewall-cmd --add-port=8080/tcp --permanent $ network\u003e firewall-cmd --reload Swift Stoage Node 설치 $ storage all\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-swift-account openstack-swift-container openstack-swift-object openstack-selinux xfsprogs rsync rsync-daemon openssh-clients # swift 밒 관련 모듈을 설치합니다. $ storage all\u003e mkfs.xfs -i size=1024 -s size=4096 /dev/sdb1 $ storage all\u003e mkdir -p /srv/node/device $ storage all\u003e mount -o noatime,nodiratime /dev/sdb1 /srv/node/device $ storage all\u003e chown -R swift. /srv/node # 하드 디스크를 임포트 후, XFS로 포맷을 진행합니다. $ storage all\u003e vi /etc/fstab /dev/sdb1 /srv/node/device xfs noatime,nodiratime 0 0 # 설정을 fstab의 등록합니다. $ network\u003e scp /etc/swift/*.gz storage1:/etc/swift/ $ network\u003e scp /etc/swift/*.gz storage2:/etc/swift/ $ network\u003e scp /etc/swift/*.gz storage3:/etc/swift/ # 설정을 복사합니다. $ storage all\u003e chown swift. /etc/swift/*.gz $ storage all\u003e vi /etc/swift/swift.conf [swift-hash] swift_hash_path_suffix = swift_shared_path swift_hash_path_prefix = swift_shared_path $ storage all\u003e vi /etc/swift/account-server.conf bind_ip = 0.0.0.0 bind_port = 6202 $ storage all\u003e vi /etc/swift/container-server.conf bind_ip = 0.0.0.0 bind_port = 6201 $ storage all\u003e vi /etc/swift/object-server.conf bind_ip = 0.0.0.0 bind_port = 6200 $ storage all\u003e vi /etc/rsyncd.conf pid file = /var/run/rsyncd.pid log file = /var/log/rsyncd.log uid = swift gid = swift pid file = /var/run/rsyncd.pid log file = /var/log/rsyncd.log uid = swift gid = swift address = storage1 or storage2 or storage3 [account] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/account.lock [container] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/container.lock [object] path = /srv/node read only = false write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 25 lock file = /var/lock/object.lock [swift_server] path = /etc/swift read only = true write only = no list = yes incoming chmod = 0644 outgoing chmod = 0644 max connections = 5 lock file = /var/lock/swift_server.lock $ storage all\u003e semanage fcontext -a -t swift_data_t /srv/node/device $ storage all\u003e restorecon /srv/node/device $ storage all\u003e firewall-cmd --add-port={873/tcp,6200/tcp,6201/tcp,6202/tcp} --permanent $ storage all\u003e firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. $ storage all\u003e systemctl enable --now rsyncd \\ openstack-swift-account-auditor \\ openstack-swift-account-replicator \\ openstack-swift-account \\ openstack-swift-container-auditor \\ openstack-swift-container-replicator \\ openstack-swift-container-updater \\ openstack-swift-container \\ openstack-swift-object-auditor \\ openstack-swift-object-replicator \\ openstack-swift-object-updater \\ openstack-swift-object # swift 서비스를 등록 및 시작합니다. 확인 $ controller ~(keystone)\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install python3-openstackclient python3-keystoneclient python3-swiftclient # swift 사용을 위해 관련 모듈을 설치합니다. $ controller ~(keystone)\u003e openstack project create --domain default --description \"Swift Service Project\" swiftservice +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Swift Service Project | | domain_id | default | | enabled | True | | id | ab658f35464e49b7a3df626e09feab91 | | is_domain | False | | name | swiftservice | | options | {} | | parent_id | default | | tags | [] | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role create SwiftOperator +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | None | | domain_id | None | | id | 3818d26e54244c1ba5d0481e9ad44e6e | | name | SwiftOperator | | options | {} | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack user create --domain default --project swiftservice --password qwer1234 swiftuser01 +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | ab658f35464e49b7a3df626e09feab91 | | domain_id | default | | enabled | True | | id | 2ac2c69fd55a4bef95b2a8b728f131a7 | | name | swiftuser01 | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project swiftservice --user swiftuser01 SwiftOperator $ controller ~(keystone)\u003e vi ~/swift export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=swiftservice export OS_USERNAME=swiftuser01 export OS_PASSWORD=qwer1234 export OS_AUTH_URL=http://controller:5000/v3 export OS_IDENTITY_API_VERSION=3 export PS1='[\\u@\\h \\W(swift)]\\$ ' $ controller ~(keystone)\u003e chmod 600 ~/swift $ controller ~(keystone)\u003e source ~/swift $ controller ~(keystone)\u003e echo \"source ~/swift \" \u003e\u003e ~/.bash_profile $ controller ~(swift)\u003e swift stat Account: AUTH_ab658f35464e49b7a3df626e09feab91 Containers: 0 Objects: 0 Bytes: 0 Content-Type: text/plain; charset=utf-8 X-Timestamp: 1597360203.35834 X-Put-Timestamp: 1597360203.35834 Vary: Accept X-Trans-Id: tx09982b0a02ac4b7eac244-005f35c849 X-Openstack-Request-Id: tx09982b0a02ac4b7eac244-005f35c849 $ controller ~(swift)\u003e openstack container create test +---------------------------------------+-----------+------------------------------------+ | account | container | x-trans-id | +---------------------------------------+-----------+------------------------------------+ | AUTH_ab658f35464e49b7a3df626e09feab91 | test | txce00712612794927965f7-005f35c864 | +---------------------------------------+-----------+------------------------------------+ $ controller ~(swift)\u003e openstack container list +------+ | Name | +------+ | test | +------+ $ controller ~(swift)\u003e openstack object create testfile.txt test $ controller ~(swift)\u003e openstack object list test $ controller ~(swift)\u003e rm testfile.txt $ controller ~(swift)\u003e openstack object save test testfile.txt $ controller ~(swift)\u003e ll testfile.txt $ controller ~(swift)\u003e openstack object delete test testfile.txt $ controller ~(swift)\u003e openstack object list test ","#":"","openstack-ussuri--swift#\u003cstrong\u003eOpenStack Ussuri : Swift\u003c/strong\u003e":"","openstack-ussuri--swift-1#\u003cstrong\u003eOpenStack Ussuri : Swift\u003c/strong\u003e":"","swift-stoage-node-설치#\u003cstrong\u003eSwift Stoage Node 설치\u003c/strong\u003e":"","확인#\u003cstrong\u003e확인\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Swift"},"/system/openstack/openstacktraining/openstack-ussuri-10/":{"data":{"":"OpenStack Ussuri : Heat ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | ----------------------- ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- OpenStack Ussuri : Heat 클라우딩 컴퓨팅이 꽃인 Orchestaration 기능을 수행하는 Heat 서비스를 설치해보도록 하겠습니다. Heat 설치는 controller, network 노드 순으로 이루어집니다. 단 Heat는 controller에서는 API의 Endpoint만을 제공하며, 대부분의 설정은 network node에서 이루어집니다. Heat*에 대한 설명은 Heat을 참조해주세요. Heat service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 heat +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 148bafa480d84f87ba939968edb2585f | | name | heat | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user heat admin $ controller ~(keystone)\u003e openstack role create heat_stack_owner +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | None | | domain_id | None | | id | d46789e4326e4055aa8f6fead7c777bb | | name | heat_stack_owner | | options | {} | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role create heat_stack_user +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | None | | domain_id | None | | id | ff45744ddbe247919034cea7c3f309e7 | | name | heat_stack_user | | options | {} | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project admin --user admin heat_stack_owner $ controller ~(keystone)\u003e openstack service create --name heat --description \"Openstack Orchestration\" orchestration +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Openstack Orchestration | | enabled | True | | id | 6cd5b7c7a3234b39998073587c2d9f9a | | name | heat | | type | orchestration | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack service create --name heat-cfn --description \"Openstack Orchestration\" cloudformation +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Openstack Orchestration | | enabled | True | | id | 2fb2087bf8da472d8c51e9fee39c93ad | | name | heat-cfn | | type | cloudformation | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne orchestration public http://network:8004/v1/AUTH_%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 50481bc9998b454a9f70682132ecb026 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 6cd5b7c7a3234b39998073587c2d9f9a | | service_name | heat | | service_type | orchestration | | url | http://network:8004/v1/AUTH_%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne orchestration internal http://network:8004/v1/AUTH_%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 1015f4c570a747349109b76b7295876c | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 6cd5b7c7a3234b39998073587c2d9f9a | | service_name | heat | | service_type | orchestration | | url | http://network:8004/v1/AUTH_%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne orchestration admin http://network:8004/v1/AUTH_%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | ed21251a3f274ba6bb35061cef6cac1d | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 6cd5b7c7a3234b39998073587c2d9f9a | | service_name | heat | | service_type | orchestration | | url | http://network:8004/v1/AUTH_%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne cloudformation public http://network:8000/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | fb2b67b2a13d43e1a55f775857908a5f | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 2fb2087bf8da472d8c51e9fee39c93ad | | service_name | heat-cfn | | service_type | cloudformation | | url | http://network:8000/v1 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne cloudformation internal http://network:8000/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | a8f4517ecf4d4370beecee9e17183c6b | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 2fb2087bf8da472d8c51e9fee39c93ad | | service_name | heat-cfn | | service_type | cloudformation | | url | http://network:8000/v1 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne cloudformation admin http://network:8000/v1 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 66db714e538545879b7121f7150e72fc | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 2fb2087bf8da472d8c51e9fee39c93ad | | service_name | heat-cfn | | service_type | cloudformation | | url | http://network:8000/v1 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack domain create --description \"Stack projects and users\" heat +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Stack projects and users | | enabled | True | | id | 36fa9838b2fa43f6a6bbc95f0cdfd0a7 | | name | heat | | options | {} | | tags | [] | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack user create --domain heat --password qwer1234 heat_domain_admin +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | domain_id | 36fa9838b2fa43f6a6bbc95f0cdfd0a7 | | enabled | True | | id | c77bd90604254f8097aed49ea17f6fb3 | | name | heat_domain_admin | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --domain heat --user heat_domain_admin admin Heat 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database heat; $ MariaDB\u003e grant all privileges on heat.* to heat@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on heat.* to heat@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Network Node Heat 설치 $ Network\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-heat-api openstack-heat-api-cfn openstack-heat-engine python3-heatclient # Heat 및 관련 모듈을 설치합니다. $ Network\u003e vi /etc/heat/heat.conf [DEFAULT] deferred_auth_method = trusts trusts_delegated_roles = heat_stack_owner heat_metadata_server_url = http://network:8000 heat_waitcondition_server_url = http://network:8000/v1/waitcondition heat_watch_server_url = http://network:8003 heat_stack_user_role = heat_stack_user stack_user_domain_name = heat stack_domain_admin = heat_domain_admin stack_domain_admin_password = qwer1234 transport_url = rabbit://openstack:qwer1234@controller [database] connection = mysql+pymysql://heat:qwer1234@controller/heat [clients_keystone] auth_uri = http://controller:5000 [ec2authtoken] auth_uri = http://controller:5000 [heat_api] bind_host = 0.0.0.0 bind_port = 8004 [heat_api_cfn] bind_host = 0.0.0.0 bind_port = 8000 [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = heat password = qwer1234 [trustee] auth_plugin = password auth_url = http://controller:5000 username = heat password = qwer1234 user_domain_name = default $ network\u003e chgrp heat /etc/heat/heat.conf $ network\u003e chmod 640 /etc/heat/heat.conf $ network\u003e su -s /bin/bash heat -c \"heat-manage db_sync\" $ network\u003e systemctl enable --now openstack-heat-api openstack-heat-api-cfn openstack-heat-engine # DB를 import 시키고, haet 서비스를 등록 및 시작합니다. $ network\u003e firewall-cmd --add-port={8000/tcp,8004/tcp} --permanent $ network\u003e firewall-cmd --reload # 방화벽을 설정합니다. 확인 $ controller ~(keystone)\u003e vi sample-stack.yml heat_template_version: 2018-08-31 description: Heat Sample Template parameters: ImageID: type: string description: Image used to boot a server NetID: type: string description: Network ID for the server resources: server1: type: OS::Nova::Server properties: name: \"Heat_Deployed_Server\" image: { get_param: ImageID } flavor: \"m1.tiny\" networks: - network: { get_param: NetID } outputs: server1_private_ip: description: IP address of the server in the private network value: { get_attr: [ server1, first_address ] } $ controller ~(keystone)\u003e openstack stack create -t sample-stack.yml --parameter \"ImageID=cirros;NetID=Int_net\" Sample-Stack # controller ~(keystone)\u003e openstack stack list +--------------------------------------+--------------+----------------------------------+-----------------+----------------------+--------------+ | ID | Stack Name | Project | Stack Status | Creation Time | Updated Time | +--------------------------------------+--------------+----------------------------------+-----------------+----------------------+--------------+ | 4cb88c32-24f9-41cf-a44d-e18593c5eb2f | Sample-Stack | edd7025c02574d3aa2d3ab6e56208320 | CREATE_COMPLETE | 2020-08-13T09:39:16Z | None | +--------------------------------------+---- # controller ~(keystone)\u003e openstack server list +--------------------------------------+----------------------+--------+-----------------------+--------+---------+ | ID | Name | Status | Networks | Image | Flavor | +--------------------------------------+----------------------+--------+-----------------------+--------+---------+ | ab20d06c-955a-404b-9525-11e3e4b09484 | Heat_Deployed_Server | ACTIVE | int_net=192.168.100.6 | cirros | m1.tiny | +--------------------------------------+----------------------+--------+-----------------------+--------+---------+ ","#":"","openstack-ussuri--heat#\u003cstrong\u003eOpenStack Ussuri : Heat\u003c/strong\u003e":"","openstack-ussuri--heat-1#\u003cstrong\u003eOpenStack Ussuri : Heat\u003c/strong\u003e":"","확인#\u003cstrong\u003e확인\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Heat"},"/system/openstack/openstacktraining/openstack-ussuri-11/":{"data":{"":"OpenStack Ussuri : Gnocch ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi | ----------------------- ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- OpenStack Ussuri : Gnnoch Gnnoch Gnocchi service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 gnocchi +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 3217be4917454641994660bd1f3ea007 | | name | gnocchi | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user gnocchi admin $ controller ~(keystone)\u003e openstack service create --name gnocchi --description \"Metric Service\" metric -------------------------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Metric Service | | enabled | True | | id | 6ac9ec31386f4291b582bd5b504ac485 | | name | gnocchi | | type | metric | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne metric public http://controller:8041 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 12f4410ed82240b0b1340d48b0627612 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 6ac9ec31386f4291b582bd5b504ac485 | | service_name | gnocchi | | service_type | metric | | url | http://controller:8041 | +-------------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne metric internal http://controller:8041 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 70f43453f93b407e94d2dd11ddce7260 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 6ac9ec31386f4291b582bd5b504ac485 | | service_name | gnocchi | | service_type | metric | | url | http://controller:8041 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne metric admin http://controller:8041 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | bb9a955359fd4af18599913465f46958 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 6ac9ec31386f4291b582bd5b504ac485 | | service_name | gnocchi | | service_type | metric | | url | http://controller:8041 | +--------------+----------------------------------+ Gnocchi 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database gnocchi; $ MariaDB\u003e grant all privileges on gnocchi.* to gnocchi@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on gnocchi.* to gnocchi@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; controller node Gnoochi 설치 $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-gnocchi-api openstack-gnocchi-metricd python3-gnocchiclient # gnoochi 및 관련 모듈을 설치합니다. $ controller\u003e vi /etc/gnocchi/gnocchi.conf [DEFAULT] log_dir = /var/log/gnocchi [api] auth_mode = keystone [database] backend = sqlalchemy [indexer] url = mysql+pymysql://gnocchi:qwer1234@controller/gnocchi [storage] driver = file file_basepath = /var/lib/gnocchi [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = gnocchi password = qwer1234 service_token_roles_required = true $ controller\u003e vi /etc/httpd/conf.d/10-gnocchi_wsgi.conf Listen 8041 \u003cVirtualHost *:8041\u003e \u003cDirectory /usr/bin\u003e AllowOverride None Require all granted \u003c/Directory\u003e CustomLog /var/log/httpd/gnocchi_wsgi_access.log combined ErrorLog /var/log/httpd/gnocchi_wsgi_error.log SetEnvIf X-Forwarded-Proto https HTTPS=1 WSGIApplicationGroup %{GLOBAL} WSGIDaemonProcess gnocchi display-name=gnocchi_wsgi user=gnocchi group=gnocchi processes=6 threads=6 WSGIProcessGroup gnocchi WSGIScriptAlias / /usr/bin/gnocchi-api \u003c/VirtualHost\u003e # 새로 생성합니다. $ controller\u003e su -s /bin/bash gnocchi -c \"gnocchi-upgrade\" $ controller\u003e systemctl enable --now openstack-gnocchi-metricd $ controller\u003e systemctl restart httpd # gnocchi DB를 import 시킨 후 서비스를 등록합니다. $ controller\u003e semanage port -a -t http_port_t -p tcp 8041 $ controller\u003e firewall-cmd --add-port=8041/tcp --permanent $ controller\u003e firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. ","#":"","openstack-ussuri--gnnoch#\u003cstrong\u003eOpenStack Ussuri : Gnnoch\u003c/strong\u003e":"","openstack-ussuri--gnocch#\u003cstrong\u003eOpenStack Ussuri : Gnocch\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Gnocch"},"/system/openstack/openstacktraining/openstack-ussuri-12/":{"data":{"":"! 아직 수정 중 문제있음OpenStack Ussuri : Trove ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | ----------------------- ----------------------- --------------------------------- | [ Storage Node 1, 2, 3 ] | | | | Swift-account-auditor | | Swift-account-replicator | | Swift-account | | Swift-container-auditor | | Swift-container-replicator | | Swift-container-updater | | Swift-container | | Swift-object-auditor | | Swift-object-replicator | | Swift-object-updater | | Swift-swift-object | --------------------------------- OpenStack Ussuri : Trove Trove는 관리형 데이터베이스 서비스 입니다. Trove*에 대한 설명은 Heat을 참조해주세요. Trove service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 trove +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 3cdb0abe0a3a429ba08f98d8db786b6d | | name | trove | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user trove admin $ controller ~(keystone)\u003e openstack service create --name trove --description \"Database\" database +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | Database | | enabled | True | | id | 701b7dca93e74509aaf811eafc29cc03 | | name | trove | | type | database | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne database public http://controller:8779/v1.0/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 58fa28821c444c869be463b550f48651 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 701b7dca93e74509aaf811eafc29cc03 | | service_name | trove | | service_type | database | | url | http://controller:8779/v1.0/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne database internal http://controller:8779/v1.0/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | 2809c9cb57884ceabf2902c6b6e62ced | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 701b7dca93e74509aaf811eafc29cc03 | | service_name | trove | | service_type | database | | url | http://controller:8779/v1.0/%(tenant_id)s | +--------------+-------------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne database admin http://controller:8779/v1.0/%\\(tenant_id\\)s +--------------+-------------------------------------------+ | Field | Value | +--------------+-------------------------------------------+ | enabled | True | | id | d86280b59bf04be28e71a57ff8b36a0b | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 701b7dca93e74509aaf811eafc29cc03 | | service_name | trove | | service_type | database | | url | http://controller:8779/v1.0/%(tenant_id)s | +--------------+-------------------------------------------+ Trove 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database trove; $ MariaDB\u003e grant all privileges on trove.* to trove@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on trove.* to trove@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Trove 설치 $ controller ~(keystone)\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-trove python-troveclient # Trove 서비스 및 관련 모듈을 설치합니다. $ controller ~(keystone)\u003e vi /etc/trove/trove.conf [DEFAULT] network_driver = trove.network.neutron.NeutronDriver management_networks = ef7541ad-9599-4285-878a-e0ab62032b03 management_security_groups = d0d797f7-11d4-436e-89a3-ac8bca829f81 cinder_volume_type = lvmdriver-1 nova_keypair = trove-mgmt default_datastore = mysql taskmanager_manager = trove.taskmanager.manager.Manager trove_api_workers = 5 transport_url = rabbit://openstack:qwer1234@controller control_exchange = trove rpc_backend = rabbit reboot_time_out = 300 usage_timeout = 900 agent_call_high_timeout = 1200 use_syslog = False debug = True [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = trove password = qwer1234 service_token_roles_required = true [service_credentials] auth_url = http://controller/identity/v3 region_name = RegionOne project_name = service password = qwer1234 project_domain_name = Default user_domain_name = Default username = trove [database] connection = mysql+pymysql://trove:qwer1234@controller/trove [mariadb] tcp_ports = 3306,4444,4567,4568 [mysql] tcp_ports = 3306 [postgresql] tcp_ports = 5432 $ controller ~(keystone)\u003e vi /etc/trove/trove-guestagent.conf [DEFAULT] log_file = trove-guestagent.log log_dir = /var/log/trove/ ignore_users = os_admin control_exchange = trove transport_url = rabbit://openstack:qwer1234@controller rpc_backend = rabbit command_process_timeout = 60 use_syslog = False debug = True [service_credentials] auth_url = http://controller/identity/v3 region_name = RegionOne project_name = service password = qwer1234 project_domain_name = Default user_domain_name = Default username = trove $ controller ~(keystone)\u003e su -s /bin/sh -c \"trove-manage db_sync\" trove $ controller ~(keystone)\u003e systemctl enable --now openstack-trove-api.service openstack-trove-taskmanager.service openstack-trove-conductor.service $ controller ~(keystone)\u003e ","#":"","-아직-수정-중-문제있음#! 아직 수정 중 문제있음":"","openstack-ussuri--trove#\u003cstrong\u003eOpenStack Ussuri : Trove\u003c/strong\u003e":"","openstack-ussuri--trove-1#\u003cstrong\u003eOpenStack Ussuri : Trove\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Trove"},"/system/openstack/openstacktraining/openstack-ussuri-13/":{"data":{"":"OpenStack Ussuri : Designate ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | | Designate Services | ----------------------- ----------------------- OpenStack Ussuri : Designate Designate는 OpenStack 서비스에서 DNS 서비스를 배포, 관리를 담당합니다. Desigante는 Network node의 설치를 진행하고, controller node의 API를 이용하겠습니다. Designate의 보다 자세한 설명은 Designate를 참조해주세요. Designate service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 designate +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 7563701765d24b4884c0b324b7997530 | | name | designate | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user designate admin $ controller ~(keystone)\u003e openstack service create --name designate --description \"OpenStack DNS Service\" dns +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack DNS Service | | enabled | True | | id | 0e7dacc11b5b48c099d3fe110f8b8197 | | name | designate | | type | dns | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne dns public http://network:9001/ +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 20b26900a14d44209ade2fab0a0f3bbc | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 0e7dacc11b5b48c099d3fe110f8b8197 | | service_name | designate | | service_type | dns | | url | http://network:9001/ | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne dns internal http://network:9001/ +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 856883757b604b93a1273ecc4775f549 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 0e7dacc11b5b48c099d3fe110f8b8197 | | service_name | designate | | service_type | dns | | url | http://network:9001/ | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne dns admin http://network:9001/ +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | c6fef7cbb6a848228fa8ef4067ebcc49 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 0e7dacc11b5b48c099d3fe110f8b8197 | | service_name | designate | | service_type | dns | | url | http://network:9001/ | +--------------+----------------------------------+ Designate 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database designate; $ MariaDB\u003e grant all privileges on designate.* to designate@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on designate.* to designate@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Network Node Desigante 설치 $ network\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-designate-api openstack-designate-central openstack-designate-worker openstack-designate-producer openstack-designate-mdns python3-designateclient bind bind-utils # designate 및 관련 모듈을 설치합니다. $ network\u003e rndc-confgen -a -k designate -c /etc/designate.key -r /dev/urandom $ network\u003e chown named:designate /etc/designate.key $ network\u003e chmod 640 /etc/designate.key # 역할기반 키를 생성하고 권한을 설정합니다. $ network\u003e cp /etc/named.conf /etc/named.conf.backup $ network\u003e vi /etc/named.conf options { listen-on port 53 { any; }; listen-on-v6 port 53 { none; }; directory \"/var/named\"; dump-file \"/var/named/data/cache_dump.db\"; statistics-file \"/var/named/data/named_stats.txt\"; memstatistics-file \"/var/named/data/named_mem_stats.txt\"; # replace query range to your environment allow-query { localhost; 10.10.10.0/24; }; allow-new-zones yes; request-ixfr no; recursion no; bindkeys-file \"/etc/named.iscdlv.key\"; managed-keys-directory \"/var/named/dynamic\"; pid-file \"/run/named/named.pid\"; session-keyfile \"/run/named/session.key\"; }; include \"/etc/designate.key\"; controls { inet 0.0.0.0 port 953 allow { localhost; } keys { \"designate\"; }; }; logging { channel default_debug { file \"data/named.run\"; severity dynamic; }; }; zone \".\" IN { type hint; file \"named.ca\"; }; $ network\u003e chmod 640 /etc/named.conf $ network\u003e chgrp named /etc/named.conf $ network\u003e chown -R named. /var/named $ network\u003e systemctl enable --now named # named dns 서비스를 시작합니다. $ network\u003e vi /etc/designate/designate.conf [DEFAULT] log_dir = /var/log/designate transport_url = rabbit://openstack:qwer1234@controller root_helper = sudo designate-rootwrap /etc/designate/rootwrap.conf [database] connection = mysql+pymysql://designate:qwer1234@controller/designate [service:api] listen = 0.0.0.0:9001 auth_strategy = keystone api_base_uri = http://network:9001 enable_api_v2 = True enabled_extensions_v2 = quotas, reports [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = designate password = qwer1234 [service:worker] enabled = True notify = True [storage:sqlalchemy] connection = mysql+pymysql://designate:qwer1234@controller/designate $ network\u003e su -s /bin/bash -c \"designate-manage database sync\" designate $ network\u003e systemctl enable --now designate-central designate-api # designate api db를 임포트 시키고 서비스를 시작 및 등록합니다. $ network\u003e vi /etc/designate/pools.yaml - name: default description: Default Pool attributes: {} ns_records: - hostname: network priority: 1 nameservers: - host: network port: 53 targets: - type: bind9 description: BIND9 Server masters: - host: network port: 5354 options: host: network port: 53 rndc_host: network rndc_port: 953 rndc_key_file: /etc/designate.key $ network\u003e chmod 640 /etc/designate/pools.yaml $ network\u003e chgrp designate /etc/designate/pools.yaml $ network\u003e su -s /bin/bash -c \"designate-manage pool update\" designate $ network\u003e systemctl enable --now designate-worker designate-producer designate-mdns # designate pool db를 임포트 시키고 서비스를 시작 및 등록합니다. $ network\u003e setsebool -P named_write_master_zones on $ network\u003e firewall-cmd --add-service=dns --permanent $ network\u003e firewall-cmd --add-port={5354/tcp,9001/tcp} --permanent $ network\u003e firewall-cmd --reload # SELinux 및 방화벽을 설정합니다. 확인 $ controller ~(keystone)\u003e openstack dns service list +--------------------------------------+----------+--------------+--------+-------+--------------+ | id | hostname | service_name | status | stats | capabilities | +--------------------------------------+----------+--------------+--------+-------+--------------+ | 43f62f8d-20bc-43b9-8c64-758ac0a2a074 | network | central | UP | - | - | | ab90b2dc-381d-4ca8-ae66-fad57a9f9c11 | network | api | UP | - | - | | 2dbc8027-0c6a-4c4a-b7a0-92a2b19517a7 | network | worker | UP | - | - | | 502ce893-3256-432f-9c6f-2353078ee585 | network | producer | UP | - | - | | 078b306a-e3bb-461d-9c97-71679c9f8830 | network | mdns | UP | - | - | +--------------------------------------+----------+--------------+--------+-------+--------------+ $ controller ~(keystone)\u003e openstack zone create --email dnsmaster@server.education server.education. $ controller ~(keystone)\u003e openstack zone list $ controller ~(keystone)\u003e openstack recordset create --record '192.168.100.10' --type A server.education. node01 $ controller ~(keystone)\u003e openstack recordset list server.education. $ controller ~(keystone)\u003e dig -p 5354 @network.srv.world node01.server.education. $ controller ~(keystone)\u003e openstack zone create --email dnsmaster@server.education 100.168.192.in-addr.arpa. $ controller ~(keystone)\u003e openstack zone list $ controller ~(keystone)\u003e openstack recordset create --record 'node01.server.education.' --type PTR 100.168.192.in-addr.arpa. 10 $ controller ~(keystone)\u003e openstack recordset list 100.168.192.in-addr.arpa. $ controller ~(keystone)\u003e dig -p 5354 @network.srv.world -x 192.168.100.10 $ controller ~(keystone)\u003e openstack recordset list server.education. $ controller ~(keystone)\u003e openstack recordset delete server.education. node01.server.education. $ controller ~(keystone)\u003e openstack recordset list server.education. $ controller ~(keystone)\u003e openstack zone list $ controller ~(keystone)\u003e openstack zone delete server.education. $ controller ~(keystone)\u003e openstack zone list ","#":"","network-node-desigante-설치#\u003cstrong\u003eNetwork Node Desigante 설치\u003c/strong\u003e":"","openstack-ussuri--designate#\u003cstrong\u003eOpenStack Ussuri : Designate\u003c/strong\u003e":"","openstack-ussuri--designate-1#\u003cstrong\u003eOpenStack Ussuri : Designate\u003c/strong\u003e":"","확인#\u003cstrong\u003e확인\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Designate"},"/system/openstack/openstacktraining/openstack-ussuri-14/":{"data":{"":"OpenStack Ussuri : Barbican ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | | Designate Services | | Barbican API | ----------------------- ----------------------- OpenStack Ussuri : Barbican Barbican은 키 관리 서비스 입니다. 비밀 데이터의 안전한 저장, 프로비저닝 및 관리를 제공합니다. 여기에는 대칭 키, 비대칭 키, 인증서 및 원시 바이너리 데이터와 같은 키 자료가 포함됩니다. 자세한 설명은 Barbican을 참조해주세요. Barbican service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 barbican +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | bc85b317bd7c4cc1a4d5aee81c383421 | | name | barbican | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user barbican admin $ controller ~(keystone)\u003e openstack service create --name barbican --description \"OpenStack Key Manager\" key-manager -------------------------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Key Manager | | enabled | True | | id | ec2cbdda740a4887b5737fe885b4b86e | | name | barbican | | type | key-manager | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne key-manager public http://controller:9311 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 3254f8ccb5894560ab3dea0268dddd03 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | ec2cbdda740a4887b5737fe885b4b86e | | service_name | barbican | | service_type | key-manager | | url | http://controller:9311 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne key-manager internal http://controller:9311 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 37a440f72212422ca7c590e322afe56c | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | ec2cbdda740a4887b5737fe885b4b86e | | service_name | barbican | | service_type | key-manager | | url | http://controller:9311 | +--------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne key-manager admin http://controller:9311 +--------------+----------------------------------+ | Field | Value | +--------------+----------------------------------+ | enabled | True | | id | 2ad3a9aabcb840cc832470039ee37b00 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | ec2cbdda740a4887b5737fe885b4b86e | | service_name | barbican | | service_type | key-manager | | url | http://controller:9311 | +--------------+----------------------------------+ Barbican 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database barbican; $ MariaDB\u003e grant all privileges on barbican.* to barbican@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on barbican.* to barbican@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; contoller node Barbican 설치 $ controller ~(keystone)\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-barbican # Barbucan 서비스 및 관련 모듈을 설치합니다. $ controller ~(keystone)\u003e vi /etc/barbican/barbican.conf [DEFAULT] bind_host = 0.0.0.0 bind_port = 9311 host_href = http://controller:9311 log_file = /var/log/barbican/api.log sql_connection = mysql+pymysql://barbican:qwer1234@controller/barbican transport_url = rabbit://openstack:qwer1234@controller [oslo_policy] policy_file = /etc/barbican/policy.json policy_default_rule = default [secretstore] namespace = barbican.secretstore.plugin enabled_secretstore_plugins = store_crypto [crypto] namespace = barbican.crypto.plugin enabled_crypto_plugins = simple_crypto [simple_crypto_plugin] kek = 'YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=' [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = barbican password = qwer1234 $ controller ~(keystone)\u003e su -s /bin/bash barbican -c \"barbican-manage db upgrade\" $ controller ~(keystone)\u003e systemctl enable --now openstack-barbican-api # Barbican 서비스를 DB에 임포트 시킨 후, 서비스를 등록합니다. $ controller ~(keystone)\u003e firewall-cmd --add-port=9311/tcp --permanent $ controller ~(keystone)\u003e firewall-cmd --reload # 방화벽을 설정합니다. 확인 $ controller ~(keystone)\u003e openstack secret store --name secret01 --payload secretkey +---------------+------------------------------------------------------------------------+ | Field | Value | +---------------+------------------------------------------------------------------------+ | Secret href | http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d | | Name | secret01 | | Created | None | | Status | None | | Content types | None | | Algorithm | aes | | Bit length | 256 | | Secret type | opaque | | Mode | cbc | | Expiration | None | +---------------+------------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack secret list +------------------------------------------------------------------------+----------+---------------------------+--------+---------------------------+-----------+------------+-------------+------+------------+ | Secret href | Name | Created | Status | Content types | Algorithm | Bit length | Secret type | Mode | Expiration | +------------------------------------------------------------------------+----------+---------------------------+--------+---------------------------+-----------+------------+-------------+------+------------+ | http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d | secret01 | 2020-08-16T09:00:00+00:00 | ACTIVE | {'default': 'text/plain'} | aes | 256 | opaque | cbc | None | +------------------------------------------------------------------------+----------+---------------------------+--------+---------------------------+-----------+------------+-------------+------+------------+ $ controller ~(keystone)\u003e openstack secret get http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d +---------------+------------------------------------------------------------------------+ | Field | Value | +---------------+------------------------------------------------------------------------+ | Secret href | http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d | | Name | secret01 | | Created | 2020-08-16T09:00:00+00:00 | | Status | ACTIVE | | Content types | {'default': 'text/plain'} | | Algorithm | aes | | Bit length | 256 | | Secret type | opaque | | Mode | cbc | | Expiration | None | +---------------+------------------------------------------------------------------------+ # get 뒤에는 키 생성시 생성되는 값을 입력해주셔야 됩니다 ! $ controller ~(keystone)\u003e openstack secret get http://controller:9311/v1/secrets/86cbaa20-0cb9-479f-82ed-80a02f34b83d --payload +---------+-----------+ | Field | Value | +---------+-----------+ | Payload | secretkey | +---------+-----------+ $ controller ~(keystone)\u003e openstack secret order create --name secret02 --algorithm aes --bit-length 256 --mode cbc --payload-content-type application/octet-stream key +----------------+-----------------------------------------------------------------------+ | Field | Value | +----------------+-----------------------------------------------------------------------+ | Order href | http://controller:9311/v1/orders/ffe9a05e-db5e-4b7d-8b5a-86f1349863c3 | | Type | Key | | Container href | N/A | | Secret href | None | | Created | None | | Status | None | | Error code | None | | Error message | None | +----------------+-----------------------------------------------------------------------+ $ controller ~(keystone)\u003e openstack secret order list +-----------------------------------------------------------------------+------+----------------+------------------------------------------------------------------------+---------------------------+--------+------------+---------------+ | Order href | Type | Container href | Secret href | Created | Status | Error code | Error message | +-----------------------------------------------------------------------+------+----------------+------------------------------------------------------------------------+---------------------------+--------+------------+---------------+ | http://controller:9311/v1/orders/ffe9a05e-db5e-4b7d-8b5a-86f1349863c3 | Key | N/A | http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 | 2020-08-16T09:08:06+00:00 | ACTIVE | None | None | +-----------------------------------------------------------------------+------+----------------+------------------------------------------------------------------------+---------------------------+--------+------------+---------------+ $ controller ~(keystone)\u003e openstack secret order get http://controller:9311/v1/orders/ffe9a05e-db5e-4B7D-8B5A-86f1349863c3 +----------------+------------------------------------------------------------------------+ | Field | Value | +----------------+------------------------------------------------------------------------+ | Order href | http://controller:9311/v1/orders/ffe9a05e-db5e-4b7d-8b5a-86f1349863c3 | | Type | Key | | Container href | N/A | | Secret href | http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 | | Created | 2020-08-16T09:08:06+00:00 | | Status | ACTIVE | | Error code | None | | Error message | None | +----------------+------------------------------------------------------------------------+ # get 뒤에는 키 생성시 생성되는 값을 입력해주셔야 됩니다 ! $ controller ~(keystone)\u003e openstack secret get http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 +---------------+------------------------------------------------------------------------+ | Field | Value | +---------------+------------------------------------------------------------------------+ | Secret href | http://controller:9311/v1/secrets/4c3e2e5b-3585-44ae-901a-25dee6ede5a7 | | Name | secret02 | | Created | 2020-08-16T09:08:06+00:00 | | Status | ACTIVE | | Content types | {'default': 'application/octet-stream'} | | Algorithm | aes | | Bit length | 256 | | Secret type | symmetric | | Mode | cbc | | Expiration | None | +---------------+------------------------------------------------------------------------+ # get 뒤에는 키 생성시 생성되는 값을 입력해주셔야 됩니다 ! ","#":"","openstack-ussuri--barbican#\u003cstrong\u003eOpenStack Ussuri : Barbican\u003c/strong\u003e":"","openstack-ussuri--barbican-1#\u003cstrong\u003eOpenStack Ussuri : Barbican\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Barbican"},"/system/openstack/openstacktraining/openstack-ussuri-15/":{"data":{"":"OpenStack Ussuri : Rally ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | ----------------------- | API-CFN | | Neutron Server | | Heat Engine | | Gnocchi Trove API | | Designate Services | | Barbican API | ----------------------- | Rally | ----------------------- OpenStack Ussuri : Rally Rally는 오픈스택 소스를 GUI 환경으로 보여주는 서비스입니다. Rally의 자세한 설명은 Rally를 참조해주세요. Rally 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database rally; $ MariaDB\u003e grant all privileges on Rally.* to rally@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on Rally.* to rally@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; Rally 설치 $ controller ~(keystone)\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-rally openstack-rally-plugins python3-fixtures # Rally 서비스 및 관련 모듈을 설치합니다. $ controller ~(keystone)\u003e vi /etc/rally/rally.conf [DEFAULT] log_file = rally.log log_dir = /var/log/rally connection = mysql+pymysql://rally:qwer1234@controller/rally $ controller ~(keystone)\u003e mkdir /var/log/rally $ controller ~(keystone)\u003e rally db create # log 파일을 저장할 폴더를 만들고 db를 임포트 시킵니다. $ controller ~(keystone)\u003e rally deployment create --fromenv --name=my_cloud +--------------------------------------+----------------------------+----------+------------------+--------+ | uuid | created_at | name | status | active | +--------------------------------------+----------------------------+----------+------------------+--------+ | 35f9c79c-a47e-49d3-af88-b06b6020b92a | 2020-08-16T09:16:23.793238 | my_cloud | deploy-\u003efinished | | +--------------------------------------+----------------------------+----------+------------------+--------+ $ controller ~(keystone)\u003e source ~/.rally/openrc $ controller ~(keystone)\u003e rally deployment show my_cloud +---------------------------+----------+----------+-------------+-------------+---------------+ | auth_url | username | password | tenant_name | region_name | endpoint_type | +---------------------------+----------+----------+-------------+-------------+---------------+ | http://controller:5000/v3 | admin | *** | admin | | None | +---------------------------+----------+----------+-------------+-------------+---------------+ $ controller ~(keystone)\u003e rally deployment check -------------------------------------------------------------------------------- Platform openstack: -------------------------------------------------------------------------------- Available services: +-------------+----------------+-----------+ | Service | Service Type | Status | +-------------+----------------+-----------+ | __unknown__ | placement | Available | | barbican | key-manager | Available | | cinder | volumev3 | Available | | cloud | cloudformation | Available | | glance | image | Available | | gnocchi | metric | Available | | heat | orchestration | Available | | keystone | identity | Available | | neutron | network | Available | | nova | compute | Available | | swift | object-store | Available | | trove | database | Available | +-------------+----------------+-----------+ $ controller ~(keystone)\u003e vi ~/boot-and-delete.json { \"NovaServers.boot_and_delete_server\": [ { \"args\": { \"flavor\": { \"name\": \"m1.small\" }, \"image\": { \"name\": \"Ubuntu1804\" }, \"force_delete\": false }, \"runner\": { \"type\": \"constant\", \"times\": 10, \"concurrency\": 2 }, \"context\": {} } ] } $ controller ~(keystone)\u003e rally task start ~/boot-and-delete.json -------------------------------------------------------------------------------- Preparing input task -------------------------------------------------------------------------------- Task is: { \"NovaServers.boot_and_delete_server\": [ { \"args\": { \"flavor\": { \"name\": \"m1.small\" }, \"image\": { \"name\": \"Ubuntu1804\" }, \"force_delete\": false }, \"runner\": { \"type\": \"constant\", \"times\": 10, \"concurrency\": 2 }, \"context\": {} } ] } Task syntax is correct :) Running Rally version 3.0.0 -------------------------------------------------------------------------------- Task 887a0d20-37ad-4351-aeca-00f646634552: started -------------------------------------------------------------------------------- .... .... -------------------------------------------------------------------------------- Task 887a0d20-37ad-4351-aeca-00f646634552 has 0 error(s) -------------------------------------------------------------------------------- +-----------------------------------------------------------------------------------------------------------------------+ | Response Times (sec) | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ | Action | Min (sec) | Median (sec) | 90%ile (sec) | 95%ile (sec) | Max (sec) | Avg (sec) | Success | Count | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ | nova.boot_server | 4.466 | 5.409 | 34.472 | 40.002 | 45.533 | 14.471 | 100.0% | 10 | | nova.delete_server | 2.412 | 2.736 | 13.708 | 15.417 | 17.127 | 6.352 | 100.0% | 10 | | total | 6.922 | 12.091 | 40.809 | 44.416 | 48.023 | 20.823 | 100.0% | 10 | | -\u003e duration | 5.922 | 11.091 | 39.809 | 43.416 | 47.023 | 19.823 | 100.0% | 10 | | -\u003e idle_duration | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 100.0% | 10 | +--------------------+-----------+--------------+--------------+--------------+-----------+-----------+---------+-------+ Load duration: 108.251619 Full duration: 134.177109 HINTS: * To plot HTML graphics with this data, run: rally task report 887a0d20-37ad-4351-aeca-00f646634552 --out output.html * To generate a JUnit report, run: rally task export 887a0d20-37ad-4351-aeca-00f646634552 --type junit-xml --to output.xml * To get raw JSON output of task results, run: rally task report 887a0d20-37ad-4351-aeca-00f646634552 --json --out output.json ","#":"","openstack-ussuri--rally#\u003cstrong\u003eOpenStack Ussuri : Rally\u003c/strong\u003e":"","openstack-ussuri--rally-1#\u003cstrong\u003eOpenStack Ussuri : Rally\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Rally"},"/system/openstack/openstacktraining/openstack-ussuri-16/":{"data":{"":"OpenStack Ussuri : Manila ----------------------- ----------------------- ----------------------- | [ Controller Node ] | | [ Compute Node ] | | [ Network Node ] | | | | Libvirt | | Open vSwitch | | MariaDB RabbitMQ | | Nova compute | | L2 Agent | | Memcached Keystone | | Open vSwitch | | L3 Agent | | httpd Cinder API | | L2 Agent | | metadata agent | | Nova-API Compute | | Cinder-LVM | | Swift-proxy | | L2 agent L3 agent | | NFS | | Heat API | | metadata agent | | Manila Share | | API-CFN | | Neutron Server | ----------------------- | Heat Engine | | Gnocchi Trove API | | Designate Services | | Barbican API | ----------------------- | Rally Manila API | ----------------------- OpenStack Ussuri : Manila Manila는 OpenStack에서 맡는 서비스입니다. Manila의 대한 보다 자세한 설명은 Manila를 참조해주세요. Manila service 및 User 생성 $ controller ~(keystone)\u003e openstack user create --domain default --project service --password qwer1234 manila +---------------------+----------------------------------+ | Field | Value | +---------------------+----------------------------------+ | default_project_id | b470c69e28db47cdbfc81e06cc67f627 | | domain_id | default | | enabled | True | | id | 4ea7c62d89194d9883e6773a977133b6 | | name | manila | | options | {} | | password_expires_at | None | +---------------------+----------------------------------+ $ controller ~(keystone)\u003e openstack role add --project service --user manila admin $ controller ~(keystone)\u003e openstack service create --name manila --description \"OpenStack Shared Filesystem\" share +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Shared Filesystem | | enabled | True | | id | 1129696ac3f5449293b638e0daec3bde | | name | manila | | type | share | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack service create --name manilav2 --description \"OpenStack Shared Filesystem V2\" sharev2 +-------------+----------------------------------+ | Field | Value | +-------------+----------------------------------+ | description | OpenStack Shared Filesystem V2 | | enabled | True | | id | 1d94787a2d34489dbe880faa5e165e5e | | name | manilav2 | | type | sharev2 | +-------------+----------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne share public http://controller:8786/v1/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | d3d6590a342047eab8abca304701d90d | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 1129696ac3f5449293b638e0daec3bde | | service_name | manila | | service_type | share | | url | http://controller:8786/v1/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne share internal http://controller:8786/v1/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 0a6516d199d346febe62800b87a10eb9 | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 1129696ac3f5449293b638e0daec3bde | | service_name | manila | | service_type | share | | url | http://controller:8786/v1/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne share admin http://controller:8786/v1/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 6153d88f7eab40caa669c3130f03226a | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 1129696ac3f5449293b638e0daec3bde | | service_name | manila | | service_type | share | | url | http://controller:8786/v1/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne sharev2 public http://controller:8786/v2/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 22e9d4e2b62a4203ae182041c9c10049 | | interface | public | | region | RegionOne | | region_id | RegionOne | | service_id | 1d94787a2d34489dbe880faa5e165e5e | | service_name | manilav2 | | service_type | sharev2 | | url | http://controller:8786/v2/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne sharev2 internal http://controller:8786/v2/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | eb4e0b33fae7432d87078a0ba2c2e8de | | interface | internal | | region | RegionOne | | region_id | RegionOne | | service_id | 1d94787a2d34489dbe880faa5e165e5e | | service_name | manilav2 | | service_type | sharev2 | | url | http://controller:8786/v2/%(tenant_id)s | +--------------+-----------------------------------------+ $ controller ~(keystone)\u003e openstack endpoint create --region RegionOne sharev2 admin http://controller:8786/v2/%\\(tenant_id\\)s +--------------+-----------------------------------------+ | Field | Value | +--------------+-----------------------------------------+ | enabled | True | | id | 212c9ddfbc554dfb83f80e3a252db235 | | interface | admin | | region | RegionOne | | region_id | RegionOne | | service_id | 1d94787a2d34489dbe880faa5e165e5e | | service_name | manilav2 | | service_type | sharev2 | | url | http://controller:8786/v2/%(tenant_id)s | +--------------+-----------------------------------------+ Manila 유저의 DB를 생성합니다. $ controller\u003e mysql -u root -p $ MariaDB\u003e create database manila; $ MariaDB\u003e grant all privileges on manila.* to manila@'localhost' identified by 'qwer1234'; $ MariaDB\u003e grant all privileges on manila.* to manila@'%' identified by 'qwer1234'; $ MariaDB\u003e flush privileges; $ MariaDB\u003e exit; controller node manila api 설치 $ controller\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-manila python3-manilaclient # manila 및 관련 모듈을 설치합니다. $ controller\u003e vi /etc/manila/manila.conf [DEFAULT] my_ip = controller api_paste_config = /etc/manila/api-paste.ini rootwrap_config = /etc/manila/rootwrap.conf state_path = /var/lib/manila auth_strategy = keystone default_share_type = default_share_type share_name_template = share-%s transport_url = rabbit://openstack:qwer1234@controller [database] connection = mysql+pymysql://manila:qwer1234@controller/manila [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = manila password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ controller\u003e su -s /bin/bash manila -c \"manila-manage db sync\" $ controller\u003e systemctl enable --now openstack-manila-api openstack-manila-scheduler $ controller\u003e firewall-cmd --add-port=8786/tcp --permanent $ controller\u003e firewall-cmd --reload # 방화벽을 설정합니다. $ controller ~(keystone)\u003e manila service-list +----+------------------+------------+------+---------+-------+----------------------------+ | Id | Binary | Host | Zone | Status | State | Updated_at | +----+------------------+------------+------+---------+-------+----------------------------+ | 1 | manila-scheduler | controller | nova | enabled | up | 2020-08-21T01:27:53.000000 | +----+------------------+------------+------+---------+-------+----------------------------+ # 확인 compute node manila share 설차 $ compute\u003e dnf --enablerepo=centos-openstack-ussuri,powertools,epel -y install openstack-manila-share python3-manilaclient mariadb-devel python3-devel gcc make $ compute\u003e pip3 install mysqlclient $ compute\u003e vi /etc/manila/manila.conf [DEFAULT] my_ip = compute api_paste_config = /etc/manila/api-paste.ini rootwrap_config = /etc/manila/rootwrap.conf state_path = /var/lib/manila auth_strategy = keystone default_share_type = default_share_type share_name_template = share-%s transport_url = rabbit://openstack:qwer1234@controller [database] connection = mysql+pymysql://manila:qwer1234@controller/manila [keystone_authtoken] www_authenticate_uri = http://controller:5000 auth_url = http://controller:5000 memcached_servers = controller:11211 auth_type = password project_domain_name = default user_domain_name = default project_name = service username = manila password = qwer1234 [oslo_concurrency] lock_path = $state_path/tmp $ compute\u003e mkdir /var/lib/manila $ compute\u003e chown manila. /var/lib/manila $ compute\u003e firewall-cmd --add-service=nfs --permanent $ compute\u003e firewall-cmd --reload # 새로운 Disk install $ compute\u003e dnf -y install nfs-utils nfs4-acl-tools $ compute\u003e fdisk /dev/sdc ... ... $ compute\u003e pvcreate /dev/sdc1 $ compute\u003e vgcreate manila-volumes /dev/sdc1 $ compute\u003e vi /etc/manila/manila.conf [DEFAULT] enabled_share_backends = lvm [lvm] share_backend_name = LVM share_driver = manila.share.drivers.lvm.LVMShareDriver driver_handles_share_servers = False lvm_share_volume_group = manila-volumes lvm_share_export_ips = compute $ compute\u003e systemctl enable --now openstack-manila-share nfs-server 확인 $ controller ~(keystone)\u003e manila type-create default_share_type False +----------------------+--------------------------------------+ | Property | Value | +----------------------+--------------------------------------+ | ID | 9f6323f6-7443-4a83-ba70-7c10f78366c9 | | Name | default_share_type | | Visibility | public | | is_default | YES | | required_extra_specs | driver_handles_share_servers : False | | optional_extra_specs | | | Description | None | +----------------------+--------------------------------------+ $ controller ~(keystone)\u003e manila type-list +--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+ | ID | Name | visibility | is_default | required_extra_specs | optional_extra_specs | Description | +--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+ | 9f6323f6-7443-4a83-ba70-7c10f78366c9 | default_share_type | public | YES | driver_handles_share_servers : False | | None | +--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+ $ controller ~(keystone)\u003e manila create NFS 10 --name share01 +---------------------------------------+--------------------------------------+ | Property | Value | +---------------------------------------+--------------------------------------+ | id | 72ab96c5-80f5-401a-b414-ab76a240acf1 | | size | 10 | | availability_zone | None | | created_at | 2020-08-21T01:52:16.000000 | | status | creating | | name | share01 | | description | None | | project_id | edd7025c02574d3aa2d3ab6e56208320 | | snapshot_id | None | | share_network_id | None | | share_proto | NFS | | metadata | {} | | share_type | 9f6323f6-7443-4a83-ba70-7c10f78366c9 | | is_public | False | | snapshot_support | False | | task_state | None | | share_type_name | default_share_type | | access_rules_status | active | | replication_type | None | | has_replicas | False | | user_id | 4ebf85318da84b5cb1257152f9fc35ba | | create_share_from_snapshot_support | False | | revert_to_snapshot_support | False | | share_group_id | None | | source_share_group_snapshot_member_id | None | | mount_snapshot_support | False | | progress | None | | share_server_id | None | | host | | +---------------------------------------+--------------------------------------+ $ controller ~(keystone)\u003e manila list $ controller ~(keystone)\u003e manila access-allow share01 ip 1.1.1.0/24 --access-level rw $ controller ~(keystone)\u003e manila access-list share01 $ controller ~(keystone)\u003e openstack server start CentOS_8 $ controller ~(keystone)\u003e manila show share01 | grep path | cut -d'|' -f3 $ controller ~(keystone)\u003e ssh centos@10.0.0.247 $ controller ~(keystone)\u003e sudo mount -t nfs \\ 10.0.0.50:/var/lib/manila/mnt/share-3544d5a3-7157-4c10-aaa3-edd4b6fd2512 /mnt $ controller ~(keystone)\u003e df -hT $ controller ~(keystone)\u003e ","#":"","compute-node-manila-share-설차#\u003cstrong\u003ecompute node manila share 설차\u003c/strong\u003e":"","openstack-ussuri--manila#\u003cstrong\u003eOpenStack Ussuri : Manila\u003c/strong\u003e":"","openstack-ussuri--manila-1#\u003cstrong\u003eOpenStack Ussuri : Manila\u003c/strong\u003e":"","확인#\u003cstrong\u003e확인\u003c/strong\u003e":""},"title":"OpenStack Ussuri : Manila"},"/system/openstack/openstacktraining/packstack/":{"data":{"":"Packstack Stein 설치Packstack Redhat 계열 ( ex : CentOS )의 OpenStack 자동화 설치 툴 Packstack stain 설치 기본적으로 PackStack은 올인원 or 다중노드로 구성할 수 있으며, 여기서는 올인원으로 설치를 진행하며, 다중노드에 대한 설정은 추가하도록 하겠습니다. 설치사양 OS CPU RAM DISK CenOS7 4/ 2 10240 100G 만약 다중 노드에 경우 소스를 분산시키고 각 노드에 설정을 추가합니다. hosts, hostname 등록 및 설정 다중 노드의 경우 controller node에서 다른 노드의 ssh 접속을 위한 키를 등록시킵니다. $ controller\u003e $ ssh-keygen $ controller\u003e $ ssh-copy-id network $ controller\u003e $ ssh-copy-id compute $ controller\u003e $ ssh-copy-id ... 다른 노드 설치 순서 firewalld 설정 setenforce을 진행합니다. $ systemctl stop firewalld $ systemctl disable firewalld $ systemctl stop NetworkManagaer $ systemctl disable NetworkManagaer # 방화벽 및 네트워크 매니저 설정을 진행합니다. $ setenforce 0 $ sed -i 's/=enforcing/=disabled/g' /etc/sysconfig/selinux # setenforce 설정을 진행합니다. OpenStack stain release를 등록합니다. $ yum -y update # 기본 패키지를 업데이트 합니다. $ yum install -y centos-release-openstack-stein $ yum -y update # stein 레포지터리를 등록 후, 다시 업데이트를 진행합니다. 올인원의 경우 $ yum install -y openstack-packstack $ packstack --allinone # packstack을 통해 OpenStack 설치를 진행합니다. 다중노드의 경우 $ packstack --gen-answer-file=/root/stein-answer.txt # Packstack 설정 파일을 설치합니다. $ vi /root/stein.answer.txt CONFIG_CONTROLLER_HOST=contoller CONFIG_COMPUTE_HOSTS=compute1,compute2,compute3.... CONFIG_NETWORK_HOSTS=network1,network2.... CONFIG_PROVISION_DEMO=n CONFIG_NTP_SERVERS=0.centos.pool.ntp.org iburst, 1.centos.pool.ntp.org iburst, 2.centos.pool.ntp.org iburst, 3.centos.pool.ntp.org iburst CONFIG_CINDER_VOLUMES_SIZE=100G # 기본적인 설정을 진행합니다. # 설치 시 각 OpenStack의 서비스들을 원하는 Node의 설치할 수 있습니다. $ packstack --answer-file=/root/stein-answer.txt # packstack 설치를 진행합니다. 접속 IP, PW 확인 $ /var/tmp/packstack/....../openstack-setup.log | cat USERNAME= $ /var/tmp/packstack/....../openstack-setup.log | cat ADMIN_PW= # 사용자 이름 및 암호 출력 ","#":"","packstack#\u003cstrong\u003ePackstack\u003c/strong\u003e":"","packstack-stein-설치#\u003cstrong\u003ePackstack Stein 설치\u003c/strong\u003e":""},"title":"Packstack"},"/system/server/app/email/":{"data":{"-email-서버-구축-smtpimappop3-가이드-#📧 \u003cstrong\u003eEmail 서버 구축 (SMTP/IMAP/POP3) 가이드\u003c/strong\u003e 🚀":"","1-email-서버란#1️⃣ Email 서버란?":"","2-email-서버-설치-postfix--dovecot#2️⃣ Email 서버 설치 (Postfix + Dovecot)":"","3-smtp-서버-설정-postfix#3️⃣ SMTP 서버 설정 (Postfix)":"","4-imappop3-서버-설정-dovecot#4️⃣ IMAP/POP3 서버 설정 (Dovecot)":"","5-email-서버-방화벽-설정#5️⃣ Email 서버 방화벽 설정":"","6-이메일-송수신-테스트#6️⃣ 이메일 송수신 테스트":"","7-웹메일-roundcube-설정-선택#7️⃣ 웹메일 (Roundcube) 설정 (선택)":"","8-email-서버-로그-및-문제-해결#8️⃣ Email 서버 로그 및 문제 해결":"","9-결론-#9️⃣ 결론 🚀":"📧 Email 서버 구축 (SMTP/IMAP/POP3) 가이드 🚀 1️⃣ Email 서버란? 이메일 서버는 메일을 송수신 및 저장하는 역할을 합니다.\n대표적인 이메일 프로토콜은 다음과 같습니다.\n✅ SMTP (Simple Mail Transfer Protocol)\n메일을 보내는 프로토콜 (발신)\n✅ IMAP (Internet Message Access Protocol)\n서버에서 메일을 관리하는 프로토콜 (수신, 서버 동기화 가능)\n✅ POP3 (Post Office Protocol 3)\n메일을 클라이언트로 다운로드하는 프로토콜 (수신, 서버에서 삭제 가능)\n💡 IMAP과 POP3 차이점\n프로토콜 방식 장점 단점 IMAP 서버 동기화 여러 기기에서 이메일 동기화 가능 서버 저장 공간 필요 POP3 다운로드 후 삭제 서버 용량 절약 한 기기에서만 사용 가능 2️⃣ Email 서버 설치 (Postfix + Dovecot) ✅ Postfix: SMTP(메일 발송) 서버\n✅ Dovecot: IMAP/POP3(메일 수신) 서버\n🔹 Ubuntu/Debian에서 설치 sudo apt update sudo apt install postfix dovecot-core dovecot-imapd dovecot-pop3d -y 🔹 CentOS/RHEL에서 설치 sudo yum install postfix dovecot -y 설치 후 서비스 활성화 및 확인\nsudo systemctl enable postfix dovecot sudo systemctl start postfix dovecot sudo systemctl status postfix dovecot 3️⃣ SMTP 서버 설정 (Postfix) 설정 파일: /etc/postfix/main.cf\nsudo nano /etc/postfix/main.cf 📌 1. 도메인 및 호스트 설정 myhostname = mail.example.com mydomain = example.com myorigin = $mydomain inet_interfaces = all 📌 2. 메일 릴레이 차단 (스팸 방지) smtpd_recipient_restrictions = permit_mynetworks permit_sasl_authenticated reject_unauth_destination 📌 3. 설정 적용 및 서비스 재시작 sudo systemctl restart postfix 4️⃣ IMAP/POP3 서버 설정 (Dovecot) 설정 파일: /etc/dovecot/dovecot.conf\nsudo nano /etc/dovecot/dovecot.conf 📌 1. 프로토콜 활성화 protocols = imap pop3 lmtp 📌 2. 메일 저장 위치 지정 mail_location = maildir:~/Maildir 📌 3. 서비스 재시작 sudo systemctl restart dovecot 5️⃣ Email 서버 방화벽 설정 ✅ 메일 서비스에 필요한 포트 개방\n서비스 포트 SMTP 25, 587 (TLS) IMAP 143, 993 (SSL) POP3 110, 995 (SSL) 🔹 UFW (Ubuntu/Debian) sudo ufw allow 25/tcp sudo ufw allow 587/tcp sudo ufw allow 143/tcp sudo ufw allow 993/tcp sudo ufw allow 110/tcp sudo ufw allow 995/tcp sudo ufw reload 🔹 firewalld (CentOS/RHEL) sudo firewall-cmd --permanent --add-service=smtp sudo firewall-cmd --permanent --add-service=imap sudo firewall-cmd --permanent --add-service=pop3 sudo firewall-cmd --reload 6️⃣ 이메일 송수신 테스트 📌 1. SMTP 테스트 (메일 보내기) echo \"Test Email\" | mail -s \"Test Subject\" user@example.com 📌 2. IMAP/POP3 연결 테스트 telnet mail.example.com 143 # IMAP telnet mail.example.com 110 # POP3 TLS/SSL을 사용할 경우\nopenssl s_client -connect mail.example.com:993 -crlf openssl s_client -connect mail.example.com:995 -crlf 7️⃣ 웹메일 (Roundcube) 설정 (선택) ✅ 웹에서 이메일을 확인하려면 Roundcube 같은 웹메일 클라이언트를 설치 가능\nsudo apt install roundcube -y 설치 후 /etc/apache2/sites-available/roundcube.conf 설정을 추가하여 웹메일 사용 가능 🚀\n8️⃣ Email 서버 로그 및 문제 해결 📌 1. Postfix 로그 확인 sudo journalctl -u postfix -f 📌 2. Dovecot 로그 확인 sudo journalctl -u dovecot -f 📌 3. 메일 큐 확인 mailq 📌 4. DNS 레코드 (MX, SPF, DKIM) 확인 dig MX example.com 9️⃣ 결론 🚀 ✅ SMTP(Postfix) + IMAP/POP3(Dovecot) 조합으로 Email 서버를 구축할 수 있습니다.\n✅ DNS 설정 (MX, SPF, DKIM, DMARC)을 추가하면 보안 강화 가능합니다.\n✅ 웹메일(Roundcube)과 연동하면 브라우저에서도 이메일을 관리할 수 있습니다.\n📚 추가 자료\n1️⃣ Postfix 공식 문서\n2️⃣ Dovecot 공식 문서\n3️⃣ Roundcube 웹메일"},"title":"SMTP/IMAP/POP3"},"/system/server/app/http/":{"data":{"":"","-http-server-guide#📌 HTTP Server Guide":"","-https-server-guide#📌 HTTPS Server Guide":"","-결론#📌 결론":"📌 HTTP Server Guide 1️⃣ HTTP란? HTTP(HyperText Transfer Protocol) 는 웹 브라우저와 웹 서버 간 데이터를 주고받는 프로토콜입니다.\n인터넷에서 웹 페이지를 요청하고 표시하는 데 사용됩니다.\n✅ HTTP 특징\n텍스트, 이미지, 비디오 등의 데이터 전송 가능 상태를 유지하지 않는(stateless) 프로토콜 기본적으로 보안이 없으며, 데이터를 평문으로 전송 2️⃣ HTTP 서버 구축 (Apache 기준) 📌 1. Apache 웹 서버 설치 (Ubuntu 기준) sudo apt update sudo apt install apache2 -y 📌 2. 서비스 시작 및 자동 실행 설정 sudo systemctl start apache2 sudo systemctl enable apache2 📌 3. 방화벽 설정 (UFW 사용) sudo ufw allow 80/tcp sudo ufw enable 📌 4. 웹 페이지 배포 Apache 기본 문서 루트: /var/www/html\n기본 페이지 수정:\nsudo nano /var/www/html/index.html \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eMy HTTP Server\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to My HTTP Server!\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e 📌 5. 서버 확인 웹 브라우저에서 http://서버_IP 입력하여 페이지 확인\n📌 HTTPS Server Guide 1️⃣ HTTPS란? HTTPS(HyperText Transfer Protocol Secure) 는 HTTP에 TLS(SSL) 암호화가 추가된 보안 프로토콜입니다.\n데이터를 암호화하여 안전한 통신을 보장합니다.\n✅ HTTPS 특징\n데이터 암호화 (TLS/SSL 사용) 사용자 정보 보호 (MITM 공격 방지) 신뢰할 수 있는 인증서 필요 2️⃣ HTTPS 서버 구축 (Let’s Encrypt 인증서 사용) 📌 1. Certbot 설치 sudo apt install certbot python3-certbot-apache -y 📌 2. HTTPS 인증서 발급 sudo certbot --apache -d example.com -d www.example.com 🔹 example.com을 실제 도메인으로 변경\n🔹 인증 과정에서 이메일 및 약관 동의 필요\n📌 3. HTTPS 자동 갱신 설정 sudo systemctl enable certbot.timer 📌 4. 방화벽 설정 (HTTPS 허용) sudo ufw allow 443/tcp sudo ufw reload 3️⃣ HTTP → HTTPS 리디렉션 설정 Apache 설정 파일 수정 (/etc/apache2/sites-available/000-default.conf)\n\u003cVirtualHost *:80\u003e ServerName example.com Redirect permanent / https://example.com/ \u003c/VirtualHost\u003e sudo systemctl restart apache2 📌 결론 HTTP는 웹 통신의 기본 프로토콜이며, HTTPS를 통해 보안을 강화할 수 있습니다.\nLet’s Encrypt 인증서를 사용하면 무료로 HTTPS를 적용할 수 있습니다. 🚀\n📌 참고자료 1️⃣ Apache 공식 문서\n2️⃣ Let’s Encrypt \u0026 Certbot 가이드","1-https란#1️⃣ HTTPS란?":"","1-http란#1️⃣ HTTP란?":"","2-http-서버-구축-apache-기준#2️⃣ HTTP 서버 구축 (Apache 기준)":"","2-https-서버-구축-lets-encrypt-인증서-사용#2️⃣ HTTPS 서버 구축 (Let\u0026rsquo;s Encrypt 인증서 사용)":"","3-http--https-리디렉션-설정#3️⃣ HTTP → HTTPS 리디렉션 설정":""},"title":"HTTP, HTTPS"},"/system/server/auth/ldap/":{"data":{"":"","-ldap-lightweight-directory-access-protocol-guide#📌 LDAP (Lightweight Directory Access Protocol) Guide":"","1-ldap란#1️⃣ LDAP란?":"","2-ldap-서버-설치-linux-기반#2️⃣ LDAP 서버 설치 (Linux 기반)":"","3-ldap-서버-설정#3️⃣ LDAP 서버 설정":"","4-ldap-데이터-조회-및-관리#4️⃣ LDAP 데이터 조회 및 관리":"","5-ldap-연동-설정#5️⃣ LDAP 연동 설정":"","6-ldap-포트-및-방화벽-설정#6️⃣ LDAP 포트 및 방화벽 설정":"","7-결론-#7️⃣ 결론 🚀":"📌 LDAP (Lightweight Directory Access Protocol) Guide 1️⃣ LDAP란? **LDAP (Lightweight Directory Access Protocol)**은 디렉터리 서비스에 저장된 정보를 검색하고 수정하는 오픈 프로토콜입니다.\nLDAP는 사용자 계정, 그룹, 네트워크 리소스 등의 정보를 중앙 집중적으로 관리할 수 있도록 도와줍니다.\n✅ LDAP 주요 기능\n🔑 중앙 집중식 사용자 관리 – 여러 시스템에서 동일한 사용자 인증 정보 사용 🔍 빠른 검색 및 조회 – 계층적 구조를 이용한 효율적인 데이터 검색 🔗 다양한 시스템과 연동 가능 – Linux, Windows, 애플리케이션과의 인증 연동 🏢 기업 환경에서의 사용자 관리 최적화 – Active Directory(AD)와 연동하여 사용 가능 2️⃣ LDAP 서버 설치 (Linux 기반) 🔹 Ubuntu/Debian에서 OpenLDAP 서버 설치 sudo apt update sudo apt install slapd ldap-utils -y 설치 중 관리자 비밀번호(root DN password) 설정을 입력해야 합니다.\n설치 후, 설정을 구성합니다.\nsudo dpkg-reconfigure slapd 🔹 CentOS/RHEL에서 OpenLDAP 서버 설치 sudo yum install openldap openldap-servers openldap-clients -y 설치 후, LDAP 서비스를 자동 시작하도록 설정합니다.\nsudo systemctl enable slapd sudo systemctl start slapd LDAP가 정상적으로 실행되고 있는지 확인합니다.\nsudo systemctl status slapd 3️⃣ LDAP 서버 설정 LDAP의 기본 설정 파일은 /etc/ldap/ldap.conf (Ubuntu) 또는 /etc/openldap/slapd.conf (CentOS)입니다.\n📌 1. 기본 설정 파일 (/etc/ldap/ldap.conf) BASE dc=example,dc=com URI ldap://127.0.0.1 TLS_CACERT /etc/ssl/certs/ca-certificates.crt 🔹 설명\nBASE – 기본 검색 경로 (예: dc=example,dc=com) URI – LDAP 서버 주소 (ldap://127.0.0.1 또는 ldaps:// 사용 가능) TLS_CACERT – TLS 인증서 경로 (보안 연결 설정 시 사용) 📌 2. 관리자 계정 추가 LDAP 관리자를 추가하려면 ldapadd 명령어를 사용합니다.\n먼저, 관리자 계정 정보를 정의하는 .ldif 파일을 생성합니다.\nnano admin.ldif 다음 내용을 입력합니다.\ndn: cn=admin,dc=example,dc=com objectClass: organizationalRole cn: admin description: LDAP Administrator 관리자 계정을 추가합니다.\nldapadd -x -D \"cn=admin,dc=example,dc=com\" -W -f admin.ldif 📌 3. 사용자 추가 새로운 사용자 계정을 추가하려면 .ldif 파일을 만들어야 합니다.\nnano user.ldif 다음 내용을 입력합니다.\ndn: uid=johndoe,ou=users,dc=example,dc=com objectClass: inetOrgPerson objectClass: posixAccount objectClass: top cn: John Doe sn: Doe uid: johndoe mail: johndoe@example.com homeDirectory: /home/johndoe loginShell: /bin/bash uidNumber: 1001 gidNumber: 1001 userPassword: {SSHA}hashed_password_here 사용자를 추가합니다.\nldapadd -x -D \"cn=admin,dc=example,dc=com\" -W -f user.ldif 4️⃣ LDAP 데이터 조회 및 관리 📌 1. 사용자 목록 조회 ldapsearch -x -LLL -b \"dc=example,dc=com\" 📌 2. 특정 사용자 조회 ldapsearch -x -LLL -b \"dc=example,dc=com\" \"(uid=johndoe)\" 📌 3. 사용자 비밀번호 변경 ldappasswd -x -D \"cn=admin,dc=example,dc=com\" -W -S \"uid=johndoe,ou=users,dc=example,dc=com\" 5️⃣ LDAP 연동 설정 LDAP 서버는 다양한 애플리케이션과 연동하여 사용할 수 있습니다.\n✅ Linux 시스템 로그인 연동 (PAM, NSS 사용)\nsudo apt install libnss-ldap libpam-ldap ldap-utils ✅ Active Directory와 연동 (SSSD 사용)\nsudo apt install sssd ✅ 웹 애플리케이션과 연동 (Apache, Nginx)\nsudo apt install libapache2-mod-authnz-ldap 6️⃣ LDAP 포트 및 방화벽 설정 LDAP는 389/TCP (기본) 및 636/TCP (SSL/TLS) 포트를 사용합니다.\n📌 1. 포트 확인 sudo netstat -tulnp | grep slapd 📌 2. 방화벽 설정 ✅ UFW 사용 (Ubuntu/Debian)\nsudo ufw allow 389/tcp sudo ufw allow 636/tcp sudo ufw reload ✅ firewalld 사용 (CentOS/RHEL)\nsudo firewall-cmd --permanent --add-port=389/tcp sudo firewall-cmd --permanent --add-port=636/tcp sudo firewall-cmd --reload 7️⃣ 결론 🚀 LDAP는 기업 및 서버 환경에서 사용자 인증과 디렉터리 서비스를 중앙 집중적으로 관리하는 필수 프로토콜입니다.\n위의 설정을 적용하면 LDAP 서버를 구축하고, 사용자 계정을 생성하며, 인증 시스템과 연동할 수 있습니다.\n연동이 필요한 경우 PAM, SSSD, Apache/Nginx 등의 추가 설정을 진행하세요.\n📚 추가 자료\n1️⃣ LDAP 공식 문서\n2️⃣ OpenLDAP 설정 가이드\n3️⃣ Linux LDAP 인증 설정"},"title":"LDAP"},"/system/server/auth/radius/":{"data":{"-radius--tacacs-서버-구축-및-설정-가이드-#🔐 RADIUS \u0026amp; TACACS+ 서버 구축 및 설정 가이드 🚀":"","1-radius--tacacs란#1️⃣ RADIUS \u0026amp; TACACS+란?":"","2-radius-서버-설치-linux-기반#2️⃣ RADIUS 서버 설치 (Linux 기반)":"","3-radius-서버-설정#3️⃣ RADIUS 서버 설정":"","4-tacacs-서버-설치#4️⃣ TACACS+ 서버 설치":"","5-tacacs-서버-설정#5️⃣ TACACS+ 서버 설정":"","6-방화벽-설정-필요한-경우#6️⃣ 방화벽 설정 (필요한 경우)":"","7-radius--tacacs-클라이언트-설정#7️⃣ RADIUS \u0026amp; TACACS+ 클라이언트 설정":"","8-로그-확인-및-문제-해결#8️⃣ 로그 확인 및 문제 해결":"","9-결론-#9️⃣ 결론 🚀":"🔐 RADIUS \u0026 TACACS+ 서버 구축 및 설정 가이드 🚀 1️⃣ RADIUS \u0026 TACACS+란? **RADIUS (Remote Authentication Dial-In User Service)**와 **TACACS+ (Terminal Access Controller Access-Control System Plus)**는 네트워크 장비 및 시스템에 **사용자 인증, 권한 부여 및 계정 관리(AAA: Authentication, Authorization, Accounting)**를 제공하는 프로토콜입니다.\n✅ RADIUS와 TACACS+의 차이점\n프로토콜 보안성 포트 암호화 방식 사용처 RADIUS ✅ 사용자 비밀번호만 암호화 UDP 1812, 1813 PAP, CHAP, EAP VPN, Wi-Fi 인증 TACACS+ ✅ 전체 패킷 암호화 TCP 49 SSH, Telnet 인증 네트워크 장비 (Cisco 등) 2️⃣ RADIUS 서버 설치 (Linux 기반) 🔹 Ubuntu/Debian에서 FreeRADIUS 설치 sudo apt update sudo apt install freeradius -y 🔹 CentOS/RHEL에서 FreeRADIUS 설치 sudo yum install freeradius freeradius-utils -y 설치 후, 서비스 활성화 및 상태 확인\nsudo systemctl enable freeradius sudo systemctl start freeradius sudo systemctl status freeradius 3️⃣ RADIUS 서버 설정 RADIUS 주요 설정 파일은 /etc/freeradius/3.0/ 디렉토리에 있습니다.\n📌 1. 클라이언트(네트워크 장비) 추가 sudo nano /etc/freeradius/3.0/clients.conf 다음 내용을 추가하여 RADIUS 클라이언트(네트워크 장비)를 등록합니다.\nclient router1 { ipaddr = 192.168.1.1 secret = myradiussecret } 📌 2. 사용자 계정 추가 sudo nano /etc/freeradius/3.0/users testuser Cleartext-Password := \"password123\" Service-Type = Framed-User, Framed-Protocol = PPP 설정을 저장한 후 RADIUS 서비스를 재시작합니다.\nsudo systemctl restart freeradius 4️⃣ TACACS+ 서버 설치 🔹 Ubuntu/Debian에서 TACACS+ 설치 sudo apt install tacacs+ -y 🔹 CentOS/RHEL에서 TACACS+ 설치 sudo yum install tacacs+ -y 설치 후, 서비스 활성화 및 상태 확인\nsudo systemctl enable tacacs+ sudo systemctl start tacacs+ sudo systemctl status tacacs+ 5️⃣ TACACS+ 서버 설정 TACACS+ 주요 설정 파일은 /etc/tacacs+/tacacs.conf 입니다.\nsudo nano /etc/tacacs+/tacacs.conf 📌 1. 공유 키(Shared Secret) 설정 key = \"tacacs_secret\" 📌 2. 사용자 계정 추가 user = admin { login = cleartext \"adminpassword\" service = exec { priv-lvl = 15 } } 설정을 저장한 후 TACACS+ 서비스를 재시작합니다.\nsudo systemctl restart tacacs+ 6️⃣ 방화벽 설정 (필요한 경우) ✅ RADIUS (UDP 1812, 1813) \u0026 TACACS+ (TCP 49) 포트 허용\n🔹 UFW (Ubuntu/Debian)\nsudo ufw allow 1812/udp sudo ufw allow 1813/udp sudo ufw allow 49/tcp sudo ufw reload 🔹 firewalld (CentOS/RHEL)\nsudo firewall-cmd --permanent --add-port=1812/udp sudo firewall-cmd --permanent --add-port=1813/udp sudo firewall-cmd --permanent --add-port=49/tcp sudo firewall-cmd --reload 7️⃣ RADIUS \u0026 TACACS+ 클라이언트 설정 📌 1. RADIUS 클라이언트(네트워크 장비) 설정 예시 Cisco 라우터에서 RADIUS 인증 활성화\nconfigure terminal radius-server host 192.168.1.100 key myradiussecret aaa new-model aaa authentication login default group radius local aaa authorization exec default group radius local exit 📌 2. TACACS+ 클라이언트(네트워크 장비) 설정 예시 Cisco 라우터에서 TACACS+ 인증 활성화\nconfigure terminal tacacs-server host 192.168.1.100 key tacacs_secret aaa new-model aaa authentication login default group tacacs+ local aaa authorization exec default group tacacs+ local exit 8️⃣ 로그 확인 및 문제 해결 📌 1. RADIUS 로그 확인 sudo journalctl -u freeradius -f 📌 2. TACACS+ 로그 확인 sudo tail -f /var/log/tacacs.log 📌 3. 인증 테스트 (RADIUS) radtest testuser password123 127.0.0.1 0 myradiussecret 📌 4. 인증 테스트 (TACACS+) echo \"adminpassword\" | tac_pwd 9️⃣ 결론 🚀 ✅ RADIUS와 TACACS+는 네트워크 및 시스템 인증을 강화하는 중요한 프로토콜입니다.\n✅ RADIUS는 VPN, Wi-Fi 인증에 주로 사용, TACACS+는 Cisco 네트워크 장비 인증에 최적화되어 있습니다.\n✅ 보안이 중요한 환경에서는 TACACS+를, 일반적인 네트워크 인증 환경에서는 RADIUS를 사용하는 것이 좋습니다.\n📚 추가 자료\n1️⃣ FreeRADIUS 공식 문서\n2️⃣ TACACS+ 공식 문서\n3️⃣ Cisco RADIUS \u0026 TACACS+ 설정 가이드\n이제 RADIUS \u0026 TACACS+ 서버도 완벽하게 설정할 수 있어요! 🔥\n추가적으로 원하는 기능이 있다면 언제든지 요청해 주세요. 😃"},"title":"RADIUS/TACACS"},"/system/server/monitoring/syslog/":{"data":{"":"","-syslog-server-구축-및-설정-가이드-#📌 Syslog Server 구축 및 설정 가이드 🚀":"","1-syslog란#1️⃣ Syslog란?":"","2-syslog-서버-설치-linux-기반#2️⃣ Syslog 서버 설치 (Linux 기반)":"","3-syslog-서버-설정#3️⃣ Syslog 서버 설정":"","4-클라이언트에서-syslog-서버로-로그-전송#4️⃣ 클라이언트에서 Syslog 서버로 로그 전송":"","5-syslog-로그-확인#5️⃣ Syslog 로그 확인":"","6-방화벽-설정-필요한-경우#6️⃣ 방화벽 설정 (필요한 경우)":"","7-syslog-로그-분석-및-모니터링-#7️⃣ Syslog 로그 분석 및 모니터링 📊":"","8-고급-기능-elk-스택-연동#8️⃣ 고급 기능 (ELK 스택 연동)":"","9-결론-#9️⃣ 결론 🚀":"📌 Syslog Server 구축 및 설정 가이드 🚀 1️⃣ Syslog란? **Syslog (System Logging Protocol)**은 서버, 네트워크 장비, 애플리케이션 등의 로그를 중앙 집중식으로 수집하는 프로토콜입니다.\n이를 통해 서버 및 네트워크 상태를 모니터링하고, 장애 발생 시 원인을 분석할 수 있습니다.\n✅ Syslog의 주요 기능\n📥 로그 중앙 집중화 – 여러 장치에서 로그를 한 곳에서 관리 🔍 실시간 모니터링 – 로그 분석을 통해 서버 상태 및 보안 감시 🔄 자동화 및 대응 – 특정 이벤트 발생 시 자동 알림 및 조치 💾 장기적인 로그 보관 – 보안 및 감사 목적의 로그 저장 2️⃣ Syslog 서버 설치 (Linux 기반) 🔹 Ubuntu/Debian에서 Rsyslog 설치 sudo apt update sudo apt install rsyslog -y 설치 후, 서비스 활성화 및 확인\nsudo systemctl enable rsyslog sudo systemctl start rsyslog sudo systemctl status rsyslog 🔹 CentOS/RHEL에서 Rsyslog 설치 sudo yum install rsyslog -y 설치 후, 서비스 활성화 및 확인\nsudo systemctl enable rsyslog sudo systemctl start rsyslog sudo systemctl status rsyslog 3️⃣ Syslog 서버 설정 Syslog 설정 파일은 /etc/rsyslog.conf 입니다.\n이 파일을 수정하여 원격 로그 수집 및 저장 방식을 구성할 수 있습니다.\n📌 1. 원격 로그 수집 활성화 /etc/rsyslog.conf 파일을 열어 아래 설정을 추가합니다.\nsudo nano /etc/rsyslog.conf 🔹 UDP(514번 포트)로 로그 수신 설정\n# UDP 포트 514에서 로그 수신 허용 module(load=\"imudp\") input(type=\"imudp\" port=\"514\") 🔹 TCP(514번 포트)로 로그 수신 설정\n# TCP 포트 514에서 로그 수신 허용 module(load=\"imtcp\") input(type=\"imtcp\" port=\"514\") 🔹 로그 파일 저장 경로 설정\n$template RemoteLogs,\"/var/log/remote_logs/%HOSTNAME%/%PROGRAMNAME%.log\" *.* ?RemoteLogs 설정을 저장한 후, rsyslog 서비스를 재시작합니다.\nsudo systemctl restart rsyslog 4️⃣ 클라이언트에서 Syslog 서버로 로그 전송 📌 1. 클라이언트에서 로그 전송 설정 (/etc/rsyslog.conf) 클라이언트 시스템에서 로그를 중앙 Syslog 서버로 전송하려면 아래 설정을 추가합니다.\nsudo nano /etc/rsyslog.conf 🔹 Syslog 서버로 로그 전송 (UDP)\n*.* @192.168.1.100:514 # Syslog 서버 IP와 포트 입력 🔹 Syslog 서버로 로그 전송 (TCP)\n*.* @@192.168.1.100:514 # '@@'는 TCP 전송을 의미 설정을 저장한 후, rsyslog 서비스를 재시작합니다.\nsudo systemctl restart rsyslog 5️⃣ Syslog 로그 확인 📌 1. Syslog 서버에서 수집된 로그 확인 ls /var/log/remote_logs/ 특정 호스트의 로그 확인\ncat /var/log/remote_logs/client-server-name/syslog.log 6️⃣ 방화벽 설정 (필요한 경우) Syslog 서버가 원격에서 로그를 수집하려면 방화벽에서 514/UDP, 514/TCP 포트를 허용해야 합니다.\n✅ UFW 사용 (Ubuntu/Debian)\nsudo ufw allow 514/udp sudo ufw allow 514/tcp sudo ufw reload ✅ firewalld 사용 (CentOS/RHEL)\nsudo firewall-cmd --permanent --add-port=514/udp sudo firewall-cmd --permanent --add-port=514/tcp sudo firewall-cmd --reload 7️⃣ Syslog 로그 분석 및 모니터링 📊 📌 1. 로그 필터링 (grep 사용) cat /var/log/syslog | grep \"error\" 📌 2. 실시간 로그 모니터링 sudo tail -f /var/log/syslog 8️⃣ 고급 기능 (ELK 스택 연동) ✅ Syslog 데이터를 ELK(Elasticsearch + Logstash + Kibana)로 전송하여 실시간 분석 가능\n🔹 Logstash를 사용하여 Syslog 로그를 수집하려면 아래 설정을 추가합니다.\ninput { udp { port =\u003e 514 type =\u003e \"syslog\" } } output { elasticsearch { hosts =\u003e [\"http://localhost:9200\"] index =\u003e \"syslog-%{+YYYY.MM.dd}\" } } 9️⃣ 결론 🚀 Syslog 서버를 구축하면 서버 및 네트워크 장비의 로그를 중앙에서 수집하고 분석할 수 있습니다.\n위의 방법을 따라 설치하고 설정하면 효율적인 로그 관리 시스템을 운영할 수 있습니다.\n로그 분석을 자동화하려면 ELK 스택과 연동하여 대시보드로 시각화하는 것도 고려해 보세요!\n📚 추가 자료\n1️⃣ Rsyslog 공식 문서\n2️⃣ ELK 스택 설정 가이드\n3️⃣ Syslog 로그 분석 사례"},"title":"Syslog"},"/system/server/network/dhcp/":{"data":{"":"","-dhcp-server-guide#📌 DHCP Server Guide":"","1-dhcp란#1️⃣ DHCP란?":"","2-dhcp-서버-설치-linux-기반#2️⃣ DHCP 서버 설치 (Linux 기반)":"","3-dhcp-서버-설정#3️⃣ DHCP 서버 설정":"","4-클라이언트에서-ip-주소-할당-확인#4️⃣ 클라이언트에서 IP 주소 할당 확인":"","5-dhcp-로그-및-문제-해결#5️⃣ DHCP 로그 및 문제 해결":"","6-결론-#6️⃣ 결론 🚀":"📌 DHCP Server Guide 1️⃣ DHCP란? DHCP(Dynamic Host Configuration Protocol) 는 네트워크 장치(클라이언트)에 IP 주소, 서브넷 마스크, 게이트웨이, DNS 서버 등의 네트워크 설정을 자동으로 할당하는 프로토콜입니다.\n✅ DHCP 주요 기능\n📌 IP 주소 자동 할당 – 네트워크에 연결된 장치에 유효한 IP를 자동으로 할당 ⏳ IP 주소 임대(Lease) 관리 – 일정 기간 동안 IP 주소를 유지하고, 만료 시 회수 후 재할당 🌐 네트워크 설정 제공 – 서브넷 마스크, 게이트웨이, DNS 서버 등 추가 설정 제공 🚀 IP 주소 충돌 방지 – 동일한 IP 주소가 중복되지 않도록 관리 2️⃣ DHCP 서버 설치 (Linux 기반) 🔹 Ubuntu/Debian에서 DHCP 서버 설치 sudo apt update sudo apt install isc-dhcp-server -y 🔹 CentOS/RHEL에서 DHCP 서버 설치 sudo yum install dhcp-server -y 설치 후, DHCP 서비스를 자동 시작하도록 설정합니다.\nsudo systemctl enable isc-dhcp-server sudo systemctl start isc-dhcp-server 3️⃣ DHCP 서버 설정 DHCP 서버의 주요 설정 파일은 /etc/dhcp/dhcpd.conf입니다.\n📌 1. 기본 설정 예제 (/etc/dhcp/dhcpd.conf) default-lease-time 600; max-lease-time 7200; subnet 192.168.1.0 netmask 255.255.255.0 { range 192.168.1.100 192.168.1.200; option routers 192.168.1.1; option domain-name-servers 8.8.8.8, 8.8.4.4; option broadcast-address 192.168.1.255; } 🔹 설명\nrange – 클라이언트에게 할당할 IP 주소 범위 option routers – 기본 게이트웨이 주소 option domain-name-servers – 사용할 DNS 서버 📌 2. 고정 IP 할당 (Static IP) 특정 장치(MAC 주소 기준)에 고정된 IP 주소를 할당하려면 다음과 같이 설정합니다.\nhost my-server { hardware ethernet 00:1A:2B:3C:4D:5E; fixed-address 192.168.1.50; } 🔹 설명\nhardware ethernet – 장치의 MAC 주소 fixed-address – 특정 장치에 할당할 고정 IP 주소 📌 3. 설정 적용 및 서비스 재시작 설정을 변경한 후 DHCP 서버를 재시작해야 합니다.\nsudo systemctl restart isc-dhcp-server DHCP 서버가 정상적으로 실행되고 있는지 확인하려면 다음 명령어를 사용합니다.\nsudo systemctl status isc-dhcp-server 4️⃣ 클라이언트에서 IP 주소 할당 확인 DHCP 서버에서 정상적으로 IP 주소를 할당하는지 확인하려면 클라이언트에서 다음 명령어를 실행합니다.\n📌 1. Linux에서 IP 주소 갱신 sudo dhclient -r sudo dhclient 📌 2. Windows에서 IP 주소 갱신 ipconfig /release ipconfig /renew 클라이언트에서 ipconfig 또는 ifconfig를 사용하여 IP 주소를 확인할 수 있습니다.\n5️⃣ DHCP 로그 및 문제 해결 DHCP 서버의 로그를 확인하면 장애가 발생했을 때 원인을 분석할 수 있습니다.\n📌 1. 로그 파일 확인 ✅ Ubuntu/Debian 기반\nsudo tail -f /var/log/syslog | grep dhcp ✅ CentOS/RHEL 기반\nsudo journalctl -u dhcpd -f 📌 2. DHCP 포트 확인 DHCP 서버가 67/UDP 포트에서 정상적으로 실행되고 있는지 확인합니다.\nsudo netstat -tulnp | grep dhcp 📌 3. 방화벽 설정 (필요한 경우) ✅ UFW 사용 (Ubuntu/Debian)\nsudo ufw allow 67/udp sudo ufw reload ✅ firewalld 사용 (CentOS/RHEL)\nsudo firewall-cmd --permanent --add-port=67/udp sudo firewall-cmd --reload 6️⃣ 결론 🚀 DHCP 서버는 네트워크 장치의 IP 주소를 자동으로 관리하는 중요한 역할을 합니다.\n위의 방법을 따라 설치하고 설정하면 효율적으로 DHCP 서버를 운영할 수 있습니다.\n네트워크 환경에 맞게 적절한 설정을 적용하고, 로그를 확인하면서 문제를 해결하세요!\n📚 추가 자료\n1️⃣ DHCP 개념 이해 (Cloudflare)\n2️⃣ Linux DHCP 서버 설정 공식 문서"},"title":"DHCP"},"/system/server/network/dns/":{"data":{"":"","#":"","1-dns의-역할#1️⃣ DNS의 역할":"","2-dns-작동-원리#2️⃣ DNS 작동 원리":"","3-dns-서버-구성-및-설정#3️⃣ DNS 서버 구성 및 설정":"","4-공용-dns-서버#4️⃣ 공용 DNS 서버":"","5-결론#5️⃣ 결론":"📌 DNS Server 개요 DNS(Domain Name System)는 도메인 이름을 IP 주소로 변환해 주는 시스템으로, 인터넷의 전화번호부 역할을 합니다.\n웹사이트를 방문할 때 www.example.com과 같은 도메인을 입력하면, DNS 서버가 해당 도메인에 매핑된 IP 주소를 찾아 연결해 줍니다.\n1️⃣ DNS의 역할 DNS는 사용자가 도메인 이름을 입력하면 해당하는 IP 주소를 반환하여, 브라우저가 해당 서버에 접속할 수 있도록 도와줍니다.\n즉, 사람이 이해하기 쉬운 도메인 이름을 컴퓨터가 이해할 수 있는 IP 주소로 변환하는 기능을 수행합니다.\n🔹 예를 들어, 다음과 같이 동작합니다.\n서비스 도메인 주소 IP 주소 Google google.com 142.250.207.14 Naver naver.com 223.130.200.104 Daum daum.net 203.133.167.81 2️⃣ DNS 작동 원리 웹사이트에 접속할 때, 브라우저가 DNS를 통해 IP 주소를 찾는 과정은 다음과 같습니다.\n1️⃣ 사용자가 www.example.com을 입력하면, 먼저 로컬 DNS 캐시에서 해당 도메인의 IP 주소를 찾습니다.\n2️⃣ 캐시에 없으면 ISP(인터넷 서비스 제공업체)의 DNS 서버에 질의합니다.\n3️⃣ ISP DNS 서버에서도 정보가 없으면 **계층적 DNS 서버(루트 → TLD → 권한 DNS 서버)**를 따라가며 IP 주소를 찾습니다.\n4️⃣ 최종적으로 도메인에 매핑된 IP 주소를 얻어 사용자의 브라우저가 해당 서버에 접속합니다.\n💡 PING 명령어로 DNS 확인하기\nC:\\Users\\mung\u003e ping google.com Ping google.com [142.250.207.14] 32바이트 데이터 사용: 142.250.207.14의 응답: 바이트=32 시간=20ms TTL=52 142.250.207.14의 응답: 바이트=32 시간=25ms TTL=52 위와 같이 특정 도메인에 대한 IP 주소를 확인할 수 있습니다.\n3️⃣ DNS 서버 구성 및 설정 📌 1. DNS 서버 종류 🔹 권한(Authoritative) DNS 서버\n특정 도메인에 대한 정보를 저장하고, 해당 도메인에 대한 공식적인 응답을 제공하는 서버입니다. 🔹 재귀(Recursive) DNS 서버\n사용자의 DNS 질의를 받아 필요한 정보를 찾고 응답하는 역할을 합니다. ISP나 공용 DNS(예: Google Public DNS, Cloudflare 1.1.1.1)에서 제공됩니다. 🔹 캐시(Cache) DNS 서버\n자주 사용되는 도메인의 IP 정보를 캐싱하여, 빠르게 응답하는 역할을 합니다. 📌 2. Linux에서 DNS 서버(Bind) 설치 및 설정 🔹 1. Bind 설치\nsudo apt update sudo apt install bind9 -y 🔹 2. 설정 파일 편집 (/etc/bind/named.conf.local)\nzone \"example.com\" { type master; file \"/etc/bind/db.example.com\"; }; 🔹 3. 존 파일 생성 (/etc/bind/db.example.com)\n$TTL 604800 @ IN SOA example.com. root.example.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800) ; Negative Cache TTL @ IN NS ns.example.com. @ IN A 192.168.1.100 ns IN A 192.168.1.100 🔹 4. Bind 재시작 및 확인\nsudo systemctl restart bind9 nslookup example.com 127.0.0.1 위 설정을 완료하면, example.com 도메인에 대한 DNS 질의가 192.168.1.100 IP 주소로 응답됩니다.\n4️⃣ 공용 DNS 서버 일반적으로 사용되는 공용 DNS 서버는 다음과 같습니다.\n제공업체 기본 DNS 보조 DNS Google Public DNS 8.8.8.8 8.8.4.4 Cloudflare DNS 1.1.1.1 1.0.0.1 OpenDNS 208.67.222.222 208.67.220.220 이러한 공용 DNS를 사용하면 인터넷 속도가 개선될 수 있습니다.\n5️⃣ 결론 DNS 서버는 인터넷을 사용할 때 필수적인 역할을 수행하는 중요한 인프라입니다.\n특정 환경에서는 자체 DNS 서버를 구축하여 내부 네트워크에서 빠르고 안정적인 네임 서비스를 제공할 수도 있습니다.\n이번 글에서는 DNS의 개념, 동작 방식, 그리고 직접 DNS 서버를 설정하는 방법을 설명했습니다.\n이제 직접 실습해보면서 DNS에 대해 더 깊이 이해해 보세요! 🚀\n📌 참고자료 1️⃣ Cloudflare DNS 설명\n2️⃣ DNS 개념 정리"},"title":"DNS"},"/system/server/network/ftp/":{"data":{"-ftp--sftp-server-구축-및-설정-가이드-#📌 FTP \u0026amp; SFTP Server 구축 및 설정 가이드 🚀":"","1-ftp--sftp란#1️⃣ FTP \u0026amp; SFTP란?":"","2-ftp-서버-설치-linux-기반#2️⃣ FTP 서버 설치 (Linux 기반)":"","3-ftp-서버-설정#3️⃣ FTP 서버 설정":"","4-sftp-서버-설정-openssh-기반#4️⃣ SFTP 서버 설정 (OpenSSH 기반)":"","5-사용자-계정-생성-및-권한-설정#5️⃣ 사용자 계정 생성 및 권한 설정":"","6-방화벽-설정-필요한-경우#6️⃣ 방화벽 설정 (필요한 경우)":"","7-ftpsftp-클라이언트-접속-방법#7️⃣ FTP/SFTP 클라이언트 접속 방법":"","8-ftpsftp-로그-확인-및-문제-해결#8️⃣ FTP/SFTP 로그 확인 및 문제 해결":"","9-결론-#9️⃣ 결론 🚀":"📌 FTP \u0026 SFTP Server 구축 및 설정 가이드 🚀 1️⃣ FTP \u0026 SFTP란? **FTP (File Transfer Protocol)**는 네트워크를 통해 파일을 전송하는 프로토콜입니다.\n**SFTP (SSH File Transfer Protocol)**는 SSH 기반의 보안 파일 전송 프로토콜로, FTP보다 보안성이 뛰어납니다.\n✅ FTP와 SFTP의 차이점\n프로토콜 보안성 포트 인증 방식 특징 FTP ❌ 보안 없음 21 (기본) 사용자 ID/PW 암호화 없이 파일 전송 SFTP ✅ 암호화 지원 22 (SSH) SSH Key 또는 PW 보안성이 뛰어난 파일 전송 2️⃣ FTP 서버 설치 (Linux 기반) 🔹 Ubuntu/Debian에서 VSFTPD 설치 sudo apt update sudo apt install vsftpd -y 🔹 CentOS/RHEL에서 VSFTPD 설치 sudo yum install vsftpd -y 설치 후, 서비스 활성화 및 상태 확인\nsudo systemctl enable vsftpd sudo systemctl start vsftpd sudo systemctl status vsftpd 3️⃣ FTP 서버 설정 FTP 설정 파일은 /etc/vsftpd.conf 입니다.\n이 파일을 수정하여 익명 접속, 사용자 인증, 보안 설정을 구성할 수 있습니다.\nsudo nano /etc/vsftpd.conf 📌 1. 익명 접속 비활성화 anonymous_enable=NO 📌 2. 로컬 사용자 로그인 허용 local_enable=YES write_enable=YES 📌 3. FTP 전송 시 암호화 활성화 (TLS) ssl_enable=YES rsa_cert_file=/etc/ssl/private/vsftpd.pem 설정을 저장한 후, FTP 서비스를 재시작합니다.\nsudo systemctl restart vsftpd 4️⃣ SFTP 서버 설정 (OpenSSH 기반) SFTP는 SSH와 함께 제공되므로 별도 설치가 필요하지 않습니다.\n✅ SFTP 사용자 전용 설정 (/etc/ssh/sshd_config)\nsudo nano /etc/ssh/sshd_config 🔹 SFTP 전용 사용자 그룹 생성 및 제한\nSubsystem sftp internal-sftp Match Group sftpusers ChrootDirectory /sftp ForceCommand internal-sftp AllowTcpForwarding no X11Forwarding no 설정을 저장한 후, SSH 서비스를 재시작합니다.\nsudo systemctl restart sshd 5️⃣ 사용자 계정 생성 및 권한 설정 📌 1. FTP 사용자 생성 sudo useradd -m -s /sbin/nologin ftpuser sudo passwd ftpuser 📌 2. SFTP 사용자 생성 sudo useradd -m -s /sbin/nologin sftpuser sudo passwd sftpuser sudo usermod -aG sftpusers sftpuser 📌 3. SFTP 디렉토리 권한 설정 sudo mkdir -p /sftp/uploads sudo chown root:sftpusers /sftp sudo chmod 755 /sftp sudo chown sftpuser:sftpusers /sftp/uploads sudo chmod 700 /sftp/uploads 6️⃣ 방화벽 설정 (필요한 경우) ✅ FTP 포트(21) 및 SFTP 포트(22) 허용\n🔹 UFW (Ubuntu/Debian)\nsudo ufw allow 21/tcp sudo ufw allow 22/tcp sudo ufw reload 🔹 firewalld (CentOS/RHEL)\nsudo firewall-cmd --permanent --add-port=21/tcp sudo firewall-cmd --permanent --add-port=22/tcp sudo firewall-cmd --reload 7️⃣ FTP/SFTP 클라이언트 접속 방법 📌 1. FTP 접속 (리눅스) ftp 192.168.1.100 📌 2. SFTP 접속 (리눅스) sftp sftpuser@192.168.1.100 📌 3. FTP 클라이언트 사용 (Windows) ✅ FileZilla 또는 WinSCP를 사용하여 FTP/SFTP 접속 가능\nFTP 접속 정보\n호스트: 192.168.1.100 포트: 21 사용자: ftpuser 프로토콜: FTP SFTP 접속 정보\n호스트: 192.168.1.100 포트: 22 사용자: sftpuser 프로토콜: SFTP (SSH) 8️⃣ FTP/SFTP 로그 확인 및 문제 해결 📌 1. FTP 로그 확인 sudo cat /var/log/vsftpd.log 📌 2. SFTP 접속 로그 확인 sudo journalctl -u sshd | grep sftp 📌 3. 실시간 로그 모니터링 sudo tail -f /var/log/auth.log 9️⃣ 결론 🚀 FTP와 SFTP 서버를 구축하면 파일을 안전하게 전송하고 공유할 수 있습니다.\nSFTP는 보안성이 뛰어나므로 보안이 중요한 환경에서는 SFTP를 사용하는 것이 좋습니다.\n📚 추가 자료\n1️⃣ VSFTPD 공식 문서\n2️⃣ OpenSSH SFTP 설정 가이드\n3️⃣ FileZilla 사용법"},"title":"FTP/TFTP"},"/system/server/network/loadbalancer/":{"data":{"-load-balancer-로드-밸런서-구축-가이드-#⚖️ \u003cstrong\u003eLoad Balancer (로드 밸런서) 구축 가이드\u003c/strong\u003e 🚀":"","-결론-#🔟 결론 🚀":"⚖️ Load Balancer (로드 밸런서) 구축 가이드 🚀 1️⃣ Load Balancer란? 로드 밸런서(Load Balancer) 는 여러 서버로 트래픽을 분산하여 부하를 분산시키고 가용성을 높이는 기술입니다.\n웹 서비스, 데이터베이스, API 서버 등 다양한 환경에서 사용됩니다.\n✅ 로드 밸런서의 주요 기능\n트래픽 분산 → 서버 부하 방지 및 성능 향상 고가용성(HA) → 장애 발생 시 자동으로 다른 서버로 트래픽 전달 SSL 종료(Offloading) → HTTPS 트래픽의 암호화/복호화 부담을 감소 세션 유지(Sticky Session) → 특정 사용자가 동일한 서버로 접속 유지 2️⃣ 로드 밸런서 유형 유형 설명 대표 솔루션 L4 (Layer 4) LB IP 및 포트 기반 트래픽 분산 HAProxy, Nginx, LVS L7 (Layer 7) LB HTTP/HTTPS 기반 트래픽 분산 Nginx, Traefik, Envoy DNS 기반 LB 도메인 수준 트래픽 분산 Route 53, Cloudflare 클라우드 LB 클라우드 제공 로드 밸런서 AWS ALB, GCP LB 3️⃣ 로드 밸런서 솔루션 비교 솔루션 특징 추천 환경 Nginx L7 지원, SSL 종료 가능 웹 서비스 HAProxy 고성능 L4/L7 지원 API 서버, 데이터베이스 LVS (IPVS) 커널 기반 L4 LB 대규모 트래픽 환경 Traefik 컨테이너 친화적 Kubernetes, Docker Envoy 마이크로서비스 아키텍처 서비스 메시 4️⃣ HAProxy 기반 L4 로드 밸런서 구축 (Ubuntu 22.04) 🔹 1. HAProxy 설치 sudo apt update sudo apt install haproxy -y 🔹 2. HAProxy 설정 (/etc/haproxy/haproxy.cfg) frontend http_front bind *:80 default_backend web_servers backend web_servers balance roundrobin server web1 192.168.1.10:80 check server web2 192.168.1.11:80 check 🔹 3. HAProxy 재시작 및 확인 sudo systemctl restart haproxy sudo systemctl enable haproxy haproxy -c -f /etc/haproxy/haproxy.cfg 5️⃣ Nginx 기반 L7 로드 밸런서 구축 🔹 1. Nginx 설치 sudo apt update sudo apt install nginx -y 🔹 2. Nginx 설정 (/etc/nginx/nginx.conf) http { upstream backend { server 192.168.1.10; server 192.168.1.11; } server { listen 80; location / { proxy_pass http://backend; } } } 🔹 3. Nginx 재시작 및 확인 sudo systemctl restart nginx sudo systemctl enable nginx 6️⃣ 클라우드 로드 밸런서 설정 (AWS ALB 예시) 🔹 1. AWS ALB 생성 1️⃣ AWS 콘솔 → EC2 → 로드 밸런서 → 새 ALB 생성\n2️⃣ 타겟 그룹 추가 (서버 리스트 등록)\n3️⃣ 리스너 설정 (HTTP/HTTPS 트래픽 처리)\n4️⃣ 보안 그룹 및 라우팅 설정 후 생성\n🔹 2. AWS ALB 상태 확인 aws elbv2 describe-load-balancers 7️⃣ 로드 밸런서 상태 확인 및 문제 해결 ✅ HAProxy 상태 확인\nsudo systemctl status haproxy haproxy -c -f /etc/haproxy/haproxy.cfg ✅ Nginx 상태 확인\nsudo systemctl status nginx nginx -t ✅ 활성화된 백엔드 서버 확인 (HAProxy)\necho \"show stat\" | sudo socat /var/lib/haproxy/stats stdio ✅ 로드 밸런서 포트 확인\nsudo netstat -tulnp | grep LISTEN 🔟 결론 🚀 ✅ 로드 밸런서는 서비스의 가용성과 성능을 높이는 핵심 기술\n✅ L4(LVS, HAProxy) / L7(Nginx, Traefik) 등 환경에 맞는 솔루션 선택\n✅ 클라우드 환경에서는 AWS ALB, GCP LB 활용 가능\n📚 추가 자료\n1️⃣ HAProxy 공식 문서\n2️⃣ Nginx 로드 밸런싱\n3️⃣ AWS ALB 개요","1-load-balancer란#1️⃣ Load Balancer란?":"","2-로드-밸런서-유형#2️⃣ 로드 밸런서 유형":"","3-로드-밸런서-솔루션-비교#3️⃣ 로드 밸런서 솔루션 비교":"","4-haproxy-기반-l4-로드-밸런서-구축-ubuntu-2204#4️⃣ HAProxy 기반 L4 로드 밸런서 구축 (Ubuntu 22.04)":"","5-nginx-기반-l7-로드-밸런서-구축#5️⃣ Nginx 기반 L7 로드 밸런서 구축":"","6-클라우드-로드-밸런서-설정-aws-alb-예시#6️⃣ 클라우드 로드 밸런서 설정 (AWS ALB 예시)":"","7-로드-밸런서-상태-확인-및-문제-해결#7️⃣ 로드 밸런서 상태 확인 및 문제 해결":""},"title":"Load Balancer"},"/system/server/network/ntp/":{"data":{"-ntp-network-time-protocol-서버-구축-및-설정-가이드-#⏰ \u003cstrong\u003eNTP (Network Time Protocol) 서버 구축 및 설정 가이드\u003c/strong\u003e 🚀":"","1-ntp란#1️⃣ NTP란?":"","2-ntp-서버-설치-linux-기반#2️⃣ NTP 서버 설치 (Linux 기반)":"","3-ntp-서버-설정#3️⃣ NTP 서버 설정":"","4-ntp-방화벽-설정-필요한-경우#4️⃣ NTP 방화벽 설정 (필요한 경우)":"","5-ntp-클라이언트-설정#5️⃣ NTP 클라이언트 설정":"","6-로그-확인-및-문제-해결#6️⃣ 로그 확인 및 문제 해결":"","7-결론-#7️⃣ 결론 🚀":"⏰ NTP (Network Time Protocol) 서버 구축 및 설정 가이드 🚀 1️⃣ NTP란? **NTP (Network Time Protocol)**는 네트워크 상의 시계를 동기화하는 프로토콜입니다.\n인터넷 및 로컬 네트워크에서 서버와 클라이언트의 시간을 자동으로 조정하여 시스템 간의 시간 차이를 최소화하는 역할을 합니다.\n✅ NTP의 주요 기능\n정확한 시간 동기화: 인터넷의 표준 타임 서버와 동기화 자동 조정: 시스템 시간이 틀어지면 자동으로 보정 분산 네트워크 지원: 계층적으로 NTP 서버를 구성하여 로드 밸런싱 가능 로그 및 보안성 강화: 시간 정보가 정확해야 보안 로그, 인증 시스템이 올바르게 동작 2️⃣ NTP 서버 설치 (Linux 기반) 🔹 Ubuntu/Debian에서 NTP 서버 설치 sudo apt update sudo apt install ntp -y 🔹 CentOS/RHEL에서 NTP 서버 설치 sudo yum install ntp -y 설치 후, NTP 서비스 활성화 및 상태 확인\nsudo systemctl enable ntpd sudo systemctl start ntpd sudo systemctl status ntpd 3️⃣ NTP 서버 설정 NTP의 주요 설정 파일은 /etc/ntp.conf 입니다.\nsudo nano /etc/ntp.conf 📌 1. 표준 타임 서버 지정 (공용 NTP 서버 사용) server 0.pool.ntp.org iburst server 1.pool.ntp.org iburst server 2.pool.ntp.org iburst server 3.pool.ntp.org iburst 💡 iburst 옵션: 초기 동기화를 빠르게 수행\n📌 2. 내부 네트워크 NTP 클라이언트 허용 내부 네트워크의 클라이언트들이 이 서버를 NTP 서버로 사용할 수 있도록 설정합니다.\nrestrict 192.168.1.0 mask 255.255.255.0 nomodify notrap 📌 3. 설정 적용 및 서비스 재시작 sudo systemctl restart ntpd 4️⃣ NTP 방화벽 설정 (필요한 경우) ✅ UDP 123번 포트 개방 (NTP 통신 포트)\n🔹 UFW (Ubuntu/Debian)\nsudo ufw allow 123/udp sudo ufw reload 🔹 firewalld (CentOS/RHEL)\nsudo firewall-cmd --permanent --add-port=123/udp sudo firewall-cmd --reload 5️⃣ NTP 클라이언트 설정 NTP 서버가 정상적으로 동작하는지 확인하기 위해 클라이언트에서 NTP 설정을 진행합니다.\n📌 1. Linux 클라이언트에서 NTP 서버 설정 sudo nano /etc/ntp.conf 다음 내용을 추가하여 내부 NTP 서버를 지정합니다.\nserver 192.168.1.100 prefer 설정을 저장한 후 NTP 서비스를 재시작합니다.\nsudo systemctl restart ntpd NTP 상태 확인\nntpq -p 📌 2. Windows 클라이언트에서 NTP 서버 설정 제어판 → 날짜 및 시간 → 인터넷 시간 탭으로 이동 설정 변경 버튼 클릭 서버에 192.168.1.100 입력 후 지금 업데이트 클릭 ✅ 이제 Windows PC도 NTP 서버와 동기화됩니다!\n6️⃣ 로그 확인 및 문제 해결 📌 1. NTP 동기화 상태 확인 ntpq -p 📌 2. NTP 로그 확인 Ubuntu/Debian\nsudo journalctl -u ntp -f CentOS/RHEL\nsudo tail -f /var/log/messages | grep ntp 📌 3. NTP 시간 강제 동기화 (필요한 경우) sudo ntpdate -q 192.168.1.100 📌 4. NTP 포트 확인 sudo netstat -tulnp | grep ntp 7️⃣ 결론 🚀 ✅ NTP 서버를 구성하면 네트워크 내 모든 장치의 시간이 동기화되어 로그 분석, 보안 및 시스템 관리가 편리해집니다.\n✅ 인터넷의 공용 NTP 서버 또는 내부 NTP 서버를 활용하여 시간 동기화를 최적화할 수 있습니다.\n✅ 클라이언트 설정을 통해 Linux 및 Windows에서도 쉽게 동기화가 가능합니다.\n📚 추가 자료\n1️⃣ NTP 공식 문서\n2️⃣ Ubuntu NTP 설정 가이드\n3️⃣ Windows NTP 설정 가이드"},"title":"NTP"},"/system/server/network/proxy/":{"data":{"-프록시-서버-proxy-server-구축-가이드-#🌐 \u003cstrong\u003e프록시 서버 (Proxy Server) 구축 가이드\u003c/strong\u003e 🚀":"","1-프록시-서버란#1️⃣ 프록시 서버란?":"","2-프록시-서버의-종류#2️⃣ 프록시 서버의 종류":"","3-프록시-서버-구축-nginx#3️⃣ 프록시 서버 구축 (Nginx)":"","4-프록시-서버-설정#4️⃣ 프록시 서버 설정":"","5-클라이언트에서-프록시-서버-사용하기#5️⃣ 클라이언트에서 프록시 서버 사용하기":"","6-방화벽-설정-필요한-경우#6️⃣ 방화벽 설정 (필요한 경우)":"","7-프록시-서버-로그-확인-및-문제-해결#7️⃣ 프록시 서버 로그 확인 및 문제 해결":"","8-프록시-서버-배포-docker-사용#8️⃣ 프록시 서버 배포 (Docker 사용)":"","9-결론-#9️⃣ 결론 🚀":"🌐 프록시 서버 (Proxy Server) 구축 가이드 🚀 1️⃣ 프록시 서버란? 프록시 서버(Proxy Server) 는 클라이언트와 인터넷 사이에서 중계 역할을 하는 서버입니다.\n클라이언트가 인터넷에 직접 연결되지 않고 프록시를 통해 요청을 보내므로 보안, 성능, 접근 제어 등의 이점을 제공합니다.\n✅ 프록시 서버의 주요 기능\n익명성 제공 → 클라이언트의 IP 주소를 숨김 캐싱(Cache) 기능 → 자주 요청되는 데이터를 저장하여 빠르게 제공 보안 강화 → 악성 웹사이트 접근 차단 및 트래픽 필터링 콘텐츠 필터링 → 특정 웹사이트 및 서비스 접근 제한 가능 부하 분산 → 여러 서버로 트래픽을 분산하여 성능 최적화 2️⃣ 프록시 서버의 종류 유형 설명 정방향 프록시 (Forward Proxy) 클라이언트가 인터넷에 접속할 때 중개하는 프록시 역방향 프록시 (Reverse Proxy) 인터넷에서 들어오는 요청을 백엔드 서버로 전달 투명 프록시 (Transparent Proxy) 클라이언트가 인식하지 못하는 프록시 캐싱 프록시 (Caching Proxy) 요청된 데이터를 저장하여 빠르게 제공 웹 필터링 프록시 특정 사이트 차단, 콘텐츠 필터링 수행 3️⃣ 프록시 서버 구축 (Nginx) Nginx를 사용하여 역방향 프록시(Reverse Proxy) 서버를 구축해보겠습니다.\n🔹 1. Nginx 설치 Ubuntu/Debian 기반:\nsudo apt update sudo apt install nginx -y CentOS/RHEL 기반:\nsudo yum install epel-release -y sudo yum install nginx -y 설치 후 서비스 시작:\nsudo systemctl start nginx sudo systemctl enable nginx 4️⃣ 프록시 서버 설정 🔹 1. Nginx 프록시 설정 파일 생성 sudo nano /etc/nginx/sites-available/proxy.conf 📄 /etc/nginx/sites-available/proxy.conf 예제\nserver { listen 80; server_name proxy.example.com; location / { proxy_pass http://backend-server-ip:8080; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } 🔹 2. 설정 적용 및 Nginx 재시작 설정 파일을 sites-enabled 디렉터리에 심볼릭 링크 추가\nsudo ln -s /etc/nginx/sites-available/proxy.conf /etc/nginx/sites-enabled/ 설정 테스트 후 적용\nsudo nginx -t sudo systemctl restart nginx 5️⃣ 클라이언트에서 프록시 서버 사용하기 클라이언트에서 프록시 서버를 사용하려면 브라우저 또는 터미널에서 설정해야 합니다.\n🔹 1. Linux CLI에서 프록시 사용 export http_proxy=\"http://proxy.example.com:80\" export https_proxy=\"http://proxy.example.com:80\" 🔹 2. Windows에서 프록시 설정 제어판 → 네트워크 및 인터넷 → 인터넷 옵션 → 연결 → LAN 설정 프록시 서버 사용 체크 후 proxy.example.com 및 포트 입력 6️⃣ 방화벽 설정 (필요한 경우) Ubuntu/Debian (UFW 사용):\nsudo ufw allow 80/tcp sudo ufw allow 443/tcp sudo ufw enable CentOS/RHEL (firewalld 사용):\nsudo firewall-cmd --permanent --add-service=http sudo firewall-cmd --permanent --add-service=https sudo firewall-cmd --reload 7️⃣ 프록시 서버 로그 확인 및 문제 해결 🔹 1. 로그 파일 확인 sudo tail -f /var/log/nginx/access.log sudo tail -f /var/log/nginx/error.log 🔹 2. Nginx 상태 확인 sudo systemctl status nginx 🔹 3. 포트 사용 여부 확인 netstat -tulnp | grep 80 8️⃣ 프록시 서버 배포 (Docker 사용) 프록시 서버를 컨테이너화하려면 Nginx Docker 이미지를 사용할 수 있습니다.\n🔹 1. Docker 컨테이너 실행 docker run -d --name nginx-proxy -p 80:80 -v /path/to/proxy.conf:/etc/nginx/nginx.conf:ro nginx 🔹 2. Docker Compose 파일 예제 📄 docker-compose.yml\nversion: \"3\" services: proxy: image: nginx ports: - \"80:80\" volumes: - ./proxy.conf:/etc/nginx/nginx.conf:ro restart: always 컨테이너 실행:\ndocker-compose up -d 9️⃣ 결론 🚀 ✅ 프록시 서버는 보안, 속도 향상, 부하 분산 등 다양한 용도로 활용 가능\n✅ Nginx를 사용하여 간단한 역방향 프록시 서버 구축 가능\n✅ 필요에 따라 방화벽 설정, 로그 분석 및 Docker 배포 가능\n📚 추가 자료\n1️⃣ Nginx Reverse Proxy 공식 문서\n2️⃣ 프록시 서버 개념과 종류\n3️⃣ Docker + Nginx 프록시 설정"},"title":"Proxy"},"/system/server/network/vpn/":{"data":{"-vpn-virtual-private-network-구축-가이드-#🔒 \u003cstrong\u003eVPN (Virtual Private Network) 구축 가이드\u003c/strong\u003e 🚀":"","-결론-#🔟 결론 🚀":"🔒 VPN (Virtual Private Network) 구축 가이드 🚀 1️⃣ VPN이란? VPN(Virtual Private Network) 은 공용 네트워크(인터넷)를 통해 암호화된 안전한 연결을 제공하는 기술입니다.\n이를 통해 원격 사용자가 내부 네트워크에 안전하게 접속할 수 있으며, IP 우회 및 보안 강화에도 활용됩니다.\n✅ VPN의 주요 기능\n데이터 암호화 → 안전한 통신을 보장 원격 접속 → 내부 네트워크에 외부에서 접근 가능 IP 주소 숨김 → 보안 및 프라이버시 보호 방화벽 우회 → 특정 지역에서 차단된 서비스 접근 가능 2️⃣ VPN의 유형 유형 설명 Site-to-Site VPN 두 개 이상의 네트워크를 VPN 터널로 연결 Remote Access VPN 개별 사용자가 기업 네트워크에 안전하게 접속 SSL VPN 웹 브라우저를 통한 보안 연결 (예: OpenVPN, AnyConnect) IPsec VPN 강력한 보안 프로토콜 기반 VPN (예: StrongSwan) WireGuard VPN 최신 경량화 VPN 기술 (빠르고 보안성 우수) 3️⃣ VPN 솔루션 비교 VPN 솔루션 설명 특징 OpenVPN 가장 널리 사용되는 오픈소스 VPN 강력한 암호화, 다수 플랫폼 지원 WireGuard 최신 VPN 기술, 성능 우수 빠르고 경량, 설정 간편 IPsec (StrongSwan) 기업 환경에서 주로 사용 네이티브 지원, 강력한 보안 L2TP/IPsec 기본 제공 VPN 프로토콜 비교적 설정이 쉬움 SoftEther VPN 다목적 VPN 솔루션 다양한 VPN 프로토콜 지원 4️⃣ OpenVPN 서버 구축 (Ubuntu 22.04) 🔹 1. OpenVPN 설치 sudo apt update sudo apt install openvpn easy-rsa -y 🔹 2. 인증서 및 키 생성 make-cadir ~/openvpn-ca cd ~/openvpn-ca source vars ./clean-all ./build-ca ./build-key-server server ./build-dh ./build-key client 🔹 3. 서버 설정 OpenVPN 설정 파일 편집\nsudo nano /etc/openvpn/server.conf port 1194 proto udp dev tun ca ca.crt cert server.crt key server.key dh dh2048.pem cipher AES-256-CBC auth SHA256 keepalive 10 120 user nobody group nogroup persist-key persist-tun status /var/log/openvpn-status.log verb 3 🔹 4. OpenVPN 서비스 시작 sudo systemctl start openvpn@server sudo systemctl enable openvpn@server 5️⃣ WireGuard VPN 구축 🔹 1. WireGuard 설치 sudo apt update sudo apt install wireguard -y 🔹 2. 키 생성 wg genkey | tee privatekey | wg pubkey \u003e publickey 🔹 3. 서버 설정 sudo nano /etc/wireguard/wg0.conf [Interface] PrivateKey = \u003c서버의 개인키\u003e Address = 10.0.0.1/24 ListenPort = 51820 [Peer] PublicKey = \u003c클라이언트의 공개키\u003e AllowedIPs = 10.0.0.2/32 🔹 4. WireGuard 서비스 시작 sudo systemctl start wg-quick@wg0 sudo systemctl enable wg-quick@wg0 6️⃣ VPN 클라이언트 설정 🔹 1. OpenVPN 클라이언트 설정 1️⃣ client.ovpn 파일 생성\n2️⃣ VPN 접속\nsudo openvpn --config client.ovpn 🔹 2. WireGuard 클라이언트 설정 1️⃣ wg0.conf 생성\n[Interface] PrivateKey = \u003c클라이언트의 개인키\u003e Address = 10.0.0.2/24 [Peer] PublicKey = \u003c서버의 공개키\u003e Endpoint = \u003cVPN 서버 IP\u003e:51820 AllowedIPs = 0.0.0.0/0 2️⃣ VPN 접속\nsudo wg-quick up wg0 7️⃣ VPN 방화벽 및 포트 설정 ✅ OpenVPN 포트 열기\nsudo ufw allow 1194/udp ✅ WireGuard 포트 열기\nsudo ufw allow 51820/udp ✅ IP Forwarding 활성화\necho \"net.ipv4.ip_forward = 1\" | sudo tee -a /etc/sysctl.conf sudo sysctl -p 8️⃣ VPN 상태 확인 및 문제 해결 ✅ OpenVPN 상태 확인\nsudo systemctl status openvpn@server ✅ WireGuard 상태 확인\nsudo wg show ✅ 로그 확인\nsudo journalctl -u openvpn@server -f ✅ 클라이언트 접속 여부 확인\nsudo wg 🔟 결론 🚀 ✅ VPN은 보안이 중요한 네트워크 환경에서 필수적인 기술\n✅ OpenVPN, WireGuard, IPsec 등 다양한 솔루션을 활용 가능\n✅ 보안 설정 및 암호화를 철저히 적용하여 안전한 VPN 운영\n📚 추가 자료\n1️⃣ OpenVPN 공식 문서\n2️⃣ WireGuard 공식 사이트\n3️⃣ StrongSwan IPsec VPN","1-vpn이란#1️⃣ VPN이란?":"","2-vpn의-유형#2️⃣ VPN의 유형":"","3-vpn-솔루션-비교#3️⃣ VPN 솔루션 비교":"","4-openvpn-서버-구축-ubuntu-2204#4️⃣ OpenVPN 서버 구축 (Ubuntu 22.04)":"","5-wireguard-vpn-구축#5️⃣ WireGuard VPN 구축":"","6-vpn-클라이언트-설정#6️⃣ VPN 클라이언트 설정":"","7-vpn-방화벽-및-포트-설정#7️⃣ VPN 방화벽 및 포트 설정":"","8-vpn-상태-확인-및-문제-해결#8️⃣ VPN 상태 확인 및 문제 해결":""},"title":"VPN"},"/system/server/os/centos/":{"data":{"":"","1-centos란-#1. CentOS란? 🖥️🐧":"","2-centos의-특징-#2. CentOS의 특징 🌟":"","3-centos-설치-방법-#3. CentOS 설치 방법 💿":"","4-centos-기본-설정-#4. CentOS 기본 설정 ⚙️":"","5-centos-주요-패키지-및-서비스-#5. CentOS 주요 패키지 및 서비스 📦":"","6-centos-활용-예제-#6. CentOS 활용 예제 🔥":"","7-centos-관리-및-문제-해결-#7. CentOS 관리 및 문제 해결 🛠️":"","8-결론-#8. 결론 🎯":"1. CentOS란? 🖥️🐧 CentOS(CentOS Stream 포함)는 Red Hat Enterprise Linux(RHEL) 기반의 무료 오픈소스 운영체제로, RHEL과 거의 동일한 기능을 제공하지만, 상용 지원 없이 커뮤니티가 유지 및 관리합니다.\nCentOS는 서버 환경에서 많이 사용되며, 웹 서버, 데이터베이스 서버, 네트워크 장비, 클러스터링 환경 등 다양한 용도로 활용됩니다.\n2. CentOS의 특징 🌟 ✅ RHEL과 높은 호환성 – Red Hat Enterprise Linux(RHEL) 기반\n✅ 무료로 사용 가능 – 기업 환경에서도 추가 비용 없이 사용 가능\n✅ 안정성과 보안성 – 커뮤니티 기반으로 보안 패치 및 업데이트 제공\n✅ 서버 및 클라우드 최적화 – 가상화, 컨테이너, 클라우드 환경에서 많이 사용\n✅ 패키지 관리 시스템 – yum 또는 dnf를 이용한 패키지 관리\n3. CentOS 설치 방법 💿 3.1 CentOS 다운로드 및 설치 1️⃣ 공식 사이트에서 CentOS Stream 또는 CentOS 7/8 ISO 다운로드\n2️⃣ 부팅 가능한 USB 또는 가상 머신에서 ISO 마운트\n3️⃣ 설치 화면에서 “Install CentOS” 선택\n4️⃣ 네트워크 설정 및 파티션 설정 후 설치 진행\n5️⃣ 설치 완료 후 root 비밀번호 설정\n4. CentOS 기본 설정 ⚙️ 4.1 패키지 업데이트 sudo yum update -y # CentOS 7 sudo dnf update -y # CentOS 8/Stream 4.2 방화벽 설정 sudo systemctl start firewalld sudo systemctl enable firewalld 4.3 사용자 계정 추가 sudo useradd newuser sudo passwd newuser 4.4 SELinux 상태 확인 sestatus 5. CentOS 주요 패키지 및 서비스 📦 서비스 패키지 설명 웹 서버 httpd Apache 웹 서버 데이터베이스 mariadb-server MariaDB 데이터베이스 SSH openssh-server 원격 접속을 위한 SSH 서비스 방화벽 firewalld 네트워크 보안을 위한 방화벽 6. CentOS 활용 예제 🔥 6.1 Apache 웹 서버 설치 sudo yum install httpd -y # CentOS 7 sudo dnf install httpd -y # CentOS 8/Stream sudo systemctl start httpd sudo systemctl enable httpd 6.2 MariaDB 설치 및 설정 sudo yum install mariadb-server -y sudo systemctl start mariadb sudo mysql_secure_installation 6.3 방화벽에서 HTTP/HTTPS 열기 sudo firewall-cmd --add-service=http --permanent sudo firewall-cmd --add-service=https --permanent sudo firewall-cmd --reload 7. CentOS 관리 및 문제 해결 🛠️ 7.1 시스템 로그 확인 sudo journalctl -xe 7.2 네트워크 상태 확인 ip a 7.3 서비스 상태 확인 sudo systemctl status \u003c서비스명\u003e 8. 결론 🎯 CentOS는 무료이면서도 RHEL과 호환성이 높은 강력한 서버용 운영체제입니다.\n웹 서버, 데이터베이스 서버, 클라우드 인프라 등 다양한 환경에서 활용되며, yum 또는 dnf를 사용하여 패키지를 쉽게 관리할 수 있습니다.\n최신 CentOS Stream은 롤링 업데이트 방식으로 제공되므로, 장기 지원이 필요한 경우 Rocky Linux 또는 AlmaLinux를 고려할 수도 있습니다.\n더 많은 정보는 공식 문서를 참고하세요. 🚀"},"title":"CentOS"},"/system/server/os/ubuntu/":{"data":{"-ubuntu-linux-운영체제-가이드-#🐧 \u003cstrong\u003eUbuntu Linux 운영체제 가이드\u003c/strong\u003e 🚀":"","-결론-#🔟 결론 🚀":"🐧 Ubuntu Linux 운영체제 가이드 🚀 1️⃣ Ubuntu란? Ubuntu 는 Debian 기반의 오픈소스 Linux 배포판 으로, 사용자 친화적인 인터페이스와 강력한 커뮤니티 지원을 제공하는 운영체제(OS)입니다.\nCanonical사가 개발 및 유지보수하며, 데스크톱, 서버, 클라우드, IoT 환경에서 널리 사용됩니다.\n✅ Ubuntu의 주요 특징\n무료 오픈소스 운영체제 → 누구나 자유롭게 다운로드 및 사용 가능 LTS(Long Term Support) 지원 → 5년간 보안 및 패키지 업데이트 제공 광범위한 패키지 지원 → apt 패키지 관리 시스템 제공 보안 강화 → AppArmor, 자동 보안 패치 기능 내장 클라우드 친화적 → AWS, Azure, Google Cloud 등과 완벽 호환 2️⃣ Ubuntu 에디션 비교 에디션 용도 특징 Ubuntu Desktop 개인용 GUI 기반, 사용이 쉬움 Ubuntu Server 서버용 최소한의 패키지로 경량 운영 Ubuntu Core IoT, 임베디드 컨테이너 기반 보안 강화 Ubuntu Cloud 클라우드 OpenStack, Kubernetes 지원 3️⃣ Ubuntu 설치 방법 🔹 1. Ubuntu 다운로드 Ubuntu 공식 사이트 에서 원하는 버전 다운로드\nUbuntu LTS (권장) → 장기 지원 안정 버전 최신 버전 → 최신 기능 포함, 하지만 지원 기간 짧음 🔹 2. 부팅 USB 만들기 (Windows 기준) # Rufus 또는 balenaEtcher 사용하여 USB에 Ubuntu ISO 굽기 🔹 3. Ubuntu 설치 과정 1️⃣ 언어 선택 및 네트워크 설정\n2️⃣ 설치 유형 선택 (기본 설치 / 최소 설치)\n3️⃣ 디스크 파티션 설정\n4️⃣ 사용자 계정 및 비밀번호 생성\n5️⃣ 설치 완료 후 재부팅\n4️⃣ Ubuntu 기본 명령어 ✅ 시스템 업데이트 및 패키지 관리\nsudo apt update \u0026\u0026 sudo apt upgrade -y # 시스템 업데이트 sudo apt install \u003c패키지명\u003e -y # 패키지 설치 sudo apt remove \u003c패키지명\u003e -y # 패키지 삭제 ✅ 파일 및 디렉토리 관리\nls -l # 파일 목록 확인 cd /path # 디렉토리 이동 cp file1 file2 # 파일 복사 mv file1 file2 # 파일 이동/이름 변경 rm -rf folder/ # 폴더 삭제 ✅ 사용자 및 권한 관리\nsudo adduser username # 새 사용자 추가 sudo passwd username # 사용자 비밀번호 변경 sudo usermod -aG sudo username # sudo 권한 추가 ✅ 서비스 및 프로세스 관리\nsudo systemctl status apache2 # 서비스 상태 확인 sudo systemctl start apache2 # 서비스 시작 sudo systemctl restart apache2 # 서비스 재시작 sudo kill -9 \u003cPID\u003e # 특정 프로세스 강제 종료 5️⃣ Ubuntu 네트워크 설정 ✅ IP 주소 확인 및 설정\nip a # 네트워크 인터페이스 확인 ifconfig # (net-tools 패키지 필요) ✅ 고정 IP 설정 (Netplan)\nsudo nano /etc/netplan/00-installer-config.yaml network: ethernets: ens33: dhcp4: no addresses: [192.168.1.100/24] gateway4: 192.168.1.1 nameservers: addresses: [8.8.8.8, 8.8.4.4] version: 2 sudo netplan apply ✅ 방화벽(UFW) 설정\nsudo ufw enable # 방화벽 활성화 sudo ufw allow 22/tcp # SSH 허용 sudo ufw allow 80/tcp # HTTP 허용 sudo ufw status # 방화벽 상태 확인 6️⃣ Ubuntu 보안 강화 ✅ SSH 포트 변경\nsudo nano /etc/ssh/sshd_config Port 2222 # 기본 22번 포트를 2222로 변경 sudo systemctl restart ssh ✅ 불필요한 서비스 중지\nsudo systemctl disable avahi-daemon sudo systemctl stop cups ✅ 로그 및 감시 시스템 활성화\nsudo journalctl -xe # 시스템 로그 확인 sudo tail -f /var/log/auth.log # 인증 로그 확인 7️⃣ Ubuntu 시스템 모니터링 ✅ CPU 및 메모리 사용량 확인\ntop # 실시간 프로세스 모니터링 htop # (설치 필요) 인터랙티브 프로세스 뷰어 free -m # 메모리 사용량 확인 df -h # 디스크 사용량 확인 ✅ 시스템 로그 확인\nsudo dmesg | tail sudo journalctl -u nginx --since \"1 hour ago\" ✅ 네트워크 트래픽 모니터링\nsudo iftop # (설치 필요) 실시간 네트워크 트래픽 확인 8️⃣ Ubuntu 백업 및 복구 ✅ 시스템 전체 백업 (tar 이용)\nsudo tar -cvpzf /backup/ubuntu_backup.tar.gz --exclude=/backup --one-file-system / ✅ 시스템 복원\nsudo tar -xvpzf /backup/ubuntu_backup.tar.gz -C / ✅ 개별 디렉토리 백업 및 복원 (rsync 사용)\nsudo rsync -av --delete /home /backup/ sudo rsync -av /backup/home/ /home/ 🔟 결론 🚀 ✅ Ubuntu는 가장 널리 사용되는 Linux 배포판\n✅ 서버 및 클라우드 환경에서 강력한 보안과 성능 제공\n✅ 네트워크, 패키지, 보안 설정을 최적화하여 안정적으로 운영 가능\n📚 추가 자료\n1️⃣ Ubuntu 공식 문서\n2️⃣ Ubuntu Server 관리 가이드\n3️⃣ Netplan 네트워크 설정","1-ubuntu란#1️⃣ Ubuntu란?":"","2-ubuntu-에디션-비교#2️⃣ Ubuntu 에디션 비교":"","3-ubuntu-설치-방법#3️⃣ Ubuntu 설치 방법":"","4-ubuntu-기본-명령어#4️⃣ Ubuntu 기본 명령어":"","5-ubuntu-네트워크-설정#5️⃣ Ubuntu 네트워크 설정":"","6-ubuntu-보안-강화#6️⃣ Ubuntu 보안 강화":"","7-ubuntu-시스템-모니터링#7️⃣ Ubuntu 시스템 모니터링":"","8-ubuntu-백업-및-복구#8️⃣ Ubuntu 백업 및 복구":""},"title":"Ubuntu"},"/system/server/os/windows/":{"data":{"":"","1-windows란-#1. Windows란? 🖥️🪟":"","2-windows의-특징-#2. Windows의 특징 🌟":"","3-windows의-주요-에디션-#3. Windows의 주요 에디션 🏢🎮":"","4-windows-설치-및-설정-#4. Windows 설치 및 설정 ⚙️":"","5-windows-기본-설정-#5. Windows 기본 설정 🛠️":"","6-windows-주요-기능-#6. Windows 주요 기능 💡":"","7-windows-활용-예제-#7. Windows 활용 예제 🔥":"","8-windows-문제-해결-및-관리-#8. Windows 문제 해결 및 관리 🔍":"","9-결론-#9. 결론 🎯":"1. Windows란? 🖥️🪟 Windows는 **마이크로소프트(Microsoft)**에서 개발한 운영체제로, 개인용 PC부터 서버, 클라우드 환경까지 폭넓게 사용됩니다.\nWindows는 GUI(그래픽 사용자 인터페이스)를 제공하는 직관적인 OS로, 다양한 소프트웨어 및 하드웨어와 호환성이 뛰어나며, 기업 및 개인 사용자 모두에게 널리 사용됩니다.\n2. Windows의 특징 🌟 ✅ 사용자 친화적인 인터페이스 – GUI 기반으로 누구나 쉽게 사용 가능\n✅ 광범위한 소프트웨어 지원 – 다양한 응용 프로그램과 호환\n✅ 강력한 보안 기능 – Windows Defender, BitLocker, Windows Firewall 제공\n✅ 클라우드 및 서버 지원 – Azure, Windows Server 등과 통합 가능\n✅ 자동 업데이트 기능 – 보안 패치 및 시스템 최적화를 자동으로 수행\n3. Windows의 주요 에디션 🏢🎮 에디션 특징 Windows Home 가정용 기본 버전, 필수 기능 제공 Windows Pro 비즈니스용 추가 기능 포함 (BitLocker, 원격 데스크톱 등) Windows Enterprise 대기업 및 조직을 위한 보안 및 관리 기능 강화 Windows Server 기업 서버 운영을 위한 버전 (Active Directory, Hyper-V 등 포함) 4. Windows 설치 및 설정 ⚙️ 4.1 Windows 설치 방법 1️⃣ 마이크로소프트 공식 사이트에서 Windows ISO 다운로드\n2️⃣ 부팅 가능한 USB 생성 (Rufus 또는 Windows Media Creation Tool 활용)\n3️⃣ 컴퓨터 BIOS/UEFI에서 USB로 부팅 설정\n4️⃣ Windows 설치 화면에서 언어 및 지역 선택 후 설치 진행\n5️⃣ Windows 계정 및 네트워크 설정 후 완료\n5. Windows 기본 설정 🛠️ 5.1 Windows 업데이트 wuauclt /detectnow # Windows 업데이트 확인 설정 → Windows 업데이트에서 최신 패치 적용 가능 5.2 네트워크 설정 확인 ipconfig /all 설정 → 네트워크 및 인터넷에서 어댑터 및 Wi-Fi 설정 변경 가능 5.3 방화벽 설정 netsh advfirewall set allprofiles state on # 방화벽 활성화 제어판 → Windows Defender 방화벽에서 설정 가능 6. Windows 주요 기능 💡 기능 설명 Windows Defender 기본 제공되는 보안 및 바이러스 방지 프로그램 BitLocker 디스크 암호화를 통해 데이터 보호 Hyper-V 가상화 기술을 통해 가상 머신 운영 원격 데스크톱 원격으로 다른 PC에 접속 가능 파일 탐색기 파일 및 폴더 관리 기능 제공 7. Windows 활용 예제 🔥 7.1 원격 데스크톱 활성화 Set-ItemProperty 'HKLM:\\System\\CurrentControlSet\\Control\\Terminal Server' -Name \"fDenyTSConnections\" -Value 0 Enable-NetFirewallRule -DisplayGroup \"Remote Desktop\" 7.2 Windows 서비스 확인 및 관리 Get-Service # 실행 중인 서비스 목록 확인 Restart-Service Spooler # 프린터 서비스 재시작 7.3 네트워크 포트 확인 netstat -ano # 현재 열려 있는 포트 확인 8. Windows 문제 해결 및 관리 🔍 8.1 시스템 로그 확인 Get-EventLog -LogName System -Newest 20 이벤트 뷰어 → Windows 로그에서 오류 및 경고 확인 가능 8.2 디스크 상태 확인 chkdsk /f /r # 디스크 오류 검사 및 복구 8.3 시스템 리소스 사용량 확인 tasklist # 실행 중인 프로세스 목록 확인 작업 관리자 (Ctrl + Shift + Esc)에서 CPU, 메모리 사용량 확인 가능 9. 결론 🎯 Windows는 개인 사용자부터 기업 환경까지 널리 사용되는 강력한 운영체제입니다.\nGUI 환경과 강력한 기능을 제공하며, PowerShell, CMD, 레지스트리 편집기 등을 활용하여 다양한 시스템 설정 및 관리를 수행할 수 있습니다.\n서버 환경에서는 Windows Server가 사용되며, 가상화, 보안, 네트워크 기능이 추가 제공됩니다.\n자세한 내용은 Microsoft 공식 문서에서 확인하세요. 🚀"},"title":"Windows"},"/system/server/storage/iscsi/":{"data":{"":"","-iscsi-server-guide#📌 iSCSI Server Guide":"","1-iscsi란#1️⃣ iSCSI란?":"","2-iscsi-구성-요소#2️⃣ iSCSI 구성 요소":"","3-iscsi-서버-설정-ubuntu-기준#3️⃣ iSCSI 서버 설정 (Ubuntu 기준)":"","4-iscsi-클라이언트-설정-ubuntu-기준#4️⃣ iSCSI 클라이언트 설정 (Ubuntu 기준)":"","5-결론#5️⃣ 결론":"📌 iSCSI Server Guide 1️⃣ iSCSI란? iSCSI(Internet Small Computer Systems Interface)는 IP 네트워크를 통해 스토리지를 연결하는 프로토콜입니다.\n서버(이니시에이터, Initiator)와 스토리지 타겟(Target) 간에 SCSI 명령을 TCP/IP 네트워크로 전송하여 디스크를 공유할 수 있도록 합니다.\n✅ iSCSI의 특징\n기존 이더넷 네트워크를 사용하여 SAN(Storage Area Network) 구축 가능 별도의 전용 하드웨어가 필요하지 않아 비용 절감 가능 스토리지 확장이 용이하며, 데이터센터에서 널리 사용됨 2️⃣ iSCSI 구성 요소 🔹 iSCSI Initiator (클라이언트)\niSCSI 스토리지를 마운트하여 로컬 디스크처럼 사용 🔹 iSCSI Target (서버)\niSCSI 클라이언트에 디스크를 제공하는 서버 역할 수행 🔹 iSCSI LUN (Logical Unit Number)\n실제 할당된 스토리지 단위 (예: 논리 볼륨) 3️⃣ iSCSI 서버 설정 (Ubuntu 기준) 📌 1. iSCSI Target 설치 sudo apt update sudo apt install tgt -y 📌 2. iSCSI 디스크 생성 sudo mkdir -p /iscsi sudo fallocate -l 10G /iscsi/iscsi_disk.img sudo chmod 600 /iscsi/iscsi_disk.img 📌 3. Target 구성 (/etc/tgt/conf.d/iscsi.conf) \u003ctarget iqn.2025-03.mungdocs:storage.target1\u003e backing-store /iscsi/iscsi_disk.img initiator-address 192.168.1.100 incominguser iscsiuser password123 \u003c/target\u003e 📌 4. iSCSI 서비스 재시작 및 확인 sudo systemctl restart tgt sudo tgtadm --mode target --op show 4️⃣ iSCSI 클라이언트 설정 (Ubuntu 기준) 📌 1. iSCSI Initiator 설치 sudo apt install open-iscsi -y 📌 2. iSCSI Target 검색 sudo iscsiadm -m discovery -t sendtargets -p 192.168.1.10 📌 3. Target 로그인 sudo iscsiadm -m node --targetname \"iqn.2025-03.mungdocs:storage.target1\" --portal 192.168.1.10 --login 📌 4. 디스크 확인 및 마운트 lsblk sudo mkfs.ext4 /dev/sdX sudo mount /dev/sdX /mnt 5️⃣ 결론 iSCSI는 기존 네트워크를 활용하여 비용 효율적인 스토리지 솔루션을 제공합니다.\n위의 설정을 통해 iSCSI 서버 및 클라이언트를 구성할 수 있으며, SAN 환경을 구축할 수도 있습니다. 🚀\n📌 참고자료 1️⃣ Linux iSCSI 설정 가이드\n2️⃣ iSCSI 개념 및 활용"},"title":"ISCSI"},"/system/server/storage/nfs/":{"data":{"":"","-nfs-server-guide#📌 NFS Server Guide":"","1-nfs란#1️⃣ NFS란?":"","2-nfs-구성-요소#2️⃣ NFS 구성 요소":"","3-nfs-서버-설정-ubuntu-기준#3️⃣ NFS 서버 설정 (Ubuntu 기준)":"","4-nfs-클라이언트-설정-ubuntu-기준#4️⃣ NFS 클라이언트 설정 (Ubuntu 기준)":"","5-결론#5️⃣ 결론":"📌 NFS Server Guide 1️⃣ NFS란? NFS(Network File System) 는 네트워크를 통해 파일 시스템을 공유하는 프로토콜입니다.\n서버와 클라이언트 간에 원격 디렉터리를 마운트하여 로컬 디스크처럼 사용할 수 있습니다.\n✅ NFS의 특징\n여러 클라이언트가 동일한 디렉터리에 접근 가능 파일 및 디렉터리 공유가 용이 리눅스 및 유닉스 환경에서 널리 사용 2️⃣ NFS 구성 요소 🔹 NFS Server\n파일을 공유하는 서버 🔹 NFS Client\n공유된 파일 시스템을 마운트하여 사용하는 클라이언트 3️⃣ NFS 서버 설정 (Ubuntu 기준) 📌 1. NFS 서버 패키지 설치 sudo apt update sudo apt install nfs-kernel-server -y 📌 2. 공유할 디렉터리 생성 sudo mkdir -p /mnt/nfs_share sudo chmod 777 /mnt/nfs_share 📌 3. NFS 익스포트 설정 (/etc/exports) /mnt/nfs_share 192.168.1.0/24(rw,sync,no_root_squash,no_subtree_check) 📌 192.168.1.0/24: 특정 네트워크 대역에 공유\n📌 rw: 읽기/쓰기 권한 제공\n📌 sync: 동기화하여 데이터 손실 방지\n📌 no_root_squash: 클라이언트에서 root 권한 유지\n📌 4. NFS 서비스 재시작 sudo exportfs -a sudo systemctl restart nfs-kernel-server 4️⃣ NFS 클라이언트 설정 (Ubuntu 기준) 📌 1. NFS 클라이언트 패키지 설치 sudo apt install nfs-common -y 📌 2. NFS 마운트 확인 showmount -e 192.168.1.10 📌 3. NFS 공유 디렉터리 마운트 sudo mount 192.168.1.10:/mnt/nfs_share /mnt 📌 /mnt/nfs_share를 /mnt에 마운트\n📌 4. fstab에 자동 마운트 설정 (/etc/fstab) 192.168.1.10:/mnt/nfs_share /mnt nfs defaults 0 0 5️⃣ 결론 NFS는 간단하면서도 강력한 네트워크 파일 공유 솔루션입니다.\n위의 설정을 통해 여러 클라이언트가 동일한 디렉터리를 공유할 수 있습니다. 🚀\n📌 참고자료 1️⃣ NFS 개념 및 설정\n2️⃣ NFS 공유 디렉터리 마운트"},"title":"NFS"},"/system/server/storage/smb/":{"data":{"":"","-smb-server-guide#📌 SMB Server Guide":"","1-smb란#1️⃣ SMB란?":"","2-smb-구성-요소#2️⃣ SMB 구성 요소":"","3-smb-서버-설정-ubuntu-기준#3️⃣ SMB 서버 설정 (Ubuntu 기준)":"","4-smb-클라이언트-설정-windows-기준#4️⃣ SMB 클라이언트 설정 (Windows 기준)":"","5-smb-클라이언트-설정-linux-기준#5️⃣ SMB 클라이언트 설정 (Linux 기준)":"","6-결론#6️⃣ 결론":"📌 SMB Server Guide 1️⃣ SMB란? SMB(Server Message Block) 는 네트워크를 통해 파일 및 프린터를 공유하는 프로토콜입니다.\n주로 Windows 환경에서 사용되지만, Linux 및 macOS에서도 지원됩니다.\n✅ SMB의 특징\nWindows 및 Linux 간 파일 공유 가능 인증 기반 접근 제어 제공 CIFS(Common Internet File System)으로도 알려짐 2️⃣ SMB 구성 요소 🔹 SMB Server\n공유 디렉터리를 제공하는 서버 🔹 SMB Client\n공유된 디렉터리에 접근하는 클라이언트 3️⃣ SMB 서버 설정 (Ubuntu 기준) 📌 1. Samba 패키지 설치 sudo apt update sudo apt install samba -y 📌 2. 공유할 디렉터리 생성 sudo mkdir -p /mnt/smb_share sudo chmod 777 /mnt/smb_share 📌 3. Samba 설정 수정 (/etc/samba/smb.conf) [smb_share] path = /mnt/smb_share browseable = yes writable = yes guest ok = yes create mask = 0777 directory mask = 0777 📌 4. Samba 서비스 재시작 sudo systemctl restart smbd sudo systemctl enable smbd 4️⃣ SMB 클라이언트 설정 (Windows 기준) 📌 1. 공유 폴더 접근 1️⃣ 윈도우 + R 키 입력\n2️⃣ \\\\192.168.1.10\\smb_share 입력 후 Enter\n📌 2. 네트워크 드라이브 연결 1️⃣ 파일 탐색기 → 내 PC\n2️⃣ 네트워크 드라이브 연결 클릭\n3️⃣ 폴더에 \\\\192.168.1.10\\smb_share 입력\n4️⃣ 마침 버튼 클릭\n5️⃣ SMB 클라이언트 설정 (Linux 기준) 📌 1. CIFS 패키지 설치 sudo apt install cifs-utils -y 📌 2. SMB 마운트 sudo mount -t cifs -o username=guest,password= //192.168.1.10/smb_share /mnt 📌 3. fstab 자동 마운트 설정 (/etc/fstab) //192.168.1.10/smb_share /mnt cifs guest,uid=1000,gid=1000 0 0 6️⃣ 결론 SMB는 Windows와 Linux 간 파일 공유를 간편하게 설정할 수 있는 강력한 프로토콜입니다.\n위 설정을 통해 네트워크에서 손쉽게 파일을 공유할 수 있습니다. 🚀\n📌 참고자료 1️⃣ SMB 설정 가이드\n2️⃣ Linux에서 SMB 마운트하기"},"title":"SMB"},"/system/server/virtualization/virtualization/":{"data":{"-가상화-virtualization-구축-가이드-#🖥️ \u003cstrong\u003e가상화 (Virtualization) 구축 가이드\u003c/strong\u003e 🚀":"","-결론-#🔟 결론 🚀":"🖥️ 가상화 (Virtualization) 구축 가이드 🚀 1️⃣ 가상화(Virtualization)란? 가상화(Virtualization) 는 하나의 물리적인 하드웨어 리소스를 여러 개의 가상 환경으로 나누어 사용하는 기술입니다.\n이를 통해 서버, 스토리지, 네트워크, 애플리케이션 등의 IT 리소스를 효율적으로 운영할 수 있습니다.\n✅ 가상화의 주요 이점\n하드웨어 비용 절감 → 한 대의 서버에서 여러 개의 가상 머신(VM) 실행 리소스 활용도 향상 → CPU, RAM, 디스크 사용률 최적화 유연한 확장성 → 필요에 따라 가상 환경을 빠르게 생성 및 제거 가능 백업 및 복구 용이 → VM의 스냅샷 및 마이그레이션 기능 지원 보안 및 격리 → 서로 다른 환경을 분리하여 안정성 확보 2️⃣ 가상화의 유형 유형 설명 서버 가상화 하나의 물리 서버에서 여러 개의 VM 실행 (예: VMware ESXi, Proxmox, KVM, Hyper-V) 네트워크 가상화 네트워크 리소스를 가상화하여 소프트웨어 기반 네트워크 구현 (예: SDN, VLAN, VXLAN) 스토리지 가상화 여러 개의 물리 디스크를 하나의 가상 스토리지로 통합 (예: Ceph, LVM, ZFS) 데스크톱 가상화 (VDI) 원격 환경에서 가상 데스크톱 제공 (예: Citrix, VMware Horizon) 애플리케이션 가상화 애플리케이션을 독립된 환경에서 실행 (예: Docker, Kubernetes) 3️⃣ 가상화 솔루션 비교 가상화 플랫폼 설명 특징 VMware ESXi 기업용 서버 가상화 솔루션 안정적, 관리 도구 풍부, 유료 라이선스 Proxmox VE 오픈소스 가상화 솔루션 KVM + LXC 지원, 웹 UI 제공 KVM Linux 기반 가상화 기술 고성능, 오픈소스, 커널 통합 Hyper-V Windows 기반 가상화 솔루션 Windows 서버와 최적화 VirtualBox 데스크톱 가상화 솔루션 무료, 크로스 플랫폼 지원 4️⃣ KVM 기반 가상화 환경 구축 🔹 1. KVM 설치 (Ubuntu 22.04) sudo apt update sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virt-manager -y 🔹 2. 가상화 활성화 확인 sudo kvm-ok ✅ “KVM acceleration can be used” 메시지가 나오면 정상 ✅\n5️⃣ 가상 머신(VM) 생성 및 관리 🔹 1. VM 생성 (CLI 방식) sudo virt-install \\ --name ubuntu-vm \\ --ram 2048 \\ --vcpus 2 \\ --disk path=/var/lib/libvirt/images/ubuntu-vm.qcow2,size=20 \\ --os-type linux \\ --os-variant ubuntu22.04 \\ --network bridge=virbr0 \\ --graphics none \\ --location 'http://releases.ubuntu.com/22.04/ubuntu-22.04.1-live-server-amd64.iso' \\ --extra-args \"console=ttyS0,115200n8\" 🔹 2. VM 리스트 확인 sudo virsh list --all 🔹 3. VM 시작/정지 sudo virsh start ubuntu-vm sudo virsh shutdown ubuntu-vm 6️⃣ Proxmox 기반 가상화 환경 구축 🔹 1. Proxmox 설치 1️⃣ Proxmox 공식 사이트에서 ISO 다운로드\n2️⃣ 부팅 USB 생성 후 설치 진행\n3️⃣ 설치 후 웹 UI 접속:\nhttps://\u003cProxmox_IP\u003e:8006 🔹 2. VM 생성 (Proxmox Web UI) 1️⃣ “Datacenter” → “Create VM” 클릭\n2️⃣ ISO 이미지 선택 후 설정 진행\n3️⃣ 리소스 (CPU, RAM, Storage) 설정 후 “Finish”\n7️⃣ 가상화 네트워크 구성 🔹 1. Bridge 네트워크 설정 (KVM) sudo nano /etc/network/interfaces auto br0 iface br0 inet static address 192.168.1.100 netmask 255.255.255.0 gateway 192.168.1.1 bridge_ports eno1 bridge_stp off bridge_fd 0 적용 후 네트워크 재시작\nsudo systemctl restart networking 8️⃣ 컨테이너 기반 가상화 (Docker \u0026 Kubernetes) 🔹 1. Docker 설치 (Ubuntu) curl -fsSL https://get.docker.com | sudo bash sudo usermod -aG docker $USER 🔹 2. 컨테이너 실행 docker run -d --name webserver -p 8080:80 nginx 🔹 3. Kubernetes 설치 (Minikube) curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube minikube start 9️⃣ 가상화 모니터링 및 관리 ✅ 가상 머신 모니터링\nsudo virsh dominfo ubuntu-vm sudo virsh domstats ubuntu-vm ✅ Proxmox 모니터링\n1️⃣ Proxmox Web UI → “Datacenter” → “Nodes”\n2️⃣ CPU, RAM, Disk 사용량 실시간 확인\n✅ Docker 컨테이너 모니터링\ndocker stats ✅ Kubernetes 모니터링\nkubectl top nodes kubectl top pods 🔟 결론 🚀 ✅ 가상화는 서버, 네트워크, 스토리지 등의 IT 리소스를 효율적으로 관리할 수 있는 핵심 기술\n✅ KVM, Proxmox, VMware 등 다양한 솔루션을 활용하여 환경 구축 가능\n✅ Docker 및 Kubernetes를 활용하여 애플리케이션 가상화까지 확장 가능\n📚 추가 자료\n1️⃣ KVM 공식 문서\n2️⃣ Proxmox VE 공식 사이트\n3️⃣ VMware ESXi 개요\n4️⃣ Docker 공식 문서\n5️⃣ Kubernetes 공식 사이트","1-가상화virtualization란#1️⃣ 가상화(Virtualization)란?":"","2-가상화의-유형#2️⃣ 가상화의 유형":"","3-가상화-솔루션-비교#3️⃣ 가상화 솔루션 비교":"","4-kvm-기반-가상화-환경-구축#4️⃣ KVM 기반 가상화 환경 구축":"","5-가상-머신vm-생성-및-관리#5️⃣ 가상 머신(VM) 생성 및 관리":"","6-proxmox-기반-가상화-환경-구축#6️⃣ Proxmox 기반 가상화 환경 구축":"","7-가상화-네트워크-구성#7️⃣ 가상화 네트워크 구성":"","8-컨테이너-기반-가상화-docker--kubernetes#8️⃣ 컨테이너 기반 가상화 (Docker \u0026amp; Kubernetes)":"","9-가상화-모니터링-및-관리#9️⃣ 가상화 모니터링 및 관리":""},"title":"Virtualization"},"/system/server/virtualization/websocket/":{"data":{"-websocket-서버-구축-가이드-#🌐 \u003cstrong\u003eWebSocket 서버 구축 가이드\u003c/strong\u003e 🚀":"","1-websocket이란#1️⃣ WebSocket이란?":"","2-websocket-서버-구축-python--fastapi#2️⃣ WebSocket 서버 구축 (Python + FastAPI)":"","3-websocket-서버-코드-fastapi#3️⃣ WebSocket 서버 코드 (FastAPI)":"","4-websocket-서버-실행#4️⃣ WebSocket 서버 실행":"","5-websocket-클라이언트-테스트-python#5️⃣ WebSocket 클라이언트 테스트 (Python)":"","6-websocket-방화벽-설정#6️⃣ WebSocket 방화벽 설정":"","7-websocket-서버-배포-gunicorn--uvicorn#7️⃣ WebSocket 서버 배포 (Gunicorn + Uvicorn)":"","8-websocket-로그-및-문제-해결#8️⃣ WebSocket 로그 및 문제 해결":"","9-결론-#9️⃣ 결론 🚀":"🌐 WebSocket 서버 구축 가이드 🚀 1️⃣ WebSocket이란? WebSocket은 브라우저와 서버 간 양방향 통신을 위한 프로토콜입니다.\n기존의 HTTP는 요청-응답 방식이지만, WebSocket은 서버가 클라이언트에게 실시간으로 데이터를 보낼 수 있는 특징이 있습니다.\n✅ 주요 특징\n양방향 통신 지원 (서버 → 클라이언트, 클라이언트 → 서버) Persistent Connection (연결 유지) 낮은 네트워크 오버헤드 (헤더가 적음) 실시간 데이터 전송 가능 (채팅, 게임, 주식 데이터 등에 활용) 2️⃣ WebSocket 서버 구축 (Python + FastAPI) Python의 FastAPI + WebSockets 라이브러리를 사용하여 간단한 WebSocket 서버를 만들어보겠습니다.\n🔹 1. WebSocket 서버 설치 pip install fastapi uvicorn 3️⃣ WebSocket 서버 코드 (FastAPI) 📂 websocket_server.py\nfrom fastapi import FastAPI, WebSocket from fastapi.responses import HTMLResponse import uvicorn app = FastAPI() # 클라이언트 관리 clients = [] @app.websocket(\"/ws\") async def websocket_endpoint(websocket: WebSocket): await websocket.accept() # WebSocket 연결 수락 clients.append(websocket) try: while True: data = await websocket.receive_text() # 클라이언트로부터 메시지 수신 for client in clients: await client.send_text(f\"Message: {data}\") # 모든 클라이언트에 메시지 전송 except: clients.remove(websocket) # 간단한 HTML 페이지 제공 @app.get(\"/\") async def home(): return HTMLResponse(\"\"\" \u003chtml\u003e \u003chead\u003e \u003cscript\u003e var ws = new WebSocket(\"ws://localhost:8000/ws\"); ws.onmessage = function(event) { document.getElementById(\"output\").innerHTML += event.data + \"\u003cbr\u003e\"; }; function sendMessage() { var input = document.getElementById(\"message\"); ws.send(input.value); input.value = \"\"; } \u003c/script\u003e \u003c/head\u003e \u003cbody\u003e \u003ch2\u003eWebSocket Chat\u003c/h2\u003e \u003cinput id=\"message\" type=\"text\"\u003e \u003cbutton onclick=\"sendMessage()\"\u003eSend\u003c/button\u003e \u003cdiv id=\"output\"\u003e\u003c/div\u003e \u003c/body\u003e \u003c/html\u003e \"\"\") if __name__ == \"__main__\": uvicorn.run(app, host=\"0.0.0.0\", port=8000) 4️⃣ WebSocket 서버 실행 python websocket_server.py 🌍 브라우저에서 실행\nhttp://localhost:8000/ 에 접속 후 메시지를 입력하면 WebSocket을 통해 실시간으로 송수신할 수 있습니다.\n5️⃣ WebSocket 클라이언트 테스트 (Python) 아래 코드를 실행하면 WebSocket 서버에 메시지를 전송하고 응답을 받을 수 있습니다.\nimport asyncio import websockets async def test_websocket(): uri = \"ws://localhost:8000/ws\" async with websockets.connect(uri) as websocket: await websocket.send(\"Hello WebSocket!\") response = await websocket.recv() print(f\"Received: {response}\") asyncio.run(test_websocket()) 6️⃣ WebSocket 방화벽 설정 ✅ WebSocket은 기본적으로 8000번 포트를 사용하므로 방화벽을 열어야 합니다.\n🔹 UFW (Ubuntu/Debian) sudo ufw allow 8000/tcp 🔹 firewalld (CentOS/RHEL) sudo firewall-cmd --permanent --add-port=8000/tcp sudo firewall-cmd --reload 7️⃣ WebSocket 서버 배포 (Gunicorn + Uvicorn) FastAPI WebSocket 서버를 Gunicorn + Uvicorn을 사용하여 실행할 수 있습니다.\n🔹 1. Gunicorn 설치 pip install gunicorn 🔹 2. Gunicorn을 사용하여 실행 gunicorn -w 4 -k uvicorn.workers.UvicornWorker websocket_server:app --bind 0.0.0.0:8000 ✅ -w 4: 워커 프로세스 4개 사용\n✅ -k uvicorn.workers.UvicornWorker: Uvicorn 기반의 Gunicorn 워커 사용\n8️⃣ WebSocket 로그 및 문제 해결 🔹 1. WebSocket 로그 확인 sudo journalctl -u websocket_server -f 🔹 2. 포트 사용 여부 확인 netstat -tulnp | grep 8000 🔹 3. WebSocket 연결 테스트 curl -i -N -H \"Connection: Upgrade\" -H \"Upgrade: websocket\" http://localhost:8000/ws 9️⃣ 결론 🚀 ✅ WebSocket을 사용하면 실시간 데이터를 빠르게 주고받을 수 있습니다.\n✅ FastAPI + Uvicorn을 활용하면 Python으로 간단하게 WebSocket 서버를 구축할 수 있습니다.\n✅ Gunicorn과 함께 배포하면 성능을 더욱 향상시킬 수 있습니다.\n📚 추가 자료\n1️⃣ FastAPI WebSocket 공식 문서\n2️⃣ WebSocket 프로토콜 공식 문서\n3️⃣ Gunicorn + Uvicorn 설정"},"title":"WebSocket"},"/travel/":{"data":{"":" RSS Feed "},"title":"Travel"}}